[
  {
    "filename": "1109.5951v2.pdf",
    "total_citations": 15,
    "valid_format_count": 15,
    "existing_count": 15,
    "invalid_format_count": 0,
    "nonexistent_count": 0,
    "citations": [
      {
        "text": "Dowe, D. L. and Hajek, A. R. (1998). A non-behavioural, compu tational extension to the\nTuring Test. In Intl. Conf. on Computational Intelligence & multimedia app lications\n(ICCIMA\u201998), Gippsland, Australia , pages 101\u2013106.\n\u00b4Etor\u00b4 e, P. and Jourdain, B. (2010). Adaptive optimal alloca tion in strati\ufb01ed sampling methods.\nMethodology and Computing in Applied Probability , 12(3):335\u2013360.\nHern\u00b4 andez-Orallo, J. (2000). Beyond the Turing Test. J. Logic, Language & Information ,\n9(4):447\u2013466.\nHern\u00b4 andez-Orallo, J. (2010). A (hopefully) unbiased univ ersal environment class for measuring\nintelligence ofbiological and arti\ufb01cialsystems. In Proc. of the Third Conference on Arti\ufb01cial\nGeneral Intelligence , Lugano.\nHern\u00b4 andez-Orallo, J. and Dowe, D. L. (2010). Measuring uni versal intelligence: Towards an\nanytime intelligence test. Arti\ufb01cial Intelligence , 174(18):1508 \u2013 1539.\nHernandez-orallo, J. and Minaya-collado, N. (1998). A form al de\ufb01nition of intelligence based\non an intensional variant of algorithmic complexity. In EIS\u201998.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Dowe, D. L. and Hajek, A. R. (1998). A non-behavioural, compu tational extension to the\nTuring Test. In Intl. Conf. on Computational Intelligence & multimedia app lications\n(ICCIMA\u201998), Gippsland, Australia , pages 101\u2013106.\n\u00b4Etor\u00b4 e, P. and Jourdain, B. (2010). Adaptive optimal alloca tion in strati\ufb01ed sampling methods.\nMethodology and Computing in Applied Probability , 12(3):335\u2013360.\nHern\u00b4 andez-Orallo, J. (2000). Beyond the Turing Test. J. Logic, Language & Information ,\n9(4):447\u2013466.\nHern\u00b4 andez-Orallo, J. (2010). A (hopefully) unbiased univ ersal environment class for measuring\nintelligence ofbiological and arti\ufb01cialsystems. In Proc. of the Third Conference on Arti\ufb01cial\nGeneral Intelligence , Lugano.\nHern\u00b4 andez-Orallo, J. and Dowe, D. L. (2010). Measuring uni versal intelligence: Towards an\nanytime intelligence test. Arti\ufb01cial Intelligence , 174(18):1508 \u2013 1539.\nHernandez-orallo, J. and Minaya-collado, N. (1998). A form al de\ufb01nition of intelligence based\non an intensional variant of algorithmic complexity. In EIS\u201998."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "INFORMATION-THEORETIC APPROACHES TO BIOLOGY",
          "authors": [
            "Dowe",
            "Prank"
          ],
          "year": 1998,
          "confidence": "medium"
        }
      },
      {
        "text": "Hibbard, B. (2009). Bias and no free lunch in formal measures of intelligence. Journal of\nArti\ufb01cial General Intelligence , 1(1):54\u201361.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hibbard, B. (2009). Bias and no free lunch in formal measures of intelligence. Journal of\nArti\ufb01cial General Intelligence , 1(1):54\u201361."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Bias and No Free Lunch in Formal Measures of Intelligence",
          "authors": [
            "Hibbard"
          ],
          "year": 2009,
          "confidence": "medium"
        }
      },
      {
        "text": "Hutter, M. (2001). Towards a universal theory of arti\ufb01cial i ntelligence based on algorithmic\nprobability and sequential decisions. Proc. 12th Eurpean Conference on Machine Learning\n(ECML-2001) , pages 226\u2013238.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hutter, M. (2001). Towards a universal theory of arti\ufb01cial i ntelligence based on algorithmic\nprobability and sequential decisions. Proc. 12th Eurpean Conference on Machine Learning\n(ECML-2001) , pages 226\u2013238."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Towards a Universal Theory of Artificial Intelligence Based on Algorithmic Probability and Sequential Decisions",
          "authors": [
            "Hutter"
          ],
          "year": 2001,
          "confidence": "medium"
        }
      },
      {
        "text": "Hutter, M. (2005). Universal Arti\ufb01cial Intelligence: Sequential Decisions b ased on Algorithmic\nProbability . Springer, Berlin. 300 pages, http://www.hutter1.net/ai /uaibook.htm.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hutter, M. (2005). Universal Arti\ufb01cial Intelligence: Sequential Decisions b ased on Algorithmic\nProbability . Springer, Berlin. 300 pages, http://www.hutter1.net/ai /uaibook.htm."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Algorithmic probability",
          "authors": [
            "Hutter",
            "Legg",
            "Vitanyi"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "Hutter, M. and Legg, S. (2007). Temporal di\ufb00erence updating without a learning rate. In\nNeural Information Processing Systems (NIPS \u201907) .\nInsa-Cabrera, J., Dowe, D. L., Espana-Cubillo, S., Hernand ez-Lloreda, M., and Hernandez-",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hutter, M. and Legg, S. (2007). Temporal di\ufb00erence updating without a learning rate. In\nNeural Information Processing Systems (NIPS \u201907) .\nInsa-Cabrera, J., Dowe, D. L., Espana-Cubillo, S., Hernand ez-Lloreda, M., and Hernandez-"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Marx and temporal di\ufb00erence",
          "authors": [],
          "year": 2010,
          "confidence": "medium"
        }
      },
      {
        "text": "Orallo, J. (2011a). Comparing humans and ai agents. In Juerg en Schmidhuber, Kristinn\nR. Thorisson, M. L., editor, Arti\ufb01cial General Intelligence, 4th Intl Conf, Mountain Vi ew,\nSan Francisco . Lecture Notes in Arti\ufb01cial Intelligence, Springer.\nInsa-Cabrera, J., Dowe, D. L., and Hernandez-Orallo, J. (20 11b). Evaluating a reinforcement\nlearning algorithm with a general intelligence test. In Jos e A. Lozano, Jose A. Gamez, J.\nA. M., editor, Current Topics in Arti\ufb01cial Intelligence. 14th Conference of the Spanish As-\nsociation for Arti\ufb01cial Intelligence, CAEPIA 2011 . Lecture Notes in Arti\ufb01cial Intelligence,\nSpringer.",
        "style": "mla",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Orallo, J. (2011a). Comparing humans and ai agents. In Juerg en Schmidhuber, Kristinn\nR. Thorisson, M. L., editor, Arti\ufb01cial General Intelligence, 4th Intl Conf, Mountain Vi ew,\nSan Francisco . Lecture Notes in Arti\ufb01cial Intelligence, Springer.\nInsa-Cabrera, J., Dowe, D. L., and Hernandez-Orallo, J. (20 11b). Evaluating a reinforcement\nlearning algorithm with a general intelligence test. In Jos e A. Lozano, Jose A. Gamez, J.\nA. M., editor, Current Topics in Arti\ufb01cial Intelligence. 14th Conference of the Spanish As-\nsociation for Arti\ufb01cial Intelligence, CAEPIA 2011 . Lecture Notes in Arti\ufb01cial Intelligence,\nSpringer."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Comparing Humans and AI Agents",
          "authors": [
            "Insa-Cabrera",
            "Dowe",
            "Espa\u00f1a-Cubillo",
            "Hern\u00e1ndez-Lloreda",
            "Hern\u00e1ndez-Orallo"
          ],
          "year": 2011,
          "confidence": "medium"
        }
      },
      {
        "text": "Legg, S. and Hutter, M. (2007). Universal intelligence: A de \ufb01nition of machine intelligence.\nMinds and Machines , 17(4):391\u2013444.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Legg, S. and Hutter, M. (2007). Universal intelligence: A de \ufb01nition of machine intelligence.\nMinds and Machines , 17(4):391\u2013444."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Universal Intelligence: A Definition of Machine Intelligence",
          "authors": [
            "Legg",
            "Hutter"
          ],
          "year": 2007,
          "confidence": "medium"
        }
      },
      {
        "text": "Li, M. and Vit\u00b4 anyi, P. M. B. (2008). An introduction to Kolmogorov complexity and its\napplications . Springer, 3nd edition.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Li, M. and Vit\u00b4 anyi, P. M. B. (2008). An introduction to Kolmogorov complexity and its\napplications . Springer, 3nd edition."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Message from the INVITE 2008 Chairs",
          "authors": [],
          "year": 2008,
          "confidence": "medium"
        }
      },
      {
        "text": "Mahoney, M. (2008). Generic compression benchmark. http://www.mattmahoney.net/dc/uiq .",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Mahoney, M. (2008). Generic compression benchmark. http://www.mattmahoney.net/dc/uiq ."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "MyMultiMediaWorld.com: A benchmark platform for 3D compression algorithms",
          "authors": [
            "Le Bonhomme",
            "Preda",
            "Preteux"
          ],
          "year": 2008,
          "confidence": "medium"
        }
      },
      {
        "text": "Schaul, T., Togelius, J., and Schmidhuber, J. (2011). Measu ring Intelligence through Games.\nArXiv e-prints .\nSolomono\ufb00, R. J. (1964). A formaltheory of inductive infere nce: Part 1 and 2. Inform. Control ,\n7:1\u201322, 224\u2013254.\nSolomono\ufb00, R. J. (1978). Complexity-based induction syste ms: comparisons and convergence\ntheorems. IEEE Trans. Information Theory , IT-24:422\u2013432.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Schaul, T., Togelius, J., and Schmidhuber, J. (2011). Measu ring Intelligence through Games.\nArXiv e-prints .\nSolomono\ufb00, R. J. (1964). A formaltheory of inductive infere nce: Part 1 and 2. Inform. Control ,\n7:1\u201322, 224\u2013254.\nSolomono\ufb00, R. J. (1978). Complexity-based induction syste ms: comparisons and convergence\ntheorems. IEEE Trans. Information Theory , IT-24:422\u2013432."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "A scalable neural network architecture for board games",
          "authors": [
            "Schaul",
            "Schmidhuber"
          ],
          "year": 2008,
          "confidence": "medium"
        }
      },
      {
        "text": "Sutton, R. and Barto, A. (1998). Reinforcement learning: An introduction . Cambridge, MA,\nMIT Press.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Sutton, R. and Barto, A. (1998). Reinforcement learning: An introduction . Cambridge, MA,\nMIT Press."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Introduction: The Challenge of Reinforcement Learning",
          "authors": [
            "Sutton"
          ],
          "year": 1992,
          "confidence": "medium"
        }
      },
      {
        "text": "Turing, A. M. (1950). Computing machinery and intelligence .Mind.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Turing, A. M. (1950). Computing machinery and intelligence .Mind."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Computing Machinery and Intelligence (1950)",
          "authors": [
            "Turing"
          ],
          "year": 2004,
          "confidence": "medium"
        }
      },
      {
        "text": "Veness, J., Ng, K. S., Hutter, M., and Silver, D. (2010). Rein forcement learning via AIXI\napproximation. In Proc. Association for the advancesment of arti\ufb01cial intell igence.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Veness, J., Ng, K. S., Hutter, M., and Silver, D. (2010). Rein forcement learning via AIXI\napproximation. In Proc. Association for the advancesment of arti\ufb01cial intell igence."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Reinforcement Learning via AIXI Approximation",
          "authors": [
            "Veness",
            "Ng",
            "Hutter",
            "Silver"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "Veness, J., Ng, K. S., Hutter, M., Uther, W., and Silver, D. (2 011). A Monte-Carlo AIXI\nApproximation. Journal of Arti\ufb01cial Intelligence Research (JAIR) , 40(1).",
        "style": "mla",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Veness, J., Ng, K. S., Hutter, M., Uther, W., and Silver, D. (2 011). A Monte-Carlo AIXI\nApproximation. Journal of Arti\ufb01cial Intelligence Research (JAIR) , 40(1)."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Veness, Sir David (Christopher), (born 20 Sept. 1947), Under-Secretary-General for Safety and Security, United Nations, 2005\u201309",
          "authors": [],
          "year": 2007,
          "confidence": "medium"
        }
      },
      {
        "text": "Watkins, C. (1989). Learning from Delayed Rewards . PhD thesis, King\u2019s College, Oxford.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Watkins, C. (1989). Learning from Delayed Rewards . PhD thesis, King\u2019s College, Oxford."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Efficient Learning from Delayed Rewards through Symbiotic Evolution",
          "authors": [
            "Moriarty",
            "Miikkulainen"
          ],
          "year": 1995,
          "confidence": "medium"
        }
      }
    ],
    "in_text_citations": [
      {
        "type": "author_year",
        "year": "1950",
        "context": "or empirical aspects of broadly intelligent machines. Of course there\nis the well-known Turing Test **(Turing, 1950), howe**ver this paradox ically seems\nto be more about dodging the di\ufb03cult problem of ex"
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "One recent attempt at an explicit de\ufb01nition of intelligence is the Univer-\nsal Intelligence Measure **(Legg and Hutter, 20**07). This is a mathematical, non-\nanthropocentric de\ufb01nition of intelligence that"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "rmance of broadly intelligent agen ts.\n2.1 Universal Intelligence Tests\nHern\u00b4 andez-Orallo and Dowe **(2010) introduce the** notion of a Universal Intelli-\ngence Test , a test designed to be able to quant"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": "opocentric bias. Related discussion on the motivation b ehind such\ntests is given by Dowe and Hajek **(1998); Hern\u00b4 andez-**Orallo(2000 ); Schaul et al.\n(2011). With respect to our goal of wanting to buil"
      },
      {
        "type": "author_year",
        "year": "2011",
        "context": "tion b ehind such\ntests is given by Dowe and Hajek (1998); Hern\u00b4 andez-Orallo(2000 ); Schaul et al.\n**(2011). With respect** to our goal of wanting to build more powerful arti\ufb01cial\nagents, we strongly sup"
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "of intelligence propose d by various\npsychologistsandarti\ufb01cialintelligenceresearchers,LeggandHut ter**(2007)argue\nthat the** informal de\ufb01nition:\n\u201cintelligence measures an agent\u2019s ability to achieve goals "
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": "erties associated with intellig ence. To\nformalise this intuition, they used reinforcement learning **(Sutton and Barto,\n1**998), a general framework for goal achieving agents in unknown environments.\nIn "
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "d\ninto a single result. To encourage agents to apply Occam\u2019s Razor, as advocated\nby Legg and Hutter **(2007), each environ**ment is weighted accordin g to its com-\nplexity,with simplerenvironmentsbeing we"
      },
      {
        "type": "author_year",
        "year": "2005",
        "context": "hat converges to optimal performance in any e nvironment\nwhere this is possible for a general agent **(Hutter, 2005). At t**he o ther end of the\nscale, it can be shownthat the UniversalIntelligence Measur"
      },
      {
        "type": "author_year",
        "year": "2008",
        "context": "ling has been\nused to create the test data sequences that make up the Generic Compression\nBenchmark **(Mahoney, 2008). Her**e we will use this technique to sam ple envir-\nonments for the Universal Intelli"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": " of performing a Monte Ca rlo\nsample over environments is also used by Hern\u00b4 andez-Orallo and Dow e **(2010)\nand Schaul et** al. (2011) in their related work.\n3.2 Environment simulation\nWe need to be able"
      },
      {
        "type": "author_year",
        "year": "2011",
        "context": " rlo\nsample over environments is also used by Hern\u00b4 andez-Orallo and Dow e (2010)\nand Schaul et al. **(2011) in their rela**ted work.\n3.2 Environment simulation\nWe need to be able to run each sampled prog"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": " knowing if a program will respect the bound.\nA more practical alternative is geometric discounting **(Sutton and Barto,\n1**998) where we allow the environment to generate any reward in any cycle so\nlong "
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "g we allow agent s to train\nfrom samples prior to taking the test, as suggested in Legg and Hut ter **(2007).\nFor more lim**ited agents however, the choice of reference machine is important.\nIndeed, in th"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "t, randomly acting agent s have an\nAIQofzero,anaturalbaselinesuggestedbyHern\u00b4 andez-Orallo a nd Dowe**(2010).\n3.6 Variance** Reduction Techniques for AIQ Estimation\nObtaining an accurate estimate of an ag"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": " adaptive strati\ufb01ed samp ling, however\nwe have chosen the method developed by \u00b4Etor\u00b4 e and Jourdain **(2010) as they\nhave **derived the con\ufb01dence intervals for the estimate of the mean , a feature we\nwill"
      },
      {
        "type": "author_year",
        "year": "1989",
        "context": "a \ufb01xed fraction of the time when it tries a random action. We have imple-\nmented the Q(\u03bb) algorithm **(Watkins, 1989), whi**ch subsumes the simpler Q(0)\nalgorithm as a special case, and also HLQ(\u03bb) which "
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "pecial case, and also HLQ(\u03bb) which is similar except that it\nautomatically adapts its learning rate **(Hutter and Legg, 20**07). Fin ally, we have\n2 4 8 16 32\nContext Depth0102030405060AIQ Score\n5 10 25 5"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": "s) seemed\nquite incomprehensible.\n5 Related Work and Discussion\nHernandez-orallo and Minaya-collado **(1998) developed a r**elated te st, called the\nC-test, that is also based on a very simple reference m"
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "cular program that computes it.\n6 Conclusion\nWe have taken the theoretical model of Legg and Hutter **(2007) a nd converte**d\nit into a practical test for machine intelligence. To do this we have ran doml"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": " simple universal Turing machine, drawing ins pira-\ntion at points from Hern\u00b4 andez-Orallo and Dowe **(2010), and the re l**ated work in\nHern\u00b4 andez-Orallo (2010). In all of our tests the AIQ scores beh a"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "tion at points from Hern\u00b4 andez-Orallo and Dowe (2010), and the re lated work in\nHern\u00b4 andez-Orallo **(2010). In all of ou**r tests the AIQ scores beh aved sensibly,\nwith agents expected to be more intell"
      },
      {
        "type": "author_year",
        "year": "2009",
        "context": "telligence\nMeasure is its dependence on the choice of reference machine, as h ighlighted by\nHibbard **(2009). While we acc**ept that problematic reference machin es exist, it\nwas our belief that if we cho"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": "s National Science\nFoundation grant number PBTIP2-133701.\nBibliography\nDowe, D. L. and Hajek, A. R. **(1998). A non-behavi**oural, compu tational extension to the\nTuring Test. In Intl. Conf. on Computatio"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "edia app lications\n(ICCIMA\u201998), Gippsland, Australia , pages 101\u2013106.\n\u00b4Etor\u00b4 e, P. and Jourdain, B. **(2010). Adaptive opt**imal alloca tion in strati\ufb01ed sampling methods.\nMethodology and Computing in App"
      },
      {
        "type": "author_year",
        "year": "2000",
        "context": "g methods.\nMethodology and Computing in Applied Probability , 12(3):335\u2013360.\nHern\u00b4 andez-Orallo, J. **(2000). Beyond the T**uring Test. J. Logic, Language & Information ,\n9(4):447\u2013466.\nHern\u00b4 andez-Orallo,"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "0). Beyond the Turing Test. J. Logic, Language & Information ,\n9(4):447\u2013466.\nHern\u00b4 andez-Orallo, J. **(2010). A (hopefully**) unbiased univ ersal environment class for measuring\nintelligence ofbiological "
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "Third Conference on Arti\ufb01cial\nGeneral Intelligence , Lugano.\nHern\u00b4 andez-Orallo, J. and Dowe, D. L. **(2010). Measuring un**i versal intelligence: Towards an\nanytime intelligence test. Arti\ufb01cial Intellige"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": "nce test. Arti\ufb01cial Intelligence , 174(18):1508 \u2013 1539.\nHernandez-orallo, J. and Minaya-collado, N. **(1998). A form al de**\ufb01nition of intelligence based\non an intensional variant of algorithmic complexit"
      },
      {
        "type": "author_year",
        "year": "2009",
        "context": "n of intelligence based\non an intensional variant of algorithmic complexity. In EIS\u201998.\nHibbard, B. **(2009). Bias and no **free lunch in formal measures of intelligence. Journal of\nArti\ufb01cial General Inte"
      },
      {
        "type": "author_year",
        "year": "2001",
        "context": "formal measures of intelligence. Journal of\nArti\ufb01cial General Intelligence , 1(1):54\u201361.\nHutter, M. **(2001). Towards a un**iversal theory of arti\ufb01cial i ntelligence based on algorithmic\nprobability and s"
      },
      {
        "type": "author_year",
        "year": "2005",
        "context": "ecisions. Proc. 12th Eurpean Conference on Machine Learning\n(ECML-2001) , pages 226\u2013238.\nHutter, M. **(2005). Universal Ar**ti\ufb01cial Intelligence: Sequential Decisions b ased on Algorithmic\nProbability . S"
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "lity . Springer, Berlin. 300 pages, http://www.hutter1.net/ai /uaibook.htm.\nHutter, M. and Legg, S. **(2007). Temporal di\ufb00**erence updating without a learning rate. In\nNeural Information Processing System"
      },
      {
        "type": "author_year",
        "year": "2011a",
        "context": "nsa-Cabrera, J., Dowe, D. L., Espana-Cubillo, S., Hernand ez-Lloreda, M., and Hernandez-\nOrallo, J. **(2011a). Comparing h**umans and ai agents. In Juerg en Schmidhuber, Kristinn\nR. Thorisson, M. L., edit"
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "elligence, CAEPIA 2011 . Lecture Notes in Arti\ufb01cial Intelligence,\nSpringer.\nLegg, S. and Hutter, M. **(2007). Universal in**telligence: A de \ufb01nition of machine intelligence.\nMinds and Machines , 17(4):391"
      },
      {
        "type": "author_year",
        "year": "2008",
        "context": "\ufb01nition of machine intelligence.\nMinds and Machines , 17(4):391\u2013444.\nLi, M. and Vit\u00b4 anyi, P. M. B. **(2008). An introduct**ion to Kolmogorov complexity and its\napplications . Springer, 3nd edition.\nMahon"
      },
      {
        "type": "author_year",
        "year": "2008",
        "context": " An introduction to Kolmogorov complexity and its\napplications . Springer, 3nd edition.\nMahoney, M. **(2008). Generic comp**ression benchmark. http://www.mattmahoney.net/dc/uiq .\nSchaul, T., Togelius, J.,"
      },
      {
        "type": "author_year",
        "year": "2011",
        "context": "ession benchmark. http://www.mattmahoney.net/dc/uiq .\nSchaul, T., Togelius, J., and Schmidhuber, J. **(2011). Measu ring I**ntelligence through Games.\nArXiv e-prints .\nSolomono\ufb00, R. J. (1964). A formalthe"
      },
      {
        "type": "author_year",
        "year": "1964",
        "context": "nd Schmidhuber, J. (2011). Measu ring Intelligence through Games.\nArXiv e-prints .\nSolomono\ufb00, R. J. **(1964). A formaltheo**ry of inductive infere nce: Part 1 and 2. Inform. Control ,\n7:1\u201322, 224\u2013254.\nSol"
      },
      {
        "type": "author_year",
        "year": "1978",
        "context": "altheory of inductive infere nce: Part 1 and 2. Inform. Control ,\n7:1\u201322, 224\u2013254.\nSolomono\ufb00, R. J. **(1978). Complexity-b**ased induction syste ms: comparisons and convergence\ntheorems. IEEE Trans. Infor"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": " and convergence\ntheorems. IEEE Trans. Information Theory , IT-24:422\u2013432.\nSutton, R. and Barto, A. **(1998). Reinforcemen**t learning: An introduction . Cambridge, MA,\nMIT Press.\nTuring, A. M. (1950). Co"
      },
      {
        "type": "author_year",
        "year": "1950",
        "context": "Barto, A. (1998). Reinforcement learning: An introduction . Cambridge, MA,\nMIT Press.\nTuring, A. M. **(1950). Computing ma**chinery and intelligence .Mind.\nVeness, J., Ng, K. S., Hutter, M., and Silver, D"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "950). Computing machinery and intelligence .Mind.\nVeness, J., Ng, K. S., Hutter, M., and Silver, D. **(2010). Rein forceme**nt learning via AIXI\napproximation. In Proc. Association for the advancesment of"
      },
      {
        "type": "author_year",
        "year": "1989",
        "context": "te-Carlo AIXI\nApproximation. Journal of Arti\ufb01cial Intelligence Research (JAIR) , 40(1).\nWatkins, C. **(1989). Learning fro**m Delayed Rewards . PhD thesis, King\u2019s College, Oxford.\n"
      }
    ]
  },
  {
    "filename": "1307.3176v4.pdf",
    "total_citations": 2,
    "valid_format_count": 2,
    "existing_count": 2,
    "invalid_format_count": 0,
    "nonexistent_count": 0,
    "citations": [
      {
        "text": "Francis Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine\nlearning. Advances in Neural Information Processing Systems (NIPS) , 2011.\nVarsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. In\nProceedings of the 21st Annual Conference on Learning Theory (COLT) , pages 355\u2013366, 2008.\n15\nNoufel Frikha and Stphane Menozzi. Concentration Bounds for Stochastic Approximations. Electron. Commun.\nProbab. , 17:1\u201315, 2012.\nElad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-\nconvex optimization. Journal of Machine Learning Research , 19:421\u2013436, 2011.\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In\nAdvances in Neural Information Processing Systems (NIPS) , pages 315\u2013323, 2013.\nLihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news\narticle recommendation. In Proceedings of the 19th international conference on World wide web , pages 661\u2013",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Francis Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine\nlearning. Advances in Neural Information Processing Systems (NIPS) , 2011.\nVarsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. In\nProceedings of the 21st Annual Conference on Learning Theory (COLT) , pages 355\u2013366, 2008.\n15\nNoufel Frikha and Stphane Menozzi. Concentration Bounds for Stochastic Approximations. Electron. Commun.\nProbab. , 17:1\u201315, 2012.\nElad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-\nconvex optimization. Journal of Machine Learning Research , 19:421\u2013436, 2011.\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In\nAdvances in Neural Information Processing Systems (NIPS) , pages 315\u2013323, 2013.\nLihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news\narticle recommendation. In Proceedings of the 19th international conference on World wide web , pages 661\u2013"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Asymptotic Properties of Stochastic Approximation Algorithms",
          "authors": [],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "670. ACM, 2010.\nJ. Mary, Aurlien Garivier, L. Li, R. Munos, O. Nicol, R. Ortner, and P. Preux. ICML Exploration and Exploitation\n3 - New Challenges, 2012.\nAS Nemirovsky and DB Yudin. Problem complexity and method ef\ufb01ciency in optimization, 1983.\nAlexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex\nstochastic optimization. arXiv preprint arXiv:1109.5647 , 2011.\nNicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence\nrate for \ufb01nite training sets. arXiv preprint arXiv:1202.6258 , 2012.\nPaat Rusmevichientong and John N. Tsitsiklis. Linearly parameterized bandits. Math. Oper. Res. , 35(2):395\u2013411,\nMay 2010.\nShai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-\ntion. arXiv preprint arXiv:1209.1873 , 2012.\nPierre Tarr `es and Yuan Yao. Online learning as stochastic approximation of regularization paths. arXiv preprint\narXiv:1103.5538 , 2011.\nYahoo! Webscope. Yahoo! webscope dataset ydata-frontpage-todaymodule-clicks-v2 0, 2011. URL \"http:\n//research.yahoo.com/Academic_Relations\" .\nMartin Zinkevich. Online convex programming and generalized in\ufb01nitesimal gradient ascent. In Proceedings of\nthe 20th International Conference on Machine Learning , pages 928\u2013925, 2003.\n16",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "670. ACM, 2010.\nJ. Mary, Aurlien Garivier, L. Li, R. Munos, O. Nicol, R. Ortner, and P. Preux. ICML Exploration and Exploitation\n3 - New Challenges, 2012.\nAS Nemirovsky and DB Yudin. Problem complexity and method ef\ufb01ciency in optimization, 1983.\nAlexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex\nstochastic optimization. arXiv preprint arXiv:1109.5647 , 2011.\nNicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence\nrate for \ufb01nite training sets. arXiv preprint arXiv:1202.6258 , 2012.\nPaat Rusmevichientong and John N. Tsitsiklis. Linearly parameterized bandits. Math. Oper. Res. , 35(2):395\u2013411,\nMay 2010.\nShai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-\ntion. arXiv preprint arXiv:1209.1873 , 2012.\nPierre Tarr `es and Yuan Yao. Online learning as stochastic approximation of regularization paths. arXiv preprint\narXiv:1103.5538 , 2011.\nYahoo! Webscope. Yahoo! webscope dataset ydata-frontpage-todaymodule-clicks-v2 0, 2011. URL \"http:\n//research.yahoo.com/Academic_Relations\" .\nMartin Zinkevich. Online convex programming and generalized in\ufb01nitesimal gradient ascent. In Proceedings of\nthe 20th International Conference on Machine Learning , pages 928\u2013925, 2003.\n16"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "2010 Miracle Yearbook",
          "authors": [],
          "year": 2010,
          "confidence": "medium"
        }
      }
    ],
    "in_text_citations": [
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "irst, we consider the PEGE algorithm for linear bandits proposed by Rusmevichientong and Tsitsiklis **[2010]**.\nThis algorithm is designed for action sets Dsatisfying a strong convexity property (see assu"
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "only O(log4n)in the regret performance.\nSecond, we consider the LinUCB algorithm proposed Li et al. **[2010]**. Here we investigate computationally\nef\ufb01cient variants of LinUCB. We begin by replacing the O"
      },
      {
        "type": "numbered",
        "ref_num": "2013",
        "context": "-GD iterate, and then compare\nthis to two other state-of-the-art OLS schemes from Johnson and Zhang **[2013]** and Roux et al. [2012]. The\nLinUCB algorithm is designed for situations where at each time, n"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "ompare\nthis to two other state-of-the-art OLS schemes from Johnson and Zhang [2013] and Roux et al. **[2012]**. The\nLinUCB algorithm is designed for situations where at each time, n, the agent can choose "
      },
      {
        "type": "numbered",
        "ref_num": "2013",
        "context": "om the numerical experiments, we observe that the fRLS-GD iterate as well\nas SVRG Johnson and Zhang **[2013]** and SAG Roux et al. [2012] variants consistently track the true RLS solutions\nin each iterati"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "s, we observe that the fRLS-GD iterate as well\nas SVRG Johnson and Zhang [2013] and SAG Roux et al. **[2012]** variants consistently track the true RLS solutions\nin each iteration of LinUCB, while the run"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "exity. Non-asymptotic bounds in expectation for SGD schemes have been provided by\nBach and Moulines **[2011]**. In the machine learning community, several algorithms have been proposed for min-\nimising th"
      },
      {
        "type": "numbered",
        "ref_num": "2003",
        "context": "mmunity, several algorithms have been proposed for min-\nimising the regret, for instance, Zinkevich **[2003]**, Hazan and Kale [2011], Rakhlin et al. [2011] and these can be\nconverted to \ufb01nd the minimiser"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "thms have been proposed for min-\nimising the regret, for instance, Zinkevich [2003], Hazan and Kale **[2011]**, Rakhlin et al. [2011] and these can be\nconverted to \ufb01nd the minimiser of a (usually convex) "
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": " for min-\nimising the regret, for instance, Zinkevich [2003], Hazan and Kale [2011], Rakhlin et al. **[2011]** and these can be\nconverted to \ufb01nd the minimiser of a (usually convex) function. A closely rel"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "oximation\n(SA), and concentration bounds for SA algorithms have been provided by Frikha and Menozzi **[2012]**. Adaptive\nregularisation in the context of least squares regression has been analysed in Tarr"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "ive\nregularisation in the context of least squares regression has been analysed in Tarr `es and Yao **[2011]**. For recent\nalgorithmic improvements to solving batch problems, the reader is referred to the"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "orithmic improvements to solving batch problems, the reader is referred to the works of Roux et al. **[2012]**,\nShalev-Shwartz and Zhang [2012], Johnson and Zhang [2013].\nIn general, none of the schemes p"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "batch problems, the reader is referred to the works of Roux et al. [2012],\nShalev-Shwartz and Zhang **[2012]**, Johnson and Zhang [2013].\nIn general, none of the schemes proposed above are directly applic"
      },
      {
        "type": "numbered",
        "ref_num": "2013",
        "context": " is referred to the works of Roux et al. [2012],\nShalev-Shwartz and Zhang [2012], Johnson and Zhang **[2013]**.\nIn general, none of the schemes proposed above are directly applicable in our setting due to"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "Sketch In order to prove the bound in expectation, following the proof scheme of Frikha and Menozzi\n**[2012]**, we expand the error at time ninto an initial error term, a (martingale) sampling error term,"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "etails).\nThe initial and sampling errors appear as in previous works on SGD (cf. Frikha and Menozzi **[2012]** and Bach\nand Moulines [2011]), and can be treated similarly, except that here we can make all"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "ling errors appear as in previous works on SGD (cf. Frikha and Menozzi [2012] and Bach\nand Moulines **[2011]**), and can be treated similarly, except that here we can make all the constants explicit, usin"
      },
      {
        "type": "numbered",
        "ref_num": "2008",
        "context": "onvergence of the least squares solution ^\u0012nto\u0012\u0003. Adapting a con\ufb01dence ball result\nfrom Dani et al. **[2008]**, we derive the third term of K1.\nHaving bounded the mean error, we can bound separately the d"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "bound separately the deviation of the error from its mean. To do\nthis, following Frikha and Menozzi **[2012]**, we decompose k\u0012n\u0000^\u0012nk2\u0000Ek\u0012n\u0000^\u0012nk2into a sum of martingale\ndifferences as follows: Let Hndeno"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "are Lipschitz continuous in the noise \u0018i,\nwith Lipschitz constants Li. Unlike in Frikha and Menozzi **[2012]** we use the exact form of the update to derive the\nexact constants Li. The \ufb01nal step of the pr"
      },
      {
        "type": "numbered",
        "ref_num": "1983",
        "context": "ergence rate for SGD type schemes that do not involve a drifting target (see Ne-\nmirovsky and Yudin **[1983]**).\nDependence on dThe dependence of the rate derived above on the dimension dofxiis indirect, "
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "overns the\nlosses of the bandit algorithm (see (A5) below). PEGE of Rusmevichientong and Tsitsiklis **[2010]** is a well-known\nalgorithm in this setting. Recall from the introduction that it gathers data "
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "nly O(md2).\nResults We require the following extra assumptions from Rusmevichientong and Tsitsiklis **[2010]**:\n(A4\u2019) A basisfb1;:::;bdg2D forRdis known to the algorithm.\n(A5) The function G(\u0012)isJ-Lipschi"
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": " (8)\nNow to complete the proof we only need to reprove Lemma 3.6 of Rusmevichientong and Tsitsiklis **[2010]**, which\nstates that for all n\u0015d,Ek\u0012\u0003(G(\u0012\u0003)\u0000G(\u0012md))k2\u0014K1(n)\ndmk\u0012\u0003k2:\nk\u0012\u0003(G(\u0012\u0003)\u0000G(\u0012md))k2=\r\r(\u0012\u0003\u0000"
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "we have used that G(\u0012) =G(a\u0012)for alla > 0, (A5), and Lemma 3.5 of Rus-\nmevichientong and Tsitsiklis **[2010]**.\nThe rest of the proof follows that of Theorem 3.1 of Rusmevichientong and Tsitsiklis [2010]."
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "siklis [2010].\nThe rest of the proof follows that of Theorem 3.1 of Rusmevichientong and Tsitsiklis **[2010]**.\n4 Online GD for Regularized Least Squares\nIdeally an online algorithm would not need to sati"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "r (note, \u000bmust be chosen in (1=2;1)to ensure (A1) holds).\nUnlike in the setting of Tarr `es and Yao **[2011]**, we do not assume that the data arrive from a distribution, and hence\nthe bias error is dif\ufb01c"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "e recommendation platform provided for the ICML\nexploration and exploitation challenge (Mary et al. **[2012]**). This platform is based on the user click log dataset\nfrom the Yahoo! front page, provided u"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "he user click log dataset\nfrom the Yahoo! front page, provided under the Webscope program (Webscope **[2011]**). An algorithm for this\nplatform is required to repeatedly select a news article from a pool "
      },
      {
        "type": "numbered",
        "ref_num": "2013",
        "context": "similar to the above algorithm, except that the SGD scheme used is derived from John-\nson and Zhang **[2013]**. The \ufb01rst scheme is derived from Johnson and Zhang [2013] and updates the parameter\nas follow"
      },
      {
        "type": "numbered",
        "ref_num": "2013",
        "context": " used is derived from John-\nson and Zhang [2013]. The \ufb01rst scheme is derived from Johnson and Zhang **[2013]** and updates the parameter\nas follows: Let fi;n(\u0012) :=1\n2(yi\u0000\u0012Txi)2+\u0015nk\u0012k2\n2,Fn(\u0012) =1\nnn\u00001P\ni=1"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "andom in f1;:::;ng.\nfLinUCB-SAG. This is a variant that uses the SGD scheme proposed by Roux et al. **[2012]**. The updates here are\naccording to\n\u0012n=\u0012n\u00001\u0000\rn\nnnX\ni=1yn;i;whereyn;i=(\nf0\ni(\u0012n\u00001)ifi=in,\nyn\u00001;"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "times observed on two different data \ufb01les corresponding to days 2and4in October, 2009 (see\nWebscope **[2011]**) of the dataset. It is evident that the SGD schemes result in signi\ufb01cant computational gains "
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "error, zn:=\u0012n\u0000^\u0012n,\nfrom its mean. The proof technique is similar to that used by Frikha and Menozzi **[2012]**. However, our analysis is\nmuch simpler, we make all the constants explicit for the problem at"
      },
      {
        "type": "numbered",
        "ref_num": "2008",
        "context": " 1\u0000\u000e, where\n\fn= max\u0010\n128dlognlogn2\u000e\u00001;\u0000\n2 logn2\u000e\u00001\u00012\u0011\n.\nProof. Follows from Theorem 5of Dani et al. **[2008]** and (A3).\nUsing the above lemma, we bound the drift error as follows:\n nX\nk=n0+1exp(\u00002\u0016(\u0000n\u0000\u0000k"
      }
    ]
  },
  {
    "filename": "1312.5602v1.pdf",
    "total_citations": 26,
    "valid_format_count": 26,
    "existing_count": 26,
    "invalid_format_count": 0,
    "nonexistent_count": 0,
    "citations": [
      {
        "text": "[1] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In\nProceedings of the 12th International Conference on Machine Learning (ICML 1995) , pages\n30\u201337. Morgan Kaufmann, 1995.",
        "ref_num": "1",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "1",
          "Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In\nProceedings of the 12th International Conference on Machine Learning (ICML 1995) , pages\n30\u201337. Morgan Kaufmann, 1995."
        ],
        "doi": null,
        "in_text_mentions": [
          "such as Q-\nlearning with non-linear function approximators [25], or indeed with off-policy learning **[1]** could\ncause the Q-network to diverge. Subsequently, the majority of work in reinforcement learni",
          "seven games it was tested on, with no adjustment of the\narchitecture or hyperparameters.\nReferences\n**[1]** Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In\nProcee"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Residual Algorithms: Reinforcement Learning with Function Approximation",
          "authors": [
            "Baird"
          ],
          "year": 1995,
          "confidence": "medium"
        }
      },
      {
        "text": "[2] Marc Bellemare, Joel Veness, and Michael Bowling. Sketch-based linear value function ap-\nproximation. In Advances in Neural Information Processing Systems 25 , pages 2222\u20132230,\n2012.",
        "ref_num": "2",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "2",
          "Marc Bellemare, Joel Veness, and Michael Bowling. Sketch-based linear value function ap-\nproximation. In Advances in Neural Information Processing Systems 25 , pages 2222\u20132230,\n2012."
        ],
        "doi": null,
        "in_text_mentions": [
          "tures, and\nusing tug-of-war hashing to randomly project the features into a lower-dimensional space **[2]**. The\nHyperNEAT evolutionary architecture [8] has also been applied to the Atari platform, where ",
          "12th International Conference on Machine Learning (ICML 1995) , pages\n30\u201337. Morgan Kaufmann, 1995.\n**[2]** Marc Bellemare, Joel Veness, and Michael Bowling. Sketch-based linear value function ap-\nproxima"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Context Tree Switching",
          "authors": [
            "Veness",
            "Ng",
            "Hutter",
            "Bowling"
          ],
          "year": 2012,
          "confidence": "medium"
        }
      },
      {
        "text": "[3] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Arti\ufb01cial Intelligence\nResearch , 47:253\u2013279, 2013.",
        "ref_num": "3",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "3",
          "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Arti\ufb01cial Intelligence\nResearch , 47:253\u2013279, 2013."
        ],
        "doi": null,
        "in_text_mentions": [
          " our approach to a range of Atari 2600 games implemented in The Arcade Learning Envi-\nronment (ALE) **[3]**. Atari 2600 is a challenging RL testbed that presents agents with a high dimen-\nsional visual in",
          "l inputs.\nThe use of the Atari 2600 emulator as a reinforcement learning platform was introduced by **[3]**, who\napplied standard reinforcement learning algorithms with linear function approximation and g",
          "llowing previous approaches to playing Atari games, we also use a simple frame-skipping tech-\nnique **[3]**. More precisely, the agent sees and selects actions on every kthframe instead of every\nframe, an",
          "ogress of an agent during training can be challenging. Since our evaluation metric, as suggested\nby **[3]**, is the total reward the agent collects in an episode or game averaged over a number of\ngames, w",
          "\n5.3 Main Evaluation\nWe compare our results with the best performing methods from the RL literature **[3, 4]**. The method\nlabeled Sarsa used the Sarsa algorithm to learn linear policies on several differ",
          "ets hand-\nengineered for the Atari task and we report the score for the best performing feature set **[3]**. Con-\ntingency used the same basic approach as Sarsa but augmented the feature sets with a learn",
          "ng each game. Note that our reported human scores are much higher\nthan the ones in Bellemare et al. **[3]**. For the learned methods, we follow the evaluation strategy used\nin Bellemare et al. [3, 5] and ",
          "are et al. [3]. For the learned methods, we follow the evaluation strategy used\nin Bellemare et al. **[3, 5]** and report the average score obtained by running an \u000f-greedy policy with\n\u000f= 0:05for a \ufb01xed nu",
          "7\nB. Rider Breakout Enduro Pong Q*bert Seaquest S. Invaders\nRandom 354 1:2 0\u000020:4 157 110 179\nSarsa **[3]** 996 5:2 129\u000019 614 665 271\nContingency [4] 1743 6 159\u000017 960 723 268\nDQN 4092 168 470 20 1952 17",
          "n ap-\nproximation. In Advances in Neural Information Processing Systems 25 , pages 2222\u20132230,\n2012.\n**[3]** Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironmen"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
          "authors": [
            "Bellemare",
            "Naddaf",
            "Veness",
            "Bowling"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "[4] Marc G Bellemare, Joel Veness, and Michael Bowling. Investigating contingency awareness\nusing atari 2600 games. In AAAI , 2012.",
        "ref_num": "4",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "4",
          "Marc G Bellemare, Joel Veness, and Michael Bowling. Investigating contingency awareness\nusing atari 2600 games. In AAAI , 2012."
        ],
        "doi": null,
        "in_text_mentions": [
          "\n5.3 Main Evaluation\nWe compare our results with the best performing methods from the RL literature **[3, 4]**. The method\nlabeled Sarsa used the Sarsa algorithm to learn linear policies on several differ",
          "re sets with a learned\nrepresentation of the parts of the screen that are under the agent\u2019s control **[4]**. Note that both of these\nmethods incorporate signi\ufb01cant prior knowledge about the visual problem",
          "uest S. Invaders\nRandom 354 1:2 0\u000020:4 157 110 179\nSarsa [3] 996 5:2 129\u000019 614 665 271\nContingency **[4]** 1743 6 159\u000017 960 723 268\nDQN 4092 168 470 20 1952 1705 581\nHuman 7456 31 368\u00003 18900 28010 3690",
          "luation platform for general agents. Journal of Arti\ufb01cial Intelligence\nResearch , 47:253\u2013279, 2013.\n**[4]** Marc G Bellemare, Joel Veness, and Michael Bowling. Investigating contingency awareness\nusing at"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Investigating Contingency Awareness Using Atari 2600 Games",
          "authors": [
            "Bellemare",
            "Veness",
            "Bowling"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "[5] Marc G. Bellemare, Joel Veness, and Michael Bowling. Bayesian learning of recursively fac-\ntored environments. In Proceedings of the Thirtieth International Conference on Machine\nLearning (ICML 2013) , pages 1211\u20131219, 2013.\n8",
        "ref_num": "5",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "5",
          "Marc G. Bellemare, Joel Veness, and Michael Bowling. Bayesian learning of recursively fac-\ntored environments. In Proceedings of the Thirtieth International Conference on Machine\nLearning (ICML 2013) , pages 1211\u20131219, 2013.\n8"
        ],
        "doi": null,
        "in_text_mentions": [
          "are et al. [3]. For the learned methods, we follow the evaluation strategy used\nin Bellemare et al. **[3, 5]** and report the average score obtained by running an \u000f-greedy policy with\n\u000f= 0:05for a \ufb01xed nu",
          "s, and Michael Bowling. Investigating contingency awareness\nusing atari 2600 games. In AAAI , 2012.\n**[5]** Marc G. Bellemare, Joel Veness, and Michael Bowling. Bayesian learning of recursively fac-\ntored"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Hals-, Nasen- und Ohrenheilkunde",
          "authors": [
            "Larsen"
          ],
          "year": 2013,
          "confidence": "medium"
        }
      },
      {
        "text": "[6] George E. Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep\nneural networks for large-vocabulary speech recognition. Audio, Speech, and Language Pro-\ncessing, IEEE Transactions on , 20(1):30 \u201342, January 2012.",
        "ref_num": "6",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "6",
          "George E. Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep\nneural networks for large-vocabulary speech recognition. Audio, Speech, and Language Pro-\ncessing, IEEE Transactions on , 20(1):30 \u201342, January 2012."
        ],
        "doi": null,
        "in_text_mentions": [
          "raw sen-\nsory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition **[6, 7]**.\nThese methods utilise a range of neural network architectures, including convolutional netwo",
          "f the Thirtieth International Conference on Machine\nLearning (ICML 2013) , pages 1211\u20131219, 2013.\n8\n**[6]** George E. Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep\nneural netw"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition",
          "authors": [
            "Dahl",
            "Dong Yu",
            "Li Deng",
            "Acero"
          ],
          "year": 2012,
          "confidence": "medium"
        }
      },
      {
        "text": "[7] Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep\nrecurrent neural networks. In Proc. ICASSP , 2013.",
        "ref_num": "7",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "7",
          "Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep\nrecurrent neural networks. In Proc. ICASSP , 2013."
        ],
        "doi": null,
        "in_text_mentions": [
          "raw sen-\nsory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition **[6, 7]**.\nThese methods utilise a range of neural network architectures, including convolutional netwo",
          "ition. Audio, Speech, and Language Pro-\ncessing, IEEE Transactions on , 20(1):30 \u201342, January 2012.\n**[7]** Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep\nrecurren"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Speech recognition with deep recurrent neural networks",
          "authors": [
            "Graves",
            "Mohamed",
            "Hinton"
          ],
          "year": 2013,
          "confidence": "medium"
        }
      },
      {
        "text": "[8] Matthew Hausknecht, Risto Miikkulainen, and Peter Stone. A neuro-evolution approach to\ngeneral atari game playing. 2013.",
        "ref_num": "8",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "8",
          "Matthew Hausknecht, Risto Miikkulainen, and Peter Stone. A neuro-evolution approach to\ngeneral atari game playing. 2013."
        ],
        "doi": null,
        "in_text_mentions": [
          "ly project the features into a lower-dimensional space [2]. The\nHyperNEAT evolutionary architecture **[8]** has also been applied to the Atari platform, where it was\nused to evolve (separately, for each d",
          "edge about the inputs.\nWe also include a comparison to the evolutionary policy search approach from **[8]** in the last three\nrows of table 1. We report two sets of results for this method. The HNeat Best",
          "59\u000017 960 723 268\nDQN 4092 168 470 20 1952 1705 581\nHuman 7456 31 368\u00003 18900 28010 3690\nHNeat Best **[8]** 3616 52 106 19 1800 920 1720\nHNeat Pixel [8] 1332 4 91\u000016 1325 800 1145\nDQN Best 5184 225 661 21",
          "05 581\nHuman 7456 31 368\u00003 18900 28010 3690\nHNeat Best [8] 3616 52 106 19 1800 920 1720\nHNeat Pixel **[8]** 1332 4 91\u000016 1325 800 1145\nDQN Best 5184 225 661 21 4500 1740 1075\nTable 1: The upper table comp",
          "Geoffrey E. Hinton. Speech recognition with deep\nrecurrent neural networks. In Proc. ICASSP , 2013.\n**[8]** Matthew Hausknecht, Risto Miikkulainen, and Peter Stone. A neuro-evolution approach to\ngeneral a"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "HyperNEAT-GGP",
          "authors": [
            "Hausknecht",
            "Khandelwal",
            "Miikkulainen",
            "Stone"
          ],
          "year": 2012,
          "confidence": "medium"
        }
      },
      {
        "text": "[9] Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with\nenergy-based policies. In European Workshop on Reinforcement Learning , page 43, 2012.",
        "ref_num": "9",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "9",
          "Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with\nenergy-based policies. In European Workshop on Reinforcement Learning , page 43, 2012."
        ],
        "doi": null,
        "in_text_mentions": [
          " E; restricted Boltzmann\nmachines have been used to estimate the value function [21]; or the policy **[9]**. In addition, the\ndivergence issues with Q-learning have been partially addressed by gradient te",
          "isto Miikkulainen, and Peter Stone. A neuro-evolution approach to\ngeneral atari game playing. 2013.\n**[9]** Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with\nenergy-b"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Table 8: Output of actor-critic reinforcement learning.",
          "authors": [],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "[10] Kevin Jarrett, Koray Kavukcuoglu, MarcAurelio Ranzato, and Yann LeCun. What is the best\nmulti-stage architecture for object recognition? In Proc. International Conference on Com-\nputer Vision and Pattern Recognition (CVPR 2009) , pages 2146\u20132153. IEEE, 2009.",
        "ref_num": "10",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "10",
          "Kevin Jarrett, Koray Kavukcuoglu, MarcAurelio Ranzato, and Yann LeCun. What is the best\nmulti-stage architecture for object recognition? In Proc. International Conference on Com-\nputer Vision and Pattern Recognition (CVPR 2009) , pages 2146\u20132153. IEEE, 2009."
        ],
        "doi": null,
        "in_text_mentions": [
          "layer convolves 16 8\u00028\n\ufb01lters with stride 4with the input image and applies a recti\ufb01er nonlinearity **[10, 18]**. The second\nhidden layer convolves 32 4\u00024\ufb01lters with stride 2, again followed by a recti\ufb01er",
          "earning with\nenergy-based policies. In European Workshop on Reinforcement Learning , page 43, 2012.\n**[10]** Kevin Jarrett, Koray Kavukcuoglu, MarcAurelio Ranzato, and Yann LeCun. What is the best\nmulti-s"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Depletion Effects Massively Change Chromatin Properties and Influence Genome Folding",
          "authors": [
            "Diesinger",
            "Heermann"
          ],
          "year": 2009,
          "confidence": "medium"
        }
      },
      {
        "text": "[11] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. In Advances in Neural Information Processing Systems 25 , pages\n1106\u20131114, 2012.",
        "ref_num": "11",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "11",
          "Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. In Advances in Neural Information Processing Systems 25 , pages\n1106\u20131114, 2012."
        ],
        "doi": null,
        "in_text_mentions": [
          "to extract high-level features from raw sen-\nsory data, leading to breakthroughs in computer vision **[11, 22, 16]** and speech recognition [6, 7].\nThese methods utilise a range of neural network architec",
          "eep neural networks, it is often possible to learn better representations than\nhandcrafted features **[11]**. These successes motivate our approach to reinforcement learning. Our\ngoal is to connect a rein",
          " \ufb01nal cropping\nstage is only required because we use the GPU implementation of 2D convolutions from **[11]**, which\nexpects square inputs. For the experiments in this paper, the function \u001efrom algorithm 1",
          " Conference on Com-\nputer Vision and Pattern Recognition (CVPR 2009) , pages 2146\u20132153. IEEE, 2009.\n**[11]** Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con-\nvoluti"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "ImageNet classification with deep convolutional neural networks",
          "authors": [
            "Krizhevsky",
            "Sutskever",
            "Hinton"
          ],
          "year": 2017,
          "confidence": "medium"
        }
      },
      {
        "text": "[12] Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement\nlearning. In Neural Networks (IJCNN), The 2010 International Joint Conference on , pages\n1\u20138. IEEE, 2010.",
        "ref_num": "12",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "12",
          "Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement\nlearning. In Neural Networks (IJCNN), The 2010 International Joint Conference on , pages\n1\u20138. IEEE, 2010."
        ],
        "doi": null,
        "in_text_mentions": [
          "to learn a low dimensional representation of the task, and then applying NFQ to this\nrepresentation **[12]**. In contrast our approach applies reinforcement learning end-to-end, directly\nfrom the visual i",
          "e history and the action have been used as inputs\nto the neural network by some previous approaches **[20, 12]**. The main drawback of this type\nof architecture is that a separate forward pass is required",
          "l neural networks. In Advances in Neural Information Processing Systems 25 , pages\n1106\u20131114, 2012.\n**[12]** Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement\nlearning"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "The 2010 International Joint Conference on Neural Networks (IJCNN)",
          "authors": [],
          "year": 2010,
          "confidence": "medium"
        }
      },
      {
        "text": "[13] Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC\nDocument, 1993.",
        "ref_num": "13",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "13",
          "Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC\nDocument, 1993."
        ],
        "doi": null,
        "in_text_mentions": [
          "Left-to-right ) Pong, Breakout, Space Invaders,\nSeaquest, Beam Rider\nan experience replay mechanism **[13]** which randomly samples previous transitions, and thereby\nsmooths the training distribution over",
          "es. Q-learning has also previously been combined with experience replay and a simple\nneural network **[13]**, but again starting with a low-dimensional state rather than raw visual inputs.\nThe use of the ",
          "ast to TD-Gammon and similar online approaches, we utilize a technique known as expe-\nrience replay **[13]** where we store the agent\u2019s experiences at each time-step, et= (st;at;rt;st+1)\nin a data-setD=e1",
          "ng. In Neural Networks (IJCNN), The 2010 International Joint Conference on , pages\n1\u20138. IEEE, 2010.\n**[13]** Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC\nDo"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Teaching and learning of deburring robots using neural networks",
          "authors": [
            "Liu",
            "Asada"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "[14] Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Rich\nSutton. Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approxi-\nmation. In Advances in Neural Information Processing Systems 22 , pages 1204\u20131212, 2009.",
        "ref_num": "14",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "14",
          "Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Rich\nSutton. Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approxi-\nmation. In Advances in Neural Information Processing Systems 22 , pages 1204\u20131212, 2009."
        ],
        "doi": null,
        "in_text_mentions": [
          "methods are proven to converge when evaluating a \ufb01xed policy with a nonlinear\nfunction approximator **[14]**; or when learning a control policy with linear function approximation\nusing a restricted varian",
          "in. Reinforcement learning for robots using neural networks. Technical report, DTIC\nDocument, 1993.\n**[14]** Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Rich\nSutton. C"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
          "authors": [
            "Sutton",
            "Maei",
            "Precup",
            "Bhatnagar",
            "Silver",
            "Szepesv\u00e1ri",
            "Wiewiora"
          ],
          "year": 2009,
          "confidence": "medium"
        }
      },
      {
        "text": "[15] Hamid Maei, Csaba Szepesv \u00b4ari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy\nlearning control with function approximation. In Proceedings of the 27th International Con-\nference on Machine Learning (ICML 2010) , pages 719\u2013726, 2010.",
        "ref_num": "15",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "15",
          "Hamid Maei, Csaba Szepesv \u00b4ari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy\nlearning control with function approximation. In Proceedings of the 27th International Con-\nference on Machine Learning (ICML 2010) , pages 719\u2013726, 2010."
        ],
        "doi": null,
        "in_text_mentions": [
          "arning a control policy with linear function approximation\nusing a restricted variant of Q-learning **[15]**. However, these methods have not yet been extended to\nnonlinear control.\nPerhaps the most simil",
          "n Approxi-\nmation. In Advances in Neural Information Processing Systems 22 , pages 1204\u20131212, 2009.\n**[15]** Hamid Maei, Csaba Szepesv \u00b4ari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy\nlea"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Shiitake",
          "authors": [
            "Wasser"
          ],
          "year": 2010,
          "confidence": "medium"
        }
      },
      {
        "text": "[16] V olodymyr Mnih. Machine Learning for Aerial Image Labeling . PhD thesis, University of\nToronto, 2013.",
        "ref_num": "16",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "16",
          "V olodymyr Mnih. Machine Learning for Aerial Image Labeling . PhD thesis, University of\nToronto, 2013."
        ],
        "doi": null,
        "in_text_mentions": [
          "to extract high-level features from raw sen-\nsory data, leading to breakthroughs in computer vision **[11, 22, 16]** and speech recognition [6, 7].\nThese methods utilise a range of neural network architec",
          "dings of the 27th International Con-\nference on Machine Learning (ICML 2010) , pages 719\u2013726, 2010.\n**[16]** V olodymyr Mnih. Machine Learning for Aerial Image Labeling . PhD thesis, University of\nToronto"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Interpretability of machine learning models for medical image analysis",
          "authors": [
            "",
            "Konate"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "[17] Andrew Moore and Chris Atkeson. Prioritized sweeping: Reinforcement learning with less\ndata and less real time. Machine Learning , 13:103\u2013130, 1993.",
        "ref_num": "17",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "17",
          "Andrew Moore and Chris Atkeson. Prioritized sweeping: Reinforcement learning with less\ndata and less real time. Machine Learning , 13:103\u2013130, 1993."
        ],
        "doi": null,
        "in_text_mentions": [
          "ategy might\nemphasize transitions from which we can learn the most, similar to prioritized sweeping **[17]**.\n4.1 Preprocessing and Model Architecture\nWorking directly with raw Atari frames, which are 210",
          "lodymyr Mnih. Machine Learning for Aerial Image Labeling . PhD thesis, University of\nToronto, 2013.\n**[17]** Andrew Moore and Chris Atkeson. Prioritized sweeping: Reinforcement learning with less\ndata and"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Prioritized sweeping: Reinforcement learning with less data and less time",
          "authors": [
            "Moore",
            "Atkeson"
          ],
          "year": 1993,
          "confidence": "medium"
        }
      },
      {
        "text": "[18] Vinod Nair and Geoffrey E Hinton. Recti\ufb01ed linear units improve restricted boltzmann ma-\nchines. In Proceedings of the 27th International Conference on Machine Learning (ICML\n2010) , pages 807\u2013814, 2010.",
        "ref_num": "18",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "18",
          "Vinod Nair and Geoffrey E Hinton. Recti\ufb01ed linear units improve restricted boltzmann ma-\nchines. In Proceedings of the 27th International Conference on Machine Learning (ICML\n2010) , pages 807\u2013814, 2010."
        ],
        "doi": null,
        "in_text_mentions": [
          "layer convolves 16 8\u00028\n\ufb01lters with stride 4with the input image and applies a recti\ufb01er nonlinearity **[10, 18]**. The second\nhidden layer convolves 32 4\u00024\ufb01lters with stride 2, again followed by a recti\ufb01er",
          "ing: Reinforcement learning with less\ndata and less real time. Machine Learning , 13:103\u2013130, 1993.\n**[18]** Vinod Nair and Geoffrey E Hinton. Recti\ufb01ed linear units improve restricted boltzmann ma-\nchines"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Abk\u00fcrzungsverzeichnis",
          "authors": [],
          "year": 2010,
          "confidence": "medium"
        }
      },
      {
        "text": "[19] Jordan B. Pollack and Alan D. Blair. Why did td-gammon work. In Advances in Neural\nInformation Processing Systems 9 , pages 10\u201316, 1996.",
        "ref_num": "19",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "19",
          "Jordan B. Pollack and Alan D. Blair. Why did td-gammon work. In Advances in Neural\nInformation Processing Systems 9 , pages 10\u201316, 1996."
        ],
        "doi": null,
        "in_text_mentions": [
          "\nthe dice rolls helps explore the state space and also makes the value function particularly smooth\n**[19]**.\nFurthermore, it was shown that combining model-free reinforcement learning algorithms such as ",
          "eedings of the 27th International Conference on Machine Learning (ICML\n2010) , pages 807\u2013814, 2010.\n**[19]** Jordan B. Pollack and Alan D. Blair. Why did td-gammon work. In Advances in Neural\nInformation "
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "TD-Gammon",
          "authors": [],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "[20] Martin Riedmiller. Neural \ufb01tted q iteration\u2013\ufb01rst experiences with a data ef\ufb01cient neural re-\ninforcement learning method. In Machine Learning: ECML 2005 , pages 317\u2013328. Springer,\n2005.",
        "ref_num": "20",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "20",
          "Martin Riedmiller. Neural \ufb01tted q iteration\u2013\ufb01rst experiences with a data ef\ufb01cient neural re-\ninforcement learning method. In Machine Learning: ECML 2005 , pages 317\u2013328. Springer,\n2005."
        ],
        "doi": null,
        "in_text_mentions": [
          "r control.\nPerhaps the most similar prior work to our own approach is neural \ufb01tted Q-learning (NFQ) **[20]**.\nNFQ optimises the sequence of loss functions in Equation 2, using the RPROP algorithm to updat",
          "e history and the action have been used as inputs\nto the neural network by some previous approaches **[20, 12]**. The main drawback of this type\nof architecture is that a separate forward pass is required",
          "Why did td-gammon work. In Advances in Neural\nInformation Processing Systems 9 , pages 10\u201316, 1996.\n**[20]** Martin Riedmiller. Neural \ufb01tted q iteration\u2013\ufb01rst experiences with a data ef\ufb01cient neural re-\nin"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Breaker pages",
          "authors": [],
          "year": 2005,
          "confidence": "medium"
        }
      },
      {
        "text": "[21] Brian Sallans and Geoffrey E. Hinton. Reinforcement learning with factored states and actions.\nJournal of Machine Learning Research , 5:1063\u20131088, 2004.",
        "ref_num": "21",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "21",
          "Brian Sallans and Geoffrey E. Hinton. Reinforcement learning with factored states and actions.\nJournal of Machine Learning Research , 5:1063\u20131088, 2004."
        ],
        "doi": null,
        "in_text_mentions": [
          "mate the environment E; restricted Boltzmann\nmachines have been used to estimate the value function **[21]**; or the policy [9]. In addition, the\ndivergence issues with Q-learning have been partially addr",
          "al re-\ninforcement learning method. In Machine Learning: ECML 2005 , pages 317\u2013328. Springer,\n2005.\n**[21]** Brian Sallans and Geoffrey E. Hinton. Reinforcement learning with factored states and actions.\n"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Factored conditional restricted Boltzmann Machines for modeling motion style",
          "authors": [
            "Taylor",
            "Hinton"
          ],
          "year": 2009,
          "confidence": "medium"
        }
      },
      {
        "text": "[22] Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and Yann LeCun. Pedestrian de-\ntection with unsupervised multi-stage feature learning. In Proc. International Conference on\nComputer Vision and Pattern Recognition (CVPR 2013) . IEEE, 2013.",
        "ref_num": "22",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "22",
          "Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and Yann LeCun. Pedestrian de-\ntection with unsupervised multi-stage feature learning. In Proc. International Conference on\nComputer Vision and Pattern Recognition (CVPR 2013) . IEEE, 2013."
        ],
        "doi": null,
        "in_text_mentions": [
          "to extract high-level features from raw sen-\nsory data, leading to breakthroughs in computer vision **[11, 22, 16]** and speech recognition [6, 7].\nThese methods utilise a range of neural network architec",
          "earning with factored states and actions.\nJournal of Machine Learning Research , 5:1063\u20131088, 2004.\n**[22]** Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and Yann LeCun. Pedestrian de-\ntection wi"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "IEEE-NANO 2013 conference program",
          "authors": [],
          "year": 2013,
          "confidence": "medium"
        }
      },
      {
        "text": "[23] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction . MIT Press,\n1998.",
        "ref_num": "23",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "23",
          "Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction . MIT Press,\n1998."
        ],
        "doi": null,
        "in_text_mentions": [
          "0)js;a]. Such value iteration algorithms converge to the optimal action-\nvalue function, Qi!Q\u0003asi!1 **[23]**. In practice, this basic approach is totally impractical,\nbecause the action-value function is ",
          ", is presented in Algorithm 1.\nThis approach has several advantages over standard online Q-learning **[23]**. First, each step of\nexperience is potentially used in many weight updates, which allows for gr",
          "Proc. International Conference on\nComputer Vision and Pattern Recognition (CVPR 2013) . IEEE, 2013.\n**[23]** Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction . MIT Press,\n1998.\n[24"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Reinforcement Learning",
          "authors": [
            "Barto"
          ],
          "year": 1998,
          "confidence": "medium"
        }
      },
      {
        "text": "[24] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM ,\n38(3):58\u201368, 1995.",
        "ref_num": "24",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "24",
          "Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM ,\n38(3):58\u201368, 1995."
        ],
        "doi": null,
        "in_text_mentions": [
          " learnt entirely by reinforcement learning and self-play, and achieved a super-\nhuman level of play **[24]**. TD-gammon used a model-free reinforcement learning algorithm similar\nto Q-learning, and approx",
          "3.\n[23] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction . MIT Press,\n1998.\n**[24]** Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM ,\n38(3):5"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Temporal difference learning and TD-Gammon",
          "authors": [
            "Tesauro"
          ],
          "year": 1995,
          "confidence": "medium"
        }
      },
      {
        "text": "[25] John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with\nfunction approximation. Automatic Control, IEEE Transactions on , 42(5):674\u2013690, 1997.",
        "ref_num": "25",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "25",
          "John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with\nfunction approximation. Automatic Control, IEEE Transactions on , 42(5):674\u2013690, 1997."
        ],
        "doi": null,
        "in_text_mentions": [
          "l-free reinforcement learning algorithms such as Q-\nlearning with non-linear function approximators **[25]**, or indeed with off-policy learning [1] could\ncause the Q-network to diverge. Subsequently, the",
          "einforcement learning fo-\ncused on linear function approximators with better convergence guarantees **[25]**.\n1In fact TD-Gammon approximated the state value function V(s)rather than the action-value func",
          " arise and the parameters could get stuck in a poor local minimum, or\neven diverge catastrophically **[25]**. By using experience replay the behavior distribution is averaged\nover many of its previous sta",
          "Tesauro. Temporal difference learning and td-gammon. Communications of the ACM ,\n38(3):58\u201368, 1995.\n**[25]** John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with\nfuncti"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Average cost temporal-difference learning",
          "authors": [
            "Tsitsiklis",
            "Van Roy"
          ],
          "year": 1999,
          "confidence": "medium"
        }
      },
      {
        "text": "[26] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning , 8(3-4):279\u2013292,\n1992.\n9",
        "ref_num": "26",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "26",
          "Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning , 8(3-4):279\u2013292,\n1992.\n9"
        ],
        "doi": null,
        "in_text_mentions": [
          " raw video data in complex RL environments. The network is\ntrained with a variant of the Q-learning **[26]** algorithm, with stochastic gradient descent to update\nthe weights. To alleviate the problems of",
          "ur distribution\n\u001aand the emulatorErespectively, then we arrive at the familiar Q-learning algorithm **[26]**.\nNote that this algorithm is model-free : it solves the reinforcement learning task directly us",
          "earning with\nfunction approximation. Automatic Control, IEEE Transactions on , 42(5):674\u2013690, 1997.\n**[26]** Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning , 8(3-4):279\u2013292,\n1992.\n9"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Unknown",
          "authors": [
            "Watkins",
            "Dayan"
          ],
          "year": 1992,
          "confidence": "medium"
        }
      }
    ],
    "in_text_citations": [
      {
        "type": "numbered",
        "ref_num": "11",
        "context": "to extract high-level features from raw sen-\nsory data, leading to breakthroughs in computer vision **[11, 22, 16]** and speech recognition [6, 7].\nThese methods utilise a range of neural network architec"
      },
      {
        "type": "numbered",
        "ref_num": "22",
        "context": "to extract high-level features from raw sen-\nsory data, leading to breakthroughs in computer vision **[11, 22, 16]** and speech recognition [6, 7].\nThese methods utilise a range of neural network architec"
      },
      {
        "type": "numbered",
        "ref_num": "16",
        "context": "to extract high-level features from raw sen-\nsory data, leading to breakthroughs in computer vision **[11, 22, 16]** and speech recognition [6, 7].\nThese methods utilise a range of neural network architec"
      },
      {
        "type": "numbered",
        "ref_num": "6",
        "context": "raw sen-\nsory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition **[6, 7]**.\nThese methods utilise a range of neural network architectures, including convolutional netwo"
      },
      {
        "type": "numbered",
        "ref_num": "7",
        "context": "raw sen-\nsory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition **[6, 7]**.\nThese methods utilise a range of neural network architectures, including convolutional netwo"
      },
      {
        "type": "numbered",
        "ref_num": "26",
        "context": " raw video data in complex RL environments. The network is\ntrained with a variant of the Q-learning **[26]** algorithm, with stochastic gradient descent to update\nthe weights. To alleviate the problems of"
      },
      {
        "type": "numbered",
        "ref_num": "13",
        "context": "Left-to-right ) Pong, Breakout, Space Invaders,\nSeaquest, Beam Rider\nan experience replay mechanism **[13]** which randomly samples previous transitions, and thereby\nsmooths the training distribution over"
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": " our approach to a range of Atari 2600 games implemented in The Arcade Learning Envi-\nronment (ALE) **[3]**. Atari 2600 is a challenging RL testbed that presents agents with a high dimen-\nsional visual in"
      },
      {
        "type": "numbered",
        "ref_num": "23",
        "context": "0)js;a]. Such value iteration algorithms converge to the optimal action-\nvalue function, Qi!Q\u0003asi!1 **[23]**. In practice, this basic approach is totally impractical,\nbecause the action-value function is "
      },
      {
        "type": "numbered",
        "ref_num": "26",
        "context": "ur distribution\n\u001aand the emulatorErespectively, then we arrive at the familiar Q-learning algorithm **[26]**.\nNote that this algorithm is model-free : it solves the reinforcement learning task directly us"
      },
      {
        "type": "numbered",
        "ref_num": "24",
        "context": " learnt entirely by reinforcement learning and self-play, and achieved a super-\nhuman level of play **[24]**. TD-gammon used a model-free reinforcement learning algorithm similar\nto Q-learning, and approx"
      },
      {
        "type": "numbered",
        "ref_num": "19",
        "context": "\nthe dice rolls helps explore the state space and also makes the value function particularly smooth\n**[19]**.\nFurthermore, it was shown that combining model-free reinforcement learning algorithms such as "
      },
      {
        "type": "numbered",
        "ref_num": "25",
        "context": "l-free reinforcement learning algorithms such as Q-\nlearning with non-linear function approximators **[25]**, or indeed with off-policy learning [1] could\ncause the Q-network to diverge. Subsequently, the"
      },
      {
        "type": "numbered",
        "ref_num": "1",
        "context": "such as Q-\nlearning with non-linear function approximators [25], or indeed with off-policy learning **[1]** could\ncause the Q-network to diverge. Subsequently, the majority of work in reinforcement learni"
      },
      {
        "type": "numbered",
        "ref_num": "25",
        "context": "einforcement learning fo-\ncused on linear function approximators with better convergence guarantees **[25]**.\n1In fact TD-Gammon approximated the state value function V(s)rather than the action-value func"
      },
      {
        "type": "numbered",
        "ref_num": "21",
        "context": "mate the environment E; restricted Boltzmann\nmachines have been used to estimate the value function **[21]**; or the policy [9]. In addition, the\ndivergence issues with Q-learning have been partially addr"
      },
      {
        "type": "numbered",
        "ref_num": "9",
        "context": " E; restricted Boltzmann\nmachines have been used to estimate the value function [21]; or the policy **[9]**. In addition, the\ndivergence issues with Q-learning have been partially addressed by gradient te"
      },
      {
        "type": "numbered",
        "ref_num": "14",
        "context": "methods are proven to converge when evaluating a \ufb01xed policy with a nonlinear\nfunction approximator **[14]**; or when learning a control policy with linear function approximation\nusing a restricted varian"
      },
      {
        "type": "numbered",
        "ref_num": "15",
        "context": "arning a control policy with linear function approximation\nusing a restricted variant of Q-learning **[15]**. However, these methods have not yet been extended to\nnonlinear control.\nPerhaps the most simil"
      },
      {
        "type": "numbered",
        "ref_num": "20",
        "context": "r control.\nPerhaps the most similar prior work to our own approach is neural \ufb01tted Q-learning (NFQ) **[20]**.\nNFQ optimises the sequence of loss functions in Equation 2, using the RPROP algorithm to updat"
      },
      {
        "type": "numbered",
        "ref_num": "12",
        "context": "to learn a low dimensional representation of the task, and then applying NFQ to this\nrepresentation **[12]**. In contrast our approach applies reinforcement learning end-to-end, directly\nfrom the visual i"
      },
      {
        "type": "numbered",
        "ref_num": "13",
        "context": "es. Q-learning has also previously been combined with experience replay and a simple\nneural network **[13]**, but again starting with a low-dimensional state rather than raw visual inputs.\nThe use of the "
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "l inputs.\nThe use of the Atari 2600 emulator as a reinforcement learning platform was introduced by **[3]**, who\napplied standard reinforcement learning algorithms with linear function approximation and g"
      },
      {
        "type": "numbered",
        "ref_num": "2",
        "context": "tures, and\nusing tug-of-war hashing to randomly project the features into a lower-dimensional space **[2]**. The\nHyperNEAT evolutionary architecture [8] has also been applied to the Atari platform, where "
      },
      {
        "type": "numbered",
        "ref_num": "8",
        "context": "ly project the features into a lower-dimensional space [2]. The\nHyperNEAT evolutionary architecture **[8]** has also been applied to the Atari platform, where it was\nused to evolve (separately, for each d"
      },
      {
        "type": "numbered",
        "ref_num": "11",
        "context": "eep neural networks, it is often possible to learn better representations than\nhandcrafted features **[11]**. These successes motivate our approach to reinforcement learning. Our\ngoal is to connect a rein"
      },
      {
        "type": "numbered",
        "ref_num": "13",
        "context": "ast to TD-Gammon and similar online approaches, we utilize a technique known as expe-\nrience replay **[13]** where we store the agent\u2019s experiences at each time-step, et= (st;at;rt;st+1)\nin a data-setD=e1"
      },
      {
        "type": "numbered",
        "ref_num": "23",
        "context": ", is presented in Algorithm 1.\nThis approach has several advantages over standard online Q-learning **[23]**. First, each step of\nexperience is potentially used in many weight updates, which allows for gr"
      },
      {
        "type": "numbered",
        "ref_num": "25",
        "context": " arise and the parameters could get stuck in a poor local minimum, or\neven diverge catastrophically **[25]**. By using experience replay the behavior distribution is averaged\nover many of its previous sta"
      },
      {
        "type": "numbered",
        "ref_num": "17",
        "context": "ategy might\nemphasize transitions from which we can learn the most, similar to prioritized sweeping **[17]**.\n4.1 Preprocessing and Model Architecture\nWorking directly with raw Atari frames, which are 210"
      },
      {
        "type": "numbered",
        "ref_num": "11",
        "context": " \ufb01nal cropping\nstage is only required because we use the GPU implementation of 2D convolutions from **[11]**, which\nexpects square inputs. For the experiments in this paper, the function \u001efrom algorithm 1"
      },
      {
        "type": "numbered",
        "ref_num": "20",
        "context": "e history and the action have been used as inputs\nto the neural network by some previous approaches **[20, 12]**. The main drawback of this type\nof architecture is that a separate forward pass is required"
      },
      {
        "type": "numbered",
        "ref_num": "12",
        "context": "e history and the action have been used as inputs\nto the neural network by some previous approaches **[20, 12]**. The main drawback of this type\nof architecture is that a separate forward pass is required"
      },
      {
        "type": "numbered",
        "ref_num": "10",
        "context": "layer convolves 16 8\u00028\n\ufb01lters with stride 4with the input image and applies a recti\ufb01er nonlinearity **[10, 18]**. The second\nhidden layer convolves 32 4\u00024\ufb01lters with stride 2, again followed by a recti\ufb01er"
      },
      {
        "type": "numbered",
        "ref_num": "18",
        "context": "layer convolves 16 8\u00028\n\ufb01lters with stride 4with the input image and applies a recti\ufb01er nonlinearity **[10, 18]**. The second\nhidden layer convolves 32 4\u00024\ufb01lters with stride 2, again followed by a recti\ufb01er"
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "llowing previous approaches to playing Atari games, we also use a simple frame-skipping tech-\nnique **[3]**. More precisely, the agent sees and selects actions on every kthframe instead of every\nframe, an"
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "ogress of an agent during training can be challenging. Since our evaluation metric, as suggested\nby **[3]**, is the total reward the agent collects in an episode or game averaged over a number of\ngames, w"
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "\n5.3 Main Evaluation\nWe compare our results with the best performing methods from the RL literature **[3, 4]**. The method\nlabeled Sarsa used the Sarsa algorithm to learn linear policies on several differ"
      },
      {
        "type": "numbered",
        "ref_num": "4",
        "context": "\n5.3 Main Evaluation\nWe compare our results with the best performing methods from the RL literature **[3, 4]**. The method\nlabeled Sarsa used the Sarsa algorithm to learn linear policies on several differ"
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "ets hand-\nengineered for the Atari task and we report the score for the best performing feature set **[3]**. Con-\ntingency used the same basic approach as Sarsa but augmented the feature sets with a learn"
      },
      {
        "type": "numbered",
        "ref_num": "4",
        "context": "re sets with a learned\nrepresentation of the parts of the screen that are under the agent\u2019s control **[4]**. Note that both of these\nmethods incorporate signi\ufb01cant prior knowledge about the visual problem"
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "ng each game. Note that our reported human scores are much higher\nthan the ones in Bellemare et al. **[3]**. For the learned methods, we follow the evaluation strategy used\nin Bellemare et al. [3, 5] and "
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "are et al. [3]. For the learned methods, we follow the evaluation strategy used\nin Bellemare et al. **[3, 5]** and report the average score obtained by running an \u000f-greedy policy with\n\u000f= 0:05for a \ufb01xed nu"
      },
      {
        "type": "numbered",
        "ref_num": "5",
        "context": "are et al. [3]. For the learned methods, we follow the evaluation strategy used\nin Bellemare et al. **[3, 5]** and report the average score obtained by running an \u000f-greedy policy with\n\u000f= 0:05for a \ufb01xed nu"
      },
      {
        "type": "numbered",
        "ref_num": "8",
        "context": "edge about the inputs.\nWe also include a comparison to the evolutionary policy search approach from **[8]** in the last three\nrows of table 1. We report two sets of results for this method. The HNeat Best"
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "7\nB. Rider Breakout Enduro Pong Q*bert Seaquest S. Invaders\nRandom 354 1:2 0\u000020:4 157 110 179\nSarsa **[3]** 996 5:2 129\u000019 614 665 271\nContingency [4] 1743 6 159\u000017 960 723 268\nDQN 4092 168 470 20 1952 17"
      },
      {
        "type": "numbered",
        "ref_num": "4",
        "context": "uest S. Invaders\nRandom 354 1:2 0\u000020:4 157 110 179\nSarsa [3] 996 5:2 129\u000019 614 665 271\nContingency **[4]** 1743 6 159\u000017 960 723 268\nDQN 4092 168 470 20 1952 1705 581\nHuman 7456 31 368\u00003 18900 28010 3690"
      },
      {
        "type": "numbered",
        "ref_num": "8",
        "context": "59\u000017 960 723 268\nDQN 4092 168 470 20 1952 1705 581\nHuman 7456 31 368\u00003 18900 28010 3690\nHNeat Best **[8]** 3616 52 106 19 1800 920 1720\nHNeat Pixel [8] 1332 4 91\u000016 1325 800 1145\nDQN Best 5184 225 661 21"
      },
      {
        "type": "numbered",
        "ref_num": "8",
        "context": "05 581\nHuman 7456 31 368\u00003 18900 28010 3690\nHNeat Best [8] 3616 52 106 19 1800 920 1720\nHNeat Pixel **[8]** 1332 4 91\u000016 1325 800 1145\nDQN Best 5184 225 661 21 4500 1740 1075\nTable 1: The upper table comp"
      },
      {
        "type": "numbered",
        "ref_num": "1",
        "context": "seven games it was tested on, with no adjustment of the\narchitecture or hyperparameters.\nReferences\n**[1]** Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In\nProcee"
      },
      {
        "type": "numbered",
        "ref_num": "2",
        "context": "12th International Conference on Machine Learning (ICML 1995) , pages\n30\u201337. Morgan Kaufmann, 1995.\n**[2]** Marc Bellemare, Joel Veness, and Michael Bowling. Sketch-based linear value function ap-\nproxima"
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "n ap-\nproximation. In Advances in Neural Information Processing Systems 25 , pages 2222\u20132230,\n2012.\n**[3]** Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironmen"
      },
      {
        "type": "numbered",
        "ref_num": "4",
        "context": "luation platform for general agents. Journal of Arti\ufb01cial Intelligence\nResearch , 47:253\u2013279, 2013.\n**[4]** Marc G Bellemare, Joel Veness, and Michael Bowling. Investigating contingency awareness\nusing at"
      },
      {
        "type": "numbered",
        "ref_num": "5",
        "context": "s, and Michael Bowling. Investigating contingency awareness\nusing atari 2600 games. In AAAI , 2012.\n**[5]** Marc G. Bellemare, Joel Veness, and Michael Bowling. Bayesian learning of recursively fac-\ntored"
      },
      {
        "type": "numbered",
        "ref_num": "6",
        "context": "f the Thirtieth International Conference on Machine\nLearning (ICML 2013) , pages 1211\u20131219, 2013.\n8\n**[6]** George E. Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep\nneural netw"
      },
      {
        "type": "numbered",
        "ref_num": "7",
        "context": "ition. Audio, Speech, and Language Pro-\ncessing, IEEE Transactions on , 20(1):30 \u201342, January 2012.\n**[7]** Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep\nrecurren"
      },
      {
        "type": "numbered",
        "ref_num": "8",
        "context": "Geoffrey E. Hinton. Speech recognition with deep\nrecurrent neural networks. In Proc. ICASSP , 2013.\n**[8]** Matthew Hausknecht, Risto Miikkulainen, and Peter Stone. A neuro-evolution approach to\ngeneral a"
      },
      {
        "type": "numbered",
        "ref_num": "9",
        "context": "isto Miikkulainen, and Peter Stone. A neuro-evolution approach to\ngeneral atari game playing. 2013.\n**[9]** Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with\nenergy-b"
      },
      {
        "type": "numbered",
        "ref_num": "10",
        "context": "earning with\nenergy-based policies. In European Workshop on Reinforcement Learning , page 43, 2012.\n**[10]** Kevin Jarrett, Koray Kavukcuoglu, MarcAurelio Ranzato, and Yann LeCun. What is the best\nmulti-s"
      },
      {
        "type": "numbered",
        "ref_num": "11",
        "context": " Conference on Com-\nputer Vision and Pattern Recognition (CVPR 2009) , pages 2146\u20132153. IEEE, 2009.\n**[11]** Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con-\nvoluti"
      },
      {
        "type": "numbered",
        "ref_num": "12",
        "context": "l neural networks. In Advances in Neural Information Processing Systems 25 , pages\n1106\u20131114, 2012.\n**[12]** Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement\nlearning"
      },
      {
        "type": "numbered",
        "ref_num": "13",
        "context": "ng. In Neural Networks (IJCNN), The 2010 International Joint Conference on , pages\n1\u20138. IEEE, 2010.\n**[13]** Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC\nDo"
      },
      {
        "type": "numbered",
        "ref_num": "14",
        "context": "in. Reinforcement learning for robots using neural networks. Technical report, DTIC\nDocument, 1993.\n**[14]** Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Rich\nSutton. C"
      },
      {
        "type": "numbered",
        "ref_num": "15",
        "context": "n Approxi-\nmation. In Advances in Neural Information Processing Systems 22 , pages 1204\u20131212, 2009.\n**[15]** Hamid Maei, Csaba Szepesv \u00b4ari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy\nlea"
      },
      {
        "type": "numbered",
        "ref_num": "16",
        "context": "dings of the 27th International Con-\nference on Machine Learning (ICML 2010) , pages 719\u2013726, 2010.\n**[16]** V olodymyr Mnih. Machine Learning for Aerial Image Labeling . PhD thesis, University of\nToronto"
      },
      {
        "type": "numbered",
        "ref_num": "17",
        "context": "lodymyr Mnih. Machine Learning for Aerial Image Labeling . PhD thesis, University of\nToronto, 2013.\n**[17]** Andrew Moore and Chris Atkeson. Prioritized sweeping: Reinforcement learning with less\ndata and"
      },
      {
        "type": "numbered",
        "ref_num": "18",
        "context": "ing: Reinforcement learning with less\ndata and less real time. Machine Learning , 13:103\u2013130, 1993.\n**[18]** Vinod Nair and Geoffrey E Hinton. Recti\ufb01ed linear units improve restricted boltzmann ma-\nchines"
      },
      {
        "type": "numbered",
        "ref_num": "19",
        "context": "eedings of the 27th International Conference on Machine Learning (ICML\n2010) , pages 807\u2013814, 2010.\n**[19]** Jordan B. Pollack and Alan D. Blair. Why did td-gammon work. In Advances in Neural\nInformation "
      },
      {
        "type": "numbered",
        "ref_num": "20",
        "context": "Why did td-gammon work. In Advances in Neural\nInformation Processing Systems 9 , pages 10\u201316, 1996.\n**[20]** Martin Riedmiller. Neural \ufb01tted q iteration\u2013\ufb01rst experiences with a data ef\ufb01cient neural re-\nin"
      },
      {
        "type": "numbered",
        "ref_num": "21",
        "context": "al re-\ninforcement learning method. In Machine Learning: ECML 2005 , pages 317\u2013328. Springer,\n2005.\n**[21]** Brian Sallans and Geoffrey E. Hinton. Reinforcement learning with factored states and actions.\n"
      },
      {
        "type": "numbered",
        "ref_num": "22",
        "context": "earning with factored states and actions.\nJournal of Machine Learning Research , 5:1063\u20131088, 2004.\n**[22]** Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and Yann LeCun. Pedestrian de-\ntection wi"
      },
      {
        "type": "numbered",
        "ref_num": "23",
        "context": "Proc. International Conference on\nComputer Vision and Pattern Recognition (CVPR 2013) . IEEE, 2013.\n**[23]** Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction . MIT Press,\n1998.\n[24"
      },
      {
        "type": "numbered",
        "ref_num": "24",
        "context": "3.\n[23] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction . MIT Press,\n1998.\n**[24]** Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM ,\n38(3):5"
      },
      {
        "type": "numbered",
        "ref_num": "25",
        "context": "Tesauro. Temporal difference learning and td-gammon. Communications of the ACM ,\n38(3):58\u201368, 1995.\n**[25]** John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with\nfuncti"
      },
      {
        "type": "numbered",
        "ref_num": "26",
        "context": "earning with\nfunction approximation. Automatic Control, IEEE Transactions on , 42(5):674\u2013690, 1997.\n**[26]** Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning , 8(3-4):279\u2013292,\n1992.\n9"
      }
    ]
  },
  {
    "filename": "1312.6055v3.pdf",
    "total_citations": 28,
    "valid_format_count": 28,
    "existing_count": 28,
    "invalid_format_count": 0,
    "nonexistent_count": 0,
    "citations": [
      {
        "text": "[1] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical\nStatistics , 22:400\u2013407, 1951.",
        "ref_num": "1",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "1",
          "H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical\nStatistics , 22:400\u2013407, 1951."
        ],
        "doi": null,
        "in_text_mentions": [
          "pen-source, extensible, and easy to apply to new algorithms.\n1 Introduction\nStochastic optimization **[1]** is among the most widely used components in large-scale machine learn-\ning, thanks to its linear",
          "owledgements\nWe thank the anonymous ICLR reviewers for their many constructive comments.\nReferences\n**[1]** H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical\nStatistics , "
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "A Stochastic Approximation Method",
          "authors": [
            "Robbins",
            "Monro"
          ],
          "year": 1951,
          "confidence": "medium"
        }
      },
      {
        "text": "[2] L \u00b4eon Bottou. Online Algorithms and Stochastic Approximations. In David Saad, editor, Online\nLearning and Neural Networks . Cambridge University Press, Cambridge, UK, 1998.",
        "ref_num": "2",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "2",
          "L \u00b4eon Bottou. Online Algorithms and Stochastic Approximations. In David Saad, editor, Online\nLearning and Neural Networks . Cambridge University Press, Cambridge, UK, 1998."
        ],
        "doi": null,
        "in_text_mentions": [
          "learn-\ning, thanks to its linear complexity, ef\ufb01cient data usage, and often superior generalization **[2, 3, 4]**.\nIn this context, numerous variants of stochastic gradient descent have been proposed, in ",
          " S. Monro. A stochastic approximation method. Annals of Mathematical\nStatistics , 22:400\u2013407, 1951.\n**[2]** L \u00b4eon Bottou. Online Algorithms and Stochastic Approximations. In David Saad, editor, Online\nLe"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Gaussian Approximations to the Algorithms",
          "authors": [
            "Benveniste",
            "M\u00e9tivier",
            "Priouret"
          ],
          "year": 1990,
          "confidence": "medium"
        }
      },
      {
        "text": "[3] L \u00b4eon Bottou and Yann LeCun. Large Scale Online Learning. In Sebastian Thrun, Lawrence\nSaul, and Bernhard Sch \u00a8olkopf, editors, Advances in Neural Information Processing Systems\n16. MIT Press, Cambridge, MA, 2004.",
        "ref_num": "3",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "3",
          "L \u00b4eon Bottou and Yann LeCun. Large Scale Online Learning. In Sebastian Thrun, Lawrence\nSaul, and Bernhard Sch \u00a8olkopf, editors, Advances in Neural Information Processing Systems\n16. MIT Press, Cambridge, MA, 2004."
        ],
        "doi": null,
        "in_text_mentions": [
          "learn-\ning, thanks to its linear complexity, ef\ufb01cient data usage, and often superior generalization **[2, 3, 4]**.\nIn this context, numerous variants of stochastic gradient descent have been proposed, in ",
          "aad, editor, Online\nLearning and Neural Networks . Cambridge University Press, Cambridge, UK, 1998.\n**[3]** L \u00b4eon Bottou and Yann LeCun. Large Scale Online Learning. In Sebastian Thrun, Lawrence\nSaul, an"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Scaling Learning Algorithms toward AI",
          "authors": [
            "Bengio",
            "LeCun"
          ],
          "year": 2007,
          "confidence": "medium"
        }
      },
      {
        "text": "[4] L \u00b4eon Bottou and Olivier Bousquet. The Tradeoffs of Large Scale Learning. In J.C. Platt,\nD. Koller, Y . Singer, and S. Roweis, editors, Advances in Neural Information Processing Sys-\ntems, volume 20, pages 161\u2013168. NIPS Foundation (http://books.nips.cc), 2008.",
        "ref_num": "4",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "4",
          "L \u00b4eon Bottou and Olivier Bousquet. The Tradeoffs of Large Scale Learning. In J.C. Platt,\nD. Koller, Y . Singer, and S. Roweis, editors, Advances in Neural Information Processing Sys-\ntems, volume 20, pages 161\u2013168. NIPS Foundation (http://books.nips.cc), 2008."
        ],
        "doi": null,
        "in_text_mentions": [
          "learn-\ning, thanks to its linear complexity, ef\ufb01cient data usage, and often superior generalization **[2, 3, 4]**.\nIn this context, numerous variants of stochastic gradient descent have been proposed, in ",
          "opf, editors, Advances in Neural Information Processing Systems\n16. MIT Press, Cambridge, MA, 2004.\n**[4]** L \u00b4eon Bottou and Olivier Bousquet. The Tradeoffs of Large Scale Learning. In J.C. Platt,\nD. Kol"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "The Tradeoffs of Large-Scale Learning",
          "authors": [
            "Bottou",
            "Bousquet"
          ],
          "year": 2011,
          "confidence": "medium"
        }
      },
      {
        "text": "[5] A. Benveniste, M. Metivier, and P. Priouret. Adaptive Algorithms and Stochastic Approxima-\ntions . Springer Verlag, Berlin, New York, 1990.",
        "ref_num": "5",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "5",
          "A. Benveniste, M. Metivier, and P. Priouret. Adaptive Algorithms and Stochastic Approxima-\ntions . Springer Verlag, Berlin, New York, 1990."
        ],
        "doi": null,
        "in_text_mentions": [
          "nt descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort **[5, 6, 7, 8, 9]**. These algorithms may\nderive from simplifying assumptions on the optimization landsc",
          "ation Processing Sys-\ntems, volume 20, pages 161\u2013168. NIPS Foundation (http://books.nips.cc), 2008.\n**[5]** A. Benveniste, M. Metivier, and P. Priouret. Adaptive Algorithms and Stochastic Approxima-\ntions"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Adaptive Algorithms and Stochastic Approximations",
          "authors": [
            "Benveniste",
            "M\u00e9tivier",
            "Priouret"
          ],
          "year": 1990,
          "confidence": "medium"
        }
      },
      {
        "text": "[6] N. Le Roux, P.A. Manzagol, and Y . Bengio. Topmoumoute online natural gradient algorithm,\n2008.",
        "ref_num": "6",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "6",
          "N. Le Roux, P.A. Manzagol, and Y . Bengio. Topmoumoute online natural gradient algorithm,\n2008."
        ],
        "doi": null,
        "in_text_mentions": [
          "nt descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort **[5, 6, 7, 8, 9]**. These algorithms may\nderive from simplifying assumptions on the optimization landsc",
          "ret. Adaptive Algorithms and Stochastic Approxima-\ntions . Springer Verlag, Berlin, New York, 1990.\n**[6]** N. Le Roux, P.A. Manzagol, and Y . Bengio. Topmoumoute online natural gradient algorithm,\n2008.\n"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Extracting and composing robust features with denoising autoencoders",
          "authors": [
            "Vincent",
            "Larochelle",
            "Bengio",
            "Manzagol"
          ],
          "year": 2008,
          "confidence": "medium"
        }
      },
      {
        "text": "[7] Antoine Bordes, L \u00b4eon Bottou, and Patrick Gallinari. SGD-QN: Careful Quasi-Newton\nStochastic Gradient Descent. Journal of Machine Learning Research , 10:1737\u20131754, July\n2009.",
        "ref_num": "7",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "7",
          "Antoine Bordes, L \u00b4eon Bottou, and Patrick Gallinari. SGD-QN: Careful Quasi-Newton\nStochastic Gradient Descent. Journal of Machine Learning Research , 10:1737\u20131754, July\n2009."
        ],
        "doi": null,
        "in_text_mentions": [
          "nt descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort **[5, 6, 7, 8, 9]**. These algorithms may\nderive from simplifying assumptions on the optimization landsc",
          "[6] N. Le Roux, P.A. Manzagol, and Y . Bengio. Topmoumoute online natural gradient algorithm,\n2008.\n**[7]** Antoine Bordes, L \u00b4eon Bottou, and Patrick Gallinari. SGD-QN: Careful Quasi-Newton\nStochastic Gr"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "COMPARISON AND COOPERATION OF SEVERAL CLASSIFIERS",
          "authors": [
            "DRIANCOURT",
            "BOTTOU",
            "GALLINARI"
          ],
          "year": 1991,
          "confidence": "medium"
        }
      },
      {
        "text": "[8] Wei Xu. Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient\nDescent. ArXiv-CoRR , abs/1107.2490, 2011.",
        "ref_num": "8",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "8",
          "Wei Xu. Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient\nDescent. ArXiv-CoRR , abs/1107.2490, 2011."
        ],
        "doi": null,
        "in_text_mentions": [
          "nt descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort **[5, 6, 7, 8, 9]**. These algorithms may\nderive from simplifying assumptions on the optimization landsc",
          "Newton\nStochastic Gradient Descent. Journal of Machine Learning Research , 10:1737\u20131754, July\n2009.\n**[8]** Wei Xu. Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient\nDescent."
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Large-Scale Machine Learning with Stochastic Gradient Descent L\u00e9on Bottou",
          "authors": [],
          "year": 2011,
          "confidence": "medium"
        }
      },
      {
        "text": "[9] Tom Schaul, Sixin Zhang, and Yann LeCun. No More Pesky Learning Rates. In International\nConference on Machine Learning (ICML) , 2013.",
        "ref_num": "9",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "9",
          "Tom Schaul, Sixin Zhang, and Yann LeCun. No More Pesky Learning Rates. In International\nConference on Machine Learning (ICML) , 2013."
        ],
        "doi": null,
        "in_text_mentions": [
          "nt descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort **[5, 6, 7, 8, 9]**. These algorithms may\nderive from simplifying assumptions on the optimization landsc",
          "s Large Scale Learning with Averaged Stochastic Gradient\nDescent. ArXiv-CoRR , abs/1107.2490, 2011.\n**[9]** Tom Schaul, Sixin Zhang, and Yann LeCun. No More Pesky Learning Rates. In International\nConferen"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Deep learning &amp; convolutional networks",
          "authors": [
            "LeCun"
          ],
          "year": 2015,
          "confidence": "medium"
        }
      },
      {
        "text": "[10] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online\nLearning and Stochastic Optimization. 2010.",
        "ref_num": "10",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "10",
          "John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online\nLearning and Stochastic Optimization. 2010."
        ],
        "doi": null,
        "in_text_mentions": [
          "6, 7, 8, 9]. These algorithms may\nderive from simplifying assumptions on the optimization landscape **[10]**, but in practice, they tend\nto be used as general-purpose tools, often outside of the space of ",
          "0, SGD with parameter averaging [] with decay term in\n[10\u00004;0:5]and exponent inf1\n2;3\n4;1g, ADAGRAD **[10]** with initial rates \u00110, ADADELTA [23] with\ndecay parameter (1\u0000\r)2[10\u00004;0:5]and regularizer in [1",
          "LeCun. No More Pesky Learning Rates. In International\nConference on Machine Learning (ICML) , 2013.\n**[10]** John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online\nLearning a"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Online Learning with Prior Knowledge",
          "authors": [
            "Hazan",
            "Megiddo"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "[11] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient\nproblem. arXiv preprint arXiv:1211.5063 , 2012.",
        "ref_num": "11",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "11",
          "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient\nproblem. arXiv preprint arXiv:1211.5063 , 2012."
        ],
        "doi": null,
        "in_text_mentions": [
          "ctitioners \ufb01nd it dif\ufb01cult to discern where potential\nweaknesses of new (or old) algorithms may lie **[11]**, and when they are applicable \u2013 an issue that\nis separate from raw performance. This results in",
          "n the loss surface are a common occurrence when training recurrent neural networks,\nas discussed in **[11]**. See the top rows of Figure 1 for some examples of shape prototypes.\n2.2 One-dimensional Concat",
          "d Yoram Singer. Adaptive Subgradient Methods for Online\nLearning and Stochastic Optimization. 2010.\n**[11]** Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient\nproblem."
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Radial Basis Functions for Speech Recognition",
          "authors": [
            "Bengio"
          ],
          "year": 1992,
          "confidence": "medium"
        }
      },
      {
        "text": "[12] X. Glorot and Y . Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetworks. In G. Orr and Muller K., editors, Proceedings of the International Conference on\nArti\ufb01cial Intelligence and Statistics (AISTATS) , pages 249\u2013256. Society for Arti\ufb01cial Intelli-\ngence and Statistics, 2010.",
        "ref_num": "12",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "12",
          "X. Glorot and Y . Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetworks. In G. Orr and Muller K., editors, Proceedings of the International Conference on\nArti\ufb01cial Intelligence and Statistics (AISTATS) , pages 249\u2013256. Society for Arti\ufb01cial Intelli-\ngence and Statistics, 2010."
        ],
        "doi": null,
        "in_text_mentions": [
          "every time that the dataset, loss\nfunction, regularization parameters, or model architecture change **[12]**.\nThe objective of this paper is to establish a collection of benchmarks to evaluate stochastic ",
          "Yoshua Bengio. Understanding the exploding gradient\nproblem. arXiv preprint arXiv:1211.5063 , 2012.\n**[12]** X. Glorot and Y . Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetwo"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Feedforward Neural Networks",
          "authors": [
            "Michelucci"
          ],
          "year": 2018,
          "confidence": "medium"
        }
      },
      {
        "text": "[13] Nikolaus Hansen, Anne Auger, Steffen Finck, Raymond Ros, et al. Real-parameter black-box\noptimization benchmarking 2010: Experimental setup. 2010.",
        "ref_num": "13",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "13",
          "Nikolaus Hansen, Anne Auger, Steffen Finck, Raymond Ros, et al. Real-parameter black-box\noptimization benchmarking 2010: Experimental setup. 2010."
        ],
        "doi": null,
        "in_text_mentions": [
          ". This is a similar approach to the very fruitful one taken by the black-box\noptimization community **[13, 14]**.\nThe core assumption we make is that stochastic optimization algorithms are acting locally ",
          "nd Statistics (AISTATS) , pages 249\u2013256. Society for Arti\ufb01cial Intelli-\ngence and Statistics, 2010.\n**[13]** Nikolaus Hansen, Anne Auger, Steffen Finck, Raymond Ros, et al. Real-parameter black-box\noptimi"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Experimental Setup",
          "authors": [
            "Stojek"
          ],
          "year": 2010,
          "confidence": "medium"
        }
      },
      {
        "text": "[14] Nikolaus Hansen, Anne Auger, Raymond Ros, Steffen Finck, and Petr Po \u02c7s\u00b4\u0131k. Comparing\nresults of 31 algorithms from the black-box optimization benchmarking BBOB-2009. In Pro-\nceedings of the 12th annual conference companion on Genetic and evolutionary computation ,\npages 1689\u20131696. ACM, 2010.",
        "ref_num": "14",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "14",
          "Nikolaus Hansen, Anne Auger, Raymond Ros, Steffen Finck, and Petr Po \u02c7s\u00b4\u0131k. Comparing\nresults of 31 algorithms from the black-box optimization benchmarking BBOB-2009. In Pro-\nceedings of the 12th annual conference companion on Genetic and evolutionary computation ,\npages 1689\u20131696. ACM, 2010."
        ],
        "doi": null,
        "in_text_mentions": [
          ". This is a similar approach to the very fruitful one taken by the black-box\noptimization community **[13, 14]**.\nThe core assumption we make is that stochastic optimization algorithms are acting locally ",
          "mond Ros, et al. Real-parameter black-box\noptimization benchmarking 2010: Experimental setup. 2010.\n**[14]** Nikolaus Hansen, Anne Auger, Raymond Ros, Steffen Finck, and Petr Po \u02c7s\u00b4\u0131k. Comparing\nresults o"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Proceedings of the 12th annual conference companion on Genetic and evolutionary computation",
          "authors": [],
          "year": 2010,
          "confidence": "medium"
        }
      },
      {
        "text": "[15] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. In Advances in Neural Information Processing Systems 25 , pages\n1106\u20131114, 2012.",
        "ref_num": "15",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "15",
          "Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. In Advances in Neural Information Processing Systems 25 , pages\n1106\u20131114, 2012."
        ],
        "doi": null,
        "in_text_mentions": [
          "h as L1\nintroduces non-differentiable bends (as do recti\ufb01ed-linear or maxout units in deep learning **[15, 16]**).\nSteep cliffs in the loss surface are a common occurrence when training recurrent neural n",
          "h annual conference companion on Genetic and evolutionary computation ,\npages 1689\u20131696. ACM, 2010.\n**[15]** Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con-\nvoluti"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "ImageNet classification with deep convolutional neural networks",
          "authors": [
            "Krizhevsky",
            "Sutskever",
            "Hinton"
          ],
          "year": 2017,
          "confidence": "medium"
        }
      },
      {
        "text": "[16] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.\nMaxout networks. arXiv preprint arXiv:1302.4389 , 2013.",
        "ref_num": "16",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "16",
          "Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.\nMaxout networks. arXiv preprint arXiv:1302.4389 , 2013."
        ],
        "doi": null,
        "in_text_mentions": [
          "h as L1\nintroduces non-differentiable bends (as do recti\ufb01ed-linear or maxout units in deep learning **[15, 16]**).\nSteep cliffs in the loss surface are a common occurrence when training recurrent neural n",
          "l neural networks. In Advances in Neural Information Processing Systems 25 , pages\n1106\u20131114, 2012.\n**[16]** Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.\nMaxout n"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Generative adversarial networks",
          "authors": [
            "Goodfellow",
            "Pouget-Abadie",
            "Mirza",
            "Xu",
            "Warde-Farley",
            "Ozair",
            "Courville",
            "Bengio"
          ],
          "year": 2020,
          "confidence": "medium"
        }
      },
      {
        "text": "[17] Y . LeCun, L. Bottou, G. Orr, and K. Muller. Ef\ufb01cient BackProp. In G. Orr and Muller K.,\neditors, Neural Networks: Tricks of the trade . Springer, 1998.",
        "ref_num": "17",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "17",
          "Y . LeCun, L. Bottou, G. Orr, and K. Muller. Ef\ufb01cient BackProp. In G. Orr and Muller K.,\neditors, Neural Networks: Tricks of the trade . Springer, 1998."
        ],
        "doi": null,
        "in_text_mentions": [
          " without substantial overhead. In many learning prob-\nlems, effort is put into proper normalization **[17]**, but that is insuf\ufb01cient to guarantee homogeneous\nscaling, for example throughout all the layer",
          " Mirza, Aaron Courville, and Yoshua Bengio.\nMaxout networks. arXiv preprint arXiv:1302.4389 , 2013.\n**[17]** Y . LeCun, L. Bottou, G. Orr, and K. Muller. Ef\ufb01cient BackProp. In G. Orr and Muller K.,\neditor"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Efficient BackProp",
          "authors": [
            "LeCun",
            "Bottou",
            "Orr",
            "M\u00fcller"
          ],
          "year": 1998,
          "confidence": "medium"
        }
      },
      {
        "text": "[18] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut-\ndinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv\npreprint arXiv:1207.0580 , 2012.",
        "ref_num": "18",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "18",
          "Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut-\ndinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv\npreprint arXiv:1207.0580 , 2012."
        ],
        "doi": null,
        "in_text_mentions": [
          "dependently for each dimension) with a certain\nprobability. This mimics both training with drop-out **[18]**, and scenarios with recti\ufb01ed\nlinear units where a unit will be inactive for some input samples,",
          " BackProp. In G. Orr and Muller K.,\neditors, Neural Networks: Tricks of the trade . Springer, 1998.\n**[18]** Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut-\nd"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "ImageNet classification with deep convolutional neural networks",
          "authors": [
            "Krizhevsky",
            "Sutskever",
            "Hinton"
          ],
          "year": 2017,
          "confidence": "medium"
        }
      },
      {
        "text": "[19] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. IEEE Transactions on\nNeural Networks , 9(5):1054\u20131054, Sep 1998.\n11",
        "ref_num": "19",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "19",
          "R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. IEEE Transactions on\nNeural Networks , 9(5):1054\u20131054, Sep 1998.\n11"
        ],
        "doi": null,
        "in_text_mentions": [
          "otstrapping : i.e.\nit pulls the value of the current state towards the value of its successor state **[19]**. These stochastic\nupdate directions are not proper gradients of any scalar energy \ufb01eld [20], bu",
          "l networks by preventing co-adaptation of feature detectors. arXiv\npreprint arXiv:1207.0580 , 2012.\n**[19]** R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. IEEE Transactions on\nNeura"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Reinforcement Learning: An Introduction",
          "authors": [
            "Sutton",
            "Barto"
          ],
          "year": 1998,
          "confidence": "medium"
        }
      },
      {
        "text": "[20] Etienne Barnard. Temporal-difference methods and Markov models. IEEE Transactions on\nSystems, Man, and Cybernetics , 23(2):357\u2013365, 1993.",
        "ref_num": "20",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "20",
          "Etienne Barnard. Temporal-difference methods and Markov models. IEEE Transactions on\nSystems, Man, and Cybernetics , 23(2):357\u2013365, 1993."
        ],
        "doi": null,
        "in_text_mentions": [
          "r state [19]. These stochastic\nupdate directions are not proper gradients of any scalar energy \ufb01eld **[20]**, but they still form a (more\ngeneral) vector \ufb01eld with non-zero curl, where the objective for t",
          "ment Learning: An Introduction. IEEE Transactions on\nNeural Networks , 9(5):1054\u20131054, Sep 1998.\n11\n**[20]** Etienne Barnard. Temporal-difference methods and Markov models. IEEE Transactions on\nSystems, M"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Temporal-difference methods and Markov models",
          "authors": [
            "Barnard"
          ],
          "year": 1993,
          "confidence": "medium"
        }
      },
      {
        "text": "[21] Richard S. Sutton, Anna Koop, and David Silver. On the role of tracking in stationary environ-\nments. In Proceedings of the Twenty-Fourth International Conference on Machine Learning\n(ICML 2007 , pages 871\u2013878. ACM Press, 2007.",
        "ref_num": "21",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "21",
          "Richard S. Sutton, Anna Koop, and David Silver. On the role of tracking in stationary environ-\nments. In Proceedings of the Twenty-Fourth International Conference on Machine Learning\n(ICML 2007 , pages 871\u2013878. ACM Press, 2007."
        ],
        "doi": null,
        "in_text_mentions": [
          "t of the problem,\nrather than attempting to converge to a global but static solution of the problem **[21]**. In addition,\nreinforcement learning (RL) tasks often involve non-stationary optimization. For ",
          "ethods and Markov models. IEEE Transactions on\nSystems, Man, and Cybernetics , 23(2):357\u2013365, 1993.\n**[21]** Richard S. Sutton, Anna Koop, and David Silver. On the role of tracking in stationary environ-\n"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Workplace Stress1",
          "authors": [
            "LUNDBERG"
          ],
          "year": 2007,
          "confidence": "medium"
        }
      },
      {
        "text": "[22] Yurii Nesterov and Arkadii Semenovich Nemirovskii. Interior-point polynomial algorithms in\nconvex programming , volume 13. SIAM, 1994.",
        "ref_num": "22",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "22",
          "Yurii Nesterov and Arkadii Semenovich Nemirovskii. Interior-point polynomial algorithms in\nconvex programming , volume 13. SIAM, 1994."
        ],
        "doi": null,
        "in_text_mentions": [
          "with decay factor in [10\u00002;1]and initial rates \u00110, SGD with momentum (regular or Nesterov\u2019s\nvariant **[22]**) [0:1;0:999] and initial rates \u00110, SGD with parameter averaging [] with decay term in\n[10\u00004;0:5",
          "ty-Fourth International Conference on Machine Learning\n(ICML 2007 , pages 871\u2013878. ACM Press, 2007.\n**[22]** Yurii Nesterov and Arkadii Semenovich Nemirovskii. Interior-point polynomial algorithms in\nconv"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Interior-Point Polynomial Algorithms in Convex Programming",
          "authors": [
            "Nesterov",
            "Nemirovskii"
          ],
          "year": 1994,
          "confidence": "medium"
        }
      },
      {
        "text": "[23] Matthew D Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv preprint\narXiv:1212.5701 , 2012.",
        "ref_num": "23",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "23",
          "Matthew D Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv preprint\narXiv:1212.5701 , 2012."
        ],
        "doi": null,
        "in_text_mentions": [
          "th decay term in\n[10\u00004;0:5]and exponent inf1\n2;3\n4;1g, ADAGRAD [10] with initial rates \u00110, ADADELTA **[23]** with\ndecay parameter (1\u0000\r)2[10\u00004;0:5]and regularizer in [10\u00006;10\u00002, the incremental delta-bar-\n",
          "ch Nemirovskii. Interior-point polynomial algorithms in\nconvex programming , volume 13. SIAM, 1994.\n**[23]** Matthew D Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv preprint\narXiv:1212.5701 , "
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "The arXiv of the future will not look like the arXiv",
          "authors": [
            "Pepe",
            "Cantiello",
            "Nicholson"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "[24] Richard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-\ndelta. In AAAI , pages 171\u2013176, 1992.",
        "ref_num": "24",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "24",
          "Richard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-\ndelta. In AAAI , pages 171\u2013176, 1992."
        ],
        "doi": null,
        "in_text_mentions": [
          "ter (1\u0000\r)2[10\u00004;0:5]and regularizer in [10\u00006;10\u00002, the incremental delta-bar-\ndelta algorithm (IDBD **[24]**), RPROP [25] with initial stepsizes \u00110, RMSprop [26] with minimal\nlearning rates \u00110, maximal le",
          "atthew D Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv preprint\narXiv:1212.5701 , 2012.\n**[24]** Richard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-\ndelta"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Sparse Incremental Delta-Bar-Delta for System Identification",
          "authors": [
            "Liu",
            "Ye",
            "Li"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "[25] Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation\nlearning: The RPROP algorithm. In Neural Networks, 1993., IEEE International Conference\non, pages 586\u2013591. IEEE, 1993.",
        "ref_num": "25",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "25",
          "Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation\nlearning: The RPROP algorithm. In Neural Networks, 1993., IEEE International Conference\non, pages 586\u2013591. IEEE, 1993."
        ],
        "doi": null,
        "in_text_mentions": [
          "\u00004;0:5]and regularizer in [10\u00006;10\u00002, the incremental delta-bar-\ndelta algorithm (IDBD [24]), RPROP **[25]** with initial stepsizes \u00110, RMSprop [26] with minimal\nlearning rates \u00110, maximal learning rates ",
          "ias by gradient descent: An incremental version of delta-bar-\ndelta. In AAAI , pages 171\u2013176, 1992.\n**[25]** Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation\nlearn"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "A direct adaptive method for faster backpropagation learning: the RPROP algorithm",
          "authors": [
            "Riedmiller",
            "Braun"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "[26] T Tieleman and G Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of\nits recent magnitude. COURSERA: Neural Networks for Machine Learning , 2012.",
        "ref_num": "26",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "26",
          "T Tieleman and G Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of\nits recent magnitude. COURSERA: Neural Networks for Machine Learning , 2012."
        ],
        "doi": null,
        "in_text_mentions": [
          "e incremental delta-bar-\ndelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u00110, RMSprop **[26]** with minimal\nlearning rates \u00110, maximal learning rates in [10;103]and decay parameter \r, as wel",
          " algorithm. In Neural Networks, 1993., IEEE International Conference\non, pages 586\u2013591. IEEE, 1993.\n**[26]** T Tieleman and G Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of\nits r"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Running a Small Flock of Sheep",
          "authors": [
            "Hinton"
          ],
          "year": 2006,
          "confidence": "medium"
        }
      },
      {
        "text": "[27] Tom Schaul and Yann LeCun. Adaptive learning rates and parallelization for stochastic, sparse,\nnon-smooth gradients. In International Conference on Learning Representations , Scottsdale,\nAZ, 2013.",
        "ref_num": "27",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "27",
          "Tom Schaul and Yann LeCun. Adaptive learning rates and parallelization for stochastic, sparse,\nnon-smooth gradients. In International Conference on Learning Representations , Scottsdale,\nAZ, 2013."
        ],
        "doi": null,
        "in_text_mentions": [
          "ere\nLinit=E[L(\u00120)]is the expected loss value at the initial point, similar to the approach taken in **[27]**,\nbut even more condensed. In other words, a normalized value near zero corresponds to no progre",
          "y a running average of\nits recent magnitude. COURSERA: Neural Networks for Machine Learning , 2012.\n**[27]** Tom Schaul and Yann LeCun. Adaptive learning rates and parallelization for stochastic, sparse,\n"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Learning Hierarchies of Sparse Features",
          "authors": [
            "Ranzato",
            "Boureau",
            "Kavukcuoglu",
            "Gregor",
            "LeCun"
          ],
          "year": 2012,
          "confidence": "medium"
        }
      },
      {
        "text": "[28] Yann LeCun and Corinna Cortes. The MNIST dataset of handwritten digits. 1998.\nhttp://yann.lecun.com/exdb/mnist/.\nA Appendix: Framework Software\nAs part of this work a software framework was developed for the computing and managing all the\nresults obtained for all the different con\ufb01gurations of function prototypes and algorithms. The main\ncomponent of the system is a database where all the results are stored and can be easily retrieved\nby querying the database accordingly. The building blocks of this database are the individual exper-\niments, where each experiment is associated to a unit test and an algorithm with \ufb01xed parameters.\nAn instance of an experiment database can either be loaded from the disk, or it can be created on the\n\ufb02y by running the associated experiments as needed. The code below creates a database and runs all\nthe experiments for all the readily available algorithms and default unit tests, and then saves them to\ndisk:\nrequire \u2019experiment\u2019\nlocal db = experimentsDB()\ndb:runExperiments()\ndb:save(\u2019experimentsDB\u2019)\nThis database now can be loaded from the disk, and the user can query it in order to retrieve speci\ufb01c\nexperiments, using \ufb01lters . An example is shown below:\nlocal db = experimentsDB()\ndb:load(\u2019experimentsDB\u2019)\nlocal experiments = db:filter({fun={\u2019quad\u2019, \u2019line\u2019},\nalgo={\u2019sgd\u2019}, learningRate=1e-4})\nThe code above loads an experiment database from the disk and it retrieves all the experiments for\nall the quadratic and line prototype shapes, for all different types of noise and all scales, further\nselecting the subset of experiments to those optimized using SGD with learningRate equal to 1e-\n4. The user can rerun the extracted experiments or have access to the associated results, i.e., the\nexpected value of the function in different optimization steps, along with the associated parameters\nvalues. In order to qualitatively assess the results the following code can be used:\ndb:ComputeReferenceValues()\ndb:cleanDB()\n12\ndb:assessPerformance()\ndb:plotExperiments()\nThe code above computes the reference expected values for each prototype function, it removes the\nexperiments for which no reference value is available, then it qualitatively assesses the performance\nof all the available experiments and \ufb01nally it plots the results given the color con\ufb01guration described\nin section 3.3. It is really easy to add a new algorithm in the database in order to evaluate its\nrobustness. The code below illustrates a simple example:\ndb:addAlgorithm(algoname, algofun, opt)\ndb:testAlgorithm(algoname)\ndb:plotExperiments({}, {algoname})\nHere a new algorithm with name algoname , function instance algo (which should satisfy the\noptim interface), and a table of different parameter con\ufb01gurations opt is added to the database\nand it is tested under all available functions prototypes. Finally, the last line plots a graph with all\nthe results for this algorithm.\nIt is also possible to add a set of new unit tests to the database, and subsequently run a set of\nexperiments associated with them. There are different parameters to be de\ufb01ned for the creation of a\nset of unit tests (that allow wildcard speci\ufb01cation too):\n1. the concatenated shape prototypes for each dimension,\n2. the noise prototype to be applied to each dimension,\n3. the scale of each dimension of the function,\n4. in case of multivariate unit tests, a parameter speci\ufb01es which p-norm is used for the com-\nbination,\n5. a rotation parameter that induces correlation of the different parameter dimensions, and\n6. a curl parameter that changes the vector \ufb01eld of a multivariate function.\n13",
        "ref_num": "28",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "28",
          "Yann LeCun and Corinna Cortes. The MNIST dataset of handwritten digits. 1998.\nhttp://yann.lecun.com/exdb/mnist/.\nA Appendix: Framework Software\nAs part of this work a software framework was developed for the computing and managing all the\nresults obtained for all the different con\ufb01gurations of function prototypes and algorithms. The main\ncomponent of the system is a database where all the results are stored and can be easily retrieved\nby querying the database accordingly. The building blocks of this database are the individual exper-\niments, where each experiment is associated to a unit test and an algorithm with \ufb01xed parameters.\nAn instance of an experiment database can either be loaded from the disk, or it can be created on the\n\ufb02y by running the associated experiments as needed. The code below creates a database and runs all\nthe experiments for all the readily available algorithms and default unit tests, and then saves them to\ndisk:\nrequire \u2019experiment\u2019\nlocal db = experimentsDB()\ndb:runExperiments()\ndb:save(\u2019experimentsDB\u2019)\nThis database now can be loaded from the disk, and the user can query it in order to retrieve speci\ufb01c\nexperiments, using \ufb01lters . An example is shown below:\nlocal db = experimentsDB()\ndb:load(\u2019experimentsDB\u2019)\nlocal experiments = db:filter({fun={\u2019quad\u2019, \u2019line\u2019},\nalgo={\u2019sgd\u2019}, learningRate=1e-4})\nThe code above loads an experiment database from the disk and it retrieves all the experiments for\nall the quadratic and line prototype shapes, for all different types of noise and all scales, further\nselecting the subset of experiments to those optimized using SGD with learningRate equal to 1e-\n4. The user can rerun the extracted experiments or have access to the associated results, i.e., the\nexpected value of the function in different optimization steps, along with the associated parameters\nvalues. In order to qualitatively assess the results the following code can be used:\ndb:ComputeReferenceValues()\ndb:cleanDB()\n12\ndb:assessPerformance()\ndb:plotExperiments()\nThe code above computes the reference expected values for each prototype function, it removes the\nexperiments for which no reference value is available, then it qualitatively assesses the performance\nof all the available experiments and \ufb01nally it plots the results given the color con\ufb01guration described\nin section 3.3. It is really easy to add a new algorithm in the database in order to evaluate its\nrobustness. The code below illustrates a simple example:\ndb:addAlgorithm(algoname, algofun, opt)\ndb:testAlgorithm(algoname)\ndb:plotExperiments({}, {algoname})\nHere a new algorithm with name algoname , function instance algo (which should satisfy the\noptim interface), and a table of different parameter con\ufb01gurations opt is added to the database\nand it is tested under all available functions prototypes. Finally, the last line plots a graph with all\nthe results for this algorithm.\nIt is also possible to add a set of new unit tests to the database, and subsequently run a set of\nexperiments associated with them. There are different parameters to be de\ufb01ned for the creation of a\nset of unit tests (that allow wildcard speci\ufb01cation too):\n1. the concatenated shape prototypes for each dimension,\n2. the noise prototype to be applied to each dimension,\n3. the scale of each dimension of the function,\n4. in case of multivariate unit tests, a parameter speci\ufb01es which p-norm is used for the com-\nbination,\n5. a rotation parameter that induces correlation of the different parameter dimensions, and\n6. a curl parameter that changes the vector \ufb01eld of a multivariate function.\n13"
        ],
        "doi": null,
        "in_text_mentions": [
          "ndom projections in parameter space of the loss function in an MNIST classi\ufb01cation task with an\nMLP **[28]**. We defer a fuller investigation of this type, namely obtaining statistics on how commonly\ndiff",
          "n-smooth gradients. In International Conference on Learning Representations , Scottsdale,\nAZ, 2013.\n**[28]** Yann LeCun and Corinna Cortes. The MNIST dataset of handwritten digits. 1998.\nhttp://yann.lecun"
        ],
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "De l'esclavage au salariat",
          "authors": [
            "Moulier-Boutang"
          ],
          "year": 1998,
          "confidence": "medium"
        }
      }
    ],
    "in_text_citations": [
      {
        "type": "numbered",
        "ref_num": "1",
        "context": "pen-source, extensible, and easy to apply to new algorithms.\n1 Introduction\nStochastic optimization **[1]** is among the most widely used components in large-scale machine learn-\ning, thanks to its linear"
      },
      {
        "type": "numbered",
        "ref_num": "2",
        "context": "learn-\ning, thanks to its linear complexity, ef\ufb01cient data usage, and often superior generalization **[2, 3, 4]**.\nIn this context, numerous variants of stochastic gradient descent have been proposed, in "
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "learn-\ning, thanks to its linear complexity, ef\ufb01cient data usage, and often superior generalization **[2, 3, 4]**.\nIn this context, numerous variants of stochastic gradient descent have been proposed, in "
      },
      {
        "type": "numbered",
        "ref_num": "4",
        "context": "learn-\ning, thanks to its linear complexity, ef\ufb01cient data usage, and often superior generalization **[2, 3, 4]**.\nIn this context, numerous variants of stochastic gradient descent have been proposed, in "
      },
      {
        "type": "numbered",
        "ref_num": "5",
        "context": "nt descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort **[5, 6, 7, 8, 9]**. These algorithms may\nderive from simplifying assumptions on the optimization landsc"
      },
      {
        "type": "numbered",
        "ref_num": "6",
        "context": "nt descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort **[5, 6, 7, 8, 9]**. These algorithms may\nderive from simplifying assumptions on the optimization landsc"
      },
      {
        "type": "numbered",
        "ref_num": "7",
        "context": "nt descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort **[5, 6, 7, 8, 9]**. These algorithms may\nderive from simplifying assumptions on the optimization landsc"
      },
      {
        "type": "numbered",
        "ref_num": "8",
        "context": "nt descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort **[5, 6, 7, 8, 9]**. These algorithms may\nderive from simplifying assumptions on the optimization landsc"
      },
      {
        "type": "numbered",
        "ref_num": "9",
        "context": "nt descent have been proposed, in order to\nimprove performance, robustness, or reduce tuning effort **[5, 6, 7, 8, 9]**. These algorithms may\nderive from simplifying assumptions on the optimization landsc"
      },
      {
        "type": "numbered",
        "ref_num": "10",
        "context": "6, 7, 8, 9]. These algorithms may\nderive from simplifying assumptions on the optimization landscape **[10]**, but in practice, they tend\nto be used as general-purpose tools, often outside of the space of "
      },
      {
        "type": "numbered",
        "ref_num": "11",
        "context": "ctitioners \ufb01nd it dif\ufb01cult to discern where potential\nweaknesses of new (or old) algorithms may lie **[11]**, and when they are applicable \u2013 an issue that\nis separate from raw performance. This results in"
      },
      {
        "type": "numbered",
        "ref_num": "12",
        "context": "every time that the dataset, loss\nfunction, regularization parameters, or model architecture change **[12]**.\nThe objective of this paper is to establish a collection of benchmarks to evaluate stochastic "
      },
      {
        "type": "numbered",
        "ref_num": "13",
        "context": ". This is a similar approach to the very fruitful one taken by the black-box\noptimization community **[13, 14]**.\nThe core assumption we make is that stochastic optimization algorithms are acting locally "
      },
      {
        "type": "numbered",
        "ref_num": "14",
        "context": ". This is a similar approach to the very fruitful one taken by the black-box\noptimization community **[13, 14]**.\nThe core assumption we make is that stochastic optimization algorithms are acting locally "
      },
      {
        "type": "numbered",
        "ref_num": "15",
        "context": "h as L1\nintroduces non-differentiable bends (as do recti\ufb01ed-linear or maxout units in deep learning **[15, 16]**).\nSteep cliffs in the loss surface are a common occurrence when training recurrent neural n"
      },
      {
        "type": "numbered",
        "ref_num": "16",
        "context": "h as L1\nintroduces non-differentiable bends (as do recti\ufb01ed-linear or maxout units in deep learning **[15, 16]**).\nSteep cliffs in the loss surface are a common occurrence when training recurrent neural n"
      },
      {
        "type": "numbered",
        "ref_num": "11",
        "context": "n the loss surface are a common occurrence when training recurrent neural networks,\nas discussed in **[11]**. See the top rows of Figure 1 for some examples of shape prototypes.\n2.2 One-dimensional Concat"
      },
      {
        "type": "numbered",
        "ref_num": "17",
        "context": " without substantial overhead. In many learning prob-\nlems, effort is put into proper normalization **[17]**, but that is insuf\ufb01cient to guarantee homogeneous\nscaling, for example throughout all the layer"
      },
      {
        "type": "numbered",
        "ref_num": "18",
        "context": "dependently for each dimension) with a certain\nprobability. This mimics both training with drop-out **[18]**, and scenarios with recti\ufb01ed\nlinear units where a unit will be inactive for some input samples,"
      },
      {
        "type": "numbered",
        "ref_num": "19",
        "context": "otstrapping : i.e.\nit pulls the value of the current state towards the value of its successor state **[19]**. These stochastic\nupdate directions are not proper gradients of any scalar energy \ufb01eld [20], bu"
      },
      {
        "type": "numbered",
        "ref_num": "20",
        "context": "r state [19]. These stochastic\nupdate directions are not proper gradients of any scalar energy \ufb01eld **[20]**, but they still form a (more\ngeneral) vector \ufb01eld with non-zero curl, where the objective for t"
      },
      {
        "type": "numbered",
        "ref_num": "21",
        "context": "t of the problem,\nrather than attempting to converge to a global but static solution of the problem **[21]**. In addition,\nreinforcement learning (RL) tasks often involve non-stationary optimization. For "
      },
      {
        "type": "numbered",
        "ref_num": "22",
        "context": "with decay factor in [10\u00002;1]and initial rates \u00110, SGD with momentum (regular or Nesterov\u2019s\nvariant **[22]**) [0:1;0:999] and initial rates \u00110, SGD with parameter averaging [] with decay term in\n[10\u00004;0:5"
      },
      {
        "type": "numbered",
        "ref_num": "10",
        "context": "0, SGD with parameter averaging [] with decay term in\n[10\u00004;0:5]and exponent inf1\n2;3\n4;1g, ADAGRAD **[10]** with initial rates \u00110, ADADELTA [23] with\ndecay parameter (1\u0000\r)2[10\u00004;0:5]and regularizer in [1"
      },
      {
        "type": "numbered",
        "ref_num": "23",
        "context": "th decay term in\n[10\u00004;0:5]and exponent inf1\n2;3\n4;1g, ADAGRAD [10] with initial rates \u00110, ADADELTA **[23]** with\ndecay parameter (1\u0000\r)2[10\u00004;0:5]and regularizer in [10\u00006;10\u00002, the incremental delta-bar-\n"
      },
      {
        "type": "numbered",
        "ref_num": "24",
        "context": "ter (1\u0000\r)2[10\u00004;0:5]and regularizer in [10\u00006;10\u00002, the incremental delta-bar-\ndelta algorithm (IDBD **[24]**), RPROP [25] with initial stepsizes \u00110, RMSprop [26] with minimal\nlearning rates \u00110, maximal le"
      },
      {
        "type": "numbered",
        "ref_num": "25",
        "context": "\u00004;0:5]and regularizer in [10\u00006;10\u00002, the incremental delta-bar-\ndelta algorithm (IDBD [24]), RPROP **[25]** with initial stepsizes \u00110, RMSprop [26] with minimal\nlearning rates \u00110, maximal learning rates "
      },
      {
        "type": "numbered",
        "ref_num": "26",
        "context": "e incremental delta-bar-\ndelta algorithm (IDBD [24]), RPROP [25] with initial stepsizes \u00110, RMSprop **[26]** with minimal\nlearning rates \u00110, maximal learning rates in [10;103]and decay parameter \r, as wel"
      },
      {
        "type": "numbered",
        "ref_num": "27",
        "context": "ere\nLinit=E[L(\u00120)]is the expected loss value at the initial point, similar to the approach taken in **[27]**,\nbut even more condensed. In other words, a normalized value near zero corresponds to no progre"
      },
      {
        "type": "numbered",
        "ref_num": "28",
        "context": "ndom projections in parameter space of the loss function in an MNIST classi\ufb01cation task with an\nMLP **[28]**. We defer a fuller investigation of this type, namely obtaining statistics on how commonly\ndiff"
      },
      {
        "type": "numbered",
        "ref_num": "1",
        "context": "owledgements\nWe thank the anonymous ICLR reviewers for their many constructive comments.\nReferences\n**[1]** H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical\nStatistics , "
      },
      {
        "type": "numbered",
        "ref_num": "2",
        "context": " S. Monro. A stochastic approximation method. Annals of Mathematical\nStatistics , 22:400\u2013407, 1951.\n**[2]** L \u00b4eon Bottou. Online Algorithms and Stochastic Approximations. In David Saad, editor, Online\nLe"
      },
      {
        "type": "numbered",
        "ref_num": "3",
        "context": "aad, editor, Online\nLearning and Neural Networks . Cambridge University Press, Cambridge, UK, 1998.\n**[3]** L \u00b4eon Bottou and Yann LeCun. Large Scale Online Learning. In Sebastian Thrun, Lawrence\nSaul, an"
      },
      {
        "type": "numbered",
        "ref_num": "4",
        "context": "opf, editors, Advances in Neural Information Processing Systems\n16. MIT Press, Cambridge, MA, 2004.\n**[4]** L \u00b4eon Bottou and Olivier Bousquet. The Tradeoffs of Large Scale Learning. In J.C. Platt,\nD. Kol"
      },
      {
        "type": "numbered",
        "ref_num": "5",
        "context": "ation Processing Sys-\ntems, volume 20, pages 161\u2013168. NIPS Foundation (http://books.nips.cc), 2008.\n**[5]** A. Benveniste, M. Metivier, and P. Priouret. Adaptive Algorithms and Stochastic Approxima-\ntions"
      },
      {
        "type": "numbered",
        "ref_num": "6",
        "context": "ret. Adaptive Algorithms and Stochastic Approxima-\ntions . Springer Verlag, Berlin, New York, 1990.\n**[6]** N. Le Roux, P.A. Manzagol, and Y . Bengio. Topmoumoute online natural gradient algorithm,\n2008.\n"
      },
      {
        "type": "numbered",
        "ref_num": "7",
        "context": "[6] N. Le Roux, P.A. Manzagol, and Y . Bengio. Topmoumoute online natural gradient algorithm,\n2008.\n**[7]** Antoine Bordes, L \u00b4eon Bottou, and Patrick Gallinari. SGD-QN: Careful Quasi-Newton\nStochastic Gr"
      },
      {
        "type": "numbered",
        "ref_num": "8",
        "context": "Newton\nStochastic Gradient Descent. Journal of Machine Learning Research , 10:1737\u20131754, July\n2009.\n**[8]** Wei Xu. Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient\nDescent."
      },
      {
        "type": "numbered",
        "ref_num": "9",
        "context": "s Large Scale Learning with Averaged Stochastic Gradient\nDescent. ArXiv-CoRR , abs/1107.2490, 2011.\n**[9]** Tom Schaul, Sixin Zhang, and Yann LeCun. No More Pesky Learning Rates. In International\nConferen"
      },
      {
        "type": "numbered",
        "ref_num": "10",
        "context": "LeCun. No More Pesky Learning Rates. In International\nConference on Machine Learning (ICML) , 2013.\n**[10]** John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online\nLearning a"
      },
      {
        "type": "numbered",
        "ref_num": "11",
        "context": "d Yoram Singer. Adaptive Subgradient Methods for Online\nLearning and Stochastic Optimization. 2010.\n**[11]** Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient\nproblem."
      },
      {
        "type": "numbered",
        "ref_num": "12",
        "context": "Yoshua Bengio. Understanding the exploding gradient\nproblem. arXiv preprint arXiv:1211.5063 , 2012.\n**[12]** X. Glorot and Y . Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetwo"
      },
      {
        "type": "numbered",
        "ref_num": "13",
        "context": "nd Statistics (AISTATS) , pages 249\u2013256. Society for Arti\ufb01cial Intelli-\ngence and Statistics, 2010.\n**[13]** Nikolaus Hansen, Anne Auger, Steffen Finck, Raymond Ros, et al. Real-parameter black-box\noptimi"
      },
      {
        "type": "numbered",
        "ref_num": "14",
        "context": "mond Ros, et al. Real-parameter black-box\noptimization benchmarking 2010: Experimental setup. 2010.\n**[14]** Nikolaus Hansen, Anne Auger, Raymond Ros, Steffen Finck, and Petr Po \u02c7s\u00b4\u0131k. Comparing\nresults o"
      },
      {
        "type": "numbered",
        "ref_num": "15",
        "context": "h annual conference companion on Genetic and evolutionary computation ,\npages 1689\u20131696. ACM, 2010.\n**[15]** Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con-\nvoluti"
      },
      {
        "type": "numbered",
        "ref_num": "16",
        "context": "l neural networks. In Advances in Neural Information Processing Systems 25 , pages\n1106\u20131114, 2012.\n**[16]** Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.\nMaxout n"
      },
      {
        "type": "numbered",
        "ref_num": "17",
        "context": " Mirza, Aaron Courville, and Yoshua Bengio.\nMaxout networks. arXiv preprint arXiv:1302.4389 , 2013.\n**[17]** Y . LeCun, L. Bottou, G. Orr, and K. Muller. Ef\ufb01cient BackProp. In G. Orr and Muller K.,\neditor"
      },
      {
        "type": "numbered",
        "ref_num": "18",
        "context": " BackProp. In G. Orr and Muller K.,\neditors, Neural Networks: Tricks of the trade . Springer, 1998.\n**[18]** Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut-\nd"
      },
      {
        "type": "numbered",
        "ref_num": "19",
        "context": "l networks by preventing co-adaptation of feature detectors. arXiv\npreprint arXiv:1207.0580 , 2012.\n**[19]** R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. IEEE Transactions on\nNeura"
      },
      {
        "type": "numbered",
        "ref_num": "20",
        "context": "ment Learning: An Introduction. IEEE Transactions on\nNeural Networks , 9(5):1054\u20131054, Sep 1998.\n11\n**[20]** Etienne Barnard. Temporal-difference methods and Markov models. IEEE Transactions on\nSystems, M"
      },
      {
        "type": "numbered",
        "ref_num": "21",
        "context": "ethods and Markov models. IEEE Transactions on\nSystems, Man, and Cybernetics , 23(2):357\u2013365, 1993.\n**[21]** Richard S. Sutton, Anna Koop, and David Silver. On the role of tracking in stationary environ-\n"
      },
      {
        "type": "numbered",
        "ref_num": "22",
        "context": "ty-Fourth International Conference on Machine Learning\n(ICML 2007 , pages 871\u2013878. ACM Press, 2007.\n**[22]** Yurii Nesterov and Arkadii Semenovich Nemirovskii. Interior-point polynomial algorithms in\nconv"
      },
      {
        "type": "numbered",
        "ref_num": "23",
        "context": "ch Nemirovskii. Interior-point polynomial algorithms in\nconvex programming , volume 13. SIAM, 1994.\n**[23]** Matthew D Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv preprint\narXiv:1212.5701 , "
      },
      {
        "type": "numbered",
        "ref_num": "24",
        "context": "atthew D Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv preprint\narXiv:1212.5701 , 2012.\n**[24]** Richard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-\ndelta"
      },
      {
        "type": "numbered",
        "ref_num": "25",
        "context": "ias by gradient descent: An incremental version of delta-bar-\ndelta. In AAAI , pages 171\u2013176, 1992.\n**[25]** Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation\nlearn"
      },
      {
        "type": "numbered",
        "ref_num": "26",
        "context": " algorithm. In Neural Networks, 1993., IEEE International Conference\non, pages 586\u2013591. IEEE, 1993.\n**[26]** T Tieleman and G Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of\nits r"
      },
      {
        "type": "numbered",
        "ref_num": "27",
        "context": "y a running average of\nits recent magnitude. COURSERA: Neural Networks for Machine Learning , 2012.\n**[27]** Tom Schaul and Yann LeCun. Adaptive learning rates and parallelization for stochastic, sparse,\n"
      },
      {
        "type": "numbered",
        "ref_num": "28",
        "context": "n-smooth gradients. In International Conference on Learning Representations , Scottsdale,\nAZ, 2013.\n**[28]** Yann LeCun and Corinna Cortes. The MNIST dataset of handwritten digits. 1998.\nhttp://yann.lecun"
      }
    ]
  },
  {
    "filename": "1402.0030v2.pdf",
    "total_citations": 29,
    "valid_format_count": 29,
    "existing_count": 29,
    "invalid_format_count": 0,
    "nonexistent_count": 0,
    "citations": [
      {
        "text": "Bradley, David M and Bagnell, J Andrew. Differential\nsparse coding. In Advances in Neural Information Pro-\ncessingSystems , volume20,2008.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Bradley, David M and Bagnell, J Andrew. Differential\nsparse coding. In Advances in Neural Information Pro-\ncessingSystems , volume20,2008."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Boosting Structured Prediction for Imitation Learning",
          "authors": [
            "Ratliff",
            "Bradley",
            "Bagnell",
            "Chestnutt"
          ],
          "year": 2007,
          "confidence": "medium"
        }
      },
      {
        "text": "Dayan, Peter and Hinton, Geoffrey E. Varieties of\nhelmholtz machine. Neural Networks , 9(8):1385\u20131403,",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Dayan, Peter and Hinton, Geoffrey E. Varieties of\nhelmholtz machine. Neural Networks , 9(8):1385\u20131403,"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Varieties of Helmholtz Machine",
          "authors": [
            "Dayan",
            "Hinton"
          ],
          "year": 1996,
          "confidence": "medium"
        }
      },
      {
        "text": "Dayan, Peter, Hinton, Geoffrey E, Neal, Radford M, and",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Dayan, Peter, Hinton, Geoffrey E, Neal, Radford M, and"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Metacognitive Information Theory",
          "authors": [
            "Dayan"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "Zemel,RichardS. Thehelmholtzmachine. Neuralcom-\nputation,7(5):889\u2013904,1995.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Zemel,RichardS. Thehelmholtzmachine. Neuralcom-\nputation,7(5):889\u2013904,1995."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "The Helmholtz Machine",
          "authors": [
            "Dayan",
            "Hinton",
            "Neal",
            "Zemel"
          ],
          "year": 1995,
          "confidence": "medium"
        }
      },
      {
        "text": "Greensmith,Evan,Bartlett, PeterL.,andBaxter,Jonathan .\nVariance reduction techniques for gradient estimates in\nreinforcement learning. Journal of Machine Learning",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Greensmith,Evan,Bartlett, PeterL.,andBaxter,Jonathan .\nVariance reduction techniques for gradient estimates in\nreinforcement learning. Journal of Machine Learning"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Greensmith, Edwin Lloydd, (23 Jan. 1900\u20138 Aug. 1993)",
          "authors": [],
          "year": 2007,
          "confidence": "medium"
        }
      },
      {
        "text": "Research,5:1471\u20131530,2004.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Research,5:1471\u20131530,2004."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Unknown",
          "authors": [],
          "year": 2004,
          "confidence": "medium"
        }
      },
      {
        "text": "Gregor,KarolandLeCun,Yann. Learningfastapproxima-\ntionsofsparsecoding.In Proc.InternationalConference\nonMachinelearning(ICML\u201910) ,2010.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Gregor,KarolandLeCun,Yann. Learningfastapproxima-\ntionsofsparsecoding.In Proc.InternationalConference\nonMachinelearning(ICML\u201910) ,2010."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Theoretical Characterization of a Nanocrystal Layer for Nonvolatile Memory Applications",
          "authors": [
            "Leroy",
            "Armeanu",
            "Cordan"
          ],
          "year": 2010,
          "confidence": "medium"
        }
      },
      {
        "text": "Gregor,Karol,Mnih,Andriy,andWierstra,Daan.Deepau-\ntoregressive networks. arXiv preprint arXiv:1310.8499 ,",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Gregor,Karol,Mnih,Andriy,andWierstra,Daan.Deepau-\ntoregressive networks. arXiv preprint arXiv:1310.8499 ,"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Unknown",
          "authors": [
            "Pako\u0144ski",
            "Tanner",
            "\u017byczkowski"
          ],
          "year": 2003,
          "confidence": "medium"
        }
      },
      {
        "text": "Hinton, Geoffrey E and Zemel, Richard S. Autoencoders,\nminimumdescriptionlength,andHelmholtzfreeenergy.\nInAdvancesin Neural Information Processing Systems ,",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hinton, Geoffrey E and Zemel, Richard S. Autoencoders,\nminimumdescriptionlength,andHelmholtzfreeenergy.\nInAdvancesin Neural Information Processing Systems ,"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Learning Population Codes by Minimizing Description Length",
          "authors": [
            "Zemel",
            "Hinton"
          ],
          "year": 1999,
          "confidence": "medium"
        }
      },
      {
        "text": "Hinton, Geoffrey E, Dayan, Peter, Frey, Brendan J, and",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hinton, Geoffrey E, Dayan, Peter, Frey, Brendan J, and"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Connectionist Symbol Processing",
          "authors": [],
          "year": 1991,
          "confidence": "medium"
        }
      },
      {
        "text": "Neal, Radford M. The \"wake-sleep\" algorithm for un-\nsupervised neural networks. Science, 268(5214):1158\u2013\n1161,1995.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Neal, Radford M. The \"wake-sleep\" algorithm for un-\nsupervised neural networks. Science, 268(5214):1158\u2013\n1161,1995."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "The \"Wake-Sleep\" Algorithm for Unsupervised Neural Networks",
          "authors": [
            "Hinton",
            "Dayan",
            "Frey",
            "Neal"
          ],
          "year": 1995,
          "confidence": "medium"
        }
      },
      {
        "text": "Hinton,GeoffreyE.,Osindero,Simon,andTeh,YeeWhye.\nA fast learning algorithm for deep belief nets. Neural\nComputation ,18(7):1527\u20131554,2006.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hinton,GeoffreyE.,Osindero,Simon,andTeh,YeeWhye.\nA fast learning algorithm for deep belief nets. Neural\nComputation ,18(7):1527\u20131554,2006."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "A Fast Learning Algorithm for Deep Belief Nets",
          "authors": [
            "Hinton",
            "Osindero",
            "Teh"
          ],
          "year": 2006,
          "confidence": "medium"
        }
      },
      {
        "text": "Jordan, Michael I., Ghahramani, Zoubin, Jaakkola,\nTommi S., and Saul, Lawrence K. An introduction\nto variational methods for graphical models. Machine",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Jordan, Michael I., Ghahramani, Zoubin, Jaakkola,\nTommi S., and Saul, Lawrence K. An introduction\nto variational methods for graphical models. Machine"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Jordan, Michael",
          "authors": [],
          "year": 2005,
          "confidence": "medium"
        }
      },
      {
        "text": "Learning,37(2):183\u2013233,1999.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Learning,37(2):183\u2013233,1999."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "World-Class Theories of Organizational Learning",
          "authors": [],
          "year": 1999,
          "confidence": "medium"
        }
      },
      {
        "text": "Kavukcuoglu, Koray, Ranzato, Marc\u2019Aurelio, and LeCun,\nYann. Fast inference in sparse coding algorithms with\napplications to object recognition. Technical report,\nCourantInstitute,NYU,2008.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Kavukcuoglu, Koray, Ranzato, Marc\u2019Aurelio, and LeCun,\nYann. Fast inference in sparse coding algorithms with\napplications to object recognition. Technical report,\nCourantInstitute,NYU,2008."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Learning Hierarchies of Sparse Features",
          "authors": [
            "Ranzato",
            "Boureau",
            "Kavukcuoglu",
            "Gregor",
            "LeCun"
          ],
          "year": 2012,
          "confidence": "medium"
        }
      },
      {
        "text": "Kingma, Diederik P and Welling, Max. Auto-encoding\nvariational bayes. arXiv preprint arXiv:1312.6114 ,",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Kingma, Diederik P and Welling, Max. Auto-encoding\nvariational bayes. arXiv preprint arXiv:1312.6114 ,"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "An Introduction to Variational Autoencoders",
          "authors": [
            "Kingma",
            "Welling"
          ],
          "year": 2019,
          "confidence": "medium"
        }
      },
      {
        "text": "Larochelle, Hugo and Lauly, Stanislas. A neural autore-\ngressivetopicmodel. In AdvancesinNeuralInformation\nProcessingSystems ,pp.2717\u20132725,2012.Larochelle,HugoandMurray,Iain. Theneuralautoregres-\nsive distribution estimator. JMLR: W&CP , 15:29\u201337,",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Larochelle, Hugo and Lauly, Stanislas. A neural autore-\ngressivetopicmodel. In AdvancesinNeuralInformation\nProcessingSystems ,pp.2717\u20132725,2012.Larochelle,HugoandMurray,Iain. Theneuralautoregres-\nsive distribution estimator. JMLR: W&CP , 15:29\u201337,"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "The",
          "authors": [
            "Kann",
            "Lauly",
            "Cho"
          ],
          "year": 2018,
          "confidence": "medium"
        }
      },
      {
        "text": "Neal, Radford M. Connectionist learning of belief net-\nworks.Arti\ufb01cialintelligence ,56(1):71\u2013113,1992.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Neal, Radford M. Connectionist learning of belief net-\nworks.Arti\ufb01cialintelligence ,56(1):71\u2013113,1992."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Connectionist learning of belief networks",
          "authors": [
            "Neal"
          ],
          "year": 1992,
          "confidence": "medium"
        }
      },
      {
        "text": "Paisley, John William, Blei, David M., and Jordan,\nMichaelI. Variationalbayesianinferencewithstochastic\nsearch. In ICML,2012.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Paisley, John William, Blei, David M., and Jordan,\nMichaelI. Variationalbayesianinferencewithstochastic\nsearch. In ICML,2012."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "The Discrete Infinite Logistic Normal Distribution",
          "authors": [
            "Paisley",
            "Wang",
            "Blei"
          ],
          "year": 2012,
          "confidence": "medium"
        }
      },
      {
        "text": "Ranganath, Rajesh, Gerrish, Sean, and Blei, David M.\nBlack box variational inference. arXiv preprint\narXiv:1401.0118 ,2013.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Ranganath, Rajesh, Gerrish, Sean, and Blei, David M.\nBlack box variational inference. arXiv preprint\narXiv:1401.0118 ,2013."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Variational inference for Dirichlet process mixtures",
          "authors": [
            "Blei",
            "Jordan"
          ],
          "year": 2006,
          "confidence": "medium"
        }
      },
      {
        "text": "Rezende,DaniloJimenez,Mohamed,Shakir,andWierstra,\nDaan. Stochastic back-propagation and variational in-\nference in deep latent gaussian models. arXiv preprint\narXiv:1401.4082 ,2014.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Rezende,DaniloJimenez,Mohamed,Shakir,andWierstra,\nDaan. Stochastic back-propagation and variational in-\nference in deep latent gaussian models. arXiv preprint\narXiv:1401.4082 ,2014."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Stochastic variational learning in recurrent spiking networks",
          "authors": [
            "Jimenez Rezende",
            "Gerstner"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "Salakhutdinov,RuslanandHinton,GeoffreyE.Deepboltz-\nmann machines. In International Conference on Arti\ufb01-\ncialIntelligenceandStatistics , pp.448\u2013455,2009a.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Salakhutdinov,RuslanandHinton,GeoffreyE.Deepboltz-\nmann machines. In International Conference on Arti\ufb01-\ncialIntelligenceandStatistics , pp.448\u2013455,2009a."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Restricted Boltzmann machines for collaborative filtering",
          "authors": [
            "Salakhutdinov",
            "Mnih",
            "Hinton"
          ],
          "year": 2007,
          "confidence": "medium"
        }
      },
      {
        "text": "Salakhutdinov,RuslanandHinton,GeoffreyE. Replicated\nsoftmax: anundirectedtopicmodel. In Advancesinneu-\nralinformationprocessingsystems , 2009b.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Salakhutdinov,RuslanandHinton,GeoffreyE. Replicated\nsoftmax: anundirectedtopicmodel. In Advancesinneu-\nralinformationprocessingsystems , 2009b."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Visual trajectory analysis via Replicated Softmax-based models",
          "authors": [
            "Chen",
            "Ye",
            "Zou",
            "Li",
            "Cui",
            "Jiao"
          ],
          "year": 2014,
          "confidence": "medium"
        }
      },
      {
        "text": "Salakhutdinov, Ruslan and Larochelle, Hugo. Ef\ufb01cient\nlearning of deep boltzmann machines. In International\nConference on Arti\ufb01cial Intelligence and Statistics , pp.\n693\u2013700,2010.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Salakhutdinov, Ruslan and Larochelle, Hugo. Ef\ufb01cient\nlearning of deep boltzmann machines. In International\nConference on Arti\ufb01cial Intelligence and Statistics , pp.\n693\u2013700,2010."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Classification using discriminative restricted Boltzmann machines",
          "authors": [
            "Larochelle",
            "Bengio"
          ],
          "year": 2008,
          "confidence": "medium"
        }
      },
      {
        "text": "Salakhutdinov, Ruslan and Murray, Iain. On the quantita-\ntive analysis of Deep Belief Networks. In Proceedings\nofthe25thAnnualInternationalConferenceonMachine\nLearning(ICML 2008) ,2008.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Salakhutdinov, Ruslan and Murray, Iain. On the quantita-\ntive analysis of Deep Belief Networks. In Proceedings\nofthe25thAnnualInternationalConferenceonMachine\nLearning(ICML 2008) ,2008."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "On the quantitative analysis of deep belief networks",
          "authors": [
            "Salakhutdinov",
            "Murray"
          ],
          "year": 2008,
          "confidence": "medium"
        }
      },
      {
        "text": "Saul, Lawrence K., Jaakkola, Tommi, and Jordan,\nMichael I. Mean \ufb01eld theory for sigmoid belief net-\nworks.JournalofArti\ufb01cialIntelligenceResearch ,4:61\u2013\n76,1996.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Saul, Lawrence K., Jaakkola, Tommi, and Jordan,\nMichael I. Mean \ufb01eld theory for sigmoid belief net-\nworks.JournalofArti\ufb01cialIntelligenceResearch ,4:61\u2013\n76,1996."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Mean Field Theory for Sigmoid Belief Networks",
          "authors": [
            "Saul",
            "Jaakkola",
            "Jordan"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "Tang, Yichuan and Salakhutdinov, Ruslan. Learning\nstochastic feedforwardneural networks. In Advancesin\nNeuralInformationProcessingSystems ,2013.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Tang, Yichuan and Salakhutdinov, Ruslan. Learning\nstochastic feedforwardneural networks. In Advancesin\nNeuralInformationProcessingSystems ,2013."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Stochastic Networks and Insensitivity",
          "authors": [
            "Mazumdar"
          ],
          "year": 2013,
          "confidence": "medium"
        }
      },
      {
        "text": "Weaver, Lex and Tao, Nigel. The optimal reward baseline\nfor gradient-based reinforcement learning. In In Pro-\nceedings of the Seventeenth Conference on Uncertainty\ninArti\ufb01cialIntelligence ,2001.",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Weaver, Lex and Tao, Nigel. The optimal reward baseline\nfor gradient-based reinforcement learning. In In Pro-\nceedings of the Seventeenth Conference on Uncertainty\ninArti\ufb01cialIntelligence ,2001."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Optimal Reward Functions in Distributed Reinforcement Learning",
          "authors": [
            "Wolpert",
            "Tumer"
          ],
          "year": 2001,
          "confidence": "medium"
        }
      },
      {
        "text": "Williams, Ronald J. Simple statistical gradient-followin g\nalgorithms for connectionist reinforcement learning.\nMachinelearning ,8(3-4):229\u2013256,1992.\nNeural Variational InferenceandLearninginBelief Networ ks\nA. Algorithmforcomputing NVILgradients\nAlgorithm 1provides an outline of our implementation of\nNVILgradientcomputationforaminibatchof nrandomly\nchosentrainingcases. Theexponentialsmoothingfactor \u03b1\nused forupdatingthe estimatesof the mean candvariance\nvof theinferencenetworklearningsignalwasset to 0.8in\nourexperiments.\nAlgorithm1 Computegradientestimatesforthemodeland\ntheinferencenetwork\n\u2206\u03b8\u21900,\u2206\u03c6\u21900,\u2206\u03c8\u21900\nL\u21900\n{Computethelearningsignalandthebound}\nfori\u21901tondo\nxi\u2190randomtrainingcase\n{Samplefromtheinferencemodel}\nhi\u223cQ\u03c6(hi|xi)\n{Computetheunnormalizedlearningsignal}\nli\u2190logP\u03b8(xi,hi)\u2212logQ\u03c6(hi|xi)\n{Addthecasecontributionto thebound}\nL\u2190L+li\n{Subtractthe input-dependentbaseline}\nli\u2190li\u2212C\u03c8(xi)\nend for\n{Updatethe learningsignal statistics}\ncb\u2190mean(l1,...,ln)\nvb\u2190variance(l1,...,ln)\nc\u2190\u03b1c+(1\u2212\u03b1)cb\nv\u2190\u03b1v+(1\u2212\u03b1)vb\nfori\u21901tondo\nli\u2190li\u2212c\nmax(1,\u221av)\n{Accumulatethemodelparametergradient}\n\u2206\u03b8\u2190\u2206\u03b8+\u2207\u03b8logP\u03b8(xi,hi)\n{Accumulatetheinferencenetgradient}\n\u2206\u03c6\u2190\u2206\u03c6+li\u2207\u03c6logQ\u03c6(hi|xi)\n{Accumulatetheinput-dependentbaselinegradient}\n\u2206\u03c8\u2190\u2206\u03c8+li\u2207\u03c8C\u03c8(xi)\nend forB. Inference network gradient derivation\nDifferentiating the variational lower bound w.r.t. to the i n-\nferencenetworkparametersgives\n\u2207\u03c6L(x) =\u2207\u03c6EQ[logP\u03b8(x,h)\u2212logQ\u03c6(h|x)]\n=\u2207\u03c6/summationdisplay\nhQ\u03c6(h|x)logP\u03b8(x,h)\u2212\n\u2207\u03c6/summationdisplay\nhQ\u03c6(h|x)logQ\u03c6(h|x)\n=/summationdisplay\nhlogP\u03b8(x,h)\u2207\u03c6Q\u03c6(h|x)\u2212\n/summationdisplay\nh(logQ\u03c6(h|x)+1)\u2207\u03c6Q\u03c6(h|x)\n=/summationdisplay\nh(logP\u03b8(x,h)\u2212logQ\u03c6(h|x))\u2207\u03c6Q\u03c6(h|x),\nwhere we used the fact that/summationtext\nh\u2207\u03c6Q\u03c6(h|x) =\n\u2207\u03c6/summationtext\nhQ\u03c6(h|x) =\u2207\u03c61 = 0. Using the identity\n\u2207\u03c6Q\u03c6(h|x) =Q\u03c6(h|x)\u2207\u03c6logQ\u03c6(h|x), thengives\n\u2207\u03c6L(x) =/summationdisplay\nh(logP\u03b8(x,h)\u2212logQ\u03c6(h|x))\n\u00d7Q\u03c6(h|x)\u2207\u03c6logQ\u03c6(h|x)\n=EQ[(logP\u03b8(x,h)\u2212logQ\u03c6(h|x))\u2207\u03c6logQ\u03c6(h|x)].",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Williams, Ronald J. Simple statistical gradient-followin g\nalgorithms for connectionist reinforcement learning.\nMachinelearning ,8(3-4):229\u2013256,1992.\nNeural Variational InferenceandLearninginBelief Networ ks\nA. Algorithmforcomputing NVILgradients\nAlgorithm 1provides an outline of our implementation of\nNVILgradientcomputationforaminibatchof nrandomly\nchosentrainingcases. Theexponentialsmoothingfactor \u03b1\nused forupdatingthe estimatesof the mean candvariance\nvof theinferencenetworklearningsignalwasset to 0.8in\nourexperiments.\nAlgorithm1 Computegradientestimatesforthemodeland\ntheinferencenetwork\n\u2206\u03b8\u21900,\u2206\u03c6\u21900,\u2206\u03c8\u21900\nL\u21900\n{Computethelearningsignalandthebound}\nfori\u21901tondo\nxi\u2190randomtrainingcase\n{Samplefromtheinferencemodel}\nhi\u223cQ\u03c6(hi|xi)\n{Computetheunnormalizedlearningsignal}\nli\u2190logP\u03b8(xi,hi)\u2212logQ\u03c6(hi|xi)\n{Addthecasecontributionto thebound}\nL\u2190L+li\n{Subtractthe input-dependentbaseline}\nli\u2190li\u2212C\u03c8(xi)\nend for\n{Updatethe learningsignal statistics}\ncb\u2190mean(l1,...,ln)\nvb\u2190variance(l1,...,ln)\nc\u2190\u03b1c+(1\u2212\u03b1)cb\nv\u2190\u03b1v+(1\u2212\u03b1)vb\nfori\u21901tondo\nli\u2190li\u2212c\nmax(1,\u221av)\n{Accumulatethemodelparametergradient}\n\u2206\u03b8\u2190\u2206\u03b8+\u2207\u03b8logP\u03b8(xi,hi)\n{Accumulatetheinferencenetgradient}\n\u2206\u03c6\u2190\u2206\u03c6+li\u2207\u03c6logQ\u03c6(hi|xi)\n{Accumulatetheinput-dependentbaselinegradient}\n\u2206\u03c8\u2190\u2206\u03c8+li\u2207\u03c8C\u03c8(xi)\nend forB. Inference network gradient derivation\nDifferentiating the variational lower bound w.r.t. to the i n-\nferencenetworkparametersgives\n\u2207\u03c6L(x) =\u2207\u03c6EQ[logP\u03b8(x,h)\u2212logQ\u03c6(h|x)]\n=\u2207\u03c6/summationdisplay\nhQ\u03c6(h|x)logP\u03b8(x,h)\u2212\n\u2207\u03c6/summationdisplay\nhQ\u03c6(h|x)logQ\u03c6(h|x)\n=/summationdisplay\nhlogP\u03b8(x,h)\u2207\u03c6Q\u03c6(h|x)\u2212\n/summationdisplay\nh(logQ\u03c6(h|x)+1)\u2207\u03c6Q\u03c6(h|x)\n=/summationdisplay\nh(logP\u03b8(x,h)\u2212logQ\u03c6(h|x))\u2207\u03c6Q\u03c6(h|x),\nwhere we used the fact that/summationtext\nh\u2207\u03c6Q\u03c6(h|x) =\n\u2207\u03c6/summationtext\nhQ\u03c6(h|x) =\u2207\u03c61 = 0. Using the identity\n\u2207\u03c6Q\u03c6(h|x) =Q\u03c6(h|x)\u2207\u03c6logQ\u03c6(h|x), thengives\n\u2207\u03c6L(x) =/summationdisplay\nh(logP\u03b8(x,h)\u2212logQ\u03c6(h|x))\n\u00d7Q\u03c6(h|x)\u2207\u03c6logQ\u03c6(h|x)\n=EQ[(logP\u03b8(x,h)\u2212logQ\u03c6(h|x))\u2207\u03c6logQ\u03c6(h|x)]."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",
          "authors": [
            "Williams"
          ],
          "year": 1992,
          "confidence": "medium"
        }
      }
    ],
    "in_text_citations": [
      {
        "type": "numbered",
        "ref_num": "1",
        "context": "at westart bynotingthat\nEQ[\u2207\u03c6logQ\u03c6(h|x)] =EQ/bracketleftbigg\u2207\u03c6Q\u03c6(h|x)\nQ\u03c6(h|x)/bracketrightbigg\n=\u2207\u03c6EQ**[1]** = 0. (8)\nThereforewe can subtractany cthat doesnot dependon h\nfromthelearningsignalinEq. 4withou"
      },
      {
        "type": "author_year",
        "year": "1996",
        "context": "nebyintr o-\nducing more variational parameters, as was done for sig-\nmoid belief nets by Saulet al. **(1996). However, thi**s tech-\nniqueincreasesthegapbetweentheboundbeingoptimized\nand the log-likelihood"
      },
      {
        "type": "author_year",
        "year": "2013",
        "context": "ariational infer-\nence,NVILcanhandlebothdiscreteandcontinuouslatent\nvariables(unlike Kingma&Welling **(2013);Rezendeetal.\n**(2014))as well variationalposteriorswith complexdepen-\ndency structures (unlike "
      },
      {
        "type": "author_year",
        "year": "2014",
        "context": "ce,NVILcanhandlebothdiscreteandcontinuouslatent\nvariables(unlike Kingma&Welling (2013);Rezendeetal.\n**(2014))as well varia**tionalposteriorswith complexdepen-\ndency structures (unlike Ranganathet al. (201"
      },
      {
        "type": "author_year",
        "year": "2013",
        "context": "al.\n(2014))as well variationalposteriorswith complexdepen-\ndency structures (unlike Ranganathet al. **(2013)). More-\nover,** the variance reduction methods we employ are sim-\nple and model-independent, un"
      },
      {
        "type": "author_year",
        "year": "2012",
        "context": "e and model-independent, unlike the more sophisticated\nmodel-speci\ufb01ccontrolvariatesof Paisleyet al. **(2012).\nThough the i**dea of training an inference model by\nfollowing the gradient of the variational "
      },
      {
        "type": "author_year",
        "year": "2001",
        "context": "ceandLearninginBelief Networ ks\nour experience works as well as the optimal approach of\nWeaver& Tao **(2001) which require**s taking into account\nthe magnitudeof the gradientof the inferencenetworkpa-\nram"
      },
      {
        "type": "author_year",
        "year": "2012",
        "context": " simple control variates.\nIn contrast to the more elaborate control variates (e.g. of\nPaisleyet al. **(2012)), baselines d**o not depend on the form\nof the model or of the variational distribution and thu"
      },
      {
        "type": "author_year",
        "year": "1994",
        "context": " network\nby optimizing a variational lower bound is not new. It\ngoes back at least to Hinton& Zemel **(1994), who derived\n**the variational objective from the Minimum Description\nLength (MDL) perspective "
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": ", and dis-\nmissed the sampling-based approach as infeasible due to\nnoise.\nSalakhutdinov& Larochelle **(2010)proposedusinga**feed-\nforward \u201crecognition\u201d model to perform ef\ufb01cient input-\ndependent initializ"
      },
      {
        "type": "author_year",
        "year": "2013",
        "context": "iables, called Stochastic Gradi-\nent Variational Bayes (SGVB), has been proposed by\nKingma& Welling **(2013) andRezendeet **al. (2014). Like\nNVIL, it involves using feedforward models to perform\napproxima"
      },
      {
        "type": "author_year",
        "year": "2014",
        "context": "c Gradi-\nent Variational Bayes (SGVB), has been proposed by\nKingma& Welling (2013) andRezendeet al. **(2014). Like\nNVIL, i**t involves using feedforward models to perform\napproximate inference and trains "
      },
      {
        "type": "author_year",
        "year": "2013",
        "context": "iables, while SGVB might converge\nfasterusingthevariancereductiontechniquesweproposed .\nGregoretal. **(2013) have recently** proposed a related al-\ngorithm for training sigmoid belief network like models\n"
      },
      {
        "type": "author_year",
        "year": "2008",
        "context": "-imag e\ntraining set and 10,000-image test set. We used the bina-\nrization of Salakhutdinov& Murray **(2008), which makes\n**ourscoresdirectlycomparabletothosein theliterature.\nWe used 3\u00d710\u22124as the learnin"
      },
      {
        "type": "author_year",
        "year": "2009b",
        "context": "est documents. We use\nthe standard preprocessed versions of the datasets from\nSalakhutdinov& Hinton **(2009b),whichhavevoc**abularies\nof2Kand10Kwordsrespectively.\nWeexperimentedwithtwosimpledocumentmodels"
      },
      {
        "type": "author_year",
        "year": "2013",
        "context": "his would make it an al-\nternative to the importance-sampling training method of\nTang&Salakhutdinov **(2013) for condition**al models with\nstructuredhigh-dimensionaloutputs.\nWe hope that the generalityand"
      }
    ]
  }
]