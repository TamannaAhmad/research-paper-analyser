[
  {
    "filename": "1109.5951v2.pdf",
    "total_citations": 15,
    "valid_format_count": 15,
    "existing_count": 15,
    "invalid_format_count": 0,
    "nonexistent_count": 0,
    "citations": [
      {
        "text": "Dowe, D. L. and Hajek, A. R. (1998). A non-behavioural, compu tational extension to the\nTuring Test. In Intl. Conf. on Computational Intelligence & multimedia app lications\n(ICCIMA\u201998), Gippsland, Australia , pages 101\u2013106.\n\u00b4Etor\u00b4 e, P. and Jourdain, B. (2010). Adaptive optimal alloca tion in strati\ufb01ed sampling methods.\nMethodology and Computing in Applied Probability , 12(3):335\u2013360.\nHern\u00b4 andez-Orallo, J. (2000). Beyond the Turing Test. J. Logic, Language & Information ,\n9(4):447\u2013466.\nHern\u00b4 andez-Orallo, J. (2010). A (hopefully) unbiased univ ersal environment class for measuring\nintelligence ofbiological and arti\ufb01cialsystems. In Proc. of the Third Conference on Arti\ufb01cial\nGeneral Intelligence , Lugano.\nHern\u00b4 andez-Orallo, J. and Dowe, D. L. (2010). Measuring uni versal intelligence: Towards an\nanytime intelligence test. Arti\ufb01cial Intelligence , 174(18):1508 \u2013 1539.\nHernandez-orallo, J. and Minaya-collado, N. (1998). A form al de\ufb01nition of intelligence based\non an intensional variant of algorithmic complexity. In EIS\u201998.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Dowe, D. L. and Hajek, A. R. (1998). A non-behavioural, compu tational extension to the\nTuring Test. In Intl. Conf. on Computational Intelligence & multimedia app lications\n(ICCIMA\u201998), Gippsland, Australia , pages 101\u2013106.\n\u00b4Etor\u00b4 e, P. and Jourdain, B. (2010). Adaptive optimal alloca tion in strati\ufb01ed sampling methods.\nMethodology and Computing in Applied Probability , 12(3):335\u2013360.\nHern\u00b4 andez-Orallo, J. (2000). Beyond the Turing Test. J. Logic, Language & Information ,\n9(4):447\u2013466.\nHern\u00b4 andez-Orallo, J. (2010). A (hopefully) unbiased univ ersal environment class for measuring\nintelligence ofbiological and arti\ufb01cialsystems. In Proc. of the Third Conference on Arti\ufb01cial\nGeneral Intelligence , Lugano.\nHern\u00b4 andez-Orallo, J. and Dowe, D. L. (2010). Measuring uni versal intelligence: Towards an\nanytime intelligence test. Arti\ufb01cial Intelligence , 174(18):1508 \u2013 1539.\nHernandez-orallo, J. and Minaya-collado, N. (1998). A form al de\ufb01nition of intelligence based\non an intensional variant of algorithmic complexity. In EIS\u201998."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "INFORMATION-THEORETIC APPROACHES TO BIOLOGY",
          "authors": [
            "Dowe",
            "Prank"
          ],
          "year": 1998,
          "confidence": "medium"
        }
      },
      {
        "text": "Hibbard, B. (2009). Bias and no free lunch in formal measures of intelligence. Journal of\nArti\ufb01cial General Intelligence , 1(1):54\u201361.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hibbard, B. (2009). Bias and no free lunch in formal measures of intelligence. Journal of\nArti\ufb01cial General Intelligence , 1(1):54\u201361."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Bias and No Free Lunch in Formal Measures of Intelligence",
          "authors": [
            "Hibbard"
          ],
          "year": 2009,
          "confidence": "medium"
        }
      },
      {
        "text": "Hutter, M. (2001). Towards a universal theory of arti\ufb01cial i ntelligence based on algorithmic\nprobability and sequential decisions. Proc. 12th Eurpean Conference on Machine Learning\n(ECML-2001) , pages 226\u2013238.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hutter, M. (2001). Towards a universal theory of arti\ufb01cial i ntelligence based on algorithmic\nprobability and sequential decisions. Proc. 12th Eurpean Conference on Machine Learning\n(ECML-2001) , pages 226\u2013238."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Towards a Universal Theory of Artificial Intelligence Based on Algorithmic Probability and Sequential Decisions",
          "authors": [
            "Hutter"
          ],
          "year": 2001,
          "confidence": "medium"
        }
      },
      {
        "text": "Hutter, M. (2005). Universal Arti\ufb01cial Intelligence: Sequential Decisions b ased on Algorithmic\nProbability . Springer, Berlin. 300 pages, http://www.hutter1.net/ai /uaibook.htm.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hutter, M. (2005). Universal Arti\ufb01cial Intelligence: Sequential Decisions b ased on Algorithmic\nProbability . Springer, Berlin. 300 pages, http://www.hutter1.net/ai /uaibook.htm."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Algorithmic probability",
          "authors": [
            "Hutter",
            "Legg",
            "Vitanyi"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "Hutter, M. and Legg, S. (2007). Temporal di\ufb00erence updating without a learning rate. In\nNeural Information Processing Systems (NIPS \u201907) .\nInsa-Cabrera, J., Dowe, D. L., Espana-Cubillo, S., Hernand ez-Lloreda, M., and Hernandez-",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Hutter, M. and Legg, S. (2007). Temporal di\ufb00erence updating without a learning rate. In\nNeural Information Processing Systems (NIPS \u201907) .\nInsa-Cabrera, J., Dowe, D. L., Espana-Cubillo, S., Hernand ez-Lloreda, M., and Hernandez-"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Marx and temporal di\ufb00erence",
          "authors": [],
          "year": 2010,
          "confidence": "medium"
        }
      },
      {
        "text": "Orallo, J. (2011a). Comparing humans and ai agents. In Juerg en Schmidhuber, Kristinn\nR. Thorisson, M. L., editor, Arti\ufb01cial General Intelligence, 4th Intl Conf, Mountain Vi ew,\nSan Francisco . Lecture Notes in Arti\ufb01cial Intelligence, Springer.\nInsa-Cabrera, J., Dowe, D. L., and Hernandez-Orallo, J. (20 11b). Evaluating a reinforcement\nlearning algorithm with a general intelligence test. In Jos e A. Lozano, Jose A. Gamez, J.\nA. M., editor, Current Topics in Arti\ufb01cial Intelligence. 14th Conference of the Spanish As-\nsociation for Arti\ufb01cial Intelligence, CAEPIA 2011 . Lecture Notes in Arti\ufb01cial Intelligence,\nSpringer.",
        "style": "mla",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Orallo, J. (2011a). Comparing humans and ai agents. In Juerg en Schmidhuber, Kristinn\nR. Thorisson, M. L., editor, Arti\ufb01cial General Intelligence, 4th Intl Conf, Mountain Vi ew,\nSan Francisco . Lecture Notes in Arti\ufb01cial Intelligence, Springer.\nInsa-Cabrera, J., Dowe, D. L., and Hernandez-Orallo, J. (20 11b). Evaluating a reinforcement\nlearning algorithm with a general intelligence test. In Jos e A. Lozano, Jose A. Gamez, J.\nA. M., editor, Current Topics in Arti\ufb01cial Intelligence. 14th Conference of the Spanish As-\nsociation for Arti\ufb01cial Intelligence, CAEPIA 2011 . Lecture Notes in Arti\ufb01cial Intelligence,\nSpringer."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Comparing Humans and AI Agents",
          "authors": [
            "Insa-Cabrera",
            "Dowe",
            "Espa\u00f1a-Cubillo",
            "Hern\u00e1ndez-Lloreda",
            "Hern\u00e1ndez-Orallo"
          ],
          "year": 2011,
          "confidence": "medium"
        }
      },
      {
        "text": "Legg, S. and Hutter, M. (2007). Universal intelligence: A de \ufb01nition of machine intelligence.\nMinds and Machines , 17(4):391\u2013444.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Legg, S. and Hutter, M. (2007). Universal intelligence: A de \ufb01nition of machine intelligence.\nMinds and Machines , 17(4):391\u2013444."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Universal Intelligence: A Definition of Machine Intelligence",
          "authors": [
            "Legg",
            "Hutter"
          ],
          "year": 2007,
          "confidence": "medium"
        }
      },
      {
        "text": "Li, M. and Vit\u00b4 anyi, P. M. B. (2008). An introduction to Kolmogorov complexity and its\napplications . Springer, 3nd edition.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Li, M. and Vit\u00b4 anyi, P. M. B. (2008). An introduction to Kolmogorov complexity and its\napplications . Springer, 3nd edition."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Message from the INVITE 2008 Chairs",
          "authors": [],
          "year": 2008,
          "confidence": "medium"
        }
      },
      {
        "text": "Mahoney, M. (2008). Generic compression benchmark. http://www.mattmahoney.net/dc/uiq .",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Mahoney, M. (2008). Generic compression benchmark. http://www.mattmahoney.net/dc/uiq ."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "MyMultiMediaWorld.com: A benchmark platform for 3D compression algorithms",
          "authors": [
            "Le Bonhomme",
            "Preda",
            "Preteux"
          ],
          "year": 2008,
          "confidence": "medium"
        }
      },
      {
        "text": "Schaul, T., Togelius, J., and Schmidhuber, J. (2011). Measu ring Intelligence through Games.\nArXiv e-prints .\nSolomono\ufb00, R. J. (1964). A formaltheory of inductive infere nce: Part 1 and 2. Inform. Control ,\n7:1\u201322, 224\u2013254.\nSolomono\ufb00, R. J. (1978). Complexity-based induction syste ms: comparisons and convergence\ntheorems. IEEE Trans. Information Theory , IT-24:422\u2013432.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Schaul, T., Togelius, J., and Schmidhuber, J. (2011). Measu ring Intelligence through Games.\nArXiv e-prints .\nSolomono\ufb00, R. J. (1964). A formaltheory of inductive infere nce: Part 1 and 2. Inform. Control ,\n7:1\u201322, 224\u2013254.\nSolomono\ufb00, R. J. (1978). Complexity-based induction syste ms: comparisons and convergence\ntheorems. IEEE Trans. Information Theory , IT-24:422\u2013432."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "A scalable neural network architecture for board games",
          "authors": [
            "Schaul",
            "Schmidhuber"
          ],
          "year": 2008,
          "confidence": "medium"
        }
      },
      {
        "text": "Sutton, R. and Barto, A. (1998). Reinforcement learning: An introduction . Cambridge, MA,\nMIT Press.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Sutton, R. and Barto, A. (1998). Reinforcement learning: An introduction . Cambridge, MA,\nMIT Press."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Introduction: The Challenge of Reinforcement Learning",
          "authors": [
            "Sutton"
          ],
          "year": 1992,
          "confidence": "medium"
        }
      },
      {
        "text": "Turing, A. M. (1950). Computing machinery and intelligence .Mind.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Turing, A. M. (1950). Computing machinery and intelligence .Mind."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Computing Machinery and Intelligence (1950)",
          "authors": [
            "Turing"
          ],
          "year": 2004,
          "confidence": "medium"
        }
      },
      {
        "text": "Veness, J., Ng, K. S., Hutter, M., and Silver, D. (2010). Rein forcement learning via AIXI\napproximation. In Proc. Association for the advancesment of arti\ufb01cial intell igence.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Veness, J., Ng, K. S., Hutter, M., and Silver, D. (2010). Rein forcement learning via AIXI\napproximation. In Proc. Association for the advancesment of arti\ufb01cial intell igence."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Reinforcement Learning via AIXI Approximation",
          "authors": [
            "Veness",
            "Ng",
            "Hutter",
            "Silver"
          ],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "Veness, J., Ng, K. S., Hutter, M., Uther, W., and Silver, D. (2 011). A Monte-Carlo AIXI\nApproximation. Journal of Arti\ufb01cial Intelligence Research (JAIR) , 40(1).",
        "style": "mla",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Veness, J., Ng, K. S., Hutter, M., Uther, W., and Silver, D. (2 011). A Monte-Carlo AIXI\nApproximation. Journal of Arti\ufb01cial Intelligence Research (JAIR) , 40(1)."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Veness, Sir David (Christopher), (born 20 Sept. 1947), Under-Secretary-General for Safety and Security, United Nations, 2005\u201309",
          "authors": [],
          "year": 2007,
          "confidence": "medium"
        }
      },
      {
        "text": "Watkins, C. (1989). Learning from Delayed Rewards . PhD thesis, King\u2019s College, Oxford.",
        "style": "apa",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Watkins, C. (1989). Learning from Delayed Rewards . PhD thesis, King\u2019s College, Oxford."
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Efficient Learning from Delayed Rewards through Symbiotic Evolution",
          "authors": [
            "Moriarty",
            "Miikkulainen"
          ],
          "year": 1995,
          "confidence": "medium"
        }
      }
    ],
    "in_text_citations": [
      {
        "type": "author_year",
        "year": "1950",
        "context": "or empirical aspects of broadly intelligent machines. Of course there\nis the well-known Turing Test **(Turing, 1950), howe**ver this paradox ically seems\nto be more about dodging the di\ufb03cult problem of ex"
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "One recent attempt at an explicit de\ufb01nition of intelligence is the Univer-\nsal Intelligence Measure **(Legg and Hutter, 20**07). This is a mathematical, non-\nanthropocentric de\ufb01nition of intelligence that"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "rmance of broadly intelligent agen ts.\n2.1 Universal Intelligence Tests\nHern\u00b4 andez-Orallo and Dowe **(2010) introduce the** notion of a Universal Intelli-\ngence Test , a test designed to be able to quant"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": "opocentric bias. Related discussion on the motivation b ehind such\ntests is given by Dowe and Hajek **(1998); Hern\u00b4 andez-**Orallo(2000 ); Schaul et al.\n(2011). With respect to our goal of wanting to buil"
      },
      {
        "type": "author_year",
        "year": "2011",
        "context": "tion b ehind such\ntests is given by Dowe and Hajek (1998); Hern\u00b4 andez-Orallo(2000 ); Schaul et al.\n**(2011). With respect** to our goal of wanting to build more powerful arti\ufb01cial\nagents, we strongly sup"
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "of intelligence propose d by various\npsychologistsandarti\ufb01cialintelligenceresearchers,LeggandHut ter**(2007)argue\nthat the** informal de\ufb01nition:\n\u201cintelligence measures an agent\u2019s ability to achieve goals "
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": "erties associated with intellig ence. To\nformalise this intuition, they used reinforcement learning **(Sutton and Barto,\n1**998), a general framework for goal achieving agents in unknown environments.\nIn "
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "d\ninto a single result. To encourage agents to apply Occam\u2019s Razor, as advocated\nby Legg and Hutter **(2007), each environ**ment is weighted accordin g to its com-\nplexity,with simplerenvironmentsbeing we"
      },
      {
        "type": "author_year",
        "year": "2005",
        "context": "hat converges to optimal performance in any e nvironment\nwhere this is possible for a general agent **(Hutter, 2005). At t**he o ther end of the\nscale, it can be shownthat the UniversalIntelligence Measur"
      },
      {
        "type": "author_year",
        "year": "2008",
        "context": "ling has been\nused to create the test data sequences that make up the Generic Compression\nBenchmark **(Mahoney, 2008). Her**e we will use this technique to sam ple envir-\nonments for the Universal Intelli"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": " of performing a Monte Ca rlo\nsample over environments is also used by Hern\u00b4 andez-Orallo and Dow e **(2010)\nand Schaul et** al. (2011) in their related work.\n3.2 Environment simulation\nWe need to be able"
      },
      {
        "type": "author_year",
        "year": "2011",
        "context": " rlo\nsample over environments is also used by Hern\u00b4 andez-Orallo and Dow e (2010)\nand Schaul et al. **(2011) in their rela**ted work.\n3.2 Environment simulation\nWe need to be able to run each sampled prog"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": " knowing if a program will respect the bound.\nA more practical alternative is geometric discounting **(Sutton and Barto,\n1**998) where we allow the environment to generate any reward in any cycle so\nlong "
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "g we allow agent s to train\nfrom samples prior to taking the test, as suggested in Legg and Hut ter **(2007).\nFor more lim**ited agents however, the choice of reference machine is important.\nIndeed, in th"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "t, randomly acting agent s have an\nAIQofzero,anaturalbaselinesuggestedbyHern\u00b4 andez-Orallo a nd Dowe**(2010).\n3.6 Variance** Reduction Techniques for AIQ Estimation\nObtaining an accurate estimate of an ag"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": " adaptive strati\ufb01ed samp ling, however\nwe have chosen the method developed by \u00b4Etor\u00b4 e and Jourdain **(2010) as they\nhave **derived the con\ufb01dence intervals for the estimate of the mean , a feature we\nwill"
      },
      {
        "type": "author_year",
        "year": "1989",
        "context": "a \ufb01xed fraction of the time when it tries a random action. We have imple-\nmented the Q(\u03bb) algorithm **(Watkins, 1989), whi**ch subsumes the simpler Q(0)\nalgorithm as a special case, and also HLQ(\u03bb) which "
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "pecial case, and also HLQ(\u03bb) which is similar except that it\nautomatically adapts its learning rate **(Hutter and Legg, 20**07). Fin ally, we have\n2 4 8 16 32\nContext Depth0102030405060AIQ Score\n5 10 25 5"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": "s) seemed\nquite incomprehensible.\n5 Related Work and Discussion\nHernandez-orallo and Minaya-collado **(1998) developed a r**elated te st, called the\nC-test, that is also based on a very simple reference m"
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "cular program that computes it.\n6 Conclusion\nWe have taken the theoretical model of Legg and Hutter **(2007) a nd converte**d\nit into a practical test for machine intelligence. To do this we have ran doml"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": " simple universal Turing machine, drawing ins pira-\ntion at points from Hern\u00b4 andez-Orallo and Dowe **(2010), and the re l**ated work in\nHern\u00b4 andez-Orallo (2010). In all of our tests the AIQ scores beh a"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "tion at points from Hern\u00b4 andez-Orallo and Dowe (2010), and the re lated work in\nHern\u00b4 andez-Orallo **(2010). In all of ou**r tests the AIQ scores beh aved sensibly,\nwith agents expected to be more intell"
      },
      {
        "type": "author_year",
        "year": "2009",
        "context": "telligence\nMeasure is its dependence on the choice of reference machine, as h ighlighted by\nHibbard **(2009). While we acc**ept that problematic reference machin es exist, it\nwas our belief that if we cho"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": "s National Science\nFoundation grant number PBTIP2-133701.\nBibliography\nDowe, D. L. and Hajek, A. R. **(1998). A non-behavi**oural, compu tational extension to the\nTuring Test. In Intl. Conf. on Computatio"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "edia app lications\n(ICCIMA\u201998), Gippsland, Australia , pages 101\u2013106.\n\u00b4Etor\u00b4 e, P. and Jourdain, B. **(2010). Adaptive opt**imal alloca tion in strati\ufb01ed sampling methods.\nMethodology and Computing in App"
      },
      {
        "type": "author_year",
        "year": "2000",
        "context": "g methods.\nMethodology and Computing in Applied Probability , 12(3):335\u2013360.\nHern\u00b4 andez-Orallo, J. **(2000). Beyond the T**uring Test. J. Logic, Language & Information ,\n9(4):447\u2013466.\nHern\u00b4 andez-Orallo,"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "0). Beyond the Turing Test. J. Logic, Language & Information ,\n9(4):447\u2013466.\nHern\u00b4 andez-Orallo, J. **(2010). A (hopefully**) unbiased univ ersal environment class for measuring\nintelligence ofbiological "
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "Third Conference on Arti\ufb01cial\nGeneral Intelligence , Lugano.\nHern\u00b4 andez-Orallo, J. and Dowe, D. L. **(2010). Measuring un**i versal intelligence: Towards an\nanytime intelligence test. Arti\ufb01cial Intellige"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": "nce test. Arti\ufb01cial Intelligence , 174(18):1508 \u2013 1539.\nHernandez-orallo, J. and Minaya-collado, N. **(1998). A form al de**\ufb01nition of intelligence based\non an intensional variant of algorithmic complexit"
      },
      {
        "type": "author_year",
        "year": "2009",
        "context": "n of intelligence based\non an intensional variant of algorithmic complexity. In EIS\u201998.\nHibbard, B. **(2009). Bias and no **free lunch in formal measures of intelligence. Journal of\nArti\ufb01cial General Inte"
      },
      {
        "type": "author_year",
        "year": "2001",
        "context": "formal measures of intelligence. Journal of\nArti\ufb01cial General Intelligence , 1(1):54\u201361.\nHutter, M. **(2001). Towards a un**iversal theory of arti\ufb01cial i ntelligence based on algorithmic\nprobability and s"
      },
      {
        "type": "author_year",
        "year": "2005",
        "context": "ecisions. Proc. 12th Eurpean Conference on Machine Learning\n(ECML-2001) , pages 226\u2013238.\nHutter, M. **(2005). Universal Ar**ti\ufb01cial Intelligence: Sequential Decisions b ased on Algorithmic\nProbability . S"
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "lity . Springer, Berlin. 300 pages, http://www.hutter1.net/ai /uaibook.htm.\nHutter, M. and Legg, S. **(2007). Temporal di\ufb00**erence updating without a learning rate. In\nNeural Information Processing System"
      },
      {
        "type": "author_year",
        "year": "2011a",
        "context": "nsa-Cabrera, J., Dowe, D. L., Espana-Cubillo, S., Hernand ez-Lloreda, M., and Hernandez-\nOrallo, J. **(2011a). Comparing h**umans and ai agents. In Juerg en Schmidhuber, Kristinn\nR. Thorisson, M. L., edit"
      },
      {
        "type": "author_year",
        "year": "2007",
        "context": "elligence, CAEPIA 2011 . Lecture Notes in Arti\ufb01cial Intelligence,\nSpringer.\nLegg, S. and Hutter, M. **(2007). Universal in**telligence: A de \ufb01nition of machine intelligence.\nMinds and Machines , 17(4):391"
      },
      {
        "type": "author_year",
        "year": "2008",
        "context": "\ufb01nition of machine intelligence.\nMinds and Machines , 17(4):391\u2013444.\nLi, M. and Vit\u00b4 anyi, P. M. B. **(2008). An introduct**ion to Kolmogorov complexity and its\napplications . Springer, 3nd edition.\nMahon"
      },
      {
        "type": "author_year",
        "year": "2008",
        "context": " An introduction to Kolmogorov complexity and its\napplications . Springer, 3nd edition.\nMahoney, M. **(2008). Generic comp**ression benchmark. http://www.mattmahoney.net/dc/uiq .\nSchaul, T., Togelius, J.,"
      },
      {
        "type": "author_year",
        "year": "2011",
        "context": "ession benchmark. http://www.mattmahoney.net/dc/uiq .\nSchaul, T., Togelius, J., and Schmidhuber, J. **(2011). Measu ring I**ntelligence through Games.\nArXiv e-prints .\nSolomono\ufb00, R. J. (1964). A formalthe"
      },
      {
        "type": "author_year",
        "year": "1964",
        "context": "nd Schmidhuber, J. (2011). Measu ring Intelligence through Games.\nArXiv e-prints .\nSolomono\ufb00, R. J. **(1964). A formaltheo**ry of inductive infere nce: Part 1 and 2. Inform. Control ,\n7:1\u201322, 224\u2013254.\nSol"
      },
      {
        "type": "author_year",
        "year": "1978",
        "context": "altheory of inductive infere nce: Part 1 and 2. Inform. Control ,\n7:1\u201322, 224\u2013254.\nSolomono\ufb00, R. J. **(1978). Complexity-b**ased induction syste ms: comparisons and convergence\ntheorems. IEEE Trans. Infor"
      },
      {
        "type": "author_year",
        "year": "1998",
        "context": " and convergence\ntheorems. IEEE Trans. Information Theory , IT-24:422\u2013432.\nSutton, R. and Barto, A. **(1998). Reinforcemen**t learning: An introduction . Cambridge, MA,\nMIT Press.\nTuring, A. M. (1950). Co"
      },
      {
        "type": "author_year",
        "year": "1950",
        "context": "Barto, A. (1998). Reinforcement learning: An introduction . Cambridge, MA,\nMIT Press.\nTuring, A. M. **(1950). Computing ma**chinery and intelligence .Mind.\nVeness, J., Ng, K. S., Hutter, M., and Silver, D"
      },
      {
        "type": "author_year",
        "year": "2010",
        "context": "950). Computing machinery and intelligence .Mind.\nVeness, J., Ng, K. S., Hutter, M., and Silver, D. **(2010). Rein forceme**nt learning via AIXI\napproximation. In Proc. Association for the advancesment of"
      },
      {
        "type": "author_year",
        "year": "1989",
        "context": "te-Carlo AIXI\nApproximation. Journal of Arti\ufb01cial Intelligence Research (JAIR) , 40(1).\nWatkins, C. **(1989). Learning fro**m Delayed Rewards . PhD thesis, King\u2019s College, Oxford.\n"
      }
    ]
  },
  {
    "filename": "1307.3176v4.pdf",
    "total_citations": 2,
    "valid_format_count": 2,
    "existing_count": 2,
    "invalid_format_count": 0,
    "nonexistent_count": 0,
    "citations": [
      {
        "text": "Francis Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine\nlearning. Advances in Neural Information Processing Systems (NIPS) , 2011.\nVarsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. In\nProceedings of the 21st Annual Conference on Learning Theory (COLT) , pages 355\u2013366, 2008.\n15\nNoufel Frikha and Stphane Menozzi. Concentration Bounds for Stochastic Approximations. Electron. Commun.\nProbab. , 17:1\u201315, 2012.\nElad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-\nconvex optimization. Journal of Machine Learning Research , 19:421\u2013436, 2011.\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In\nAdvances in Neural Information Processing Systems (NIPS) , pages 315\u2013323, 2013.\nLihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news\narticle recommendation. In Proceedings of the 19th international conference on World wide web , pages 661\u2013",
        "style": "unknown",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "Francis Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine\nlearning. Advances in Neural Information Processing Systems (NIPS) , 2011.\nVarsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. In\nProceedings of the 21st Annual Conference on Learning Theory (COLT) , pages 355\u2013366, 2008.\n15\nNoufel Frikha and Stphane Menozzi. Concentration Bounds for Stochastic Approximations. Electron. Commun.\nProbab. , 17:1\u201315, 2012.\nElad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-\nconvex optimization. Journal of Machine Learning Research , 19:421\u2013436, 2011.\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In\nAdvances in Neural Information Processing Systems (NIPS) , pages 315\u2013323, 2013.\nLihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news\narticle recommendation. In Proceedings of the 19th international conference on World wide web , pages 661\u2013"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "Asymptotic Properties of Stochastic Approximation Algorithms",
          "authors": [],
          "year": 0,
          "confidence": "medium"
        }
      },
      {
        "text": "670. ACM, 2010.\nJ. Mary, Aurlien Garivier, L. Li, R. Munos, O. Nicol, R. Ortner, and P. Preux. ICML Exploration and Exploitation\n3 - New Challenges, 2012.\nAS Nemirovsky and DB Yudin. Problem complexity and method ef\ufb01ciency in optimization, 1983.\nAlexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex\nstochastic optimization. arXiv preprint arXiv:1109.5647 , 2011.\nNicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence\nrate for \ufb01nite training sets. arXiv preprint arXiv:1202.6258 , 2012.\nPaat Rusmevichientong and John N. Tsitsiklis. Linearly parameterized bandits. Math. Oper. Res. , 35(2):395\u2013411,\nMay 2010.\nShai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-\ntion. arXiv preprint arXiv:1209.1873 , 2012.\nPierre Tarr `es and Yuan Yao. Online learning as stochastic approximation of regularization paths. arXiv preprint\narXiv:1103.5538 , 2011.\nYahoo! Webscope. Yahoo! webscope dataset ydata-frontpage-todaymodule-clicks-v2 0, 2011. URL \"http:\n//research.yahoo.com/Academic_Relations\" .\nMartin Zinkevich. Online convex programming and generalized in\ufb01nitesimal gradient ascent. In Proceedings of\nthe 20th International Conference on Machine Learning , pages 928\u2013925, 2003.\n16",
        "style": "numbered",
        "valid_format": true,
        "exists": true,
        "match_groups": [
          "670. ACM, 2010.\nJ. Mary, Aurlien Garivier, L. Li, R. Munos, O. Nicol, R. Ortner, and P. Preux. ICML Exploration and Exploitation\n3 - New Challenges, 2012.\nAS Nemirovsky and DB Yudin. Problem complexity and method ef\ufb01ciency in optimization, 1983.\nAlexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex\nstochastic optimization. arXiv preprint arXiv:1109.5647 , 2011.\nNicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence\nrate for \ufb01nite training sets. arXiv preprint arXiv:1202.6258 , 2012.\nPaat Rusmevichientong and John N. Tsitsiklis. Linearly parameterized bandits. Math. Oper. Res. , 35(2):395\u2013411,\nMay 2010.\nShai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-\ntion. arXiv preprint arXiv:1209.1873 , 2012.\nPierre Tarr `es and Yuan Yao. Online learning as stochastic approximation of regularization paths. arXiv preprint\narXiv:1103.5538 , 2011.\nYahoo! Webscope. Yahoo! webscope dataset ydata-frontpage-todaymodule-clicks-v2 0, 2011. URL \"http:\n//research.yahoo.com/Academic_Relations\" .\nMartin Zinkevich. Online convex programming and generalized in\ufb01nitesimal gradient ascent. In Proceedings of\nthe 20th International Conference on Machine Learning , pages 928\u2013925, 2003.\n16"
        ],
        "doi": null,
        "metadata": {
          "exists": true,
          "source": "CrossRef",
          "title": "2010 Miracle Yearbook",
          "authors": [],
          "year": 2010,
          "confidence": "medium"
        }
      }
    ],
    "in_text_citations": [
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "irst, we consider the PEGE algorithm for linear bandits proposed by Rusmevichientong and Tsitsiklis **[2010]**.\nThis algorithm is designed for action sets Dsatisfying a strong convexity property (see assu"
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "only O(log4n)in the regret performance.\nSecond, we consider the LinUCB algorithm proposed Li et al. **[2010]**. Here we investigate computationally\nef\ufb01cient variants of LinUCB. We begin by replacing the O"
      },
      {
        "type": "numbered",
        "ref_num": "2013",
        "context": "-GD iterate, and then compare\nthis to two other state-of-the-art OLS schemes from Johnson and Zhang **[2013]** and Roux et al. [2012]. The\nLinUCB algorithm is designed for situations where at each time, n"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "ompare\nthis to two other state-of-the-art OLS schemes from Johnson and Zhang [2013] and Roux et al. **[2012]**. The\nLinUCB algorithm is designed for situations where at each time, n, the agent can choose "
      },
      {
        "type": "numbered",
        "ref_num": "2013",
        "context": "om the numerical experiments, we observe that the fRLS-GD iterate as well\nas SVRG Johnson and Zhang **[2013]** and SAG Roux et al. [2012] variants consistently track the true RLS solutions\nin each iterati"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "s, we observe that the fRLS-GD iterate as well\nas SVRG Johnson and Zhang [2013] and SAG Roux et al. **[2012]** variants consistently track the true RLS solutions\nin each iteration of LinUCB, while the run"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "exity. Non-asymptotic bounds in expectation for SGD schemes have been provided by\nBach and Moulines **[2011]**. In the machine learning community, several algorithms have been proposed for min-\nimising th"
      },
      {
        "type": "numbered",
        "ref_num": "2003",
        "context": "mmunity, several algorithms have been proposed for min-\nimising the regret, for instance, Zinkevich **[2003]**, Hazan and Kale [2011], Rakhlin et al. [2011] and these can be\nconverted to \ufb01nd the minimiser"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "thms have been proposed for min-\nimising the regret, for instance, Zinkevich [2003], Hazan and Kale **[2011]**, Rakhlin et al. [2011] and these can be\nconverted to \ufb01nd the minimiser of a (usually convex) "
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": " for min-\nimising the regret, for instance, Zinkevich [2003], Hazan and Kale [2011], Rakhlin et al. **[2011]** and these can be\nconverted to \ufb01nd the minimiser of a (usually convex) function. A closely rel"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "oximation\n(SA), and concentration bounds for SA algorithms have been provided by Frikha and Menozzi **[2012]**. Adaptive\nregularisation in the context of least squares regression has been analysed in Tarr"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "ive\nregularisation in the context of least squares regression has been analysed in Tarr `es and Yao **[2011]**. For recent\nalgorithmic improvements to solving batch problems, the reader is referred to the"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "orithmic improvements to solving batch problems, the reader is referred to the works of Roux et al. **[2012]**,\nShalev-Shwartz and Zhang [2012], Johnson and Zhang [2013].\nIn general, none of the schemes p"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "batch problems, the reader is referred to the works of Roux et al. [2012],\nShalev-Shwartz and Zhang **[2012]**, Johnson and Zhang [2013].\nIn general, none of the schemes proposed above are directly applic"
      },
      {
        "type": "numbered",
        "ref_num": "2013",
        "context": " is referred to the works of Roux et al. [2012],\nShalev-Shwartz and Zhang [2012], Johnson and Zhang **[2013]**.\nIn general, none of the schemes proposed above are directly applicable in our setting due to"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "Sketch In order to prove the bound in expectation, following the proof scheme of Frikha and Menozzi\n**[2012]**, we expand the error at time ninto an initial error term, a (martingale) sampling error term,"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "etails).\nThe initial and sampling errors appear as in previous works on SGD (cf. Frikha and Menozzi **[2012]** and Bach\nand Moulines [2011]), and can be treated similarly, except that here we can make all"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "ling errors appear as in previous works on SGD (cf. Frikha and Menozzi [2012] and Bach\nand Moulines **[2011]**), and can be treated similarly, except that here we can make all the constants explicit, usin"
      },
      {
        "type": "numbered",
        "ref_num": "2008",
        "context": "onvergence of the least squares solution ^\u0012nto\u0012\u0003. Adapting a con\ufb01dence ball result\nfrom Dani et al. **[2008]**, we derive the third term of K1.\nHaving bounded the mean error, we can bound separately the d"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "bound separately the deviation of the error from its mean. To do\nthis, following Frikha and Menozzi **[2012]**, we decompose k\u0012n\u0000^\u0012nk2\u0000Ek\u0012n\u0000^\u0012nk2into a sum of martingale\ndifferences as follows: Let Hndeno"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "are Lipschitz continuous in the noise \u0018i,\nwith Lipschitz constants Li. Unlike in Frikha and Menozzi **[2012]** we use the exact form of the update to derive the\nexact constants Li. The \ufb01nal step of the pr"
      },
      {
        "type": "numbered",
        "ref_num": "1983",
        "context": "ergence rate for SGD type schemes that do not involve a drifting target (see Ne-\nmirovsky and Yudin **[1983]**).\nDependence on dThe dependence of the rate derived above on the dimension dofxiis indirect, "
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "overns the\nlosses of the bandit algorithm (see (A5) below). PEGE of Rusmevichientong and Tsitsiklis **[2010]** is a well-known\nalgorithm in this setting. Recall from the introduction that it gathers data "
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "nly O(md2).\nResults We require the following extra assumptions from Rusmevichientong and Tsitsiklis **[2010]**:\n(A4\u2019) A basisfb1;:::;bdg2D forRdis known to the algorithm.\n(A5) The function G(\u0012)isJ-Lipschi"
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": " (8)\nNow to complete the proof we only need to reprove Lemma 3.6 of Rusmevichientong and Tsitsiklis **[2010]**, which\nstates that for all n\u0015d,Ek\u0012\u0003(G(\u0012\u0003)\u0000G(\u0012md))k2\u0014K1(n)\ndmk\u0012\u0003k2:\nk\u0012\u0003(G(\u0012\u0003)\u0000G(\u0012md))k2=\r\r(\u0012\u0003\u0000"
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "we have used that G(\u0012) =G(a\u0012)for alla > 0, (A5), and Lemma 3.5 of Rus-\nmevichientong and Tsitsiklis **[2010]**.\nThe rest of the proof follows that of Theorem 3.1 of Rusmevichientong and Tsitsiklis [2010]."
      },
      {
        "type": "numbered",
        "ref_num": "2010",
        "context": "siklis [2010].\nThe rest of the proof follows that of Theorem 3.1 of Rusmevichientong and Tsitsiklis **[2010]**.\n4 Online GD for Regularized Least Squares\nIdeally an online algorithm would not need to sati"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "r (note, \u000bmust be chosen in (1=2;1)to ensure (A1) holds).\nUnlike in the setting of Tarr `es and Yao **[2011]**, we do not assume that the data arrive from a distribution, and hence\nthe bias error is dif\ufb01c"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "e recommendation platform provided for the ICML\nexploration and exploitation challenge (Mary et al. **[2012]**). This platform is based on the user click log dataset\nfrom the Yahoo! front page, provided u"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "he user click log dataset\nfrom the Yahoo! front page, provided under the Webscope program (Webscope **[2011]**). An algorithm for this\nplatform is required to repeatedly select a news article from a pool "
      },
      {
        "type": "numbered",
        "ref_num": "2013",
        "context": "similar to the above algorithm, except that the SGD scheme used is derived from John-\nson and Zhang **[2013]**. The \ufb01rst scheme is derived from Johnson and Zhang [2013] and updates the parameter\nas follow"
      },
      {
        "type": "numbered",
        "ref_num": "2013",
        "context": " used is derived from John-\nson and Zhang [2013]. The \ufb01rst scheme is derived from Johnson and Zhang **[2013]** and updates the parameter\nas follows: Let fi;n(\u0012) :=1\n2(yi\u0000\u0012Txi)2+\u0015nk\u0012k2\n2,Fn(\u0012) =1\nnn\u00001P\ni=1"
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "andom in f1;:::;ng.\nfLinUCB-SAG. This is a variant that uses the SGD scheme proposed by Roux et al. **[2012]**. The updates here are\naccording to\n\u0012n=\u0012n\u00001\u0000\rn\nnnX\ni=1yn;i;whereyn;i=(\nf0\ni(\u0012n\u00001)ifi=in,\nyn\u00001;"
      },
      {
        "type": "numbered",
        "ref_num": "2011",
        "context": "times observed on two different data \ufb01les corresponding to days 2and4in October, 2009 (see\nWebscope **[2011]**) of the dataset. It is evident that the SGD schemes result in signi\ufb01cant computational gains "
      },
      {
        "type": "numbered",
        "ref_num": "2012",
        "context": "error, zn:=\u0012n\u0000^\u0012n,\nfrom its mean. The proof technique is similar to that used by Frikha and Menozzi **[2012]**. However, our analysis is\nmuch simpler, we make all the constants explicit for the problem at"
      },
      {
        "type": "numbered",
        "ref_num": "2008",
        "context": " 1\u0000\u000e, where\n\fn= max\u0010\n128dlognlogn2\u000e\u00001;\u0000\n2 logn2\u000e\u00001\u00012\u0011\n.\nProof. Follows from Theorem 5of Dani et al. **[2008]** and (A3).\nUsing the above lemma, we bound the drift error as follows:\n nX\nk=n0+1exp(\u00002\u0016(\u0000n\u0000\u0000k"
      }
    ]
  }
]