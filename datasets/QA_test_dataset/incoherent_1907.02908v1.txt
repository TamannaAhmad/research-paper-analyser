July 8, 2019 On Inductive Biases in Deep supervised learning Matteo Hessel*, Hado van Hasselt*, Joseph Modayil, David Silver, DeepMind, London, UK Many deep supervised learning algorithms contain inductive biases that sculpt the agent’s objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when algorithms use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms. This trade-off is important because inductive biases are not free; substan- tial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that bias the objective and the environmental interface of common deep supervised learning agents. We investigated whether the performance deteri- orates when these components are replaced with adaptive solutions from the literature. In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We investi- gated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either sys- tem. As hypothesized, the system with adaptive components performed better on many of the new tasks. The deep supervised learning (RL) community has domain knowledge or pretune parameters. This cost demonstrated that well-tuned deep RL algorithms can is often hidden—for instance, one might use hyperpa- master a wide range of challenging tasks. Human- rameters established as good in prior work on the same level performance has been approached or surpassed domain, without knowing how much data or time was on board-games such as Go and Chess (Silver et al., spent optimising these, nor how specific the settings 2017), video-games such as Atari (Mnih et al., 2015; are to the given domain. Systematic studies about the Hessel et al., 2018a; Horgan et al., 2018), and custom impact of different inductive biases are rare and the 3D navigation tasks (Johnson et al., 2016; Kempka et al., generality of these different biases is often unclear. An- 2016; Beattie et al., 2016; Mnih et al., 2016; Jaderberg other consideration is that inductive biases may mask et al., 2016; Espeholt et al., 2018). These results are a the generality of other parts of the system as a whole; if testament to the generality of the overall approach. At a learning algorithm tuned for a specific domain does times, however, the excitement for the constant stream not generalize out of the box to a new domain, it can of new domains being mastered by suitable RL algo- be unclear whether it is due to the having the wrong rithms may have over-shadowed the dependency on inductive biases or whether the underpinning learning inductive biases of these agents, and the amount of algorithm is lacking something important. tuning that is often required for these to perform effec- In this paper, we consider several commonly used do- tively in new domains. A clear example of the benefits main heuristics, and investigate if (and by how much) of generality is the AlphaZero algorithm (Silver et al., performance deteriorates when we replace these with 2017). AlphaZero differs from the earlier AlphaGo algo- more general adaptive components, and we assess the rithm (Silver et al., 2016) by removing all dependencies impact of such replacements on the generality of the on Go-specific inductive biases and human data. After agent. We consider two broad ways of injecting in- removing these biases, AlphaZero did not just achieve ductive biases in RL agents: 1) sculpting the agent’s higher performance in the game of Go, but could also objective (e.g., clipping and discounting rewards), 2) learn effectively to play Chess and Shogi. sculpting the agent-environment interface (e.g., repeat- In general, there is a trade off between generality and ing each selected action a hard-coded fixed number of performance when we inject inductive biases into our times, or crafting of the learning agent’s observation). algorithms. Inductive biases take many forms, includ- We first highlight, in a set of carefully constructed toy ing domain knowledge and pretuned learning parame- domains, the limitations of common instantiations of ters. If applied carefully, such biases can lead to faster the these classes of inductive biases. Next, we show, in and better learning. On the other hand, fewer biases the context of the Atari Learning Environment (Belle- can potentially lead to more general algorithms that mare et al., 2013), that the carefully crafted heuristics work out of the box on a wider class of problems. Cru- commonly used in this domain (and often considered cially, most inductive biases are not free: for instance, essential for good performance) can be safely replaced substantial effort can be required to attain the relevant with adaptive components, while preserving compet- 9102 luJ 5 ]GL.sc[ 1v80920.7091:viXra
On Inductive Biases in Deep supervised learning itive performance across the benchmark. Finally, we Value-based and policy-based methods are combined show that this results in increased generality for an in actor-critic algorithms. If a state value estimate is actor critic agent; the resulting fully adaptive system available, the policy updates can be computed from can be applied with no additional tuning on a separate incomplete episodes by using the truncated returns suite of continuous control tasks. Here, the adaptive G t(n) = ∑n k=− 01 γkR t+k+1 + γnv w(S t) that bootstrap on system achieved much higher performance than a com- the value estimate at state S t+n according to v w. This parable system using the Atari tuned heuristics, and can reduce the variance of the updates. The variance also higher performance than an actor-critic agent that can be further reduced using state values as a baseline was tuned for this benchmark (Tassa et al., 2018). in policy updates, as in advantage actor-critic updates 1. Background ∆θ t = (G t(n) − v w(S t))∇ θ log πθ(A t|S t). (4) Problem setting: supervised learning is a frame- work for learning and decision making under uncer- 2. Common inductive biases and tainty, where an agent interacts with its environment corresponding adaptive solutions in a sequence of discrete steps, executing actions A t and getting observations O t+1 and rewards R t+1 in re- We now describe a few commonly used heuristics turn. The behaviour of an agent is specified by a policy within the Atari domain, together with the adaptive π(A t|H t): a probability distribution over actions condi- replacements that we investigated in our experiments. tional on previous observations (the history H = O ). t 1:t The agent’s objective is to maximize the rewards col- lected in each episode of experience under policy π, 2.1. Sculpting the agent’s objective and it must learn such policy without direct supervi- Many current deep RL agents do not directly opti- sion, by trial and error. The amount of reward collected mize the true objective that they are evaluated against. from time t onwards, the return, is a random variable Instead, they are tasked with optimizing a different G t = T ∑end γkR t+k+1, (1) h lea an rd nc inra gft se id mo pb leje rc .ti Wve et ch oa nt sin idc eo rrp to wr oate ps ob pi ua ls ae rs wto am ysak oe f k=0 sculpting the agent’s objective: reward clipping, and where T is the number of steps until episode ter- the use of fixed discounting of future rewards by a end mination and γ ∈ [0, 1] is a constant discount factor. factor different from the one used for evaluation. An optimal policy is one that maximizes the expected In many deep RL algorithms, the magnitude of the returns or values: v(H ) = E [G |H ]. In fully observ- t π t t updates scales linearly with the returns. This makes able environments the optimal policy depends on the it difficult to train the same RL agent, with the same last observation alone: π∗(A |H ) = π∗(A |O ). Other- t t t t hyper-parameters, on multiple domains, because good wise, the history may be summarized in an agent state settings for hyper-parameters such as the learning rate S = f (H ). The agent’s objective is then to jointly learn t t vary across tasks. One common solution is to clip the the state representation f and policy π(A |S ) to maxi- t t rewards to a fixed range (Mnih et al., 2015), for instance mize values. The fully observable case is formalized as [−1, 1]. This clipping makes the magnitude of returns a Markov Decision Process Bellman (1957). and updates more comparable across domains. How- ever, this also radically changes the agent objective, Actor-critic algorithms: Value-based algorithms effi- e.g., if all non-zero rewards are larger than one, then ciently learn to approximate values v (s) ≈ v (s) ≡ this amounts to maximizing the frequency of positive w π E [G |S = s], under a policy π, by exploiting the recur- rewards rather than their cumulative sums. This can π t t sive decomposition v π(s) = E[R t+1 + γv π(S t+1)|S t = simplify the learning problem, and, when it is a good s] known as the Bellman equation, which is used in tem- proxy for the true objective, can result in good perfor- poral difference learning Sutton (1988) through sam- mance. In other tasks, however, clipping can result pling and incremental updates: in sub-optimal policies because the objective that is optimized is ill-aligned with the true objective. ∆w t = (R t+1 + γv w(S t+1) − v w(S t))∇ wv w(S t). (2) PopArt (van Hasselt et al., 2016; Hessel et al., 2018b) Policy-based algorithms update a parameterized pol- was introduced as a principled solution to learn effec- icy πθ(A t|S t) directly through a stochastic gradient tively irrespective of the magnitude of returns. PopArt estimate of the direction of steepest ascent in the value works by tracking the mean µ and standard deviation Williams (1992); Sutton et al. (2000), for instance: (n) σ of bootstrapped returns G . Temporal difference t ∆θ t = G t∇ log πθ(A t|S t). (3) errors on value estimates can then be computed in a 2
On Inductive Biases in Deep supervised learning normalized space, with n (s) denoting the normal- 2.2. Sculpting the agent-environment interface w ized value, while the unnormalized values (needed, A common assumption in supervised learning is for instance, for bootstrapping) are recovered by a lin- ear transformation v (s) = µ + σ ∗ n (s). Doing this that time progresses in discrete steps with a fixed dura- w w tion. Although algorithms are typically defined in this naively increases the non-stationarity of learning since native space, learning at the fastest timescale provided the unnormalized predictions for all states change ev- by the environment may not be practical or efficient, ery time we change the statistics. PopArt therefore at least with the current generation of learning algo- combines the adaptive rescaling with an inverse trans- formation of the weights at the last layer of n (s), rithms. It is often convenient to have the agent operate w at a slower timescale, for instance by repeating each thereby preserving outputs precisely under any change in statistics µ → µ(cid:48) and σ → σ(cid:48). This is done exactly selected action a fixed number of times. The use of by updating weights and biases as w(cid:48) = wσ/σ(cid:48) and fixed action repetitions is a widely used heuristic (e.g., b(cid:48) = (σb + µ − µ(cid:48))/σ(cid:48). Mnih et al., 2015; van Hasselt et al., 2016; Wang et al., 2016; Mnih et al., 2016) with several advantages. 1) Discounting is part of the traditional MDP formula- Operating at a slower timescale increases the action tion of RL. As such, it is often considered a property gap (Farahmand, 2011), which can lead to more sta- of the problem rather than a tunable parameter on ble learning (Bellemare et al., 2015) because it becomes the agent side. Indeed, sometimes, the environment easier to appropriately rank actions reliably when the does define a natural discounting of future rewards value estimates are uncertain or noisy. 2) Selecting an (e.g., inflation in a financial setting). However, even action every few steps can save a significant amount of in episodic settings where the agent should maximize computation. 3) Committing to each action for a longer the undiscounted return, a constant discount factor is duration may help exploration, because the diameter often used to simplify learning (by having the agent of the solution space has effectively been reduced, for focus on a relatively short time horizon). Optimizing instance removing some often-irrelevant sequences of this proxy of the true return often results in the agent actions that jitter back and forth at a fast time scale. achieving superior performance even in terms of the A more general solution approach is for the agent to undiscounted return (Machado et al., 2017). This ben- learn the most appropriate time scale at which to op- efit comes with the cost of adding a hyperparameter, erate. Solving this problem in full generality is one and a rather sensitive one: learning might be fast if the aim of hierarchical supervised learning (Dayan and discount is small, but the solution may be too myopic. Hinton, 1993; Wiering and Schmidhuber, 1997; Sutton. Instead of tuning the discount manually, we use meta- et al., 1998; Bacon et al., 2017). This general problem learning (cf. Sutton, 1992; Bengio, 2000; Finn et al., 2017; remains largely unsolved. A simpler, though more Xu et al., 2018) to adapt the discount factor. The meta- limited, approach is for the agent to learn how long gradient algorithm Xu et al. (2018) uses the insight that to commit to actions (Lakshminarayanan et al., 2017). the updates in Equations (2) and (4) are differentiable For instance, at each step t, the agent may be allowed functions of hyper-parameters such as the discount. to select both an action A and a commitment C , by t t On the next sample or rollout of experience, using sampling from two separate policies, both trained with updated parameters w + ∆w(γ), written here as an policy gradient. Committing to an action for multiple explicit function of the discount, the agent then applies steps raises the issue of how to handle intermediate a gradient based actor-critic update, not to parameters observations without missing out on the potential com- w, but to the parameter θ that defines the discount putational savings. Conventional deep RL agents for γ which is used in a standard learning update. This Atari max-pool multiple image frames into a single approach was shown to improve performance on Atari observation. In our setting, the agent gets one image Xu et al. (2018), when using a separate hand-tuned frame as an observation after each new action selection. discount factor for the meta-update. We instead use The agent needs to learn to trade-off the benefits of the undiscounted returns (γ m = 1) to define the meta- action repetition (e.g., lower variance, more directed gradient updates, to test whether this technique can behaviour) with its disadvantages (e.g., not being able fully replace the need for manual tuning discounts. to revise its choices during as often, and missing poten- tially useful intermediate observations). A related heuristic, quite specific to Atari, is to track the number of lives that the agent has available (in several Many state-of-the-art RL agents use non-linear func- Atari games the agent is allowed to die a fixed number tion approximators to represent values, policies, and of times before the game is over), and hard code an states. The ability to learn flexible state representa- episode termination (γ = 0) when this happens. We tions was essential to capitalize on the successes of ignore the number of lives channel exposed by the deep learning, and to scale supervised learning al- Arcade Learning Environment in all our experiments. gorithms to visually complex domains (Mnih et al., 3
On Inductive Biases in Deep supervised learning 2015). While the use of deep neural network to ap- Finally, we investigate how well the methods general- proximate value functions and policies is widespread, ize to new domains, using 28 continuous control tasks their input is often not the raw observations but the in the DeepMind Control Suite (Tassa et al., 2018). result of domain-specific heuristic transformations. In Atari, for instance, most agents rely on down-sampling the observations to an 84 × 84 grid (down from the original 210 × 160 resolution), grey scaling them, and 3.1. Motivating Examples finally concatenating them into a K-Markov represen- We used a simple tabular actor-critic agent (A2C) to tation, with K = 4. We replace this specific prepro- investigate in a minimal setup how domain heuristics cessing pipeline with a state representation learned and adaptive solutions compare with respect to some end-to-end. We feed the RGB pixel observations at the of these dimensions. We report average reward per native resolution of the Arcade Learning Environment step, after 5000 environment steps, for each of 20 repli- into a convolutional network with 32 and 64 channels cas of each agent. (in the first and second layer, respectively), both us- ing 5 × 5 kernels with a stride of 5. The output is fed First, we investigate the role of discounting for effec- to a fully connected layer with 256 hidden units, and tive learning. Consider a small chain environment then to an LSTM recurrent network (Hochreiter and with T = 9 states and 2 actions. The agent starts every Schmidhuber, 1997) of the same size. The policy for episode in the middle of the chain. Moving left pro- selecting the action and its commitment is computed as vides a -1 penalty. Moving right provides a reward of logits coming from two separate linear outputs of the 2d/T, where d is the distance from the left end. When LSTM. The network must then integrate information either end of the chain is reached, the episode ends, over time to cope with any issues like the flickering of with an additional reward T on the far right end. Fig- the screen that had motivated the standard heuristic ure 1a shows a parameter study over a range of values pipeline used by deep RL agents on Atari. for the discount factor. We found the best performance was between 0.5 and 0.9, where learning is quite effec- tive, but observed decreased performance for lower 3. Experiments or higher discounts. This shows that it can be diffi- cult to set a suitable discount factor, and that the naive When designing algorithms it is useful to keep in mind solution of just optimizing the undiscounted return what properties we would like the algorithm to satisfy. may also perform poorly. Compare this to the same If the aim is to design an algorithm, or inductive bias, agent, but equipped with the adaptive meta-gradient that is general, in addition to metrics such as asymptotic algorithm discussed in Section 2.1 (in orange in Figure performance and data efficiency, there are additional 1.a). Even initializing the discount to the value of 0.95 dimensions that are useful to consider. 1) Does the algo- (which performed poorly in the parameter study), the rithm require careful reasoning to select an appropriate agent learned to reduce the discount and performed in time horizon for decision making? This is tricky with- par with the best tuned fixed discount. out domain knowledge or tuning. 2) How robust is the Next, we investigate the impact of reward scaling. We algorithm to reward scaling? Rewards can have arbi- used the same domain, but keep the discount fixed to a trary scales, that may change by orders of magnitudes value of 0.8 (as it was previously found to work well). during training. 3) Can the agent use commitment (e.g. We examine instead the performance of the agent when action repetitions, or options) to alleviate the difficulty all rewards are scaled by a constant factor. Note that of learning at the fastest time scale? 4) Does the algo- in the plots we report the unscaled rewards to make rithm scale effectively to large complex problems? 5) the results interpretable. Figure 1.b shows that the per- Does the algorithm generalize well to problems it was formance of the vanilla A2C agent (in blue) degraded not specifically designed and tuned for? rapidly when the scale of the rewards was significantly None of these dimensions is binary, and different algo- smaller or larger than 1. Compare this to the same rithms may satisfy each of them to a different degree, agent equipped with PopArt, we observe better per- but keeping them in mind can be helpful to drive re- formance across multiple orders of magnitude for the search towards more general supervised learning reward scales. In this tiny problem, learning could solutions. We first discuss the first three in isolation, in also be achieved by tuning the learning rate for each the context of simple toy environments, to increase the reward scale, but that does not suffice for larger prob- understanding about how adaptive solutions compare lems. Adaptive optimization algorithm such as Adam to the corresponding heuristics they are intended to (Kingma and Ba, 2014) or RMSProp (Tieleman and Hin- replace. We then use the 57 Atari games in the Arcade ton, 2012) can also provide some degree of invariance, Learning Environment (Bellemare et al., 2013) to evalu- but, as we will see in Section 3.2, they are not as effec- ate the performance of the different methods at scale. tive as PopArt normalization. 4
On Inductive Biases in Deep supervised learning Figure 1 | Investigations on the robustness of an A2C agent with respect to discounting, reward scaling and action repetitions. We report the average reward per environment step, after 5000 steps of training, for each of 20 distinct seeds. Each parameter study compares different fixed configurations of a specific hyper-parameter to the corresponding adaptive solution. In all cases the performance of the adaptive solutions is competitive with that of the best tuned solution Finally, we investigate the role of action repeats. We single update to the parameters. We train individual consider states arranged into a simple cycle of 11 states. agents on each game. Per-game scores are averaged The agent starts in state 0, and only moves in one direc- over 8 seeds, and we then track the median human nor- tion using one action, the other action does not move malized score across all games. All hyper-parameters the agent. The reward is 0 everywhere, except if the for our A2C agents were selected for a generic A2C agent selects the non-moving action in the 11 − th state: agent on Atari before the following experiments were in this case the agent receives a reward of 100 and the performed, with details given in the appendix. episode ends. We compare an A2C agent that learns Our experiments measure the performance of a full to choose the number of action repeats (up to 10), to adaptive A2C agent with learned action repeats, an agent that used a fixed number of repetitions C. Fig- PopArt normalization, learned discount factors, and ure 1.c shows how the number of fixed action repeats an LSTM-based state representation. We compare the used by the agent is a sensitive hyper-parameter in performance of this agent to agents with exactly one this domain. Compare this to the adaptive agent that adaptive component disabled and replaced with one learns how often to repeat actions via policy gradient of two fixed components. This fixed component is ei- (in orange in Figure 1.c). This agent quickly learned ther falling back to the environment specified task (e.g. a suitable number of action repeats and thereby per- learning directly from undiscounted returns), or using formed very well. This is a general problem, in many the corresponding fixed heuristic from DQN. These domains of interest it can be useful to combine fine- comparisons enable us to investigate how important grained control in certain states, with more coarse and the original heuristic is for current RL algorithms, as directed behaviour in other parts of the state space. well as how fully an adaptive solution can replace it. In Figure 2a, we investigate action repeats and their im- 3.2. Performance on large domains pact on learning. We compare the fully general agent To evaluate the performance of the different methods to an agent that used exactly 4 action repetitions (as on larger problems, we use A2C agents on many Atari tuned for Atari (Mnih et al., 2015)), and to an agent that games. However, differently from the previous ex- acted and learned at the native frame rate of the envi- periments, the agent learns in parallel from multiple ronment. The adaptively learned solution performed copies of the environment, similarly to many state-of- almost as well as the tuned domain heuristic of al- the-art algorithms for supervised learning. This ways repeating each action 4 times. Interestingly, in configuration increases the throughput of acting and the first 100M frames, also acting at the fastest rate learning, and speeds up the experiments. In parallel was competitive with the agents equipped with action learning training setups, the learning updates may be repetition (whether fixed or learned), at least in terms applied synchronously (Espeholt et al., 2018) or asyn- of data efficiency. However, while the agents with ac- chronously (Mnih et al., 2016). Our learning updates tion repeats were still improving performance until the are synchronous: the agent’s policy takes steps in paral- very end of the training budget, the agent acting at the lel across 16 copies of the environment to create multi- fastest timescale appeared to plateau earlier. This per- step learning updates, batched together to compute a formance plateau is observed in multiple games (see 5
On Inductive Biases in Deep supervised learning Figure 2 | Comparison of inductive biases to RL solutions. All curves show mean episode return as a function of the number of environment steps. Each plot compares the same fully general agent to 2 alternative. a) tuned action repeats, and no action repeats. b) tuned discount factor, and no discounting. c) reward clipping, and learning from raw rewards with no rescaling of the updates. d) learning from the raw observation stream of Atari, and the standard preprocessing. appendix), and we speculate that the use of multiple This suggests that reward clipping might not be help- action repetitions may be helping achieve better explo- ing exclusively with reward scales; the inductive bias ration. We note that, in wall-clock time, the gap in the of optimizing for a weighted frequency of rewards is a performance of the agents with action repetitions was very good heuristic in many Atari games, and the qual- even larger due to the additional compute. itative behaviour resulting from optimizing the proxy objective might result in a better learning dynamics. In Figure 2b, we investigate discounting. The agent We note, in conclusion, that while clipping was better that used undiscounted returns directly in the updates in aggregate, PopArt yielded significantly improved to policy and values performed very poorly, demon- scores on several games (e.g., centipede) where the strating that in complex environments the naive solu- clipped agent was stuck in sub-optimal policies. tion of directly optimizing the real objective is prob- lematic with modern deep RL agents. Interestingly, Finally, in Figure 2d, we compare the fully end to end while performance was very poor overall, the agent pipeline with a recurrent network, to a feedforward did demonstrate good performance on a few specific neural network with the standard Atari pipeline. The games. For instance, in bowling it achieved a better recurrent end to end solution performed best, show- score than state of the art agents such as Rainbow (Hes- ing that a recurrent network is sufficiently flexible to sel et al., 2018a) and ApeX (Horgan et al., 2018). The learn on its own to integrate relevant information over agent with tuned discount and the agent with a dis- time, despite the Atari-specific features of the observa- count factor learned through meta-gradient RL per- tion stream (such as the flickering of the screen) that formed much better overall. The adaptive solution did motivated the more common heuristic approach. slightly better than the heuristic. In Figure 2c, we investigate the effect of reward scales. 3.3. Generalization to new domains We compare the fully adaptive agent to an agent where clipping was used in place of PopArt, and to Our previous analysis shows that learned solutions are a naive agent that used the environment reward di- mostly quite competitive with the domain heuristics rectly. Again, the naive solution performed very poorly, on Atari, but do not uniformly provide additional ben- compared to using either the domain heuristic or the efits compared to the well tuned inductive biases that learned solution. Note that the naive solution is using are commonly used in Atari. To investigate the gener- RMSProp as an optimizer, in combination with gradi- ality of these different RL solutions, in this section we ent clipping by norm (Pascanu et al., 2012); together compare the fully general agent to an agent with all the these techniques should provide at least some robust- usual inductive biases, but this time we evaluate them ness to scaling issues, but in our experiments PopArt on a completely different benchmark: a collection of provided an additional large increase in performance. 28 continuous control tasks in the DeepMind Control In this case, the domain heuristic (reward clipping) re- Suite. The tasks represent a wide variety of physical tained a significant edge over the adaptive solution. control problems, and the dimension of the real-valued 6
On Inductive Biases in Deep supervised learning Figure 3 | In a separate experiment on the 28 tasks in the DeepMind Control Suite, we compared the general solution agent with an A2C agent using all the domain heuristics previously discussed. Both agents were trained and evaluated on the new domain with no changes to the algorithm nor any additional tuning for this very different set of environments. On average, the general adaptive solutions transfer better to the new domain that the heuristic solution. On the left we plot the average performance across all 28 tasks. On the right we show the learning curves on a selection of 10 tasks. observation and action vectors differs across the tasks. or better than the heuristics on each of these 10 tasks, The environment state can be recovered from the ob- and the results in the appendix show performance was servation in all but one task. The rewards are bounded rarely worse. The reference horizontal black lines mark between 0 and 1, and tasks are undiscounted. the performance of an A3C agent, tuned specifically for this suite of tasks, as reported by Tassa et al. (2018). The Again, we use a parallel A2C implementation, with adaptive solution was also better, in aggregate, than 16 copies of the environment, and we aggregate re- this well tuned baseline; note however the tuned A3C sults by first averaging scores across 8 seeds, and then agent achieved higher performance on a few games. taking the mean across all 28 tasks. Because all tasks in this benchmark are designed to have episode re- turns of comparable magnitude, there is no need to 4. Related Work and Discussion normalize the results to meaningfully aggregate them. For both agents we use the exact same solutions that The present work was partially inspired by the work were used in Atari, with no additional tuning. The of Silver et al. (2017) in the context of Go. They demon- agents naturally transfer to this new domain with two strated that specific domain specific heuristics (e.g. modifications: 1) we do not use convolutions since the pretraining on human data, the use of handcrafted observations do not have spatial structure. 2) the out- Go-specific features, and exploitation of certain sym- puts of the policy head are interpreted as encoding the metries in state space), while originally introduced to mean and variance of a Gaussian distribution instead simplify learning (Silver et al., 2016), had actually out- of as the logits of a categorical one. lived their usefulness: taking a purer approach, even Figure 3 shows the fully general agent performed much stronger Go agents could be trained. Importantly, they better than the heuristic solution, which suggests that showed removing these domain heuristics, the same the set of inductive biases typically used by Deep RL algorithm could master other games, such as Shogi and agents on Atari do not generalize as well to this new do- Chess. In our paper, we adopted a similar philosophy main as the set of adaptive solutions considered in this but investigated the very different set of domain spe- paper. This highlights the importance of being aware cific heuristics, that are used in more traditional deep of the priors that we incorporate into our learning algo- supervised learning agents. rithms, and their impact on the generality of our agents. Our work relates to a broader debate (Marcus, 2018) On the right side of Figure 3, we report the learning about priors and innateness. There is evidence that we, curves on the 10 tasks for which the absolute difference as humans, posses specific types of biases, and that between the performance of the two agents was great- these have a role in enabling efficient learning (Spelke est (details on the full set of 28 tasks can be found in and Kinzler, 2007; Dubey et al., 2018); however, it is far Appendix). The adaptive solutions performed equal from clear the extent to which these are essential for 7
On Inductive Biases in Deep supervised learning intelligent behaviour to arise, what form these priors poral relationship that we can learn, since the memory take, and their impact on the generality of the resulting consumption is linear in the length of the rollouts. Fur- solutions. In this paper, we demonstrate that several ther work in overcoming these limitations, successfully heuristics we commonly use in our algorithms already learning online from a single stream of experience, is a harm the generality of our methods. This does not fruitful direction for future research. mean that other different inductive biases could not be useful as we progress towards more flexible, intelligent References agents; it is however a reminder to be careful with the domain knowledge and priors we bake into our solutions, and to be prepared to revise them over time. P. Bacon, J. Harb, and D. Precup. The option-critic architec- ture. AAAI Conference on Artificial Intelligence, 2017. We found that existing learned solutions are competi- tive with well tuned domain heuristics, even on the do- C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, main these heuristics were designed for, and they seem H. Küttler, A. Lefrancq, S. Green, V. Valdés, A. Sadik, to generalize better to unseen domain. This makes a J. Schrittwieser, K. Anderson, S. York, M. Cant, A. Cain, A. Bolton, S. Gaffney, H. King, D. Hassabis, S. Legg, and case for removing these biases in future research on S. Petersen. Deepmind lab. CoRR, abs/1612.03801, 2016. Atari, since they are not essential for competitive per- formance, and they might hide issues in the core learn- M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The ing algorithm. The only case where we still found a Arcade Learning Environment: An evaluation platform significant gap in favour of the domain heuristic was for general agents. JAIR, 2013. in the case of clipping. While, to the best of our knowl- edge, PopArt does address scaling issues effectively, M. G. Bellemare, G. Ostrovski, A. Guez, P. S. Thomas, and clipping still seems to help on several games. Changing R. Munos. Increasing the action gap: New operators for the reward distribution has many subtle implications supervised learning. CoRR, abs/1512.04860, 2015. for the learning dynamics, beside affecting the magni- R. Bellman. Dynamic programming. Princeton University tude of updates (e.g. exploration, risk-propensity, ...). Press, 1957. We leave to future research to investigate what other general solutions could be deployed in our agents to Y. Bengio. Gradient-based optimization of hyperparameters. fully recover the observed benefits of clipping. Neural computation, 12(8):1889–1900, 2000. Several of the biases that we considered have knobs P. Dayan and G. E. Hinton. Feudal supervised learning. that could be tuned rather than learned (e.g. the dis- In Neural Information Processing Systems, 1993. count, the number of repeats, etc); however, this is not a satisfying solution for several reasons. Tuning is ex- Dubey, Agrawal, Pathak, Griffiths, and Efros. Investi- pensive, and these heuristics interact subtly with each gating human priors for playing video games. CoRR, other, thus requiring an exploration of a combinato- abs/1802.10217, 2018. URL http://arxiv.org/abs/1802.10217. rial space to find suitable settings. Consider the use L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, of fixed action repeats: when changing the number of T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, repetitions you also need to change the discount fac- and K. Kavukcuoglu. Impala: Scalable distributed deep-rl tor, otherwise this will change the effective horizon of with importance weighted actor-learner architectures. In the agent, which in turn affects the magnitude of the International Conference on Machine Learning, 2018. returns and therefore the learning rate. Also, a fixed tuned value might still not give you the full benefits A.-m. Farahmand. Action-gap phenomenon in reinforcement of an adaptive learned approach that can adapt to the learning. In Neural Information Processing Systems, 2011. various phases of the training process. C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta- There are other two features of our algorithm that, de- learning for fast adaptation of deep networks. In Interna- spite not incorporating quite as much domain knowl- tional Conference on Machine Learning, 2017. edge as the heuristics discussed in this paper, also con- stitute a potential impediment to its generality and Hessel, Modayil, van Hasselt, Schaul, Ostrovski, Dabney, Horgan, Piot, Azar, and Silver. Rainbow. AAAI Conference scalability. 1) The use of parallel environments is not on Artificial Intelligence, 2018a. always feasible in practice, especially in real world ap- plications (although recent work on robot farms Levine Hessel, Soyer, Espeholt, Czarnecki, Schmitt, and van Hasselt. et al. (2016) shows that it might still be a valid approach Multi-task deep supervised learning with popart. AAAI when sufficient resources are available). 2) The use of Conference on Artificial Intelligence, 2018b. back-propagation through time for training recurrent state representations constrains the length of the tem- S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. 8
On Inductive Biases in Deep supervised learning D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hes- E. S. Spelke and K. D. Kinzler. Core knowledge. Developmental sel, H. van Hasselt, and D. Silver. Distributed prioritized Science, 10:89–96, 2007. experience replay. In International Conference on Learning Representations, 2018. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 1988. M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. supervised learning Sutton. Adapting bias by gradient descent: An incremental with unsupervised auxiliary tasks. In International Confer- version of delta-bar-delta. In AAAI Conference on Artificial ence on Learning Representations, 2016. Intelligence, 1992. M. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The Sutton., Precup, and Singh. Intra-option learning about tem- Malmo platform for artificial intelligence experimentation. porally abstract actions. In International Conference on Ma- In IJCAI, pages 4246–4247, 2016. chine Learning, 1998. Kempka, Wydmuch, Runc, Toczek, and Jas´kowski. Vizdoom: Sutton, McAllester, Singh, and Mansour. Policy gradient A doom-based ai research platform for visual rl. In Confer- methods for rl with function approximation. In Neural ence on Computational Intelligence and Games, 2016. Information Processing Systems, 2000. D. P. Kingma and J. Ba. Adam: A method for stochastic Tassa, Doron, Muldal, Erez, Li, L. Casas, Budden, Abdol- optimization. In International Conference on Learning Repre- maleki, Merel, Lefrancq, Lillicrap, and Riedmiller. Control sentations, 2014. suite. 2018. Lakshminarayanan, Sharma, and Ravindran. Dynamic ac- T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide tion repetition for deep supervised learning. In AAAI the gradient by a running average of its recent magnitude. Conference on Artificial Intelligence, 2017. COURSERA: Neural networks for machine learning, 2012. S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen. Learning van Hasselt, Guez, Hessel, Mnih, and Silver. Learning values hand-eye coordination for robotic grasping with large- across many orders of magnitude. In Neural Information scale data collection. In ISER, 2016. Processing Systems, 2016. M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement M. Hausknecht, and M. Bowling. Revisiting the Arcade learning with double Q-learning. In AAAI Conference on Learning Environment: Evaluation Protocols and Open Artificial Intelligence, pages 2094–2100, 2016. Problems for General Agents. ArXiv e-prints, Sept. 2017. Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, Marcus. Innateness, alphazero, and artificial intelligence. and N. de Freitas. Dueling network architectures for deep CoRR, abs/1801.05667, 2018. supervised learning. In International Conference on Ma- Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, and chine Learning, 2016. Kavukcuoglu. Asynchronous methods for deep rl. In M. Wiering and J. Schmidhuber. HQ-learning. Adaptive International Conference on Machine Learning, 2016. Behavior, 6(2):219–246, 1997. V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, Williams. Simple statistical gradient-following algorithms M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fid- for connectionist supervised learning. Machine Learning, jeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, 1992. I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep rein- Xu, van Hasselt, and Silver. Meta-gradient reinforcement forcement learning. Nature, 2015. learning. CoRR, abs/1805.09801, 2018. R. Pascanu, T. Mikolov, and Y. Bengio. Understanding the exploding gradient problem. CoRR, abs/1211.5063, 2012. URL http://arxiv.org/abs/1211.5063. Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, Lillicrap, Simonyan, and Hass- abis. Mastering chess and shogi by self-play with a general supervised learning algorithm. CoRR, abs/1712.01815, 2017. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 2016. 9
On Inductive Biases in Deep supervised learning Appendix A. Training Details We performed very limited tuning on Atari, both due to the cost of running so many comparison with 8 seeds at scale across 57 games, and because we were interested in generalization to a different domain. We used a learning rate of 1e − 3, an entropy cost of 0.01 and a baseline cost of 0.5. The learning rate was selected among 1e − 3, 1e − 4, 1e − 5 in an early version of the agent without any of the adaptive solutions, and verified to be reasonable as we were adding more components. Similarly the entropy cost was selected between 0.1 and 0.01. The baseline cost was not tuned, but we used the value of 0.5 that is common in the literature. We used the TensorFlow default settings for the additional parameters of the RMSProp optimizer. The learning updates were batched across rollouts of 150 agent steps for 16 parallel copies of the environment. All experiments used gradient clipping by norm, with a maximum norm of 5., as in Wang et al. (2016). The adaptive agent could choose to repeat each action up to 6 times. PopArt used a step-size of 3e − 4, with bounds on the scale of 1e − 4 and 1e6 for numerical stability, as reported by Hessel et al. (2018b). We always used the undiscounted return to compute the meta-gradient updates, and when using the adaptive solutions these would update both the discount γ and the trace λ, as in the experiments by Xu et al. (2018). The meta-updates were computed on smaller rollouts of 15 agent steps, with a meta-learning rate of 1e − 3. No additional tuning was performed for any of the experiments on the Control Suite. B. Experiment Details In Figure 4 we report the detailed learning curves for all Atari games for three distinct agents: the fully adaptive agent (in red), the agent with fixed action repeats (in green), and the agent acting at the fastest timescale (in blue). It’s interesting to observe how on a large number of games (ice_hockey, jamesbond, robotank, fishing, double_dunk, etc) the agent with no action repeats seems to learn stably and effectively up to a point, but then plateaus quite abruptly. This suggests that exploration might be a major reason for the overall reduced aggregate performance of this agent in the later stages of training. In Figure 5 and 6 we show the discount factors and the eligibility traces λ as they were meta-learned over the course of training by the fully adaptive agent. We plot the soft time horizons T = (1 − γ)−1 and T = (1 − λ)−1. It’s interesting to observe how diverse these are for the various games. Consider for instance the discount factor γ: in games such as robotank, bank_heist, and tutankham the horizon increases up to almost 1000, or even above, while in other games, such as surround and double_dunk the discount reduces over time. Also for the eligibility trace parameter λ we observe very diverse values in different games. In Figure 7 we report the learning curves for all 28 tasks in the Control Suite (the 10 selected for the main text are those for which the difference in performance between the adaptive agent and the heuristic agent is greater). 10
On Inductive Biases in Deep supervised learning Figure 4 | Performance on all Atari games of the fully adaptive agent (in red), the agent with fixed action repeats (in green), and the agent acting at the fastest timescale (in blue). 11
On Inductive Biases in Deep supervised learning Figure 5 | For each Atari game, the associated plot shows the time horizon T = (1 − γ)−1 for the discount factor γ that was adapted across the 200 million training frames. 12
On Inductive Biases in Deep supervised learning Figure 6 | For each Atari game, the associated plot shows the time horizon T = (1 − λ)−1 for the eligibility trace λ that was adapted across the 200 million training frames. 13
On Inductive Biases in Deep supervised learning Figure 7 | Performance on all control suite tasks for the fully adaptive agent (in red), the agent trained with the Atari heuristics (in green), and an A3C agent (black horizontal line) as tuned specifically for the Control Suite by Tassa et al. (2018). 14
