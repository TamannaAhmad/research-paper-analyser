Low-Variance and Zero-Variance Baselines for Extensive-Form Games Trevor Davis,1† Martin Schmid,2 Michael Bowling,2,1 1Department of Computing Science, University of Alberta 2DeepMind trdavis1@ualberta.ca, {mschmid, bowlingm}@google.com † Work done during an internship at DeepMind. Abstract Extensive-form games (EFGs) are a common model of multi-agent interactions with imperfect information. State-of-the-art algorithms for solving these games typically perform full walks of the game tree that can prove prohibitively slow in large games. Alternatively, sampling-based methods such as Monte Carlo Counterfactual Regret Minimization walk one or more trajectories through the tree, touching only a fraction of the nodes on each iteration, at the expense of requiring more iterations to converge due to the variance of sampled values. In this paper, we extend recent work that uses baseline estimates to reduce this variance. We introduce a framework of baseline-corrected values in EFGs that generalizes the previous work. Within our framework, we propose new baseline functions that result in significantly reduced variance compared to existing techniques. We show that one particular choice of such a function — predictive baseline — is provably optimal under certain sampling schemes. This allows for efficient computation of zero-variance value estimates even along sampled trajectories. 1 Introduction Multi-agent strategic interactions are often modeled as extensive-form games (EFGs), a game tree representation that allows for hidden information, stochastic outcomes, and sequential interactions. Research on solving EFGs has been driven by the experimental domain of poker games, in which the Counterfactual Regret Minimization (CFR) algorithm [32] has been the basis of several breakthroughs. Approaches incorporating CFR have been used to essentially solve one nontrivial poker game [2], and to beat human professionals in another [19, 4]. CFR is in essence a policy improvement algorithm that iteratively evaluates and improves a strategy for playing an EFG. As part of this process, it must walk the entire game tree on every iteration. However, many games have prohibitively large trees when represented as EFGs. For example, many commonly played poker games have more possible game states than there are atoms in the universe [14]. In such cases, performing even a single iteration of traditional CFR is impossible. The prohibitive cost of CFR iterations is the motivation for Monte Carlo Counterfactual Regret Minimization (MCCFR), which samples trajectories to walk through the tree to allow for significantly faster iterations [17]. Additionally, while CFR spends equal time updating every game state, the sampling scheme of MCCFR can be altered to target updates to parts of the game that are more critical or more difficult to learn [10, 9]. As a trade-off for these benefits, MCCFR requires more iterations to converge due to the variance of sampled values. In the supervised learning (RL) community, the topic of variance reduction in sampling al- gorithms has been extensively studied. In particular, baseline functions that estimate state values Preprint. Under review. 9102 luJ 22 ]TG.sc[ 1v33690.7091:viXra
are typically used within policy gradient methods to decrease the variance of value estimates along sampled trajectories [29, 11, 1, 22]. Recent work by Schmid et al. [21] has adapted these ideas to variance reduction in MCCFR, resulting in the VR-MCCFR algorithm. In this work, we generalize and extend the ideas of Schmid et al. We introduce a framework for variance reduction of sampled values in EFGs by use of state-action baseline functions. We show that the VR-MCCFR is a specific application of our baseline framework that unnecessarily generalizes across dissimilar states. We introduce alternative baseline functions that take advantage of our access to the full hidden state during training, avoiding this generalization. Empirically, our new baselines result in significantly reduced variance and faster convergence than VR-MCCFR. Schmid et al. also discuss the idea of an oracle baseline that provably minimizes variance, but is impractical to compute. We introduce a predictive baseline that estimates this oracle value and can be efficiently computed. We show that under certain sampling schemes, the predictive baseline exactly tracks the true oracle value, thus provably computing zero-variance sampled values. For the first time, this allows for exact CFR updates to be performed along sampled trajectories. 2 Background An extensive-form game (EFG) [20] is a game tree, formally defined by a tuple (cid:104)N, H, P, σ , u, I(cid:105). c N is a finite set of players. H is a set of histories, where each history is a sequence of actions and corresponds to a vertex of the tree. For h, h(cid:48) ∈ H, we write h (cid:118) h(cid:48) if h is a prefix of h(cid:48). The set of actions available at h ∈ H that lead to a successor history (ha) ∈ H is denoted A(h). Histories with no successors are terminal histories Z ⊆ H. P : H \ Z → N ∪ {c} maps each history to the player that chooses the next action, where c is the chance player that acts according to the defined distribution σ (h) ∈ ∆ , where ∆ is the set of probability distributions over A(h). The utility c A(h) A(h) function u : N × Z → R assigns a value to each terminal history for each player. For each player i ∈ N , the collection of (augmented) information sets I ∈ I is a partition of i the histories H.a Player i does not observe the true history h, but only the information set I (h). i Necessarily, this means that A(h) = A(h(cid:48)) if I (h) = I (h(cid:48)), which we then denote A(I). P (h) P (h) Each player selects actions according to a (behavioral) strategy that maps each information set I ∈ I i where P (I) = i to a distribution over actions, σ (I) ∈ ∆ . The probability of taking a specific i A(I) action at a history is σ (h, a) = σ (I(h), a). A strategy profile, σ = {σ |i ∈ N }, specifies P (h) P (h) i a strategy for each player. The reach probability of a history h is πσ(h) = (cid:81) σ (h(cid:48), a). (h(cid:48)a)(cid:118)h P (h(cid:48)) This product can be decomposed as πσ(h) = πσi(h)πσ−i(h), where the first term contains the i −i actions of player i, and the second contains the actions of other players and chance. We also write πσ(h, h(cid:48)) for the probability of reaching h(cid:48) from h, defined to be 0 if h (cid:54)(cid:118) h(cid:48). A strategy profile defines an expected utility for each player as u (σ) = u (σ , σ ) = (cid:80) πσ(z)u (z). i i i −i z∈Z i In this work, we consider two-player zero-sum EFGs, in which N = {1, 2} and u(z) := u (z) = i −u (z). We also assume that the information sets satisfy perfect recall, which requires that players −i not forget any information that they once observed. Mathematically, this means that two histories in the same information set I must have the same sequence of past information sets and actions for i player i. All games played by humans exhibit perfect recall, and solving games without perfect recall is NP-hard. We write I (cid:118) h if there is any history h(cid:48) ∈ I such that h(cid:48) (cid:118) h, and we denote that i i history (unique by perfect recall) by I [h]. i 2.1 Solving EFGs A common solution concept for EFGs is a Nash equilibrium, in which no player has incentive to deviate from their specified strategy. We evaluate strategy profiles by their distance from equilibrium, as measured by exploitability, which is the average expected loss against a worst-case opponent: exploit(σ) = 1/2 max σ(cid:48)∈Σ(u 2(σ 1, σ 2(cid:48) ) + u 1(σ 1(cid:48) , σ 2)). Counterfactual Regret Minimization (CFR) is an algorithm for learning Nash equilibria in EFGs through iterative self play [32]. For any h ∈ H, let Z[h] = {z ∈ Z | h (cid:118) z} be the set of terminal his- tories reachable from h, and define the history’s expected utility as u(h|σ) = (cid:80) πσ(h, z)u(z). z∈Z[h] aAugmented information sets were introduced by Burch et al. [6]. 2
For each information set I and action a ∈ A(I), CFR accumulates the counterfactual regret of not choosing that action on previous iterations: T rt(I, a) = (cid:88) πσt (h) (u((ha)|σ) − u(h|σ)) RT (I, a) = (cid:88) rt(I, a) (1) −P (h) h∈I t=1 The next strategy profile is then selected with regret matching, which sets probabilities proportional to the positive regrets: σT +1(I, a) ∝ max(RT (I, a), 0). Defining the average strategy σT such that σT (h, a) ∝ (cid:80)T πσt(h)σt(h, a), CFR guarantees that exploit(σT ) → 0 as T → ∞, thus t=1 i i converging to a Nash equilibrium. The state-of-the-art CFR+ variant of CFR greedily zeroes all negative regrets on every itera- tion, replacing Rt with an accumulant Qt recursively defined with Q0(I, a) = 0, Qt(I, a) = max(Qt−1(I, a) + rt(I, a), 0) [25]. It also alternates updates for each player, and uses linear averaging, which gives greater weight to more recent strategies. CFR(+) requires a full walk of the game tree on each iteration, which can be a very costly operation on large games. Monte Carlo Counterfactual Regret Minimization (MCCFR) avoids this cost by only updating along sampled trajectories. For simplicity, we focus on the outcome sampling (OS) variant of MCCFR [17], though all results in this paper can be trivially extended to other MCCFR variants. On each iteration t, a sampling strategy qt ∈ Σ is used to sample a single terminal history zt ∼ πqt. A sampled utility is then calculated recursively for each prefix of zt as 1((ha) (cid:118) zt) (cid:88) uˆ(h, a|σt, zt) = uˆ((ha)|σt, zt) uˆ(h|σt, zt) = σt(h, a)uˆ(h, a|σt, zt) qt(h, a) a∈A(h) (2) where 1 is the indicator function and uˆ(zt|σt, zt) = u(zt). For any h (cid:118) zt, the sampled value uˆ(h, a|σt, zt) is an unbiased estimate of the expected utility u((ha)|σt), whether a is sampled or not. These sampled values are used to calculate a sample of the counterfactual regret: πσt (h) rˆt(I, a|zt) = (cid:88) −P (h) (cid:0) uˆ(h, a|σt, zt) − uˆ(h|σt, zt)(cid:1) (3) πqt(h) h∈I This gives an unbiased sample of the counterfactual regret rt(I, a) for all I ∈ I, which is then used to perform unbiased CFR updates. As long as as the sampling strategies satisfy πqt(z) > 0 for all z ∈ Z, MCCFR guarantees that exploit(σT ) → 0 with high probability as T → ∞, thus converging to a Nash equilibrium. However, the rate of convergence depends on the variance of rˆt(I, a|zt) [10]. 3 Baseline framework for EFGs We now introduce a method for calculating unbiased estimates of utilities in EFGs that has lower variance than the sampled utilities uˆ(h, a|σt, zt) defined above. We do this using baseline functions, which estimate the expected utility of actions in the game. We will describe specific examples of such functions in Section 4; for now, we assume the existence of some function bt : H × A → R such that bt(h, a) in some way approximates u((ha)|σt). We define a baseline-corrected sampled utility as uˆ (h, a|σt, zt) = 1((ha) (cid:118) zt) (cid:0) uˆ ((ha)|σt, zt) − bt(h, a)(cid:1) + bt(h, a) (4) b qt(h, a) b (cid:88) uˆ (h|σt, zt) = σt(h, a)uˆ (h, a|σt, zt) (5) b b a∈A(h) Equation (4) comes from the application of a control variate, in which we lower the variance of a random variable (X = 1((ha)(cid:118)zt) uˆ ((ha)|σt, zt)) by subtracting another random variable qt(h,a) b (Y = 1((ha)(cid:118)zt) bt(h, a)) and adding its known expectation (E [Y ] = bt(h, a)), thus keeping the qt(h,a) resulting estimate unbiased. If X and Y are correlated, then this estimate will have lower variance than X itself. Because uˆ ((ha)|σt, zt) is defined recursively, its computation includes the application b of independent control variates at every action taken between h and zt. These estimates are unbiased and, if the baseline function is chosen well, have low variance: 3
Theorem 1. For any h (cid:118) zt and any a ∈ A(h), the baseline-corrected utilities satisfy E (cid:2) uˆ (h, a|σt, zt)|zt (cid:119) h(cid:3) = u((ha)|σt) E (cid:2) uˆ (h|σt, zt)|zt (cid:119) h(cid:3) = u(h|σt) zt b zt b Theorem 2. Assume that we have a baseline that satisfies bt(h, t) = u((ha)|σt) for all h ∈ H, a ∈ A(h). Then for any h, a, zt, Var (cid:2) uˆ (h, a|σt, zt)|zt (cid:119) h(cid:3) = 0 zt b All proofs are given in the appendix. Theorem 1 show that we can use uˆ (h, a|σt, zt) in place of b uˆ(h, a|σt, zt) in equation 3 and maintain the convergence guarantees of MCCFR. Theorem 2 shows that an ideal baseline eliminates all variance in the MCCFR update. By choosing our baseline well, we decrease the MCCFR variance and speed up its convergence. Pseudocode for MCCFR with baseline-corrected values is given in Appendix A. Although we focus on using our baseline-corrected samples in MCCFR, nothing in the value definition is particular to that algorithm. In fact, a lower variance estimate of sampled utilities is useful in any algorithm that performs iterative training using sampled trajectories. Examples of such algorithms include policy gradient methods [24] and stochastic first-order methods [16]. 4 Baselines for EFGs In this section we propose several baseline functions for use during iterative training. Theorem 2 shows that we can minimize variance by choosing a baseline function bt such that bt(h, a) ≈ u((ha)|σt). No baseline. We begin by examining MCCFR under its original definition, where no baseline function is used. We note that when we run baseline-corrected MCCFR with a static choice of bt(h, a) = 0 for all h, a, the operation of the algorithm is identical to MCCFR. Thus, opting to not use a baseline is, in itself, a choice of a very particular baseline. Using bt(h, a) = 0 might seem like a reasonable choice when we expect the game’s payouts to be balanced between the players. However, even when the overall expected utility u(σ) is very close to 0, there will usually be particular histories with high magnitude expected utility u(h|σ). For example, in poker games, the expected utility of a history is heavily biased toward the player who has been dealt better cards, even if these biases cancel out when considered across all histories. In fact, often there is no strategy profile at all that satisfies u((ha)|σ) = 0, which makes bt(h, a) = 0 a poor choice in regards to the ideal criteria bt(h, a) ≈ u((ha)|σt). An example game where a zero baseline performs very poorly is explored in Section 5. Static strategy baseline. The simplest way to ensure that the baseline function does correspond to an actual strategy is to choose a static, known strategy profile σb ∈ Σ and let bt(h, a) = u((ha)|σb) for each time t. Once the strategy is chosen, the baseline values only need to be computed once and stored. In general this requires a full walk of the game tree, but it is sometimes possible to take advantage of the structure of the game to greatly reduce this cost. For an example, see Section 5. Learned history baseline. Using a static strategy for our baseline ensures that it corresponds to some expected utility, but it fails to take advantage of the iterative nature of MCCFR. In particular, when attempting to estimate u((ha)|σt), we have access to all past samples uˆ ((ha)|στ , zτ ) for b τ < t. Because the strategy is changed incrementally, we might expect the expected utility to change slowly and for these to be reasonable samples of the utility at time t as well. Define T ha(t) = {τ < t | (ha) (cid:118) zτ } to be the set of timesteps on which (ha) was sampled, and denote the jth such timestep as τ . We define the learned history baseline as j |T ha(t)| (cid:88) bt(h, a) = w uˆ ((ha)|στj , zτj ) (6) j b j=1 4
where (w )|T ha(t)| is a sequence of weights satisfying (cid:80)|T ha(t)| w = 1. Possible weighting choices j j=1 j=1 j include simple averaging, where w = 1/|T ha(t)|, and exponentially-decaying averaging, where j w = α(1 − α)|T ha(t)|−j for some α ∈ (0, 1]. In either case, the baseline can be efficiently updated j online by tracking the weighted sum and the number of times that (ha) has been sampled. Learned infoset baseline. The learned history baseline is very similar to the VR-MCCFR baseline defined by Schmid et al. [21]. The principle difference is that the VR-MCCFR baseline tracks values for each information set, rather than for each history; we thus refer to it as the learned infoset baseline. This baseline also updates values for each player separately, based on their own information sets. This can be accomplished by tracking separate values for each player throughout the tree walk, or by running MCCFR with alternating updates, where only one player’s regrets are updated on each tree walk. The VR-MCCFR baseline can be defined in our framework as |T Iia(t)| (cid:88) bt(h, a) = bt(I (h), a) where bt(I , a) = w uˆ ((I [zτj ]a)|στj , zτj ) (7) i i j b i j=1 where i is the player being updated, T Iia(t) is the set of timesteps on which (h(cid:48)a) was sampled for any h(cid:48) ∈ I , and τ is jth such timestep. Following Schmid et al. we consider both simple averaging i j and exponentially-decaying averaging for selecting the weights w . j Predictive baseline. Our last baseline takes advantage of the recursive nature of the MCCFR update. On each iteration, each history along the sampled trajectory is evaluated and updated in depth-first order. Thus when the update of history h (cid:118) zt is complete and the value is returned , we have already calculated the next regrets Rt+1(I(h(cid:48)), ·) for all h(cid:48) such that h (cid:118) h(cid:48) (cid:118) zt. These values will be the input to the regret matching procedure on the next iteration, computing σt+1(h(cid:48), ·) at these histories. Thus we can immediately compute this next strategy, and using the already sampled trajectory, compute an estimate of the strategy’s utility as uˆ ((ha)|σt+1, zt). This is an unbiased b sample of the expected utility u((ha)|σt+1), which is our target value for the next baseline bt+1(h, a). We thus use this sample to update the baseline: (cid:26) uˆ ((ha)|σt+1, zt) if (ha) (cid:118) zt bt+1(h, a) = b (8) bt(h, a) otherwise The computation for this update can be done efficiently by a simple change to MCCFR. In MCCFR, we compute uˆ (h|σt, zt) at each step by using σt to weight recursively-computed action values. In b MCCFR with predictive baseline, after updating the regrets at h, we use a second regret matching computation to compute σt+1(h, ·). We use this strategy to weight a second set of recursively- computed action values to compute uˆ (h|σt+1, zt). When we walk back up the tree, we return both b of the values uˆ (h|σt, zt) and uˆ (h|σt+1, zt), allowing this recursion to continue. The predictive b b value uˆ (h|σt+1, zt) is only used for updating the baseline function. These changes do not modify b the asymptotic time complexity of MCCFR. Pseudocode is given in Appendix A. 5 Experimental comparison We run our experiments using a commodity desktop machine in Leduc hold’em [23], a small poker game commonly used as a benchmark in games researchb. We compare the effect of the various baselines on the MCCFR convergence rate. Our experiments use the regret zeroing and linear averaging of CFR+, as these improve convergence when combined with any baseline. For the static strategy baseline, we use the “always call” strategy, which matches the opponent’s bets and makes no bets of its own. Expected utility under this strategy is determined by the current size of the pot, which is measurable at run time, and the winning chance of each player’s cards. Before training, we measure and store these odds for all possible sets of cards, which is significantly smaller than the size of the full game. For both of the learned baselines, we use simple averaging as it was found to work best in preliminary experiments. We run experiments with two sampling strategies. The first is uniform sampling, in which qt(h, a) = 1/|A(h)|. The second is opponent on-policy sampling, depends on the player i being updated: we bAn open source implementation of CFR+ and Leduc hold’em is available from the University of Alberta [8]. 5
sample uniformly (qt(h, a) = 1/|A(h)|) at histories h where P (h) = i, and sample on-policy (qt(h, a) = σt(h, a)) otherwise. For consistency, we use alternating updates for both schemes.                            L W H U D W L R Q V   H P D J  V S L K F   \ W L O L E D W L R O S [ H            Q R  E D V H O L Q H  Q R  E D V H O L Q H  D O Z D \ V  F D O O       D O Z D \ V  F D O O       Q R  E D V H O L Q H  O H D U Q H G  K L V W R U \  O H D U Q H G  K L V W R U \  O H D U Q H G  K L V W R U \  O H D U Q H G  L Q I R V H W  O H D U Q H G  L Q I R V H W  O H D U Q H G  L Q I R V H W  S U H G L F W L Y H  S U H G L F W L Y H  S U H G L F W L Y H                                  L W H U D W L R Q V  L W H U D W L R Q V (a) Uniform sampling (b) Opponent on-policy sampling (c) Shifted utilities Figure 1: Log-log plots of convergence of MCCFR strategies with various baselines. (a) and (b) Leduc with different MCCFR sampling schemes. (c) Leduc with utilities shifted by 100 and opponent on-policy sampling. Figures 1a and 1b show the convergence of MCCFR with the various baselines, as measured by exploitability (recall that exploitability converges to zero). All results in this paper are averaged over 20 runs, with 95% confidence intervals shown as error bands (often too narrow to be visible). With uniform sampling, the learned infoset (VR-MCCFR) baseline improves modestly on using no baseline at all, while the other three baselines achieve a significant improvement on top of that. With opponent on-policy sampling, the gap is smaller, but the learned infoset baseline is still noticeably worse than the other options. Many true expected values in Leduc are very close to zero, making MCCFR without a baseline (i.e. bt(h, a) = 0) better than it might otherwise be. To demonstrate the necessity of a baseline in some games, we ran MCCFR in a modified Leduc game where player 2 always transfers 100 chips to player 1 after every game. This utility change is independent of the player’s actions, so it doesn’t strategically change the game. However, it means that 0 is now an extremely inaccurate value estimate for all histories. Figure 1c shows convergence in Leduc with shifted utilities. The always call baseline is omitted, as the results would be identical to those in Figure 1b. Here we see that using any baseline at all provides a significant advantage over not using a baseline, due to the ability to adapt to the shifted utilities. We also see that the learned infoset baseline performs relatively well early on in this setting, because it generalizes across histories. 6 Public Outcome Sampling Although the results in Section 5 show large gains in convergence speed when using baselines with MCCFR, the magnitudes are not as large as those shown with the VR-MCCFR baseline by Schmid et al. [21]. This is because their experiments use a "vectorized" form of MCCFR, which avoids the sampling of individual histories within information sets. Instead, they track a vector of values on each iteration, one for each possible true history given the player’s observed information set. Schmid et al. do not formally define their algorithm. We refer to it as Public Outcome Sampling (POS) as the algorithm samples any actions that are publicly visible to both players, while exhaustively considering all possible private states. We give a full formal definition of POS in Appendix E. 6.1 Baselines in POS In MCCFR with POS, we still use action baselines bt(h, a) with the ideal baseline values being bt(h, a) = u((ha)|σt). Thus the baselines in Section 4 apply to this setting as well. For the learned infoset baseline, we have more information available to us than in the OS case. This is because when POS samples some history-action pair h, a, it also samples every pair h(cid:48), a for h(cid:48) ∈ I(h). Thus, rather, than using one sampled history value to update the baseline, we use a 6
weighted sample of all of the history values. Following Schmid et al., we weight the baseline values bt(h, a) = bt(I (h), a) where bt(I , a) = |T (cid:88)Iia(t)| w (cid:80) h(cid:48)∈Ii π −στ ij (h(cid:48))uˆ b((h(cid:48)a)|στj , zτj ) . i i j (cid:80) πστj (h(cid:48)) j=1 h(cid:48)∈Ii −i This is the same relative weighting given to each history when calculating the counterfactual regret. Zero-variance baseline. POS also has implications for the predictive baseline. In fact, we can guarantee that after every outcome of the game has been sampled, the predictive baseline will have learned the true value of the current strategy. For time t, let Zt be the set of sampled terminal histories (consistent with a public outcome), and let sampt(h) be the event that h is sampled on way to Zt. Theorem 3. If each of the terminal states Z[h] reachable from history h ∈ H has been sampled at least once under public outcome sampling (Z[h] ⊆ (cid:83) Zτ ), then the predictive baseline satisfies τ<t bt(h, a) = u((ha)|σt) and Var (cid:2) uˆ (h|σt, Zt)|sampt(h)(cid:3) = 0 ∀a ∈ A(h) Zt b The key idea behind the proof is that POS ensures that the baseline is updated at a history if and only if the expected value of the history changes. The full proof is in Appendix F. In order for the theorem to hold everywhere in the tree, all outcomes must be sampled, which could take a large number of iterations. An alternative is to guarantee that all outcomes are sampled during the early iterations of MCCFR. For example, one could do a full CFR tree walk on the very first iteration, and then sample on subsequent iterations. Alternatively, we can ensure the theorem always holds with smart initialization of the baseline. When there are no regrets accumulated, MCCFR uses an arbitrary strategy. If we have some strategy with known expected values throughout the tree, we can use this strategy as the default MCCFR strategy and initialize the baseline values to the strategy’s expected values. Either option guarantees that all regret updates will use zero-variance values. 6.2 POS results As in Section 5, we run experiments in Leduc and use CFR+ updates. For the learned baselines, we use exponentially-decaying averaging with α = 0.5, which preliminary experiments found to outperform simple averaging when combined with POS. For simplicity and consistency with the experiments of Schmid et al. [21], we use uniform sampling and simultaneous updates.                                         L W H U D W L R Q V   H P D J  V S L K F   \ W L O L E D W L R O S [ H                          Q R  E D V H O L Q H       D O Z D \ V  F D O O  O H D U Q H G  K L V W R U \       O H D U Q H G  L Q I R V H W  S U H G L F W L Y H                             L W H U D W L R Q V (a) Exploitability 2  H P D J  V S L K F   H F Q D L U D Y  Y I F  H J D U H Y D  Q R  E D V H O L Q H  D O Z D \ V  F D O O  O H D U Q H G  K L V W R U \  O H D U Q H G  L Q I R V H W  S U H G L F W L Y H (b) Counterfactual value variance Figure 2: Log-log plots of POS MCCFR strategies with various baselines. (a) Convergence as measured by exploitability. (b) Empirical variance of counterfactual values (cfvs). Figure 2a compares the baselines’ effects on POS MCCFR. We find that using any baseline provides a significant improvement on using no baseline. The always call baseline performs well early but tales off as it doesn’t learn during training. Even with POS, where we always see an entire information set at a time, the learned infoset baseline (VR-MCCFR) is significantly outperformed by the learned history and predictive baselines. This is likely because the learned infoset baseline has to learn the relative weighting between histories in an infoset, while the other baselines always use the current 7
strategy to weight the learned values. Finally, we observe that the predictive baseline has a small, but statistically significant, advantage over the learned history baseline in early iterations. In addition, we compare the baselines by directly measuring their variance. We measure the variance of the counterfactual value (cid:80) πσt (h)uˆ ((ha)|σt, zt) for each pair I, a, and we average across h∈I −P (h) b all such pairs. Full details are in Appendix G. Results are shown in Figure 2b. We see that using no baseline results in high and relatively steady variance of counterfactual values. Using the always call baselines also results in steady variance, as nothing is learned, but at approximately an order of magnitude lower than no baseline. Variance with the other baselines improves over time, as the baseline becomes more accurate. The learned history baseline mirrors the learned infoset baseline, but with more than an order of magnitude reduction in variance. The predictive baseline is best of all, and in fact we see Theorem 3 in action as the variance drops to zero. 7 Related Work As discussed in the introduction, the use of baseline functions has a long history in RL. Typically these approaches have used state value baselines, with some recent exceptions [18, 30]. Tucker et al. [27] suggest an explanation for this by isolating the variance terms that come from sampling an immediate action and from sampling the rest of a trajectory. Typical RL baselines only reduce the action variance, so the additional benefit from using a state-action baseline is insignificant when compared to the trajectory variance. In our work, we apply a recursive baseline to reduce both the action and trajectory variances, meaning state-action baselines give a noticeable benefit. In RL, the doubly-robust estimator [13] has been used to reduce variance settings by the recursive application of control variates [26]. Similarly, variance reduction in EFGs via recursive control variates is the basis of the advantage sum estimator [32] and AIVAT [7]. All of these techniques construct control variates by evaluating a static policy or strategy, either on the true game or on a static model. In this sense they are equivalent to our static strategy baseline. However, to the best of our knowledge, these techniques have only been used for evaluation of static strategies, rather than for variance reduction during training. Our work extends the EFG techniques to the training domain; we believe that similar ideas can be used in RL, and this is an interesting avenue of future research. Concurrent to this work, Zhou et al. [31] also suggested tracking true values of histories in a CFR variant, analogous to our predictive baseline. They use these values for truncating full tree walks, rather than for variance reduction along sampled trajectories. As such, they always initialize their values with a full tree walk, and don’t examine gradually learning the values during training. 8 Conclusion and Future Work In this work we introduced a new framework for variance reduction in EFGs through the application of a baseline value function. We demonstrated that the existing VR-MCCFR baseline can be described in our framework with a specific baseline function, and we introduced other baseline functions that significantly outperform it in practice. In addition, we introduced a predictive baseline and showed that it gives provably optimal performance under a sampling scheme that we formally define. There are three sources of variance when performing sampled updates in EFGs. The first is from sampling trajectory values, the second from sampling individual histories within an information set that is being updated, and the third from sampling which information sets will be updated on a given iteration. By introducing MCCFR with POS, we provably eliminate the first two sources of variance: the first because we have a zero-variance baseline, and the second because we consider all histories within the information set. For the first time, this allows us to select the MCCFR sampling strategy qt entirely on the basis of minimizing the third source of variance, by choosing the “best” information sets to update. Doing this in a principled way is an exciting avenue for future research. Finally, we close by discussing function approximation. All of the baselines introduced in this paper require an amount of memory that scales with the size of the game tree. In contrast, baseline functions in RL typically use function approximation, requiring a much smaller number of parameters. Additionally, these functions generalize across states, which can allow for learning an accurate baseline function more quickly. The framework that we introduce in this work is completely compatible with function approximation, and combining the two is an area for future research. 8
References [1] Shalabh Bhatnagar, Richard S Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor–critic algorithms. Automatica, 45(11):2471–2482, 2009. [2] Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold’em poker is solved. Science, 347(6218):145–149, January 2015. [3] Noam Brown and Tuomas Sandholm. Safe and nested subgame solving for imperfect-information games. In Advances in Neural Information Processing Systems 30, 2017. [4] Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418–424, January 2018. [5] Noam Brown, Tuomas Sandholm, and Brandon Amos. Depth-limited solving for imperfect-information games. In Advances in Neural Information Processing Systems 31, 2018. [6] Neil Burch, Michael Johanson, and Michael Bowling. Solving imperfect information games using decomposition. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014. [7] Neil Burch, Martin Schmid, Matej Moravcik, Dustin Morrill, and Michael Bowling. AIVAT: A new variance reduction technique for agent evaluation in imperfect information games. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, 2018. [8] Computer Poker Research Group, University of Alberta and Oskari Tammelin. CFR+, 2014. URL http://webdocs.cs.ualberta.ca/~games/poker/cfr_plus.html. [Online; accessed 23-May-2019]. [9] Richard Gibson, Neil Burch, Marc Lanctot, and Duane Szafron. Efficient Monte Carlo counterfactual regret minimization in games with many player actions. In Proceedings of the Twenty-Sixth Conference on Advances in Neural Information Processing Systems, 2012. [10] Richard Gibson, Marc Lanctot, Neil Burch, Duane Szafron, and Michael Bowling. Generalized sampling and variance in counterfactual regret minimization. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, 2012. [11] Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in supervised learning. Journal of Machine Learning Research, 5(Nov):1471–1530, 2004. [12] Sune K. Jakobsen, Troels B. Sørensen, and Vincent Conitzer. Timeability of extensive-form games. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, 2016. [13] Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for supervised learning. In Proceedings of The 33rd International Conference on Machine Learning, 2016. [14] Michael Johanson. Measuring the size of large no-limit poker games. Technical Report TR13-01, Department of Computing Science, University of Alberta, 2013. [15] Michael Johanson, Kevin Waugh, Michael Bowling, , and Martin Zinkevich. Accelerating best response calculation in large extensive games. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, 2011. [16] Christian Kroer, Kevin Waugh, Fatma Kilinç-Karzan, and Tuomas Sandholm. Faster first-order methods for extensive-form game solving. In Proceedings of the Sixteenth ACM Conference on Economics and Computation, 2015. [17] Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte Carlo sampling for regret minimization in extensive games. In Advances in Neural Information Processing Systems 22, 2009. [18] Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, and Qiang Liu. Action-dependent control variates for policy optimization via Stein identity. In International Conference on Learning Representations, 2018. [19] Matej Moravcˇík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael H. Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356 6337:508–513, 2017. [20] Martin J. Osborne and Ariel Rubinstein. A Course in Game Theory. The MIT Press, 1994. ISBN 0-262-65040-1. 9
[21] Martin Schmid, Neil Burch, Marc Lanctot, Matej Moravcik, Rudolf Kadlec, and Michael Bowling. Variance reduction in monte carlo counterfactual regret minimization (VR-MCCFR) for extensive form games using baselines. In Proceedings of the The Thirty-Third AAAI Conference on Artificial Intelligence, 2019. [22] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In International Conference on Learning Representations, 2016. [23] Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings, and Chris Rayner. Bayes’ bluff: Opponent modelling in poker. In Proceedings of the Twenty-First Conference on Uncertainty in Artficial Intelligence, 2005. [24] Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Pérolat, Karl Tuyls, Rémi Munos, and Michael Bowling. Actor-critic policy optimization in partially observable multiagent environments. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [25] Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit Texas hold’em. In Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015. [26] Philip S. Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for supervised learning. In Proceedings of the 33rd International Conference on International Conference on Machine Learning, 2016. [27] George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard Turner, Zoubin Ghahramani, and Sergey Levine. The mirage of action-dependent baselines in supervised learning. In Proceedings of the 35th Interna- tional Conference on Machine Learning, 2018. [28] Michal Šustr, Vojteˇch Kovaˇrík, and Viliam Lisý. Monte Carlo continual resolving for online strategy computation in imperfect information games. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, 2019. [29] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist supervised learning. Machine learning, 8(3-4):229–256, 1992. [30] Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M. Bayen, Sham M. Kakade, Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent factorized baselines. In International Conference on Learning Representations, 2018. [31] Yichi Zhou, Tongzheng Ren, Jialian Li, Dong Yan, and Jun Zhu. Lazy-CFR: a fast regret minimization algorithm for extensive games with imperfect information. CoRR, abs/1810.04433, 2018. URL http: //arxiv.org/abs/1810.04433. [32] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. In Advances in Neural Information Processing Systems 20, 2008. 10
Appendices A MCCFR with baseline-corrected values Pseudocode for MCCFR with baseline-corrected values is given in Algorithm 1. Quantities of the form σt(h, ·) refer to the vector of all quantities σt(h, a) for a ∈ A(h). A version for the predictive baseline, which must calculate extra values, is given in Algorithm 2. Each of these algorithms has the same worst- case iteration complexity as MCCFR without baselines, namely O(d|A |) where d is the tree’s depth and max |A | = max |A(h)|. max h Algorithm 1 MCCFR w/ baseline 1: function MCCFR(h) 2: if h ∈ Z then return u(h) 3: σt(h, ·) ← REGRETMATCHING(Rt−1(I(h), ·)) 4: σt(h, ·) ← t−1 σt−1(h, ·) + 1 σt t t 5: sample action a ∼ qt(h, ·) 6: uˆ b((ha)|σt, zt) ← MCCFR((ha)) 7: uˆ b(h, a(cid:48)|σt, zt) ← bt(h, a(cid:48)) ∀a(cid:48) (cid:54)= a 8: uˆ b(h, a|σt, zt) ← bt(h, a) + qt(h1 ,a) (uˆ b((ha)|σt, zt) − bt(h, a)) 9: uˆ b(h|σt, zt) ← (cid:80) a(cid:48) σt(h, a(cid:48))uˆ b(h, a(cid:48)|σt, zt) 10: if P (h) = 1 then 11: rt(I(h), a) ← π π2σ qtt (( hh )) (uˆ b(h, ·|σt, zt) − uˆ b(h|σt, zt)) 12: else if P (h) = 2 then 13: rt(I(h), a) ← π π1σ qtt (( hh )) (−uˆ b(h, ·|σt, zt) + uˆ b(h|σt, zt)) 14: end if 15: Rt(I(h), ·) ← Rt−1(I(h), ·) + rt(I(h), ·) 16: bt+1(h, a) ← UPDATEBASELINE(bt(h, a), uˆ b((ha)|σt, zt)) 17: return uˆ b(h|σt, zt) 18: end function B Proof of Theorem 1 This proof is a simplified version of the proof of Lemma 5 in Schmid et al. [21]. We directly analyze the expectation of the baseline-corrected utility: E (cid:2) uˆ (h, a|σt, zt) | zt (cid:119) h(cid:3) zt b (cid:18) (cid:19) = Pr (cid:2) (ha) (cid:118) zt | h (cid:118) zt(cid:3) 1 (cid:0)E (cid:2) uˆ ((ha)|σt, zt) | zt (cid:119) (ha)(cid:3) − bt(h, a)(cid:1) + bt(h, a) qt(h, a) zt b + Pr (cid:2) (ha) (cid:54)(cid:118) zt | h (cid:118) zt(cid:3) (bt(h, a)) (cid:18) (cid:19) = qt(h, a) 1 (cid:0)E (cid:2) uˆ ((ha)|σt, zt) | zt (cid:119) (ha)(cid:3) − bt(h, a)(cid:1) + bt(h, a) qt(h, a) zt b + (1 − qt(h, a))(bt(h, a)) = E (cid:2) uˆ ((ha)|σt, zt) | zt (cid:119) (ha)(cid:3) zt b We now proceed by induction on the height of (ha) in the tree. If (ha) has height 0, then (ha) ∈ Z and E zt (cid:2) uˆ b(h, a|σt, zt) | zt (cid:119) h(cid:3) = E zt (cid:2) uˆ b((ha)|σt, zt) | zt (cid:119) (ha)(cid:3) = u((ha)) by definition. For the inductive step, consider arbitrary h, a such that (ha) has height more than 0. We assume that E zt (cid:2) uˆ b(h(cid:48), a(cid:48)|σt, zt) | zt (cid:119) h(cid:48)(cid:3) = u((h(cid:48)a(cid:48))|σt) for all h(cid:48), a(cid:48) such that (h(cid:48)a(cid:48)) has smaller height than (ha). 11
Algorithm 2 MCCFR w/ predictive baseline 1: function MCCFR(h) 2: if h ∈ Z then return u(h), u(h) 3: σt(h, ·) ← REGRETMATCHING(Rt−1(I(h), ·)) 4: σt(h, ·) ← t−1 σt−1(h, ·) + 1 σt t t 5: sample action a ∼ qt(h, ·) 6: uˆ b((ha)|σt, zt), uˆ b((ha)|σt+1, zt) ← MCCFR((ha)) 7: uˆ b(h, a(cid:48)|σt, zt) ← bt(h, a(cid:48)) ∀a(cid:48) (cid:54)= a 8: uˆ b(h, a|σt, zt) ← bt(h, a) + qt(h1 ,a) (uˆ b((ha)|σt, zt) − bt(h, a)) 9: uˆ b(h|σt, zt) ← (cid:80) a(cid:48) σt(h, a(cid:48))uˆ b(h, a(cid:48)|σt, zt) 10: if P (h) = 1 then 11: rt(I(h), a) ← π π2σ qtt (( hh )) (uˆ b(h, ·|σt, zt) − uˆ b(h|σt, zt)) 12: else if P (h) = 2 then 13: rt(I(h), a) ← π π1σ qtt (( hh )) (−uˆ b(h, ·|σt, zt) + uˆ b(h|σt, zt)) 14: end if 15: Rt(I(h), ·) ← Rt−1(I(h), ·) + rt(I(h), ·) 16: bt+1(h, a) ← uˆ b((ha)|σt+1, zt) 17: σt+1(h, ·) ← REGRETMATCHING(Rt(I(h), ·)) 18: uˆ b(h, a(cid:48)|σt+1, zt) ← bt(h, a(cid:48)) ∀a(cid:48) (cid:54)= a 19: uˆ b(h, a|σt+1, zt) ← bt+1(h, a) 20: uˆ b(h|σt+1, zt) ← (cid:80) a(cid:48) σt+1(h, a(cid:48))uˆ b(h, a(cid:48)|σt+1, zt) 21: return uˆ b(h|σt, zt), uˆ b(h|σt+1, zt) 22: end function We then have E (cid:2) uˆ (h, a|σt, zt) | zt (cid:119) h(cid:3) zt b = E (cid:2) uˆ ((ha)|σt, zt) | zt (cid:119) (ha)(cid:3) zt b = (cid:88) σt((ha), a(cid:48))E (cid:2) uˆ ((ha), a(cid:48)|σt, zt) | zt (cid:119) (ha)(cid:3) zt b a(cid:48)∈A((ha)) = (cid:88) σt((ha), a(cid:48))u((haa(cid:48))|σt) by inductive hypothesis a(cid:48)∈A((ha)) = u((ha)|σt) by definition We are able to apply the inductive hypothesis because (haa(cid:48)) is a suffix of (ha) and thus must have smaller height. The proof follows by induction. C Proof of Theorem 2 This proof is similar to the proof of Lemma 3 in Schmid et al. [21]. We begin by proving that the assumption of the theorem necessitates that the baseline-corrected values are the true expected values. Lemma 1. Assume that we have a baseline that satisfies bt(h, a) = u((ha)|σt) for all h ∈ H, a ∈ A(h). Then for any h, a, zt, uˆ (h, a|σt, zt) = u((ha)|σt) (9) b 12
Proof. As for Theorem 1, we prove this by induction on the height of (ha). If (ha) ∈ Z, then by definition uˆ ((ha)|σt, zt) = u((ha)) = u((ha)|σt). This then means b uˆ (h, a|σt, zt) b = 1((ha) (cid:118) zt) (cid:0) uˆ ((ha)|σt, zt) − bt(h, a)(cid:1) + bt(h, a) qt(h, a) b = 1((ha) (cid:118) zt) (cid:0) u((ha)|σt) − bt(h, a)(cid:1) + bt(h, a) qt(h, a) = 1((ha) (cid:118) zt) (cid:0) u((ha)|σt) − u((ha)|σt)(cid:1) + u((ha)|σt) by lemma assumption qt(h, a) = u((ha)|σt) For the inductive step, we assume that uˆ (h(cid:48), a(cid:48)|σt, zt) = u((h(cid:48)a(cid:48))|σt) for any (h(cid:48)a(cid:48)) with smaller height than b (ha). Then uˆ ((ha)|σt, zt) = (cid:88) σt((ha), a(cid:48))uˆ ((ha), a(cid:48)|σt, zt) b b a(cid:48)∈A((ha)) = (cid:88) σt((ha), a(cid:48))u((haa(cid:48))|σt) by inductive hypothesis a(cid:48)∈A((ha)) = u((ha)|σt) and this in turn gives us uˆ (h, a|σt, zt) b = 1((ha) (cid:118) zt) (cid:0) u((ha)|σt) − bt(h, a)(cid:1) + bt(h, a) qt(h, a) = 1((ha) (cid:118) zt) (cid:0) u((ha)|σt) − u((ha)|σt)(cid:1) + u((ha)|σt) by lemma assumption qt(h, a) = u((ha)|σt) The proof of the lemma follows by induction. To finish the proof of Theorem 1, we only have to note that Var zt (cid:2) uˆ b(h, a|σt, zt)|zt (cid:119) h(cid:3) = Var zt (cid:2) u((ha)|σt)|zt (cid:119) h(cid:3) by Lemma 1 = 0 The last step follows because the expected utility is not a random variable. D Further variance analysis Theorem 2 shows that if the baseline function exactly predicts the true expected utility, then the baseline-corrected sampled values will have zero variance. However, it doesn’t show what happens to the variance when the baseline function only approximates the expected utility. In this section, we show that the variance is a function of the differences between the baseline estimates and the true expected values. Theorem 4. For any baseline function bt and any h, a Var (cid:2) uˆ (h, a|σt, zt)|zt (cid:119) h(cid:3) ≤ (cid:88) (πσt ((ha), (h(cid:48)a(cid:48))))2 (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)2 zt b πqt(h, (h(cid:48)a(cid:48))) (h(cid:48)a(cid:48))(cid:119)(ha) Theorem 4 shows that as the baseline estimates bt(h, a) approach the expected values u((ha)|σt), the variance converges to zero. It also establishes a bound on how quickly it happens. Before proving Theorem 4, we first examine how the full (trajectory) variance can be decomposed into contribu- tions from individual actions. Lemma 2. For any baseline function bt and any h ∈ H Var (cid:2) uˆ (h|σt, zt)|zt (cid:119) h(cid:3) = (cid:88) (cid:0) σt(h, a)(cid:1)2 Var (cid:2) uˆ ((ha)|σt, zt)(cid:3) zt b qt(h, a) zt b a∈A(h) + Var (cid:20) σt(h, a) (cid:0) u((ha)|σt) − bt(h, a)(cid:1)(cid:21) a qt(h, a) 13
Proof. We use the law of total variance, conditioning on which a is sampled at h. This gives us Var (cid:2) uˆ (h|σt, zt)|zt (cid:119) h(cid:3) zt b = E a (cid:2) Var zt (cid:2) uˆ b(h|σt, zt)(cid:12) (cid:12)zt (cid:119) (ha)(cid:3)(cid:3) + Var a (cid:2)E zt (cid:2) uˆ b(h|σt, zt)(cid:12) (cid:12)zt (cid:119) (ha)(cid:3)(cid:3) (10) We analyze each of these terms separately. First, to analyze the left summand in (10), we note that if ha (cid:64) zt then by the recursive definition of baseline- corrected values uˆ (h|σt, zt) = σt(h, a) uˆ ((ha)|σt, zt) − σt(h, a) bt(h, a) + (cid:88) σt(h, a(cid:48))bt(h, a(cid:48)) b qt(h, a) b qt(h, a) a(cid:48)∈A(h) Only the first term depends on the sampled trajectory zt, and thus E a (cid:2) Var zt (cid:2) uˆ b(h|σt, zt)(cid:12) (cid:12)zt (cid:119) (ha)(cid:3)(cid:3) = E a (cid:20) Var zt (cid:20) σ qtt (( hh ,, aa )) uˆ b((ha)|σt, zt)(cid:12) (cid:12) (cid:12) (cid:12)zt (cid:119) (ha)(cid:21)(cid:21) = E a (cid:34)(cid:18) σ qtt (( hh ,, aa )) (cid:19)2 Var zt (cid:2) uˆ b((ha)|σt, zt)(cid:12) (cid:12)zt (cid:119) (ha)(cid:3)(cid:35) = (cid:88) (cid:0) σ qt t( (h h, ,a a) )(cid:1)2 Var zt (cid:2) uˆ b((ha)|σt, zt)(cid:12) (cid:12)zt (cid:119) (ha)(cid:3) (11) a∈A(h) Next, we analyze the inner expectation of the right summand of (10) E zt (cid:2) uˆ b(h|σt, zt)(cid:12) (cid:12)zt (cid:119) (ha)(cid:3) = (cid:88) σt(h, a(cid:48))bt(h, a(cid:48)) + σt(h, a) (cid:0)E (cid:2) uˆ ((ha)|σt, zt)(cid:3) − bt(h, a)(cid:1) qt(h, a) zt b a(cid:48) = (cid:88) σt(h, a(cid:48))bt(h, a(cid:48)) + σt(h, a) (cid:0) u((ha)|σt) − bt(h, a)(cid:1) qt(h, a) a(cid:48) The first term here doesn’t depend on the sampled a, giving us Var a (cid:2)E zt (cid:2) uˆ b(h|σt, zt)(cid:12) (cid:12)(ha) (cid:118) zt(cid:3)(cid:3) = Var a (cid:20) σ qtt (( hh ,, aa )) (cid:0) u((ha)|σt) − bt(h, a)(cid:1)(cid:21) (12) Combining (10), (11), and (12) completes the proof. Lemma 2 decomposes the variance into a part from the immediately sampled action, and a part from the remainder of the sampled trajectory. We extend this to completely decompose the trajectory variance. Lemma 3. For any baseline function bt and any h, a Var zt (cid:2) uˆ b(h|σt, zt)|zt (cid:119) h(cid:3) = (cid:88) (π πσ qt t( (h h, ,h h(cid:48) (cid:48)) ))2 Var a(cid:48) (cid:20) σ qtt (( hh (cid:48)(cid:48) ,, aa (cid:48)(cid:48) )) (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)(cid:21) h(cid:48)(cid:119)h Proof. We proceed by induction on the height of h in the tree. If h has height 0, then A(h) = ∅, and Var zt (cid:2) uˆ b(h|σt, zt)|zt (cid:119) h(cid:3) = 0. Otherwise, we begin from Lemma 2 and apply the inductive hypothesis for h(cid:48) with height less than that of h. This gives Var (cid:2) uˆ (h|σt, zt)|zt (cid:119) h(cid:3) zt b = (cid:88) (cid:0) σt(h, a)(cid:1)2 Var (cid:2) uˆ ((ha)|σt, zt)(cid:3) + Var (cid:20) σt(h, a) (cid:0) u((ha)|σt) − bt(h, a)(cid:1)(cid:21) qt(h, a) zt b a qt(h, a) a∈A(h) = (cid:88) (cid:0) σ qt t( (h h, ,a a) )(cid:1)2 (cid:88) (π πσ qt t( (( (h ha a) ), ,h h(cid:48) (cid:48)) ))2 Var a(cid:48) (cid:20) σ qtt (( hh (cid:48)(cid:48) ,, aa (cid:48)(cid:48) )) (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)(cid:21) a∈A(h) h(cid:48)(cid:119)(ha) + Var (cid:20) σt(h, a) (cid:0) u((ha)|σt) − bt(h, a)(cid:1)(cid:21) a qt(h, a) 14
= (cid:88) (cid:88) (π πσ qt t( (h h, ,h h(cid:48) (cid:48)) ))2 Var a(cid:48) (cid:20) σ qtt (( hh (cid:48)(cid:48) ,, aa (cid:48)(cid:48) )) (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)(cid:21) a∈A(h) h(cid:48)(cid:119)(ha) + Var (cid:20) σt(h, a) (cid:0) u((ha)|σt) − bt(h, a)(cid:1)(cid:21) a qt(h, a) = (cid:88) (π πσ qt t( (h h, ,h h(cid:48) (cid:48)) ))2 Var a(cid:48) (cid:20) σ qtt (( hh (cid:48)(cid:48) ,, aa (cid:48)(cid:48) )) (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)(cid:21) h(cid:48)(cid:119)h The lemma follows by induction. Proof of Theorem 4. Starting from Lemma 3, we first bound the variance of history values Var (cid:2) uˆ (h|σt, zt)|zt (cid:119) h(cid:3) zt b = (cid:88) (π πσ qt t( (h h, ,h h(cid:48) (cid:48)) ))2 Var a(cid:48) (cid:20) σ qtt (( hh (cid:48)(cid:48) ,, aa (cid:48)(cid:48) )) (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)(cid:21) h(cid:48)(cid:119)h ≤ (cid:88) (π πσ qt t( (h h, ,h h(cid:48) (cid:48)) ))2 E a(cid:48) (cid:34)(cid:18) σ qtt (( hh (cid:48)(cid:48) ,, aa (cid:48)(cid:48) )) (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)(cid:19)2(cid:35) h(cid:48)(cid:119)h = (cid:88) (πσt (h, h(cid:48)))2 (cid:88) (cid:0) σt(h(cid:48), a(cid:48))(cid:1)2 (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)2 πqt(h, h(cid:48)) qt(h(cid:48), a(cid:48)) h(cid:48)(cid:119)h a(cid:48)∈A(h(cid:48)) = (cid:88) (πσt (h, (h(cid:48)a(cid:48))))2 (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)2 (13) πqt(h, (h(cid:48)a(cid:48))) h(cid:48)(cid:119)h a(cid:48)∈A(h(cid:48)) We then reformulate the variance of the history action value uˆ (h, a|σt, zt) in terms of the variance of the b succeeding history value uˆ ((ha)|σt, zt). To do this, we apply the law of total variance conditioning on the b random variable 1((ha) (cid:118) zt) which indicates whether a is sampled at h. Var zt (cid:2) uˆ b(h, a|σt, zt)(cid:12) (cid:12)zt (cid:119) h(cid:3) = Var zt (cid:20) 1(( qh ta (h) ,(cid:118) a)zt) (cid:0) uˆ b((ha)|σt, zt) − bt(h, a)(cid:1) + bt(h, a)(cid:12) (cid:12) (cid:12) (cid:12)zt (cid:119) h(cid:21) = Var zt (cid:20) 1(( qh ta (h) ,(cid:118) a)zt) (cid:0) uˆ b((ha)|σt, zt) − bt(h, a)(cid:1)(cid:12) (cid:12) (cid:12) (cid:12)zt (cid:119) h(cid:21) = E (cid:20) Var zt (cid:20) 1(( qh ta (h) ,(cid:118) a)zt) (cid:0) uˆ b((ha)|σt, zt) − bt(h, a)(cid:1)(cid:12) (cid:12) (cid:12) (cid:12)1((ha) (cid:118) zt)(cid:21)(cid:21) + Var (cid:20) E zt (cid:20) 1(( qh ta (h) ,(cid:118) a)zt) (cid:0) uˆ b((ha)|σt, zt) − bt(h, a)(cid:1)(cid:12) (cid:12) (cid:12) (cid:12)1((ha) (cid:118) zt)(cid:21)(cid:21) = E (cid:20) 1 (( q(h t(a h) , (cid:118) a))z 2t) Var zt (cid:2) uˆ b((ha)|σt, zt)(cid:12) (cid:12)1((ha) (cid:118) zt)(cid:3)(cid:21) + Var (cid:20) 1((ha) (cid:118) zt) (cid:0) u((ha)|σt) − bt(h, a)(cid:1)(cid:21) qt(h, a) = qt(h1 , a) Var zt (cid:2) uˆ b((ha)|σt, zt)(cid:12) (cid:12)zt (cid:119) (ha)(cid:3) + 1 (cid:0) u((ha)|σt) − bt(h, a)(cid:1)2 Var (cid:2)1((ha) (cid:118) zt)(cid:3) (qt(h, a))2 = qt(h1 , a) Var zt (cid:2) uˆ b((ha)|σt, zt)(cid:12) (cid:12)zt (cid:119) (ha)(cid:3) + 1 − qt(q ht( ,h a, )a) (cid:0) u((ha)|σt) − bt(h, a)(cid:1)2 ≤ qt(h1 , a) (cid:16) Var zt (cid:2) uˆ b((ha)|σt, zt)(cid:12) (cid:12)zt (cid:119) (ha)(cid:3) + (cid:0) u((ha)|σt) − bt(h, a)(cid:1)2(cid:17) (14) Combining (13) and (14), we get Var zt (cid:2) uˆ b(h, a|σt, zt)(cid:12) (cid:12)zt (cid:119) h(cid:3) ≤ 1 (cid:18) (cid:88) (πσt ((ha), (h(cid:48)a(cid:48))))2 (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)2 qt(h, a) πqt((ha), (h(cid:48)a(cid:48))) h(cid:48)(cid:119)(ha) a(cid:48)∈A(h(cid:48)) 15
(cid:19) + (cid:0) u((ha)|σt) − bt(h, a)(cid:1)2 = 1 (cid:88) (πσt ((ha), (h(cid:48)a(cid:48))))2 (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)2 qt(h, a) πqt((ha), (h(cid:48)a(cid:48))) (h(cid:48)a(cid:48))(cid:119)(ha) = (cid:88) (πσt ((ha), (h(cid:48)a(cid:48))))2 (cid:0) u((h(cid:48)a(cid:48))|σt) − bt(h(cid:48), a(cid:48))(cid:1)2 πqt(h, (h(cid:48)a(cid:48))) (h(cid:48)a(cid:48))(cid:119)(ha) E Public trees There are multiple sources of variance when computing the regret at an information set in MCCFR. One form of variance comes from sampling actions (and recursively, trajectories) from the information set, rather than walking the full subtree. A second form of variance comes from sampling only one of the histories in the information set itself. Our baseline framework reduces the first kind of variance, but does not take the second form of variance into account. One approach to combating this single-history variance could be to extend the use of the baseline; analogous to how we created a control variate from using bt(h, a) to evaluate unsampled actions a, we could also create a control variate that uses bt(h(cid:48), a) to evaluate all unsampled h(cid:48) ∈ I(h). However, this requires evaluating alternate histories along every step of the sampled trajectory, meaning that a single iteration of MCCFR goes from complexity O(d|A |) to O(d|A ||I )|. max max max A second approach, and the one we present in this section, is to change the sampling method used. Rather than using a baseline to consider each alternate history in the information set, we directly evaluate all such histories. Intuitively, this can be done by only sampling actions that are publicly observable, and walking all actions that change the game’s hidden state. This approach was used by Schmid et al. [21], but was never formalized. We formalize the algorithm here, after presenting some additional assumptions and definitions. We assume that the EFG is timeable [12], which informally means that no player can gain additional information by tracking how much time elapses while they are not acting. Formally, this means that we can assign a value time(h) to every h ∈ H such that time(h) = time(h(cid:48)) for any h(cid:48) ∈ I (h), and time(h) < time(h(cid:48)) for any P(h) h(cid:48) (cid:119) h. Every game played by humans must be timeable, or else the human could distinguish histories in the same information set by tracking elapsed time. If a game is timeable, players always observe the timing when they are acting, so there must be some strategically identical game where they observe the timing even when not acting. Thus we will assume that our games satisfy this requirement. We now introduce the concept of a public state [15], which groups histories based on information available to all players, or informally, based on whether they distinguishable to an outside observer. Formally, a public state is a set of histories that is (minimally) closed under the information set relation for all players. Let S be the set of public states (which partitions H), and S(h) ∈ S be the public state that h belongs to. By assumption that all players observe the game’s timing, necessarily time(h) = time(h(cid:48)) if S(h) = S(h(cid:48)). In turn, this means that if h (cid:118) h(cid:48), then S(h) (cid:54)= S(h(cid:48)). We also assume for simplicity that if S(h) = S(h(cid:48)), then P (h) = P (h(cid:48)). If necessary, this can be made true for any timeable game by splitting information sets and adding dummy actions, without strategically changing the game. We define T (S) to be the set of successor public states to S: S(cid:48) ∈ T (S) if there is some h ∈ S, a ∈ A(h), and h(cid:48) ∈ S(cid:48) such that (ha) = h(cid:48). The successor relation defines the edges of a public tree, where the public states are nodes. It should be noted that more than one action can lead to the same successor public state when some player doesn’t observe the action, and that one action can lead to more than one successor public state if some previously private information becomes public. In the statement of Theorem 3, we used sampt(h) to notate whether h was sampled on iteration t. With the notation introduced here, we can formalize this by defining sampt(h) to occur if and only if h(cid:48) (cid:118) zt for some h(cid:48) ∈ S(h) and some zt ∈ Zt. For clarity, we thus symbolize this relation as S(h) (cid:118) Zt. E.1 Public Outcome Sampling We now define our MCCFR variant, which we call Public Outcome Sampling (POS). Instead of walking trajectories through the EFG tree by sampling actions, POS walks trajectories through the public tree by sampling successor public states. For public state S, let I (S) ⊆ I be the collection of player i information sets contained within S. While i i walking down the tree, POS keeps track of reach probabilities πσt (I ) for each I ∈ I (S) and each player i at i i i i 16
public state S. To recurse, it samples some successor S(cid:48) ∈ T (S) using a probability distribution qt(S) ∈ ∆ . T (S) It updates the reach probabilities to πσt (I ) for each I ∈ I (S(cid:48)), using the current strategy σt. Ultimately, i i i i the recursion reaches a public state which only contains terminal nodes (as the end of the game is publicly observable). This public state, which defines the sampled trajectory in the public tree, we label Zt. The terminal histories are evaluated as u(z) for each z ∈ Zt. Walking back up the tree, at each recursion step we pass back the utilities uˆ (h(cid:48)|σt, Zt) for each h(cid:48) ∈ S(cid:48). From b these, we apply a baseline and recursively calculate utilities as uˆ (h, a|σt, Zt) = 1(S((ha)) (cid:118) Zt) (cid:0) uˆ ((ha)|σt, Zt) − bt(h, a)(cid:1) + bt(h, a) (15) b qt(S(h), S((ha)) b uˆ (h|σt, Zt) = (cid:88) σt(h, a)uˆ (h, a|σt, Zt) (16) b b a∈A(h) for each h ∈ S and a ∈ A(h). We then use these values to calculate regrets rt(I, a) for each I ∈ I (S) and i update the saved regrets. Algorithm 3 gives pseudocode for MCCFR with POS. Algorithm 4 gives pseudocode for a version using the predictive baseline. Algorithm 3 MCCFR w/ POS and baseline 1: function POS-MCCFR(S) 2: if S ⊆ Z then return {u(h) | ∀h ∈ S} 3: for I ∈ I (S) do P (S) 4: σt(I, ·) ← REGRETMATCHING(Rt−1(I, ·)) 5: σt(I, ·) ← t−1 σt−1(I, ·) + 1 σt t t 6: end for 7: sample successor S(cid:48) ∼ qt(S, ·) 8: {uˆ b(h(cid:48)|σt, Zt) | ∀h(cid:48) ∈ S(cid:48)} ← POS-MCCFR(S(cid:48)) 9: for I ∈ I do P (S) 10: for h ∈ I do 11: for a ∈ A(h) do 12: if (ha) ∈ S(cid:48) then 13: uˆ b(h, a|σt, Zt) ← bt(h, a) + qt(S1 ,S(cid:48)) (uˆ b((ha)|σt, Zt) − bt(h, a)) 14: bt+1(h, a) ← UPDATEBASELINE(bt(h, a), uˆ b((ha)|σt, Zt)) 15: else 16: uˆ b(h, a|σt, Zt) ← bt(h, a) 17: end if 18: end for 19: uˆ b(h|σt, Zt) ← (cid:80) a(cid:48) σt(h, a(cid:48))uˆ b(h, a(cid:48)|σt, Zt) 20: end for 21: if P (h) = 1 then 22: rt(I, a) ← πqt1 (S) (cid:80) h∈I π 2σt(h) (uˆ b(h, ·|σt, Zt) − uˆ b(h|σt, Zt)) 23: else if P (h) = 2 then 24: rt(I, a) ← πqt1 (S) (cid:80) h∈I π 1σt(h) (−uˆ b(h, ·|σt, Zt) + uˆ b(h|σt, Zt)) 25: end if 26: Rt(I, ·) ← Rt−1(I, ·) + rt(I, ·) 27: end for 28: return {uˆ b(h|σt, Zt) | ∀h ∈ S} 29: end function Updating a public state S with this algorithm requires walking through all of the possible histories in the public state, as well as all of the actions possible at each history, giving a complexity O(|S||A |), or equivalently max O(|I (S)||I (S)||A |). However, the computations for each information set I ∈ I (S) with acting player i i −i max i can be done completely independently, allowing for easy parallelization (e.g. on a GPU) to achieve complexity O(|I (S)||A |). This approach was taken with the non-sampling algorithm used in DeepStack [19]. −i max 17
Algorithm 4 MCCFR w/ POS and predictive baseline 1: function POS-MCCFR(S) 2: if S ⊆ Z then return {u(h), u(h) | ∀h ∈ S} 3: for I ∈ I (S) do P (S) 4: σt(I, ·) ← REGRETMATCHING(Rt−1(I, ·)) 5: σt(I, ·) ← t−1 σt−1(I, ·) + 1 σt t t 6: end for 7: sample successor S(cid:48) ∼ qt(S, ·) 8: {uˆ b(h(cid:48)|σt, Zt), uˆ b(h(cid:48)|σt+1, Zt) | ∀h(cid:48) ∈ S(cid:48)} ← POS-MCCFR(S(cid:48)) 9: for I ∈ I do P (S) 10: for h ∈ I do 11: for a ∈ A(h) do 12: if (ha) ∈ S(cid:48) then 13: uˆ b(h, a|σt, Zt) ← bt(h, a) + qt(S1 ,S(cid:48)) (uˆ b((ha)|σt, Zt) − bt(h, a)) 14: bt+1(h, a) ← uˆ b((ha)|σt+1, Zt) 15: uˆ b(h, a|σt+1, Zt) ← bt+1(h, a) 16: else 17: uˆ b(h, a|σt, Zt) ← bt(h, a) 18: uˆ b(h, a|σt+1, Zt) ← bt(h, a) 19: end if 20: end for 21: uˆ b(h|σt, Zt) ← (cid:80) a(cid:48) σt(h, a(cid:48))uˆ b(h, a(cid:48)|σt, Zt) 22: end for 23: if P (h) = 1 then 24: rt(I, a) ← πqt1 (S) (cid:80) h∈I π 2σt(h) (uˆ b(h, ·|σt, Zt) − uˆ b(h|σt, Zt)) 25: else if P (h) = 2 then 26: rt(I, a) ← πqt1 (S) (cid:80) h∈I π 1σt(h) (−uˆ b(h, ·|σt, Zt) + uˆ b(h|σt, Zt)) 27: end if 28: Rt(I, ·) ← Rt−1(I, ·) + rt(I, ·) 29: σt+1(I, ·) ← REGRETMATCHING(Rt(I, ·)) 30: for h ∈ I do 31: uˆ b(h|σt+1, Zt) ← (cid:80) a(cid:48) σt+1(h, a(cid:48))uˆ b(h, a(cid:48)|σt+1, Zt) 32: end for 33: end for 34: return {uˆ b(h|σt, Zt), uˆ b(h|σt+1, Zt) | ∀h ∈ S} 35: end function F Proof of Theorem 3 We introduce the following definition that tracks sampled values of terminal histories: (cid:40) u(z) if z ∈ Zτ for any τ < t u˜t(z) = (17) 0 otherwise We begin by showing that the if our baseline function maintains is a weighted sum of values u˜t(z), then our predictive baseline-corrected value estimates will be as well. Lemma 4. Let S((ha)) (cid:118) Zt, and assume that for all (h(cid:48)a(cid:48)) (cid:119) (ha), the predictive baseline function satisfies bt(h(cid:48), a(cid:48)) = (cid:80) πσt ((h(cid:48)a(cid:48)), z)u˜t(z). Then the baseline-corrected utility satisfies z∈Z[(h(cid:48)a(cid:48))] uˆ (h, a|σt+1, Zt) = (cid:88) πσt+1 ((ha), z)u˜t+1(z) (18) b z∈Z[(ha)] 18
Proof. We prove this by induction on the height of (ha) in the tree. Our base case is that (ha) = zt for some zt ∈ Zt, in which case uˆ (h, a|σt+1, Zt) b 1 = (u(zt) − bt+1(h, a)) + bt+1(h, a) qt(S(h), S((ha)) 1 = (u(zt) − u(zt)) + u(zt) by predictive baseline definition qt(S(h), S((ha)) = u(zt) = u˜t+1((ha)) as (ha) = zt ∈ Zt For the inductive step, we consider some (ha) and assume the hypothesis holds for all (h(cid:48)a(cid:48)) with smaller height. uˆ (h, a|σt+1, Zt) b = 1 (cid:0) uˆ ((ha)|σt+1, Zt) − bt+1(h, a)(cid:1) + bt+1(h, a) qt(S(h), S((ha)) b = uˆ ((ha)|σt+1, Zt) by predictive baseline definition b = (cid:88) σt+1((ha), a(cid:48))uˆ ((ha), a(cid:48)|σt+1, Zt) b a(cid:48)∈A((ha)) = (cid:88) σt+1((ha), a(cid:48)) (cid:88) πσt+1 ((haa(cid:48)), z)u˜t+1(z) by inductive hypothesis a(cid:48)∈A((ha)) z∈Z[(haa(cid:48))] = (cid:88) (cid:88) πσt+1 ((ha), z)u˜t+1(z) a(cid:48)∈A((ha)) z∈Z[(haa(cid:48))] = (cid:88) πσt+1 ((ha), z)u˜t+1(z) z∈Z[(ha)] In the last step we use that Z[(ha)] is partitioned into the sets Z[(haa(cid:48))] by which action a(cid:48) ∈ A((ha)) follows (ha). The lemma follows by induction. Next, we show that the predictive baseline update maintains an invariant that the baseline values are a weighted sum of values u˜t(z). Lemma 5. For any time step t, the predictive baseline satisfies bt(h, a) = (cid:88) πσt ((ha), z)u˜t(z) (19) z∈Z[(ha)] Proof. We proceed by induction on time. For the base case, t = 1, by definition bt(h, a) = 0, and u˜t(z) = 0 for all z ∈ Z. For the inductive step, we assume that the lemma holds at time t, and we show that it then follows for time t + 1. We break this into two cases, based on whether (ha) is sampled on time t or not. If S((ha)) (cid:54)(cid:118) Zt, then bt+1(h, a) = bt(h, a) by definition of the predictive baseline. Also by definition u˜t+1(z) = u˜t(z) for any z ∈ Z[(ha)], because otherwise z ∈ Zt. Next, we show that πσt+1 ((ha), z) = πσt ((ha), z) for any z ∈ Z[(ha)]. Assume for the sake of contradiction that this doesn’t hold. Then there must be some (h(cid:48)a(cid:48)) (cid:119) (ha) such that σt+1(h(cid:48), a(cid:48)) (cid:54)= σt(h(cid:48), a(cid:48)). By the definition of the MCCFR algorithm, this only occurs if Rt+1(I(h(cid:48)), a(cid:48)) (cid:54)= Rt(I(h(cid:48)), a(cid:48)), which in turn only occurs if S(h(cid:48)) (cid:118) Zt. But then S((ha)) (cid:118) S(h(cid:48)) (cid:118) Zt, which contradicts the premise. Putting this all together, we have bt+1(h, a) = bt(h, a) = (cid:88) πσt ((ha), z)u˜t(z) by inductive hypothesis z∈Z[(ha)] = (cid:88) πσt+1 ((ha), z)u˜t+1(z) z∈Z[(ha)] This completes the inductive step in the case that (ha) was not sampled. 19
If S((ha)) (cid:118) Zt, then the following holds bt+1(h, a) = uˆ ((ha)|σt+1, Zt) by predictive baseline definition b = (cid:88) πσt+1 ((ha), z)u˜t+1(z) by Lemma 4 z∈Z[(ha)] This completes the inductive step in the case that (ha) was sampled. Thus the inductive step always holds, and the lemma follows by induction. To prove Theorem 3, we note that if Z[h] ⊆ (cid:83) Zτ , then by definition u˜t(z) = u(z) for any z ∈ Z[h]. Thus τ<t we have bt(h, a) = (cid:88) πσt ((ha), z)u˜t(z) by Lemma 5 z∈Z[(ha)] = (cid:88) πσt ((ha), z)u(z) z∈Z[(ha)] = u((ha)|σt) by definition G Measuing counterfactual value variance In Figure 2b of the main paper, we reported empirical variance of counterfactual values for POS MCCFR with baselines. To measure these, we run MCCFR for some number of iterations, then freeze the strategy. We then walk every information set-action pair in the game tree, and for each such pair we run a large number of sampled trajectories originating at the pair. These trajectories are walked as if we were running MCCFR with POS, but we do not update the strategy. Instead, we only calculate the sampled counterfactual value (cid:80) πσt (h)uˆ ((ha)|σt, zt) at the initial I, a pair. From these samples, we compute an estimate of the h∈I −P(h) b true variance of the counterfactual value. Finally, we average these variance estimates across all information set-action pairs in the game. H Baselines in Monte Carlo continual resolving Typically, online play in games with perfect information is accomplished by performing an independent computation at each new decision point to decide the agent’s next action. Traditionally, this approach was intractable in games with imperfect information because there was no way to guarantee that these individual decisions would fit together into a cohesive equilibrium strategy. Instead, the traditional way of playing online was to use a precomputed strategy as a lookup table. Recently, however, techniques have been developed for safe and efficient online computation of strategies in imperfect information games [19, 3, 5]. In this section, we discuss how Monte Carlo baselines fit into this work. A particular example of this new paradigm, continual resolving, was used in the DeepStack agent which defeated poker professionals [19]. Continual resolving contains two key parts. First, a safe resolving method is used to compute each decision independently in an online fashion, while still guaranteeing that the agent plays an approximate equilibrium strategy. Second, value approximation is used to restrict the depth of future actions that are considered when solving each decision. With these ingredients, CFR+ is used to solve a relatively small subgame each time the agent must make an action selection. Šustr et al. [28] replaced the CFR+ solver with MCCFR, creating Monte Carlo continual resolving (MCCR). It is straightforward to use our baseline framework within MCCR. We conducted an experiment examining MCCR with baselines. We performed our experiment in Leduc. We measure the exploitability of strategy profiles that are constructed by independently solving each decision point as in online self-play. For each decision, we solve a subgame of depth three (i.e. looking a maximum of three actions into the future). After three actions, we approximately a history by running 100 iterations of CFR+ on the subtree rooted at the history and using the resulting strategyc to generate values. At each decision point, we run resolving until we have performed a maximum number of evaluations, either at terminal histories or depth-limited evaluations. We use this as an implementation-independent way of comparing algorithms, as evaluations take the vast majority of computation time in continual resolving. We compare MCCR with and without baselines. We use CFR+ updates, which we’ve found to decrease variance when combined with any baseline, and public outcome sampling with transitions sampled from the uniform cThis strategy, which contain errors because of the low number of iterations, approximates using a neural net for evaluation. 20
distribution. Because of the inexact nature of the evaluation function, Theorem 3 does not hold in this setting, and we found learned history baselines to slightly outperform predictive baselines in preliminary experiments. We also compare to (deterministic) continual resolving, with both CFR and CFR+ update rules.                                          P D [ L P X P  H Y D O X D W L R Q V   H P D J  V S L K F   \ W L O L E D W L R O S [ H  0 & & ) 5  Z   Q R  E D V H O L Q H  0 & & ) 5  Z   K L V W R U \  E D V H O L Q H  & ) 5  & ) 5  Figure 3: Exploitability of continual resolving strategies based on the maximum number of evaluations al- lowed per resolve. For MCCFR, results are averaged over 20 runs with 95% confidence intervals shown as (indiscernible) error bands. Results are shown in Figure 3. We see that the inclusion of a baseline significantly decreases the exploitability of MCCR strategies. Without a baseline, MCCR is not competitive with the deterministic continual resolving. With a baseline, it is able to clearly outperform continual resolving with CFR updates, and slightly outperform continual resolving with CFR+ updates. This is especially notable because there is still plenty of room to improve the technique, such as by tuning the baseline and especially by refining the sampling strategy. 21
