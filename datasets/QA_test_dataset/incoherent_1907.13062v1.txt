9102 luJ 03 ]SD.sc[ 1v26031.7091:viXra Iterative Budgeted Exponential Search Malte Helmert∗1 , Tor Lattimore2 , Levi H. S. Lelis3 , Laurent Orseau2 and Nathan R. Sturtevant4 1Department of Mathematics and Computer Science, University of Basel, Switzerland 2DeepMind, London, UK 3Departamento de Informática, Universidade Federal de Viçosa, Brazil 4Department of Computing Science, University of Alberta, Edmonton, AB, Canada malte.helmert@unibas.ch, {lattimore, lorseau}@google.com, levi.lelis@ufv.br, nathanst@ualberta.ca Abstract tree search algorithms perform successive depth-first searches with an increasing limit on the cost. They also forgo global We tackle two long-standing problems related to duplicate elimination, meaning that they do not detect if re-expansions in heuristic search algorithms. For multiple paths from the initial state lead to the same state, graph search, A* can require Ω(2n∗ ) expansions, which can lead to exponentially worse runtime compared where n is the number of states within the final ∗ to algorithms like A* when such duplicates are frequent. f bound. Existing algorithms that address this Hybrid algorithms that uses bounded memory for duplicate problem like B and B’ improve this bound to elimination are possible [Akagi et al., 2010, for example]. Ω(n2). For tree search, IDA* can also require ∗ IDA* [Korf, 1985] is a cautious linear-memory algorithm Ω(n2) expansions. We describe a new algorithmic ∗ that increases the f-cost bound minimally (see also RBFS framework that iteratively controls an expansion [Korf, 1993]). At each iteration, IDA* searches all nodes budget and solution cost limit, giving rise to new up to the f-cost bound. The minimum cost of the graph and tree search algorithms for which the nodes pruned in one iteration becomes the cost bound for number of expansions is O(n log C∗), where C∗ ∗ the next iteration. This approach ensures that the last is the optimal solution cost. Our experiments show cost bound will be exactly the minimum solution cost. that the new algorithms are robust in scenarios This is efficient when the number of nodes matching the where existing algorithms fail. In the case of tree current cost bound grows exponentially with the number search, our new algorithms have no overhead over of iterations, as the total number of expansions will IDA* in scenarios to which IDA* is well suited be dominated by the last iteration. In the worst case and can therefore be recommended as a general however, IDA* may expand only one new node in each replacement for IDA*. iteration, leading to a quadratic number of (re-)expansions. Several methods have been developed to mitigate the re-expansion overhead of IDA* [Burns and Ruml, 2013; 1 Introduction Sharon et al., 2014; Hatem et al., 2018; Sarkar et al., 1991; There are two long-standing problems in heuristic search Wah and Shang, 1994] by increasing the cost bound more where existing algorithms struggle to balance the number of aggressively at each iteration with the aim of achieving an expansions and re-expansions performed in comparison to an exponential growth rate. However, with this approach the last oracle. One is in graph search, the other in tree search. cost bound can be larger than the minimum solution cost, The first problem deals with admissible but inconsistent which may incur an arbitrarily large performance penalty. heuristics in graph search. With some caveats [Holte, 2010], For these algorithms, theoretical guarantees (when provided) A* with an admissible and consistent heuristic expands require strong assumptions such as uniformity of the costs or the minimum required number of states [Hart et al., 1968; branching factor [Hatem et al., 2015, for example]. Dechter and Pearl, 1985]. However, with inconsistent heuris- We propose a novel framework called Iterative Budgeted tics it may expand exponentially more states than more Exponential Search (IBEX) guaranteeing for both problems cautious algorithms such as B [Martelli, 1977] and B’ described above that the number of expansions is near- [Mérõ, 1984], which have a quadratic worst case. linear in the number of nodes whose cost is at most the The second problem is in heuristic tree search algorithms minimum solution cost. This is achieved by combining that use memory that grows only linearly with the search two ideas: (1) a budget on the number of expansions and depth. In contrast, A* memory usage grows linearly with (2) an exponential search for the maximum f-cost that can time and often exponentially with the depth of the search. be searched exhaustively within the given budget. This To satisfy such low memory requirements, linear-memory framework proves that no solution can be found for the *Alphabetical order. This paper is the result of merging current budget, and then doubles it until a solution is found. two independent submissions to IJCAI 2019 [Orseau et al., 2019; This ensures that the last budget is always within twice the Sturtevant and Helmert, 2019]. minimum required budget, while amortizing the work on
early iterations due to the exponential growth of the budget. [Mérõ, 1984], but it can even be shown that all algorithms We develop two simple and fast algorithms that enjoy may need to expand exponentially too many nodes in some near-linear expansion guarantees, propose a number of cases (see Appendix E). Hence we focus our attention on enhancements, show how the tree search and graph search the following relaxed notion of optimality. Let n = G∗ problems can be reduced to our framework, and show that |{s : min max f (s′) ≤ C∗}| be the π:state(π)=s s′∈π(s) these algorithms perform at least as well as state-of-the- number of states that can be reached by a path along art algorithms on a number of traditional domains without which all states have f -cost at most C∗; this is the exhibiting any of the catastrophic failure cases. definition used by Martelli [1977]. Then, there exist problems where A* performs up to Ω(2nG∗ ) expansions 2 Heuristic Search Problems [Martelli, 1977]. This limitation has been partially addressed with the B [Martelli, 1977] and B’ [Mérõ, 1984] algorithms A black box heuristic search problem is defined by a finite for which the number of expansions is at most O(n2 ). We state space S, a set of goal states S∗ ⊆ S, a cost function G∗ improve on this result with a new algorithm for which the c : S × S → [0, ∞] and an initial state s init ∈ S. The number of expansions is at most O(n log(C∗)). successors of a state s are those states s′ that can be reached G∗ by a finite-cost edge: succ(s) = {s′ : c(s, s′) < ∞}. This 2.2 Tree Search defines a directed graph G = (S, E) where states correspond Tree search algorithms work on the tree expansion of the to vertices in the graph and the edges are the finite-cost state space, where every path from s corresponds to a tree successors E = {(s, s′) : c(s, s′) < ∞}. A path is a init node. Consequently, states reached on multiple paths will be sequence of states π = (s )m with s = s and its cost t t=1 1 init expanded multiple times. is g(π) = m t=− 11 c(s t, s t+1), which may be infinite if there We say a path π is necessarily expanded if max s′∈π f (s′) < is no edgePbetween adjacent states. The end state of path C∗ and possibly expanded if max f (s′) ≤ C∗. A π = (s )m is state(π) = s and its successor paths are s′∈π t t=1 m tree search algorithm must always expand all necessarily succ(π) = {(s t)m t=+ 11 : s t+1 ∈ succ(s m)}. A state s is expanded paths and will usually also expand some paths expanded when the function succ(s) is called for s; a state s′ that are possibly but not necessarily expanded. To avoid the is generated when succ(s) is called creating s′ ∈ succ(s). A subtleties of tie-breaking, we discuss upper bounds in terms node corresponds to a single path π from the root of a search of possibly expanded paths, leaving a more detailed analysis tree. Expanding a node corresponds to expanding state(π) for future work. We write n for the number of possibly T ∗ and generating all the corresponding successor paths (nodes). expanded paths in a tree search. Search algorithms may expand the same state multiple times In the worst case, IDA* may perform Ω(n2 ) expan- T ∗ because multiple nodes might represent the same state. Let sions. To mitigate this issue, algorithms such as π∗(s) = argmin π:state(π)=s g(π) be a least-cost path to state IDA*_CR [Sarkar et al., 1991] and EDA* [Sharon et al., 2014] s and g∗(s) = g(π∗(s)) be the cost of such a path. Let increase the f -cost bound more aggressively. These methods Π∗ be the set of all paths from the initial state to all goal are effective when the growth of the tree is regular enough, states. Then, the cost of a least-cost path to a goal state is but can fail catastrophically when the tree grows rapidly near C∗ = min π∈Π∗ g(π). The objective is to find a least-cost the optimal f -cost, as will be observed in the experiments. path from the initial state to a goal state. We provide an algorithm that performs at most a logarithmic Let h∗(s) be the minimal cost over all paths from s to factor more expansions than n and uses memory that is T ∗ any goal state. A heuristic is a function h : S → [0, ∞] linear in the search depth. that provides an estimate of h∗. A heuristic is admissible Notation. The natural numbers are N = {0, 1, 2, . . .} and if h(s) ≤ h∗(s) for all states s ∈ S and consistent if N = {1, 2, 3, . . .}. For real-valued x0 and a let ⌈x⌉ = h(s) ≤ h(s′) + c(s, s′) for all pairs of states s, s′. The f - 1 ≥a max{a, ⌈x⌉} and similarly for ⌊x⌋ . cost of a path is f (π) = g(π) + h(state(π)). Note that if ≥a s = state(π) is a goal state and the heuristic is admissible 3 Abstract View we must have h(s) = 0. We say that a search algorithm is a graph search if it We now introduce a useful abstraction that allows us to treat eliminates duplicates of states generated by the algorithm; tree and graph search in a unified manner. Tree search is otherwise it is called a tree search. used as a motivating example. The problem with algorithms like EDA* that aggressively increase the f -cost limit is the 2.1 Graph Search possibility of a significant number of wasted expansions With a consistent heuristic, f -costs along a path are non- once the f -cost limit is above C∗. The core insight of our decreasing, thus a graph search algorithm must expand framework is that this can be mitigated by stopping the search all states in the graph with f (s) = g∗(s) + h(s) < if the number of expansions exceeds a budget and slowly C∗. In this setting, A* has an optimal behaviour increasing the budget in a careful manner. [Dechter and Pearl, 1985]. When the heuristic is admissible A depth-first search with an f -cost limit and expansion but inconsistent, for comparing algorithms one could budget reveals that either (a) the expansion budget was consider the ideal number of nodes that A* would expand insufficient to search the whole tree with f -cost smaller or if the heuristic was made consistent. Unfortunately, not equal to the limit, or (b) the expansion budget was sufficient. only does there exist no optimal algorithm for this case In the latter case, if the goal is found, then the algorithm
can return a certifiably optimal solution. Furthermore, when query (C, b) = [C, ∞] if C < C crit(b) budget sufficient , the budget is insufficient the largest f -cost of a node visited lim (cid:26)[1, C] if C ≥ C crit(b) budget exceeded . by the search serves as an upper bound on the largest f - Integer feedback. In many practical problems, the list A cost for which the budget will be exceeded. When the only contains integers. In this case we consider the feedback budget is sufficient, the smallest f -cost in the fringe is a model: lower bound on the same. This information means that combining exponential search [Bentley and Yao, 1976] with [⌊C⌋ + 1, ∞] if C < C (b) , query (C, b) = crit repeated depth-first searches with a varying f -cost limit and int (cid:26)[1, C] if C ≥ C (b) . crit fixed expansion budget can be used to quickly find a solution The discrete nature of the returned interval means that if if the budget is sufficient to expand all nodes with f -cost less than C∗ and otherwise produce a certificate that the budget is C crit(b) ∈ [C, C + 1], then insufficient, a process we explain in detail in Section 5. query (C, b) ∩ query (C + 1, b) = {C (b)} , int int crit Based on this idea, the basic version of our new algorithm In other words, the exact value of C (b) can be identified operates in iterations. Within each iteration the algorithm crit by making queries on either side of an interval of unit width makes multiple depth-first searches with a fixed expansion containing it. By contrast, query cannot be used to identify budget and varying f -cost limits. An iteration ends once the lim C (b) exactly. algorithm finds the optimal solution and expands all paths crit with f -cost less than C∗, or once it can prove that the present Extended feedback. For heuristic search problems the interval returned by the query function can be refined more expansion budget is insufficient to find the optimal solution. At the end of the iteration the expansion budget is doubled. precisely by using the smallest observed f -cost in the fringe and largest f -cost of an expanded path. Define A (C) = In what follows we abstract the search procedure into a > min{v ∈ A : v > C} and A (C) = max{v ∈ A : v ≤ C}. query function that accepts as input an f -cost limit and an ≤ When C ∈ A we define δ(C) = A (C) − A (C) and expansion budget and returns an interval that contains the > ≤ smallest f -cost limit for which the budget is insufficient or δ = min{δ(C) : C ≤ C∗, C ∈ A} . min throws an exception with an optimal solution if the f -cost These concepts are illustrated in Fig. 1. In the extended limit is at least C∗ and the expansion budget is larger or equal feedback model, when the expansion budget is sufficient the to the number of nodes with f -cost less than the limit. response of the query is the interval [A (C), ∞]. Otherwise > Formal model. We consider an increasing list A of real the query returns an interval [1, v] where v is any value in numbers v ≥ 1, possibly with repetition. Define a function A ∩ [C crit(b), C] (for example v = A ≤(C)): n : [1, ∞) → N by n(C) = |{v ∈ A : v ≤ C}|, 0 query (C, b) = where multiple occurrences are counted separately. Next, let ext C∗ ∈ A and n = n(C∗). In our application to tree search, [A (C), ∞] if C < C (b) , ∗ > crit A is the list of all node f values, including duplicates, and (cid:26)[1, v], with v ∈ [C (b), C] ∩ A if C ≥ C (b) . crit crit n(C) is the number of paths in the search tree for which the f values is at most C and C∗ is the cost of the optimal solution. In tree search the value of v when C ≥ C crit(b) is the We can require that all f values are at least 1 with no loss of largest f -cost over paths expanded by the search, which may generality: if h(s ) < 1 we introduce an artificial new initial depend on the expansion order. As for integer feedback, init the information provided by extended feedback allows the state with heuristic value 1 and an edge of cost 1 − h(s ) init algorithm to prove that an expansion budget is insufficient from the new state to s . This shifts all path costs by at init most 1, so if C′ is the original optimal solution cost, we have to find a solution. C∗ ≤ C′ + 1. Summary of results. In the following sections we describe algorithms for all query models for which the number of Query functions. Let C (b) = min{v ∈ A : n(v) > b} crit node expansions is at most a logarithmic factor more than be the smallest value in A for which expansion budget b is n . The limited feedback model is the most challenging and insufficient. We define three functions, query , query ∗ lim int is detailed last, while the extended feedback model provides and query , all accepting as input an f -cost limit C and ext the cleanest illustration of our ideas. The logarithmic factor expansion budget b. A call to any of the functions makes at depends on C∗ and δ or δ(C∗). The theorems are most min{b, n(C)} expansions and throws an exception with min summarized in Table 1, with precise statements given in the an optimal solution if n ≤ n(C) ≤ b. Otherwise all three ∗ relevant sections. functions return an interval containing C (b) on which we crit Overview. In the next section we implement query for make different assumptions as described next. The abstract ext tree search and for graph search (Section 4). We then objective is to make a query that finds an optimal solution introduce a variant of exponential search that uses the using as few expansions as possible. query function to find the ‘critical’ cost for a given budget Limited feedback. In the limited feedback model the query (Section 5). Our main algorithm (IBEX) uses the exponential function returns an interval that only provides information search with a growing budget an optimal solution is found about whether or not the expansion budget b was smaller or (Section 6). The DovIBEX algorithm is then provided to deal larger than n(C): with the more general limited feedback setting (Sections 7 and 8).
n Algorithm 1 Query with extended feedback for tree search δ δ(C) min 1 def query ext(C, budget): 2 data.min_fringe = ∞ # will be > C b = 3 3 data.max_visited = 0 # will be ≤ C data.expanded = 0 # number of expansions 1 4 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 C 5 data.best_path = none # f (none) = ∞ try: C crit(b) A ≤(C) C A >(C) C∗ 6 7 DFS(C, budget, {s init}, data) catch "budget exceeded": Figure 1: The function n(·), generated by b = 3, C = 10.2, 8 C the∗ s= ma1 ll5 esa tn vd alA ue = in A[1, fo3 r, 5 w, h5 i, c8 h, t1 h2 e, re13 a, re13 m, 1 or5 e]. thC ancr bit( =b) 3= val5 uei ss 19 0 ifre datu tar .n be[1 s, td _a pt aa t. hma 6=x_ nv oi ns ei :te #d] solution found throw data # to be dealt with by the user of at most 5. The largest value at most C in A is A (C) = 8, and 11 ≤ A >(C) = 12 is the next value in A. We also have δ min = 1 and 12 return [data.min_fringe, ∞] δ(C) = 4. Example queries include: query lim(C = 4, b = 3) = 13 # data: return info, passed by reference [4, ∞] and query ext(C = 4, b = 3) = [5, ∞] and query ext(C = 14 # π: path 9, b = 3) = [1, 8]. 15 def DFS(C, budget, π, data): 16 if f (π) > C: Limited feedback O(Z log Z), Z = n log C∗ 17 data.min_fringe = min(data.min_fringe, ∗ δ(C∗) f (π)) (cid:16) (cid:17) Extended feedback O n log C∗ 18 return (cid:16) ∗ (cid:16) δmin (cid:17)(cid:17) 19 data.max_visited = max(data.max_visited, f (π)) Integer feedback O (n log(C∗)) ∗ 20 if f (π) ≥ f (data.best_path): return # branch and bound Table 1: Number of expansions in the worst case of our algorithms 21 for the different types of feedback. 22 if is_goal(state(π)): 23 data.best_path = π # Here we could throw the solution if its 24 4 Reductions # cost is equal to a known lower bound 25 return We now explain how to reduce tree search and graph search 26 if data.expanded == budget: to the abstract framework and implement query for these 27 ext throw "budget exceeded" domains. These query functions will be used in the next 28 data.expanded++ sections as part of the main algorithms. Recall that the 29 number of expansions performed by query(C, ∞) must be at 30 for s′ ∈ succ(state(π)): most n(C) and n(·) is non-decreasing and that n = n(C∗). 31 DFS(C, budget, π + {s′}, data) ∗ The list A is composed of the f -costs of the nodes encountered during the search. Recall from our problem definition that each node corresponds to a path π, and that rithm 7 (Appendix A). The graph_search function is expanding a node corresponds to expanding a state s = equivalent to BFIDA* [Zhou and Hansen, 2004] with the state(π). For a search cost-bounded by C, let the fringe breadth-first search replaced with Uniform-Cost Search be all generated nodes with f (π) > C. Also, let the set of (UCS) [Russell and Norvig, 2009; Felner, 2011], using an visited nodes be the generated nodes such that f (π) ≤ C. f -cost limit C on the generated nodes and tracking the maximum f -cost among visited states and the minimum Tree search. For tree search we implement query in ext f -cost in the fringe. As in UCS, states are processed in Algorithm 1, which is a variant of depth-first search with increasing g-cost order. Since g is non-decreasing, states are an f -cost limit C and expansion budget b. The algorithm not expanded more than once in each call to Algorithm 7. terminates before exceeding the expansion budget and tracks Therefore the number of expansions is at most n(C) = |{s : the smallest f -cost observed in the fringe and the largest f - min max f (s′) ≤ C}| and the number of cost of any visited node, which are used to implement the π:state(π)=s s′∈π expansions made by query(C∗, ∞) is at most n(C∗) = n extended feedback model. Thus, for an f -cost bound C, G∗ as required. n(C) = |{π : max f (s′) ≤ C}|. When an optimal s′∈π solution is found, the algorithm throws an exception, which is expected to be caught and handled by the user. The function 5 Exponential Search query could be implement like query without needing to lim ext With the reductions out of the way, we now introduce a track min_fringe and max_expanded, but that would throw budgeted variant of exponential search [Bentley and Yao, 1976], away valuable information. which is closely related to the bracketed bisection method Graph search. In graph search, query is imple- [Press et al., 1992, §9]. ext mented in a similar way as Algorithm 1, but DFS is Algorithm 2 accepts as input a budget b, an initial replaced with graph_search , which appears in Algo- cost limit start ≤ C (b) and a function query ∈ crit
Algorithm 2 Exponential search with budgeted queries 1 2 3 4 5 C = 2.6 low = 1.3 high = ∞ 1 def exp_search(start, b, query): C = 5.8 low = 2.9 high = ∞ 2 low = start C = 4.0 low = 2.9 high = 5.0 3 high = ∞ C = 4.8 low = 4.5 high = 5.0 4 loop: low high = 4.5 5 if high == ∞: Ccrit(b) 6 C = 2×low # exponential phase else: Figure 2: The four queries made by exp_search with the 7 C = (low + high) / 2 # binary phase extended feedback model and start = 1.3 and budget b = 8 9 [low, high] = [low, high] ∩ query(C, b) 7. The ticks below the x-axis indicate the elements of A = until low == high [1.4, 1.5, 1.8, 2.3, 2.9, 3.5, 3.6, 3.9, 4.5, 5, 6]. The small circles are 10 the values of C in each query. In the first call to query the budget return low ext 11 was sufficient and so low is set to A>(C) = 2.9, which is doubled to produce the next query. In the second call to query, C = 5.8 leads to an insufficient budget and then high is set to 5.0. In the {query ext, query int, query lim}. The algorithm starts by third query C low is increased to 4.5, which is C crit(b). In the fourth setting low = start and initiates an exponential phase query the budget is insufficient and the algorithm halts. where low is repeatedly doubled until query(2 × low, b) has insufficient budget. The algorithm then sets high = 2 × low Proof. Define r = n (C , C∗, δ ). Let k = and performs a binary search on the interval [low, high] until 1 exp min min ∗ ⌈log n ⌉ be the first iteration k for which b ≥ n . low = high. See Fig. 2 for an illustration. 2 ∗ k ∗ Proposition 2 shows that for iterations k < k the number The discrete structure in the integer and extended feedback ∗ of queries performed in the call to exp_search is at most models ensures that the algorithm halts after at most n (C , C (b ), δ ) ≤ r . Proposition 1 shows that logarithmically many queries and returns C (b). In the exp k crit k min 1 crit the game ends during iteration k after a number of queries limited feedback model the algorithm generally does not halt, ∗ bounded by n (C , C∗, C (b )−C∗) ≤ r . Since each ab ru et w sui mll mm aa rk ie zea dte ir nm thin eat ni en xg tq tu we ory pi rf ob po≥ sin ti∗ o. nT s,h wes he icp hro up se ert ti he es call to query wex ip th bk u∗ dget b k c er xit pank d∗ s at most b k1 = 2k nodes, following definition: the total number of expansions is bounded by k k∗ =1 2kr 1 ≤ x x 2k∗+1r 1 ≤ 22+log2 n∗ r 1 = 4n ∗r 1. P n (ε, x, ∆) = 1 + log + log . exp l 2 (cid:16) ε (cid:17)m≥1 j 2 (cid:16) ∆ (cid:17)k≥0 Remark 4. Algorithm 3 also works when query is ext This is an upper bound on the number of calls to query needed replaced by query , and now δ = 1. int min when starting at start = ε, finding a upper bound high ≥ x and then reducing the interval [low, high] to a size at most ∆ When used for graph search, we call the resulting IBEX (leading to a query within that interval). Recall that making variant Budgeted Graph Search (BGS), and for tree search a query with cost limit C and expansion budget b will find an Budgeted Tree Search (BTS). optimal solution if n ∗ ≤ n(C) ≤ b. Remark 5. Observe that if DFS or graph_search are called Proposition 1. Suppose b ≥ n ∗. Then for any feedback with budget ≥ n(C) ≥ n ∗, it throws an optimal solution. model Algorithm 2 makes a query that terminates the Since the state space is finite, both BTS and BGS return an interaction after at most n (start, C∗, C (b) − C∗) optimal solution if one exists. If no solution exists, BGS will exp crit calls to query. exhaust the graph and return “no solution”, but BTS may run forever unless additional duplicate detection is performed. Proposition 2. Suppose b < n . Then, for the extended ∗ feedback model, Algorithm 2 returns C (b) with at most crit n (start, C (b), δ ) queries. 7 Uniform Budgeted Scheduler exp crit min While IBEX handles the integer and extended feedback 6 Iterative Budgeted Exponential Search models, it cannot handle the limited feedback model. This The Iterative Budgeted Exponential Search (IBEX) algorithm is handled by a new algorithm, presented in the next section, uses the extended query model, which is available in our using a finely balanced dovetailing idea that we call the applications to tree and graph search. Algorithm 3 initializes Uniform Budgeted Scheduler (UBS, see Algorithm 4) and a lower bound on the optimal cost with the lowest value in the array C = min A; we now denote this quantity by 1 Algorithm 3 Iterative Budgeted Exponential Search C . It subsequently operates in iterations k ∈ N . In min 0 iteration k it sets the budget to b k = 2k and calls C k+1 = 1 def IBEX(): # simple version exp_search(C k, b k, query ext) to obtain a better lower bound. 2 C 1 = C min Theorem 3. The number of expansions made by Algorithm 3 3 for k = 1,2,... is at most 4 b k = 2k 4n n (C , C∗, δ ) = O n log C∗ . 5 C k+1 = exp_search(C k, b k, query ext) ∗ exp min min ∗ (cid:18) (cid:18) δ (cid:19)(cid:19) min
Algorithm 4 Uniform Budgeted Scheduler Theorem 6. For any (k, r) with T (k, r − 1) < τ , k 1 def UBS(T , run_prog): T (k, r) ≤ min{τ , T (k, r)} . UBS j j 2 qq .= inm sa ek re t(_ (p 1r ,io 1r )i )ty_ #qu ke =u 1e( ,T r) =1 jX∈N1 3 while not q.empty(): 4 Corollary 7. For any (k, r) with T (k, r − 1) < τ , # Remove prog of minimum T cost k 5 6 (k, r) = q.extract_min() T (k, r) ≤ T (k, r) max{j : T (j, 1) ≤ T (k, r)} . UBS 7 budget = T (k, r) − T (k, r − 1) 8 if run_prog(k, budget) != "halted": Example 8. A good choice is T (k, r) = r2k. Then 9 q.insert((k, r + 1)) Corollary 7 implies that T (k, r) ≤ r2k ⌊k + log (r)⌋. 10 if r == 1: UBS 2 11 q.insert((k + 1, 1)) return none 8 DovIBEX: Limited Feedback 12 DovIBEX uses UBS to dovetail multiple instances of exponential search (see Algorithm 5). The algorithm can use takes inspiration from Luby et al. (1993) speedup algorithm. any of the three queries, while still exploiting the additional UBS runs a growing and unbounded number of programs in a information provided in the extended and integer feedback dovetailing fashion, for varying segments of steps. The notion models. Following Example 8, we use T (k, r) = r2k so of step is to be defined by the user; in heuristic search we that T (k, r) − T (k, r − 1) = 2k. Program k executes take it to be a single node expansion. During one segment, one iteration of the loop of exponential search with budget the selected program can make arbitrary computations but b = 2k; if the budget is not entirely used during a k must use no more steps than its current budget. Program k segment, the segment ends early. In the limited feedback halts when it reaches exactly τ steps, which may be infinite. model, exponential search may continue halving the interval k UBS maintains a priority queue of pairs (program index k, [C , C ] indefinitely and never halt. The scheduler low high segment number r), initialized with (1, 1) and ordered by a solves this issue: by interleaving multiple instantiations of function T : N × N → N with T (k, r) < T (k, r + 1) exponential search with increasing budgets, the total time can 1 0 0 and T (k, r) ≤ T (k + 1, r) for all (k, r) and T (k, 0) = 0 for be bounded as a function of the time required by the first all k. In each iteration UBS removes the T -minimal element program k that finds a solution. (k, r) from the front of the queue and calls run_prog (k, b) Theorem 9. For query ∈ {query , query , query }, the with b = T (k, r) − T (k, r − 1), which means that program ext int lim number of expansions made by Algorithm 5 is at most k is being run for its rth segment with a budget of b steps, leading for program k to a total of at most T (k, r) steps over Z ⌊log Z⌋ , Z = 2n n (C , C∗, δ(C∗)) the r segments. If this does not cause program k to halt, then 2 ∗ exp min (k, r+1) is added to the priority queue. Finally, if r = 1, then and also at most O(Z′ log Z′) with (k + 1, 1) is added too. The function run_prog is defined by the user and may store and restore the state of program k as Z′ = min b n (C , C∗, C (b) − C∗) . (1) exp min crit well as allow access to a shared memory space. b≥n∗ The monotonicity assumptions on T mean that UBS is The dependency on δ(C∗) is gentler than the dependency essentially executing program/segment pairs (k, r) according on δ in Theorem 3. It means that the behaviour of to a Uniform Cost Search where a pair (k, r) is a node in the min DovIBEX only depends on the structure of the search space search tree with cost T (k, r) (Fig. 3). In this sense UBS tries at the solution rather than on the worst case of all iterations. (asymptotically) to maintain a uniform amount of steps used among all non-halting programs. Remark 10. Sometimes there exists a b > n for which ∗ Let T (k, r) be the number of steps used by UBS after UBS executing (k, r). T (k, r) = max{T (j, m) : T (j, m) ≤ bn (C , C∗, C (b) − C∗) ≪ n n (C , C∗, δ ) . j exp min crit ∗ exp min min T (k, r), m ∈ N } is an upper bound on the number of steps 0 used by program j when UBS executes (k, r). In these cases the combination of UBS and exponential search can improve on Algorithm 3. Furthermore when using extended feedback, programs k < k will now halt if they ∗ can prove they cannot find a solution. This allows us to (k = 1, r = 1) (k = 1, r = 2) (k = 1, r = 3) provide the following complementary bound, which is only (k = 2, r = 1) (k = 2, r = 2) halts an additive 2n ∗r 2 ⌊log 2 r 2⌋ term away from Theorem 3. (k = 3, r = 1) (k = 3, r = 2) (k = 3, r = 3) Theorem 11. When query = query ext, the number of expansions made by Algorithm 5 is at most 2n (r + ∗ 1 r (1 + ⌊log r ⌋) with r = n (C , C∗, δ ), r = Figure 3: An example tree used by UBS. Program k = 2 halts after n2 (C , C2 ∗,2 δ(C∗)). 1 exp min min 2 τ ≤ T (2, 2) steps. exp min 2
Algorithm 5 Dovetailing IBEX Algorithm 6 Enhanced IBEX 1 def run_prog(k, b): 1 # α: factor on budget, which must be ≥ 2 2 [C low, C high] = get_state(k, default = [C min, ∞] 2 # is_additive: True is using additive search ) 3 def IBEX_enhanced(α=8, is_additive): 3 if C high = ∞: # exponential phase 4 C low = C min 4 C = 2 × C low 5 b = 1 5 else: # binary phase 6 for k = 1, 2, . . .: 6 C = (C low + C high)/2 7 [C low, C high], n used = query+ ext(C low, ∞) 7 [C low, C high] = [C low, C high]∩ query(C, b) 8 if n used < 2b: 8 if C low >= C high: return "halted" 9 for j = 1, 2, . . .: 9 store_state(k, [C high, C low]) 10 if C high == ∞: # exponential phase 10 def DovIBEX(): 11 if is_additive: 11 UBS((k, r) 7→ r2k, run_prog) 12 C = C low + 2j else: 13 14 C = C low × 2 else: # binary phase 15 9 Enhancements 16 C = (C low + C high)/2 17 [C l′ ow, C h′ igh], n used = query+ ext(C, α × b) 18 [C low, C high] = [C low, C high] ∩ [C l′ ow, C h′ igh] We now describe three enhancements for Algorithm 3 (see 19 if (C h′ igh == ∞ and n used ≥ 2b) or Algorithm 6). The enhancements use a modified query 20 C low == C high: function called query+ ext that returns the same interval 21 break as query ext and the number of expansions, which is 22 b = max(2b, n used) n = min{b, n(C)}. For notational simplicity we write used [x, y], n = query+ (C, b). used ext 10 Experiments First, in each iteration of the enhanced IBEX a query is performed with an infinite expansion budget and minimum We test1 IBEX (BTS, enhanced), DovIBEX (DovBTS, cost limit C low (Line 7). If the resulting number of expansions enhanced), IDA* [Korf, 1985], IDA*_CR [Sarkar et al., 1991] is at least 2b, where b is the current budget, then IBEX updates and EDA* [Sharon et al., 2014]. EDA*(γ) is a variant of C low, skips the exponential search and moves directly to the IDA* designed for polynomial domains that repeatedly calls next iteration. If furthermore the DFS algorithm is given DFS with unlimited budget and a cost threshold of γk at iter- the lower bound C k and throws a solution if its cost is C k, ation k. In our experiments we take γ ∈ {2, 1.01}. IDA*_CR then this guarantees that IBEX performs exactly like IDA* in behaves similarly, but adapts the next cost threshold by domains in which the number of expansions grows by at least collecting the costs of the nodes in the fringe into buckets and a factor 2 in each iteration. If the queries with infinite budget selecting the first cost that is likely to expand at least bk nodes (Line 7) do not help skipping iterations, in the worst case they in the next iteration. Our implementation uses 50 buckets and cost an additive 2n ∗ expansions. sets b = 2. The number of nodes (states) of cost strictly below C∗ is reported as n< (n< ). The second enhancement is an early stopping condition T ∗ G∗ These algorithms are tested for tree search on the for proceeding to the next iteration. Algorithm 3 terminates 15-Puzzle [Doran and Michie, 1966] with the Manhattan an iteration once it finds C (b), which can be slow when crit distance heuristic with unit costs and with varied edge δ(C (b)) is small. Algorithm 6 uses a budget window crit costs of 1 + 1/(t + 1) to move tile t, on (12, 4)-TopSpin defined by 2b and αb, where α ≥ 2, so that whenever a [Lammertink, 1989] with random action costs between 40 query is made and the number of expansions is in the interval and 60 and the max of 3 4-tile pattern database heuristics, [2b, αb], the algorithm moves on to the next iteration (line 19). on long chains (branching factor of 1 and unit edge costs, Hence iteration k ends when a query is made within budget solution depth in [1..10 000]), and on a novel domain, which with a cost within [C (2b), C (αb)]. crit crit we explain next. The third enhancement is the option of using an additive In order to evaluate the robustness of the search algorithms, variant of the exponential search algorithm, which increases we introduce the Coconut problem, which is a domain with C by increments of 2j at iteration j during the exponential varied branching factor and small solution density. The low phase (line 12). This variant is based on the assumption heuristic is 0 everywhere, except at the root where h = 1. that costs increase linearly when the budget doubles, which At each node there are 3 ‘actions’, {1, 2, 3}. The solution often happens in heuristic search if the search space grows path follows the same action (sampled uniformly in [1..3]) exponentially with the depth. for D steps, then it follows a random path sampled uniformly These enhancements can also be applied to DovIBEX (see 1 All these algorithms are implemented in C++ in the publicly Algorithm 8 in Appendix C). available HOG2 repository, https://github.com/nathansttt/hog2/.
Algorithm 15-Puzzle (unit) 15-Puzzle (real) (12, 4)-Topspin Chain Coconut α add? Solved Exp. Solved Exp. Solved Exp.×103 Solved Exp.×104 Solved Exp.×104 BTS 2 y 100 242.5 97 3 214.1 100 1 521.9 100 302.0 100 72.9 8 y 100 242.5 100 673.1 100 597.0 100 198.2 100 84.7 2 n 100 242.5 97 3 549.1 100 1 600.0 100 111.8 100 58.5 8 n 100 242.5 99 1 320.3 100 614.6 100 26.7 100 86.8 DovBTS 2 y 100 390.5 100 1 087.7 100 1 083.1 100 287.2 100 107.4 8 y 100 322.0 100 767.4 100 606.1 100 125.9 100 1 136.0 2 n 100 322.0 98 2 355.2 100 1 145.1 100 33.8 100 121.8 8 n 100 474.6 100 2 432.2 100 590.5 100 24.9 100 2 882.9 EDA* γ = 2 99 5 586.0 100 2 882.3 100 807.5 100 19.4 3 ≥ 554 249.0 EDA* γ = 1.01 100 1 023.8 100 742.0 100 730.2 100 990.2 10 ≥ 528 137.9 IDA*_CR 100 868.4 100 700.6 100 346.0 100 988.3 3 ≥ 516 937.7 IDA* 100 242.5 57 62 044.3 100 2 727.5 100 162 129.0 100 5 484.2 n< 100 100.8 100 258.1 100 35.8 100 4.9 100 2.7 T∗ Table 2: Results on tree search domains. Each tasks has 100 instances. Expansions (Exp.) are averaged on solved tasks only (except Coconut), and times 106 for the 15-puzzle. BTS is the implementation for tree search of the IBEX framework. add? is the is_additive parameter. Algorithm d=100 d=1 000 d=10 000 where D = 2 690 and q = 6. The cost set by EDA* α add? Exp. Exp. Exp. Time (s) (γ = 2) in the last iteration is 4 096, resulting in a search tree BGS 2 y 2 592 35 478 752 392 0.4 with approximately 3(4 096−2 690)/0.1 ≈ 106 700 nodes. The 8 y 1 276 22 275 312 497 0.2 same issue arises for IDA*_CR. EDA*(1.01) performs only 2 n 2 429 26 030 513 573 0.4 marginally better. This is not a carefully selected example, 8 n 513 8 821 84 434 0.1 and such behaviour occurs on almost all Coconut instances. DovBGS 2 y 2 195 31 862 564 720 0.1 Finally, we evaluate our algorithms in graph search 8 y 1 495 15 757 189 883 0.1 problems with inconsistent heuristics, parameterizing Mérõ’s 2 n 1 547 12 987 185 500 0.1 (1984) graph by d to have 2d + 2 states (see Fig. 4 in 8 n 449 4 017 36 093 0.1 Appendix D). All states have heuristic of 0 except each state A*/B/B’ 7 652 751 502 75 015 002 22.4 t which has heuristic d + i − 1. A*, B [Martelli, 1977], n< G∗ 200 2 000 20 000 0.0 ai nd B’ [Mérõ, 1984] are all expected to perform O(n2 ) G∗ expansions on this graph. The results are in Table 3. While Table 3: Results for inconsistent heuristics in graph search. BGS is A* shows quadratic growth on the number of expansions, our the implementation of the IBEX framework for graph search. algorithms exhibit near-linear performance as expected. of length q, where D is sampled uniformly in [1..10 000] and 11 Conclusion q is sampled from a geometric distribution of parameter 1/4. The first action costs 1. At depth less than D, taking the same We have developed a new framework called IBEX that action as at the parent node costs 1, taking another action combines exponential search with an increasing node costs 2D. At depth larger than D, each action costs 1/10. expansion budget to resolve two long-standing problems in heuristic search. The resulting algorithms for tree and 10.1 Results graph search improve existing guarantees on the number of We use 100 instances for each problem domain, a time limit expansions from Ω(n2 ∗) to O(n ∗ log C∗). Our algorithms of 4 hours for 15-Puzzle and TopSpin, 1 hour for the Coconut are fast and practical. They significantly outperform existing problem and no limit for the Chain problem. The results are baselines in known failure cases while being at least as good, shown in Table 2. We report results also for α = 2 to show if not better, on traditional domains; hence, for tree search the gain in efficiency when using a budget factor window of we recommend using our algorithms instead of IDA*. On [2, 8] instead of the narrower window [2, 2] (see Section 9). graph search problems our algorithms outperform A*, B and IBEX (BTS) and DovIBEX (DovBTS) (α = 8) are the only B’ when the heuristic is inconsistent, and pay only a small robust algorithms across all domains while being competitive log C∗ factor otherwise. on all domains, whereas all other algorithms tested fail hard We also expect the IBEX framework to be able to tackle on at least one domain. IBEX (BTS) has exactly the same re-expansions problems of other algorithms, such as Best- behaviour as IDA* when the number of expansions grows First Levin Tree Search [Orseau et al., 2018] and Weighted at least by a factor 2 at each call to query with infinite A* [Chen et al., 2019]. budget; see 15-Puzzle (unit). Taking is_additive=y helps The IBEX framework and algorithms have potential on exponential domains, whereas is_additive=n helps on applications beyond search in domains that exhibit a polynomial domains, as expected. dependency between a parameter and the amount of work To explain the behaviour of IDA*_CR and EDA* on (computation steps, energy, etc.) required to either succeed the Coconut problem, consider a randomly chosen instance or fail. Some of these applications may not be well suited
to the extended feedback model, which further justifies the [Hatem et al., 2018] Matthew Hatem, Ethan Burns, and interest in the analysis of the limited feedback model. Wheeler Ruml. Solving large problems with heuristic search: General-purpose parallel external-memory search. Acknowledgements Journal of Artificial Intelligence Research, 62:233–268, 2018. This research was enabled in part by a sabbatical grant from the University of Denver and by Compute [Holte, 2010] Robert C. Holte. Common misconceptions Canada (www.computecanada.ca). Many thanks to Csaba concerning heuristic search. In Proceedings of the Szepesvári, András György, János Kramár, Roshan Shariff, 3rd Annual Symposium on Combinatorial Search (SoCS Ariel Felner, and the reviewers for their feedback. 2010), pages 46–51, 2010. [Korf, 1985] Richard E. Korf. Depth-first iterative- References deepening: An optimal admissible tree search. Artificial [Akagi et al., 2010] Yuima Akagi, Akihiro Kishimoto, and Intelligence, 27(1):97–109, 1985. Alex Fukunaga. On transposition tables for single-agent [Korf, 1993] Richard E. Korf. Linear-space best-first search. search and planning: Summary of results. In Proceedings Artificial Intelligence, 62(1):41–78, 1993. of the 3rd Annual Symposium on Combinatorial Search [Lammertink, 1989] Ferdinand Lammertink. Puzzle or game (SoCS 2010), pages 2–9, 2010. having token filled track and turntable, October 3 1989. [Bentley and Yao, 1976] Jon Louis Bentley and Andrew US Patent 4,871,173. Chi-Chih Yao. An almost optimal algorithm for [Luby et al., 1993] Michael Luby, Alistair Sinclair, and unbounded searching. Information Processing Letters, David Zuckerman. Optimal speedup of Las Vegas 5(3):82–87, 1976. algorithms. Information Processing Letters, 47(4):173– [Burns and Ruml, 2013] Ethan Burns and Wheeler Ruml. 180, September 1993. Iterative-deepening search with on-line tree size predic- [Martelli, 1977] Alberto Martelli. On the complexity of tion. Annals of Mathematics and Artificial Intelligence, admissible search algorithms. Artificial Intelligence, 69(2):183–205, 2013. 8(1):1–13, 1977. [Chen et al., 2019] Jingwei Chen, Nathan R. Sturtevant, [Mérõ, 1984] László Mérõ. A heuristic search algorithm William Doyle, and Wheeler Ruml. Revisiting suboptimal with modifiable estimate. Artificial Intelligence, 23(1):13– search. In Proceedings of the 12th Annual Symposium on 27, 1984. Combinatorial Search (SoCS 2019), 2019. [Orseau et al., 2018] Laurent Orseau, Levi H. S. Lelis, Tor [Dechter and Pearl, 1985] Rina Dechter and Judea Pearl. Lattimore, and Theophane Weber. Single-agent policy tree Generalized best-first search strategies and the optimality search with guarantees. In Proceedings of the 31st Annual of A*. Journal of the ACM, 32(3):505–536, 1985. Conference on Neural Information Processing Systems [Doran and Michie, 1966] James E. Doran and Donald (NeurIPS 2018), pages 3205–3215, 2018. Michie. Experiments with the graph traverser program. [Orseau et al., 2019] Laurent Orseau, Tor Lattimore, and Proceedings of the Royal Society of London. Series A. Levi H. S. Lelis. Zooming cautiously: Linear-memory Mathematical and Physical Sciences, 294(1437):235–259, heuristic search with node expansion guarantees. CoRR, 1966. abs/1906.03242, 2019. [Felner et al., 2005] Ariel Felner, Uzi Zahavi, Jonathan [Press et al., 1992] William H. Press, Saul A. Teukolsky, Schaeffer, and Robert C. Holte. Dual lookups in pattern William Vetterling, and Brian P. Flannery. Numerical databases. In Proceedings of the 19th International Joint recipes in C. Cambridge University Press, 1992. Conference on Artificial Intelligence (IJCAI 2005), pages 103–108, 2005. [Russell and Norvig, 2009] Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall [Felner, 2011] Ariel Felner. Position paper: Dijkstra’s Press, Upper Saddle River, NJ, USA, 3rd edition, 2009. algorithm versus uniform cost search or a case against dijkstra’s algorithm. In Proceedings of the 4th Annual [Sarkar et al., 1991] U. K. Sarkar, P. P. Chakrabarti, Symposium on Combinatorial Search (SoCS 2011), pages S. Ghose, and S. C. De Sarkar. Reducing reexpansions in 47–51, 2011. iterative-deepening search by controlling cutoff bounds. Artificial Intelligence, 50(2):207–221, 1991. [Hart et al., 1968] Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. A formal basis for the heuristic determination [Sharon et al., 2014] Guni Sharon, Ariel Felner, and of minimum cost paths. IEEE Transactions on Systems Nathan R. Sturtevant. Exponential deepening A* for Science and Cybernetics, 4(2):100–107, 1968. real-time agent-centered search. In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI [Hatem et al., 2015] Matthew Hatem, Scott Kiesel, and 2014), pages 871–877, 2014. Wheeler Ruml. Recursive best-first search with bounded overhead. In Proceedings of the 29th AAAI Conference [Sturtevant and Helmert, 2019] Nathan R. Sturtevant and on Artificial Intelligence (AAAI 2015), pages 1151–1157, Malte Helmert. Exponential-binary state-space search. 2015. CoRR, abs/1906.02912, 2019.
[Sturtevant et al., 2008] Nathan R Sturtevant, Zhifu Zhang, Robert Holte, and Jonathan Schaeffer. Using inconsistent heuristics on A* search. In Proceedings of the AAAI Workshop on Search Techniques in Artificial Intelligence and Robotics, 2008. [Wah and Shang, 1994] Benjamin W. Wah and Yi Shang. Comparision and evaluation of a class of IDA* algorithms. International Journal on Artificial Intelligence Tools, 3(4):493–524, 1994. [Zhou and Hansen, 2004] Rong Zhou and Eric A. Hansen. Structured duplicate detection in external-memory graph search. In Proceedings of the 19th National Conference on Artificial Intelligence (AAAI 2004), pages 683–689, 2004.
A Graph Search Query B Depth-First Search: Further Enhancements The function graph_search is given in Algorithm 7. The actual query function is like Algorithm 1 where the call to There are a number of enhancements to DFS that strictly DFS is replaced with a call to graph_search . We use nodes reduce the number of expansions. instead of paths to stress that each element of the priority • Upper bounds from sub-optimal solutions. If a solution is queue uses constant memory size. The fringe is the set of found but the budget is exceeded, then keep the solution generated nodes with cost larger than C. cost as an upper bound for subsequent calls. • Detection of duplicate states can be performed along the Algorithm 7 Graph search (simple version) current trajectory to avoid loops in the underlying graph, while keeping a memory that grows only linearly with the 1 def query ext(C, budget): depth of the search, but is now a multiple of the state size. 2 data.min_fringe = ∞ # will be > C 3 data.max_visited = 0 # will be ≤ C C Enhanced DovIBEX Algorithm data.expanded = 0 # number of expansions 4 try: An enhanced version of Algorithm 5 is provided in 5 6 graph_search(C, budget, data) Algorithm 8. The call query+(C, b) is like query(C, b) but catch "budget exceeded": additionally returns the number of node expansions. The 7 8 return [0, data.max_visited] optimized version of the algorithm removes programs k from 9 return [data.min_fringe, ∞] the scheduler if it can be proven that they cannot find a solution. This pruning happens on line Line 20, which 10 # make_node: parent, state, g_cost -> node condition can be fulfilled in several circumstances. 11 12 def graph_search(C, budget, data): Let k be the current program index, then if a program 13 q = make_priority_queue(g) # g-cost ordering k′ > k has already made a call to query for which the budget 14 insert(q, make_node(∅, s init, 0)) #g(s init) = 0 was sufficient and the number of nodes expanded was at least 15 visited = {s init} B(k), then program k can never find a solution and is thus 16 while not empty(q): removed from the scheduler. Observe that if for program k 17 node = extract_min(q) we have C high ≤ C low, then we necessarily have b < b low, 18 s = node.state since b < n(C high) ≤ C low = b low. 19 if s in visited # already visited On Line 26 we know that C low is a lower bound on the cost 20 continue # at lower g-cost of the solution, so it is safe to use infinite budget. This may 21 visited += {s} lead to further pruning on Line 20. 22 data.max_visited = As for IBEX, Algorithm 8 also has two parameters. The 23 max(data.max_visited, node.g + h(s)) first one controls the budget, the second one whether we use 24 if is_goal(s): # optimal solution found to an additive exponential search, or a multiplicative one. 25 throw node # be dealt with by the user With B(k) = αk and T (k, r) = r2k, it can be worked out 26 if data.expanded >= budget: that T UBS(k, r) ≤ α−2 2 αkrlog2 α, where k = ⌈log α n ∗⌉ and 27 throw "budget exceeded" r = r 2 as in Theorem 9, leading to a number of expansions 28 data.expanded++ bounded by 2α n rlog2 α. This is more efficient than the 29 for s′ ∈ succ(s): default settinα g− w2 he∗ n2 r is small, but becomes rapidly less 30 node′ = make_node(node,s′,node.g+c(s, s′)) efficient for larger r. But note that this can be mitigated by 31 h′ = node′.g + h(s′) Eq. (1) and possibly by Theorem 11. 32 if h′ ≤ C: Further enhancements can be considered: insert(q, node′) 33 else: • If a program k has found an upper bound on the cost, 34 data.min_fringe = then this bound can be propagated to all k′ < k. 35 min(data.min_fringe,h′) 36 • If a solution has been found but the budget is exceeded, return "no solution" 37 keep the cost of the solution as a global upper bound (global branch and bound). D Worst-Case Re-Expansions for B and B’ Enhancement. When the heuristic is sufficiently consis- tent, A* has an optimal behaviour. In practice, it seldom Figure 4 is a parameterized adaptation of an example from happens that an inconsistent heuristic leads to a bad behaviour [Mérõ, 1984] published by [Sturtevant et al., 2008] where of A*. Therefore, to avoid the (small) overhead of BGS for A*, B and B’ all perform O(d2) expansions. Note that such cases, we propose the following rule of thumb: Run A* [Sturtevant et al., 2008] also introduce the Delay algorithm, for at least 1000 node expansions. Thereafter, if the number however this algorithm has a hidden assumption which is not of state re-expansions ever becomes at least half of the total necessarily true—namely that any state not counted as part of number of expansions, switch to BGS. n does not have a shorter path to a state counted as part of G∗
Algorithm 8 The Iterative Budgeted Exponential Search s algorithm with a few enhancements 1 1 1 1 1 # T: cost function for UBS 1 2 def T(k, r, C): return r2k t1 t2 t3 t4 · · · td d states 3 d d − 1 d − 2 d − 3 1 4 # α: Budget parameter # is_additive: use additive exp-search? 5 6 def DovIBEX_enhanced(α = 8, is_additive): m 7 # b low and C low are globals, but not C high 8 C low = C min # lower bound on C∗ 9 b qlo =w m= ak0 e_# pl ro iw oe rr itb yo _u qn ud euo e(n T)n ∗ b1 1 b2 1 b3 1 · · · 1 bd−1 d − 1 g 10 11 q.insert((1, 1, ∞)) # (k=1, r=1, C high = ∞) d states 12 while not q.empty(): 13 Figure 4: Worst-case example adapted from [Mérõ, 1984] by 14 (k, r, C high) = q.extract_min() [Sturtevant et al., 2008]. 15 b = αk # budget 16 if r == 1: which is the best heuristic value that can be propagated 17 18 q.insert((k+1, r, ∞)) forward to s. Note that hˆ is consistent. 19 Let C∗ be minimum cost of any solution state. Let n opt = 20 if b ≤ b low or C high ≤ C low: |{s : g∗(s) + hˆ(s) ≤ C∗}|, which is an upper bound on the 21 # Can’t find a solution with this budget number of states that A* expands when using hˆ instead of h. 22 continue # remove k from the queue The following theorem shows that no algorithm having only 23 access to h can hope to expand O(n ) without oracle access if r == 1: # IDA⁎ trick, can be omitted opt 2 24 5 C = C low to the consistent heuristic hˆ. 26 b = ∞ Theorem 12. For each deterministic search algorithm there 27 else if C high == ∞: # exponential phase exists a graph search problem with an admissible heuristic 28 if is_additive: such that the algorithm expands Ω(2nopt) nodes. 29 C = C low + 2r−1 30 else: Proof. Let d be an arbitrarily large integer. Consider the 31 C = C low × 2 following class of problems, illustrated in Fig. 5. The 32 else: # binary phase initial state is s init, which has g(s init) = 0 and successors 33 C = (C low + C high)/2 succ(s init) = {s 1, s 2}. Below s 2 is a full binary tree of depth 34 d. State s 1 has successors succ(s 1) = {l 1, r 1, . . . , l d, r d}, 35 C l′ ow, C h′ igh, b′ low = query+(C, b) each of which starts a chain of length 2d with the final state 36 [C low, C high] = [C low, C high] ∩ [C l′ ow, C h′ igh] in the chains started by l m (respectively r m) connected to all left-hand (respectively right-hand) children in the binary 37 tree at depth m. The heuristic values are 0 except h(s ) = 38 if C h′ igh == ∞: # budget not exceeded 2d + d + 3. Consider the first 2d − 1 expansions of the se1 arch 39 # Solution requires more than b′ low algorithm where all edge costs are unitary and there is no goal 40 b low = b′ low state. By the pigeonhole principle there exists a state s in the 41 leaves of the binary tree that has not been expanded. We now 42 q.insert((k, r+1, C high)) modify the graph so that (a) the behavior of the algorithm is identical (b) the goal is in state s∗ and (c) n = d + 2. To opt do this, let s∗ be the goal state and (a )d+2 be the unique t t=1 n G∗. As this is not true in general, the Delay algorithm is not path ending in s∗ and passing through s 2. Then for each completely general. 3 ≤ m ≤ d + 2 find an edge in the chain connected to state a that was not examined by the algorithm in the goal-less m graph and set its cost to infinity, which cuts the path from E Intractability of Admissible Heuristics state s to state s . Since the algorithm has not examined this 1 m Let c∗(s′, s) be the minimum cost of any path from s′ to edge, it cannot prove whether the heuristic value of s 1 should s with c∗(s, s) = 0. Assuming that the heuristic h is be propagated to the corresponding node in the full binary tree. The large heuristic in state s ensures that children in the admissible, define 1 binary tree that do not lead to the goal state have a heuristic of hˆ(s) = max{h(s′) − c∗(s′, s)} , at least d+3, whereas all states along the path a have heuristic s′∈S 0. Hence n opt = d + 2. Finally, by construction the algorithm
C (b) ≤ h, and since h = 2l we have h − l ≤ C∗. crit sinit During the binary phase, the size of the interval [l, h] at least halves after each query. Now, suppose that at some point h − l < 2(C (b) − C∗). Using C = (h + l)/2 then we crit s2 l1 obtain C < l + C crit(b) − C∗ ≤ C crit(b) since l ≤ C∗, and × r × 1 l2 s1 h 2d(s +1 d) += 3 STal ths ao e rr tieC nf go> r fe roCh m− ∈ h(( −CC ∗c lr , ≤i Ct( c Cb r) i ∗t− ,( tb h)C i) s∗ w r) eh q≥ ic uh irC eis s∗ a as tgin mac m oe e sh t-en≥ dinC gcr qi ut( eb ry). . r2 1 + min{k ∈ N : C∗/2k ≤ 2(C (b) − C∗)} l3 0 crit C∗ r3 = 1 + 1 + log × (cid:22) 2 2(C (b) − C∗) (cid:23) s∗ crit ≥0 goal 2d nodes C∗ = 1 + log (cid:22) 2 C (b) − C∗ (cid:23) crit ≥0 calls to query before ending the game. Therefore the number of calls to query is at most n (start, C∗, C (b) − exp crit Figure 5: The graph for the proof of Theorem 12 with d = 3. C∗). The large heuristic value of s propagates through all the gray 1 nodes. Red crosses in the chains indicate that the chain contains Proof of Proposition 2. Let x < y < z be three consecutive an edge with infinite cost, preventing the large heuristic from being values in A (that is, y = A >(x) and z = A >(y)) with propagated to the full binary tree. Only one path in the tree is entirely y = C crit(b). During the exponential phase, the number of red, and leads to the goal g. queries until h ≥ C (b) is at most crit C (b) min{k ∈ N : ε2k ≥ C (b)} = log crit expands at least 2d − 1 nodes in the modified graph before 1 crit (cid:24) 2 ε (cid:25) ≥1 finding the goal. The result follows since d may be chosen C∗ arbitrarily large. ≤ log . (cid:24) 2 ε (cid:25) ≥1 Remark 13. The example is complicated by the fact that we did not assume the algorithm was restricted to only call At the end of this phase, we have l ≤ C crit(b) ≤ h and since the successor function on previously expanded nodes. We h = 2l we have h − l ≤ C crit(b). Now for the binary phase, only used that an edge-cost is observed when its parent is where the interval [l, h] is always at least halved. Observe expanded. We did not even assume that the algorithm is that query ext ensures that if x ≤ C < y then l is set to y, and guaranteed to return an optimal solution. The proof is easily if y ≤ C < z then h is set to y too. Next we show that if modified to lower bound the expected number of expansions h − l < δ min then h < z and l > x: by any randomized algorithm using Yao’s minimax principle h − l < δ ≤ z − y (by assumption) and by randomizing the position of the goal in the leaves of min the binary tree and the infinite-cost edges in the chains. h < z − y + l ≤ z (using l ≤ y) h − l < δ ≤ y − x (by assumption) Remark 14. Theorem 12 also holds when using pathmax min [Mérõ, 1984] or BPMX [Felner et al., 2005], since in order l > x − y + h ≥ x . (using h ≥ y) to make the heuristic consistent the algorithm still needs to (Remembering that y = C (b), observe that if start = y expand exponentially many nodes along the chains. crit then l = y already at the beginning of Algorithm 2.) Thus h = l = y which terminates the algorithm and returns F Proofs of Propositions 1 and 2 l = C (b). crit In the following we use ε = start, h = high and l = low. Hence the number of calls to query during the binary phase before h − l < δ (which entails h = l) is at min Proof of Proposition 1. We know that C∗ < C (b) by most (remembering that h − l ≤ C (b) at the end of the crit crit definition of b through k, and that ε ∈ [C∗, C (b)). exponential phase) crit The number of queries in the exponential phase before h ≥ C∗ is at most min{k ∈ N 0 : C crit(b)/2k < δ min} C (b) C∗ = 1 + log crit min{k ∈ N 1 : ε2k ≥ C∗} = (cid:24)log 2 ε (cid:25) . (cid:22) 2 δ min (cid:23) ≥0 ≥1 C∗ At the end of the exponential phase, if h ∈ [C∗, C crit(b)) ≤ 1 + (cid:22)log 2 δ (cid:23) . then the game ends. Otherwise h ≥ C (b), and also min ≥0 crit l ≤ C∗ (otherwise a game-ending query would have been Therefore, the total number of queries is at most made; l = C∗ is possible iff ε = C∗). Thus l ≤ C∗ < n (start, C (b), δ ). exp crit min
G Proofs of Theorem 6 and Corollary 7 Proof of Theorem 6. Suppose when UBS is executing (k, r), the node (j, m) has already been executed by UBS. Then the optimality property of Uniform Cost Search [Russell and Norvig, 2009] ensures that T (j, m) ≤ T (k, r). Hence, program j has been executed for at most T (k, r) steps. The result follows by j summing over all programs and using that program j never runs for more than τ steps. j Proof of Corollary 7. The result follows from Theorem 6 and the assumptions that T (j, 0) = 0 and T (j, 1) ≤ T (j, m) for m ≥ 1. H Proofs of Theorems 9 and 11 Proof of Theorem 9. Let k∗ = ⌈log (n )⌉. By Proposition 1, 2 ∗ programs with k ≥ k∗ make a game-ending query after at most r = n (C , C∗, C (b ) − C∗) k exp min crit k calls to run_prog (k, 2k). By Theorem 6, Algorithm 5 does not use more than min r 2k ⌊k + log r ⌋ steps before k≥k∗ k 2 k exiting with a solution, which also upper bounds the number of expansions of the algorithm. The first bound is obtained by taking k = k∗ for which b k∗ ≤ 2n ∗ and C crit(2k∗ ) ≥ C (n ) = C∗ + δ(C∗). The second bound is obtained by crit ∗ noting that for any b ≥ n there exists a k such that 2k ≥ n ∗ ∗ and r 2k ≤ 2bn (C , C∗, C − C∗). k exp min crit(b) Proof of Theorem 11. Similarly to the proof of Theorem 3, let k = ⌈log (n )⌉ be the first program with enough budget, ∗ 2 ∗ that is 2k∗ ≥ n ∗. Using Proposition 1, program k ∗ terminates after at most r calls to query. Each program k < k requires 2 ∗ at most r calls to query to terminate, that is τ ≤ 2kr . 1 k 1 Each program k > k that has started when k terminates ∗ ∗ has used at most T (k , r ) steps, and only programs k ≤ k ∗ 2 k ∗ + ⌊log 2 r 2⌋ (that is 2k ≤ r 22k∗ ) have started (that is, T (k , r ) > 0). Hence, using Theorem 6, the number of k ∗ 2 steps is bounded by T (k , r ) ≤ τ + T (k , r ) UBS ∗ 2 k k ∗ 2 kX<k∗ kX≥k∗ k∗−1 k∗+⌊log2 n2⌋ ≤ 2kr + 2k∗r 1 2 Xk=1 kX=k∗ = (2k∗ − 1)r + (1 + ⌊log r ⌋)2k∗ r 1 2 2 2 ≤ 2n (r + r (1 + ⌊log r ⌋)) , ∗ 1 2 2 2 which also bounds the number of expansions of the algorithm.
