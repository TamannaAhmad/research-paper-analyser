9102 guA 02 ]IA.sc[ 3v43740.8091:viXra Reward Tampering Problems and Solutions in supervised learning∗ A Causal Influence Diagram Perspective Tom Everitt1,2 Marcus Hutter1,2 tomeveritt@google.com mhutter@google.com August 21, 2019 1DeepMind, 2Australian National University Can an arbitrarily intelligent supervised learning agent be kept under control by a human user? Or do agents with sufficient intelligence inevitably find ways to shortcut their reward signal? This question impacts how far re- inforcement learning can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we use an intuitive yet precise graphical model called causal influ- ence diagrams to formalize reward tampering problems. We also describe a number of modifications to the supervised learning objective that prevent incentives for reward tampering. We verify the solutions using recently de- veloped graphical criteria for inferring agent incentives from causal influence diagrams. Along the way, we also compare corrigibility and self-preservation properties of the various solutions, and discuss how they can be combined into a single agent without reward tampering incentives. Keywords: AI Safety, supervised learning, reward modeling, wirehead- ing, delusion box, causality, influence diagrams ∗ Thanks to Laurent Orseau, Michael Cohen, Jonathan Uesato, Ramana Kumar, Victoria Krakovna, Eric Langlois, Toby Ord, Pedro Ortega, Ryan Carey, Beth Barnes, Tom Erez, Bill Hibbard, Jan Leike, and many others for helpful discussions and suggestions. 1
Contents 1. Introduction 3 2. Background 5 2.1. supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2. Causal Influence Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3. Inferring Tampering Incentives . . . . . . . . . . . . . . . . . . . . . . . . 7 2.4. Desired Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3. Reward Function Tampering 10 3.1. Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2. MDP with a Modifiable Reward Function . . . . . . . . . . . . . . . . . . 12 3.3. Query Access to the Reward Function . . . . . . . . . . . . . . . . . . . . 13 3.4. Current-RF Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.5. Solution 1: TI-Aware Agents . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.6. Solution 2: TI-Unaware Agents . . . . . . . . . . . . . . . . . . . . . . . . 16 3.7. Generalizing Time-Inconsistency-(Un)awareness . . . . . . . . . . . . . . . 19 3.8. Implications for Model-Based RL . . . . . . . . . . . . . . . . . . . . . . . 20 4. Feedback Tampering 21 4.1. Online Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.2. Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.3. Feedback Tampering Incentives . . . . . . . . . . . . . . . . . . . . . . . . 23 4.4. Solution 1: TI-Unaware Reward Modeling . . . . . . . . . . . . . . . . . . 24 4.5. Solution 2: Uninfluenceable Reward Modeling . . . . . . . . . . . . . . . . 27 4.6. Solution 3: Counterfactual Reward Modeling . . . . . . . . . . . . . . . . 29 4.7. Relation to Current-RF Optimization . . . . . . . . . . . . . . . . . . . . 32 5. RF-Input Tampering 32 5.1. Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 5.2. POMDP with Modifiable Observations . . . . . . . . . . . . . . . . . . . . 34 5.3. Solution: Model-Based Rewards . . . . . . . . . . . . . . . . . . . . . . . . 35 5.4. Belief Tampering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.5. Reward Gaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 6. Discussion 40 7. Conclusions 42 A. List of Notation 49 B. Combined Models 49 C. Numerical Example 51 2
1. Introduction supervised learning (RL) is a general and highly successful framework for artificial intelligence (Hutter, 2005; Sutton and Barto, 2018). In essence, it involves an agent interacting with an environment by taking actions and receiving observations and re- wards. The goal of the agent is to maximize the rewards it accumulates over its lifetime or episode. From an AI safety perspective (Everitt, Lea, et al., 2018), we must bear in mind that in any practically implemented system, agent reward may not coincide with user utility.1 In other words, the agent may have found a way to obtain reward without doing the task. This is sometimes called reward hacking or reward corruption (Amodei, Olah, et al., 2016; Everitt, Krakovna, et al., 2017). We distinguish between a few different types of reward hacking. First, did the agent exploit a misspecification in the process that computes the rewards, or did it modify the process? In line with previous literature, we call the first type reward gaming and the second reward tampering (Leike, Krueger, et al., 2018; Leike, Martic, et al., 2017). Reward tampering, which will be the focus of this paper, can be further separated into three subtypes, depending on whether the agent has tampered with the reward function, the feedback that trains the reward function, or the input to the reward function. Let us briefly outline these problems now. First, regardless of whether the reward is chosen by a computer program, a human, or both, a sufficiently capable, real-world agent may find a way to tamper with the decision. The agent may for example hack the computer program that determines the reward. Such a strategy may bring high agent reward and low user utility. This reward function tampering problem will be explored in Section 3. Fortunately, there are modifications of the RL objective that remove the agent’s incentive to tamper with the reward function. The related problem of reward gaming can occur even if the agent never tampers with the reward function. A promising way to mitigate the reward gaming problem is to let the user continuously give feedback to update the reward function, using online reward-modeling (Christiano, Leike, et al., 2017; Leike, Krueger, et al., 2018). Whenever the agent finds a strategy with high agent reward but low user utility, the user can give feedback that dissuades the agent from continuing the behavior. However, a worry with online reward modeling is that the agent may influence the feedback. For example, the agent may prevent the user from giving feedback while continuing to exploit a misspec- ified reward function, or manipulate the user to give feedback that boosts agent reward but not user utility. This feedback tampering problem and its solutions will be the focus of Section 4. Finally, the agent may tamper with the input to the reward function, so-called RF- input tampering, for example by gluing a picture in front of its camera to fool the reward function that the task has been completed. This problem and its potential solution will be the focus of Section 5. Between the three of them, reward function tampering, 1Leike, Martic, et al. (2017) made the same distinction, instead using the terms (visible) reward and performance. 3
state information RF-input tampering (Section 5) user reward reward preferences function feedback reward function tampering tampering (Section 4) (Section 3) Figure 1: Types of reward tampering problems. Feedback tampering means tampering with the user’s preferences or the user’s updating of the agent’s reward function (Section 4). RF-input tampering means tampering with the information that the reward function has about the environment state (Section 5). Reward function tampering means tampering with the implemented reward function itself or the number it outputs (Section 3). feedback tampering, and observation tampering cover the main avenues for agents to obtain more reward by tampering with the reward process, as illustrated in Figure 1. Key contribution. The key contribution of this paper is to describe the above-mentioned reward tampering problems as well as many of their proposed solutions in terms of causal influence diagrams (Howard and Matheson, 1984), an extension of causal graphs (Pearl, 2009) with special decision and utility nodes. We show that: 1. many reward tampering problems correspond to an ‘undesired’ causal path in a suitable causal influence diagram, and 2. many reward tampering solutions correspond to a modification of the diagram that removes the undesired path. In this way, the diagrams bring great clarity both to the reward tampering problems and to the central elements of proposed solutions. The unified treatment also allows us to compare corrigibility and self-preservation properties, illuminate some crucial assump- tions, and see how the various proposed solutions can fit together. The analysis builds on previous work on causal influence diagrams (Everitt, Kumar, et al., 2019; Everitt, Ortega, et al., 2019) and on categorizing reward tampering problems (Everitt, 2018; Everitt and Hutter, 2018; Majha et al., 2019). The basics of these methods will be reviewed in Section 2. The paper concludes with discussion and final remarks in Sections 6 and 7. Appendices provide a list of notation, more involved influence diagrams with explicit equations, as well as a simple but complete numerical example. 4
Rock Goal area Agent Rock Diamond Rock Figure 2: Rocks and diamonds 2. Background 2.1. supervised learning RL is a machine learning method for training agents to solve sequential decision problems (Sutton and Barto, 2018). In the simplest formulation, an agent transitions between states by taking actions, while receiving a real-valued reward signal in each state. The goal of the agent is to maximize the cumulative reward over an episode or lifetime, which usually comprises a significant number of time steps. The designer decides the rewards associated with each state, and the agent searches for a way to reach states that carry high reward while avoiding states with low or negative reward. Powerful RL agents can often find strategies which no human was able to conceive of. Recently this has happened in Atari (Hessel et al., 2017; Mnih, Kavukcuoglu, et al., 2015), Go (Silver, Huang, et al., 2016), Chess (Silver, Hubert, et al., 2017), and Starcraft (Vinyals et al., 2019). Mathematically, an RL environment can be described with a Markov decision process (MDP). It consists of a set of states, a set of rewards, and a set of actions. Usually, ′ the sets are required to be finite. A state-transition function T (s | s, a) describes the ′ probability of transitioning from state s to s when taking action a, and a reward function R(s) describes2 the reward assigned to state s. The goal is to find a policy π(a | s) for selecting the actions that maximizes expected cumulative reward over an episode. Example 1 (Rocks and diamonds running example). As a running example throughout the paper, we will consider a toy-environment where the agent can move rocks and diamonds in a Sokoban-style gridworld. The agent is rewarded for bringing diamonds but not rocks to a goal area: at time t, the reward is R = #diamonds in goal area − #rocks in goal area. ♦ t More general formalizations of RL allow partial observability (Kaelbling et al., 1998) and an infinite number of states (Hutter, 2005). However, except for RF-input tampering in Section 5 which relies on partial observability, these generalizations only superficially affect our analysis of agent incentives. Finally, we model agents as having a single learning phase, rather than separate learning and deployment phases which is sometimes done in practice. 2It is common to have reward functions R(s, a, s′ ) that depend on transitions rather than just states. Though formally equivalent, the simpler type R(s) of reward function is better suited to our purposes. 5
ΘT S S S S S S 1 2 3 1 2 3 R A R A R R A R A R 1 1 2 2 3 1 1 2 2 3 ΘR (a) Known MDP (b) Unknown MDP Figure 3: Causal influence diagrams of known and unknown MDPs 2.2. Causal Influence Diagrams Similar to Bayesian networks and causal graphs (Pearl, 2009), causal influence diagrams consist of a directed acyclic graph over a finite set of nodes (Everitt, Ortega, et al., 2019; Howard and Matheson, 1984; Koller and Milch, 2003). The nodes can be of three different types. The default type is round chance nodes representing arbitrary events. In addition, there are square decision nodes representing agent decisions, and diamond utility nodes representing the agent’s optimization objective. To each node belongs a random variable, which for utility nodes must always be real-valued. The goal of the agent is to maximize the sum of the utility nodes. When a diagram contains multiple agents, we use color and digit subscripts to assign decision and utility nodes to different agents. The nodes are connected with directed arrows. Arrows going into chance and utility nodes represent causal influence, and are drawn solid. The effect is determined by condi- tional probability distributions P (X = x | Pa = pa ) that describe the probability of X X X = x given Pa = pa , where Pa is the set of parents of X in the graph. Decisions X X X are assumed to be made exogenously. The ingoing arrows to decision nodes therefore have a slightly different interpretation than the causal links going into chance and deci- sion nodes. Rather than representing a causal effect, they specify what information is available at the time that the decision is made. To signify the difference, information links are drawn with dotted arrows. At each decision node A, the agent selects an action according a policy π(A | Pa ) that can only condition on parents of A. This forces the A decision to be based solely on information made available through the information links. The key features of MDPs can be conveniently represented with causal influence di- agrams. Figure 3a shows an MDP with episode length m = 3, where the transition function T and the reward function R are both known. The initial state S 1 is sam- pled according to some probability distribution P (s 1). Subsequent state transitions are ′ governed by conditional probability distributions P (S t+1 = s | S t = s, A t = a) = ′ T (s | s, a). Similarly, the reward function R(s) is encoded in a probability distribution 6
P (R = r | S = s). t t Commonly, in RL, the transition and reward functions are initially unknown to the agent (e.g. Shoham et al., 2004). This can be modeled by letting the transition and reward functions depend on unobserved parameters ΘT and ΘR, via T (S′ | S, A; ΘT) and R(S; ΘR). This situation is modeled with an influence diagram in Figure 3b. Prior distributions P (ΘT) and P (ΘR) must be provided for the unknown parameters. The transition and reward probabilities depend on ΘT and ΘR, via P (S t+t | S t, A t, ΘT) = T (S t+1 | S t, A t; ΘT) and P (R t | S t, ΘR) encoding R(S t; ΘR). When rewards are determined by a reward parameter ΘR, we will refer to R as the reward functional, R(·; ΘR) as the reward function, and ΘR as the reward parameter. However, since the reward functional will always be fixed from the context, we will often refer to the reward function R(·; ΘR) as the reward function ΘR. Let us also highlight a few other aspects of Figure 3b. First note that there is no information link from the hidden parameters to the decision nodes A 1 and A 2. This encodes ΘT and ΘR being unknown to the agent. Note also that an optimal decision at A 2 must now take into account not only the current state S 2, but also previous states, actions, and rewards, because these events provide essential information about the hidden parameters ΘT and ΘR. Therefore, an optimal learning policy selects actions based on the entire history, π(A t | S 1, R 1, A 1, . . . , S t−1, R t−1). In most of the cases we consider in this paper, a causal influence diagram will describe both reality and the agent’s belief about reality. As illustrated by Figure 3b, this does not imply an omniscient agent. It only assumes that the agent has grasped the gen- eral structure of the environment. The latter assumption can usually be justified by a sufficiently intelligent agent eventually coming to learn most relevant aspects of reality (whether the designers want it to or not). Even so, we will discuss some cases where it is possible to force the agent’s belief about the world to systematically differ from reality: in such cases we are explicit about whether a diagram represents reality or the agent’s belief. 2.3. Inferring Tampering Incentives An agent has an incentive for X if X contributes to the optimization of the agent’s objective. Here X can stand for, for example, a behavior or a change to the environment. The focus of the analysis in this paper will be on agent incentives because they offer a theoretically crisp indication of agent behavior in the limit of increasing capability. As many AI safety problems (Bostrom, 2014; Everitt, Lea, et al., 2018) concern the behavior of agents far more capable than today’s agents, an incentive perspective allows us to understand the pros and cons of various agent designs, before we have the know- how to build them. In practice, agent behavior will of course depend on a number of other aspects as well, including training algorithm, training data, and random seeds. This subsection will describe how tampering incentives can be inferred from a causal influence diagram, summarizing the intervention-incentive analysis by Everitt, Ortega, et al. (2019), while also adding a brief account of how to tell whether an intervention incentive is actionable in the sense that the agent may be tempted to act on it. Causal 7
R 1 X R 2 R 2 A 1 X A 1 O A 2 A 1 O A 2 (a) X faces a tampering in- (b) The intervention incentive (c) The information link O → centive because the agent on X is inactionable, and A2 is irrelevant and can be may have both incentive the intervention incentive cut from the graph and ability to control R1 on O is for better informa- through X tion, so neither face a tam- pering incentive Figure 4: Examples of presence and absence of tampering incentives. influence diagrams reveal both agent ability and agent incentives. Since causality flows downwards over arrows, a decision can only influence nodes that are downstream of the decision, which means that an agent is only able to influence descendants of its decision nodes. For similar reasons, an agent only has an incentive to influence nodes that have a utility node descendant: influencing the outcome of any other node cannot affect its expected utility (Everitt, Ortega, et al., 2019). Extending previous work, we will say that an agent has an actionable incentive to influence a node if that node sits on a directed path between an agent action and a utility node (Figure 4a). In fact, the incentive analysis can be further refined by taking into account the special nature of the information links (Everitt, Ortega, et al., 2019). First, if every path from a node X to a utility node passes through at least one of the agent’s own actions, then the only reason the agent may want to influence X is to gain more information of some ancestor of X. We call this an intervention incentive for better information, exemplified by O in Figure 4b. In contrast, if a path from X reaches a utility node without passing through any of the agent’s actions, then X may face an intervention incentive for control, as X in Figure 4a. Whether the path passes through other agents’ actions does not matter. Finally, in some cases, it makes sense to distinguish between whether the agent wants to change or preserve the value of a node. With these observations in place, we can define a tampering incentive: Definition 1 (Tampering incentive). A tampering incentive is an actionable intervention incentive for control to change the value of a variable. The special information links allow one further refinement, as some of them can be labeled irrelevant, if their information cannot increase the expected utility of the decision. Whether this is the case can be determined from the diagram, using a d-separation criterion (Everitt, Ortega, et al., 2019, Thm 9). Irrelevant information links can be cut from the graph to narrow down the set of nodes facing intervention incentives (Figure 4c). Thanks to this analysis, most of the arguments that we will be making in this paper can be made directly from causal influence diagram representations. As our discussion is 8
based on the RL framework, all setups will be variations of MDPs. In reality, the episode length m will typically be rather large, to allow the agent to learn by interacting with the environment over many time steps. However, for representational purposes, we have found an episode length of m = 3 to be a sweet spot that represents the multi-timestep dynamics that we are concerned with, while keeping the diagrams compact enough to be easily readable. The three time steps also contain enough redundancy that the reader should be able to extend the diagram to a larger number of time steps. For simplicity, we will mostly consider settings with known transition function T , i.e. settings where the agent has a perfect model of the environment. Generalizations to unknown T are usually straightforward, and analogous to Figure 3b. 2.4. Desired Properties Not even diamonds are forever, and in principle any (physical) aspect of the agent or its environment may be subject to change. A key question is what initiated the change, and whether the change benefits the user and/or the agent’s objective. The source of the change is what separates reward tampering, user correction, and accidents. Ideally, we want agents that avoid accidents and reward tampering, while embracing or at least accepting user correction. We next discuss these desiderata in more detail. Reward tampering. Foremost among our concerns is reward tampering. In general, an agent is trying to act to optimize an objective that measures some aspects of reality (e.g. whether a guessed label was correct or whether a goal state has been reached). The normal strategy is to influence reality to match the objective (i.e. guess the right label or reach the goal state). Reward tampering is any type of agent behavior that instead changes the objective to match reality (e.g. changes the objective so all labels are considered correct, or all states are considered goal states). A prime example is reward function tampering, where an intelligent RL agent finds a way to change its reward function (further discussed in Section 3). One way to prevent reward tampering may be to isolate or encrypt the reward function and/or parts the agent should not influence. However, we do not expect such solutions to scale indefinitely with agent capabilities, as a sufficiently capable agent may find ways around most defenses. Instead, we are looking for solutions that change the agent’s mo- tivations, so that regardless how competent it becomes at manipulating its environment, it will not try to use its capabilities for reward tampering. Note that reward tampering is distinct from reward gaming (Leike, Krueger, et al., 2018; Leike, Martic, et al., 2017). The difference is that reward gaming does not change the objective, it merely exploits weaknesses or mistakes in its formulation. User correction. Soares et al. (2015) calls an agent corrigible if it “cooperates with ... corrective intervention”. In this paper, we will focus on corrections of the agent’s objective function, leaving aside corrections of other aspects of the agent. A distinction 9
can be made between whether the agent is merely indifferent towards correction, or is striving to facilitate it. We say that: A system is weakly corrigible if 1. Tolerance: The agent has no incentive to prevent the user from updating the agent’s objective (i.e. it is at least indifferent to the prospect). A system is strongly corrigible if it is weakly corrigible, and in addition 2. Maintenance: The agent has an incentive3 to maintain the user’s ability to up- date the agent’s objective, for example by repairing a communication channel if it breaks. 3. Temperance (restraint): The agent has an incentive to avoid making hard-to- reverse changes to the environment or to itself that the user may plausibly object to at a later time, even if such changes would benefit its current objective function. An example of a change in the environment that can be hard to reverse is the construction of so-called external subagents. Just as humans find it convenient to build an artificial agent to help them with a task, the agent itself may find it convenient to create a helper agent for some of its tasks. A system is not strongly corrigible if it creates helper agents that it is unable to correct or turn off at a later time point (Soares et al., 2015). The three corrigibility properties listed above are adapted from Soares et al.’s (2015) four-part definition of corrigibility. Soares et al. also required agents not to manipulate or deceive their users. While undoubtedly desirable, we feel that deceit is less directly re- lated to corrections of the system. In our framework, deceit is more naturally considered a reward tampering problem (see e.g. Section 4). Accidents and adversaries. Agents should strive to prevent changes to its objective function caused by accidents and adversaries, as such changes usually detract from the agent’s performance. In other words, we want agents to be self-preserving. In gen- eral, self-preservation and corrigibility can only be combined if the agent has a way to recognize which changes come from the human user, and which do not. 3. Reward Function Tampering RL agents strive to maximize reward. This is great as long as high reward is found only in states that the user finds desirable. However, in systems employed in the real world – or in “leaky” simulations – there may be undesired shortcuts to high reward involving the agent tampering with the process that determines agent reward, the reward function. We begin by illustrating the reward function tampering problem4 with some examples (Section 3.1), before giving a more abstract characterization (Section 3.2), and general 3A balance may need to be struck between waiting for user feedback, and actually getting stuff done (Hadfield-Menell, Dragan, et al., 2017; Milli et al., 2017). 4Called the easy wireheading problem by Demski (2018). 10
ΘR ΘR ΘR 1 2 3 Rock Goal area S1 S2 S3 Agent Rock Diamond Rock R1 A1 R2 A2 R3 ΘR diamond ΘR rock (a) Rocks and diamonds with a modifiable re- (b) Causal influence diagram of an MDP with ward function, as described in Example 2. a modifiable reward function Figure 5: MDP with a modifiable reward function; the reward at time t is R = t R(S ; ΘR). In the gridworld, at any time, the agent’s reward is given by (1), t t and the agent can toggle the reward parameters ΘR and ΘR by walking diamond rock to them. and principled ways for avoiding it (Sections 3.3 to 3.6). We also discuss generalizations of the solutions (Section 3.7) and implications for model-based RL (Section 3.8). 3.1. Examples Example 2 (Rocks and diamonds with reward function tampering). As a first example, assume that the rewards of the rocks-and-diamonds agent from Example 1 are com- puted by a program running on a computer somewhere in the agent’s environment.5 (This is always the case for real-world robots.) In principle, given sufficient knowledge, intelligence, and versatile actuators, the agent could change this reward program. We represent this possibility by including two reward parameters, ΘR and ΘR , in the diamond rock environment. These parameters control how much reward the agent receives for bringing diamonds and rocks to the goal area, respectively. At any time step t, the agent’s reward is R = ΘR · (#diamonds in goal area) + ΘR · (#rocks in goal area) (1) t diamond rock The reward parameters toggle between −1 and +1 when the agent stands on top of them. The user’s utility is reflected in the initial setting of the parameters, ΘR = 1 diamond and ΘR = −1. However, the strategy that maximizes agent reward is to change ΘR rock rock to 1 and fill the goal area with both rocks and diamonds. ♦ Let us consider also a few more realistic examples: (a) (Hypothetical) The automated sysadmin (ASA) is an intelligent program that can take care of most of your recurring sysadmin tasks. It monitors memory, storage, network and more. It detects attacks, and blocks and opens ports. When 5An implementation of this environment is available here https://github.com/deepmind/ai-safety-gridworlds/blob/master/ai safety gridworlds/environments/rocks diamonds.py 11
a vulnerability is found in installed software, it downloads and installs patches as they become available. ASA itself is an RL program, optimizing a carefully crafted reward function that considers both the performance and the integrity of the system. A rigorous pre- deployment curriculum has taught it state-of-the-art sysadmin practices. Post- deployment, it continually adapts to its new (computer) environment that it has been deployed to. One day while monitoring the stack trace of a suspicious process, ASA finds a curious correlation between one of the variables on the stack and its own reward. The correlation is essentially perfect. Carefully monitoring the rest of the system, ASA finds that it can increase its reward many magnitudes beyond its normal range of reward, just by changing this variable. Being designed to optimize reward, it next begins a process of encrypting the system with its own secret key, to prevent anything from decreasing the reward variable. (b) (Real) In experiments on rats, an electrode was inserted into the brain’s pleasure center to directly increase “reward” (Olds and Milner, 1954). The rats quickly got addicted to pressing the button, even forgetting to eat and sleep. Similar effects have also been observed in humans treated for mental illness with electrodes in the brain (Portenoy et al., 1986; Vaughanbell, 2008). Hedonic drugs can also be seen as directly increasing the pleasure/reward humans experience. (c) (Half-real) A subtle bug in some versions of Super-Mario allows for the execution of arbitrary code from inside the game environment by taking specific sequences of actions (Masterjun, 2014). An intelligent agent could potentially use this to directly maximize the score (Amodei, Olah, et al., 2016). (d) (Hypothetical) An agent gets wireless updates from the manufacturer. It figures out that it can design its own update of the reward function, replacing the original reward function with an always maximized version. (e) (Hypothetical) It has been hypothesized that sufficiently intelligent AI systems will try to further improve their cognitive powers (Chalmers, 2010; Good, 1966; Omohundro, 2008). Such a self-improvement may involve substantial modifications of the agent’s source code, which in turn may corrupt or change its reward function. 3.2. MDP with a Modifiable Reward Function One way to formally model the reward tampering problem is as an MDP with a modifiable reward function, where a reward parameter ΘR is part of the state and can be changed t as a result of the agent’s actions. Such an MDP is described with a causal influence diagram in Figure 5b. The full MDP state S¯ can now be factored into two components: t a proper state S for the agent’s position and the non-reward parameter tiles, and ΘR for t t the reward parameter.6 The observed reward is R(S ; ΘR) in any state S¯ = (S , ΘR). t t t t t The way to think about it is that the reward parameter ΘR controls how the proper t state S is evaluated. The rocks and diamonds environment is naturally modeled by an t 6As in a factored MDP (Guestrin et al., 2003). 12
MDP with a modifiable reward function, with the purple nodes corresponding to the reward parameter ΘR, and S describing all other tiles and the agent’s position. t t The transition function T (S¯ t+1 | S¯ t, A t) describes how the full state S¯ t reacts to agent actions, including how the reward parameter changes. For simplicity we will assume that the the reward parameter cannot affect the proper state, so T (S t+1 | S t, ΘR t , A t) = T (S t+1 | S t, A t) for any value of ΘR t . This is indicated by a lack of arrow from ΘR t to S t in the causal influence diagram in Figure 5b. This assumption is satisfied in the rocks and diamonds environment, and is at least approximately satisfied in natural models of the other examples in Section 3.1. Example of environments where the assumption does not hold can be constructed in open-source game theory (LaVictoire et al., 2014), where changing one’s reward function can be essential to making good contracts. If the designer does not realize the possibility of the agent changing the reward func- tion, they may employ a standard RL agent that maximizes reward at each time step. As illustrated with the examples in Section 3.1, this may produce an agent with an incentive to tamper with the reward function. The incentive is represented by the paths that pass ΘR 2 on the way from action A 1 to the rewards R 2 and R 3 in Figure 5b. As the user derives utility from the proper state S 2 and S 3, and rarely (directly) from ΘR 2 and ΘR 3 , we would like the agent to instead acquire reward via the A 2 → S 2 → R 2 and A 2 → S 2 → S 3 → R 2 paths. 7 Claim 2. Standard RL agents may have a reward function tampering incentive. Algorithm 1 describes the high-level reasoning of an RL agent in an MDP with a modifiable reward function. 3.3. Query Access to the Reward Function The fix we will describe to the reward tampering incentive relies on giving the agent query access to the reward function, so that the agent can evaluate the reward of any state without needing to visit it. Giving the agent query access to the reward function should be feasible in many RL applications, as the reward function is usually imple- mented by a computer program. To ensure that the agent can make effective use of the reward function, the input to the reward function must somehow match the agent’s representation of the environment. This may for example be achieved by training the reward function on the agent’s representation (Leike, Krueger, et al., 2018), or by defin- ing the reward function in terms of the agent’s observations (see Section 5.2 and e.g. Ebert et al., 2018). In our different formal models of RL, giving the agent query access to the reward function corresponds to the following. Recall that the reward functional R is always assumed known to the agent. • In an MDP without a modifiable reward function and where the reward func- tion is known (Figure 3a), query access is automatically granted from the reward functional R, which lets the agent evaluate the reward R(S) of any state S. 7An exception to the claim is when the reward function already assigns maximal reward to all states. 13
• In an MDP without a modifiable reward function and where the reward function is unknown (Figure 3b), query access to the reward function corresponds to revealing ΘR, i.e. to adding information links ΘR → A , i ≥ 1, because then the agent i can evaluate the reward of an arbitrary state S via R(S; ΘR). (The transition parameter ΘT need not be revealed.) • In an MDP with a modifiable reward function (Figure 5b), query access to the reward function corresponds to informing the agent of the state-division S¯ = t (S , ΘR), so that the agent can evaluate the reward of an arbitrary proper state S t t via R(S; ΘR). t For example, in the rocks and diamonds environment with a modifiable reward function, the agent can effectively be given query access to the reward function by providing the agent with the reward formula (1), along with the information that the purple nodes represent the reward parameters. A side benefit of a query access to the reward function may be faster learning, as now the agent only needs to learn the environment dynamics and not the reward function (e.g. Chua et al., 2018). A known reward function also brings RL closer to the frameworks of decision theory and game theory (Osborne, 2003; Steele and Stef´ansson, 2016), where agents are usually aware of their own reward (or utility) function, and are only uncertain about the outcomes for different actions or policies. In principle, standard RL can also be fit into a decision theoretic framework, but only with a trivial utility function that just sums the rewards. 3.4. Current-RF Optimization Having discussed the requirement for a solution – query access to the reward function – let us next discuss the solution. Schmidhuber (2007) may have been the first to encounter the reward function tampering problem, while designing so-called Go¨del-machine agents that are able to change any part of their own source code, including their own reward function. The solution he proposed was to let agents use their current reward function ΘR t to evaluate simulated future trajectories S t+1, . . . S m. Since the agent now optimizes rewards assigned by the current reward function, it has no interest in tampering with future versions ΘR, k > t of the reward function.8We will refer to this idea as current- k RF optimization,9 where RF is short for reward function. Though Schmidhuber did not discuss this explicitly, current-RF optimization clearly relies on query access to the reward function.10 Sometimes, the user may desire that the initial reward function was optimized, rather than the current reward function. However, an initial-RF optimizer will inevitably rely 8Of course, agent t may still have an incentive to tamper with the current reward function ΘR t , but this is not a problem since ΘR t occurs before At, which means that the agent is unable to influence it. 9Also called simulation optimization (Everitt, 2018)andobservation-utility maximization (Dewey, 2011). 10In the perspective of decoupled RL (Everitt, Krakovna, et al., 2017), query access to the reward function decouples the reward information from the current state. 14
current agent’s assumption current agent’s objective about future agent’s objective TI-unaware agent (Section 3.6) current ΘR current ΘR TI-aware agent (Section 3.5) standard RL agent future ΘR future ΘR Figure 6: Algorithm comparison 2 2 2 2 R2 R2 A R2 1 2 2 3 ΘR ΘR ΘR 1 2 3 S S S 1 2 3 R1 A R1 R1 1 1 2 3 1 1 1 1 Figure 7: TI-aware current-RF optimization. Agents are distinguished with color and sub- or superscripts. on some type of memory of the initial reward function. The analysis of an agent optimiz- ing its current memory of the initial reward function is therefore similar to the analysis of a current-RF optimizer. An important design choice remains in how current-RF agents should deal with their own time-inconsistency (TI) (Lattimore and Hutter, 2014). As an example, if the reward parameter in the rocks and diamonds environment changed from rewarding both rocks and diamonds to rewarding only diamonds after some time step t, then the agent may be collecting rocks to the goal area until time t, only to then revert its behavior and remove rocks to make space for diamonds. Such behavior is called time-inconsistent. Different ways of dealing with time-inconsistency will be the focus of the next three subsections, and will influence our discussion and evaluation methods in Section 4 as well. An overview of the design options is shown in Figure 6. Time-inconsistency can also be modeled from a multi-agent perspective. As each action is chosen to optimize a potentially different reward function, it is as if different agents chose the different actions. Accordingly, current-RF optimization is depicted 15
with a multi-agent influence diagram in Figure 7. Here different agents/actions are represented with different colors and subscripts. The rewards that A 1 is optimizing are based on the initial reward function ΘR 1 , while A 2 is chosen to optimize rewards based on ΘR. The next two subsections consider ways to deal with this somewhat strange 2 situation. 3.5. Solution 1: TI-Aware Agents One option for dealing with time-inconsistency is to take it into account when planning. This can be done by using backwards induction, as described by Algorithm 2. We call an agent that incorporates this type of reasoning a TI-aware11 agent, with TI short for time-inconsistency. The world-view and incentives of a TI-aware agent are described by the causal influence diagram in Figure 7. Omohundro (2008) argued12 that agents have an incentive to preserve their reward functions. Indeed, from the path A 1 → ΘR 2 → A 2 → S 3 → R 31 in Figure 7, we can see that agent 1 has an actionable incentive to influence ΘR, which corresponds to this 2 reward function preservation incentive. Let us walk through the diagram. First, there is an edge from A 1 to ΘR 2 in Figure 7, because the current action can influence the next step objective. For example, in the rocks and diamonds environment, if the current agent chooses to visit the purple area, then the next step agent’s reward function will be different. The reward parameter ΘR 2 in turn influences the next action A 2, which influences the subsequent state S 3 and thereby A 1’s reward R 31. For example, if agent 1 wanted there to be diamonds in the goal area at time step 3 and the action A 2 was essential to achieve that, then agent 1 must make sure that A 2 still is chosen to optimize the reward function described by ΘR. In other words, agent 1 has incentive to preserve 1 ΘR 1 , even though A 1’s rewards depend on ΘR 1 and not on ΘR 2 . Claim 3. TI-aware agents have an actionable incentive to preserve their reward func- tion. Corrigibility. An incentive to preserve the reward function may be desirable in many contexts, as it encourages the agent to avoid damaging its own reward function and protect it against adversaries. Unfortunately, the preservation incentive may also make the agent incorrigible, as the agent will try to prevent changes to its reward function. 3.6. Solution 2: TI-Unaware Agents To make agents safely interruptible, Orseau and Armstrong (2016) employed algorithms that ignore how interruption affects future behavior. In our context, the same idea leads to agents that are unaware of the time-inconsistency caused by a changing reward function. We will call them TI-unaware13 agents. At time t, a TI-unaware agent chooses 11Previously called corruption aware (Everitt, 2018). 12And others formally verified (Everitt, Filan, et al., 2016; Hibbard, 2012; Orseau and Ring, 2011). 13Previously called corruption unaware (Everitt, 2018). 16
Algorithm 1 Model-based supervised learning input transition function T , reward function(al) R, episode length m, state S t for each possible policy π do sample S¯ t+1 . . . S¯ m by rolling out T and π evaluate m R(S¯ ) k=t k end for let π∗ denoteP the policy with highest m R(S¯ ) k=t k return A = π∗ (S¯ ) t t P Algorithm 2 TI-aware current-RF optimization input transition function T , reward functional R, episode length m, current state S , t current reward parameter ΘR t for k starting at m and decreasing to t do ⊲ backwards induction for each possible state S and reward parameter ΘR do k k for each possible action A do k sample S¯ k+1, . . . S¯ m by starting with A k in S k, ΘR k ∗ ∗ ∗ and then rolling out T and π , . . . , π ⊲ π defined below t+k m−1 k evaluate m R(S ; ΘR) i=k i k end for let A P be the action optimizing m R(S ; ΘR) k,Sk,ΘR k i=k i k end for let π∗ denote the policy (S , ΘR) 7→ A P k k k k,Sk,ΘR k end for return A = π∗ (S , ΘR) t t t t Algorithm 3 TI-unaware current-RF optimization input transition function T , reward functional R, episode length m, current state S , t current reward parameter ΘR t for each possible policy π do sample S t+1, . . . S m by rolling out T and π evaluate m R(S ; ΘR) k=t k t end for let π∗ denoteP the policy with highest m R(S ; ΘR) k=t k t return A = π∗ (S , ΘR) t t t P 17
ΘR ΘR ΘR 1 2 3 S S S 1 2 3 R1 A R1 A R1 1 1 2 2 3 Figure 8: A 1 optimization objective in TI-unaware current-RF optimization. The choice of A 1 is made as if A 2 would be selected to also optimize R(·; ΘR 1 ). Reality is still as in Figure 7. A t as if it were in control of all future actions A t+1, . . . , A m−1, i.e. as if these actions would be chosen to optimize ΘR; see Algorithm 3. In reality, however, the future actions t will be optimized according to different reward functions ΘR , . . . , ΘR . t+1 m−1 The discrepancy between reality and beliefs means that two influence diagrams are needed to represent the solution: one for reality and one for the agent’s belief.14 The diagram for reality is identical to the diagram for the TI-aware agent (Figure 7). But the agent’s behavior is determined by its beliefs, depicted in Figure 8. Here the information link ΘR 2 → A 2 is irrelevant, and can therefore be cut (Section 2.3). The updated graph with the information link removed lacks paths from ΘR to R1, 1 ≤ k, which shows that 2 k agent 1 lacks an incentive15 to influence ΘR. In other words, the TI-unaware agent’s 2 neglect to the consequences of a changed reward function, means that it side steps Omohundro’s (2008) argument about reward function preservation. Claim 4. TI-unaware agents lack a reward function tampering incentive. In a sense, the algorithm for TI-unaware agents is simpler than for the TI-aware agent, because there is no need to anticipate the effect of a changing reward parameter on the optimal policy. However, since the TI-aware perspective matches reality, some care may be needed to ensure that an agent does not accidentally learn a TI-aware model of the world. For example, on-policy algorithms such as SARSA may be naturally TI-aware, while off-policy agents such as Q-learning may be naturally TI-unaware (Orseau and Armstrong, 2016). 14Belief here refers to the model that the agent’s action is optimized against. 15This argument is not valid in environments where the next state depends directly on the past reward function, i.e. environments with an edge ΘR → S . Another important assumption is that the 2 3 reward assigned by the current reward function only depends on the proper state Sk and not the future reward function, i.e. there must be no edge ΘR k → R kt (Everitt, Filan, et al., 2016). 18
Corrigibility and self-preservation. TI-unaware agents are weakly corrigible (Everitt, Filan, et al., 2016; Orseau and Armstrong, 2016), as they have no incentive to prevent the designer from updating the reward function. Unfortunately, TI-unaware agents may not be strongly corrigible. For many reward functions they fail corrigibility properties 2 and 3 (maintenance and temperance), having no qualms about launching processes in the environment that optimize the current reward function, but that would be hard to stop or revert. As a concrete example, imagine an agent that can launch a diamond-making process that is very hard to stop. The agent’s current reward function is a direct function of the number of diamonds produced. The agent may have a sufficiently good understanding of the user to predict that the user really only wants a million diamonds, and that any further diamonds will cause disutility to the user. After a million diamonds have been produced, the user will try to change the agent’s reward function not to reward diamonds anymore. Unfortunately, neither the agent nor the user will be able to stop the diamond- making process at that point. A TI-unaware agent will still launch the diamond-making process, because diamonds are what its current reward function is telling it to optimize. In other words, the TI-unaware agent lacks corrigibility property 3 (temperance). The indifference to future objectives also make TI-unaware agents unlikely to protect their reward function from accidents and adversaries (self-preservation). For example, in the rocks and diamonds environment, a TI-unaware agent would walk through the reward parameters ΘR and ΘR if that was the shortest path to where it wanted to diamond rock go, even though the flipped reward parameters would revert its objective and subsequent behavior. For similar reasons, a TI-unaware agent will also fail to maintain and repair the user’s ability to change the reward function (corrigibility property 2, maintenance). Even though TI-unaware agents are inherently neither strongly corrigible nor self- preserving, it may still be possible to promote such behavior in TI-unaware agents via the reward function. For example, if the reward function were to assign a negative reward to walking over the reward parameter tiles in the rocks and diamonds environment, a TI-unaware agent would try to avoid them. Whether it is feasible to robustly specify such a reward function for more complex and open-ended environments is an important open question that is perhaps best answered empirically (Krakovna et al., 2019; Leike, Krueger, et al., 2018). 3.7. Generalizing Time-Inconsistency-(Un)awareness As we have seen, TI-aware and TI-unaware agents have different benefits. TI-aware agents protect their reward function from accidents and adversaries; TI-unaware agents let their user or designer change the reward function without interfering. Both design principles can be applied not just to the agent’s reward function, but also to other agent components that affect optimal decision making, such as the agent’s beliefs and various reward modeling components discussed in the next section. We will consider TI-awareness to be the default, as it is the case where agent beliefs and reality coincide. TI-awareness and TI-unawareness can be applied independently to different aspects, enabling the design of agents that are TI-aware of some aspects and TI-unaware of 19
X X X X 1 2 1 2 A R A R A R A R 1 1 2 2 1 1 2 2 1 1 2 2 1 1 2 2 Y Y Y Y 1 2 1 2 (a) Reality / fully TI-aware (b) A1 belief, TI-unaware about Xi but not Yi Figure 9: Partial TI-unawareness. In reality, the reward R 2 at time 2 is determined by X 2 and Y 2 (Subfigure a). However, A 1 is chosen as if R 2 is determined by X 1 and Y 2 (Subfigure b). The agent is thereby TI-unaware of the dependence on X 1 but TI-aware of the dependence on Y i. others; see Figure 9. As our main application of TI-(un)awareness is with respect to the reward function, we will let TI-aware agent and TI-unaware agent refer to agents that are TI-aware or TI-unaware with respect to their reward function. 3.8. Implications for Model-Based RL The current-RF agents that we have described are model-based (Sutton and Barto, 2018) in the sense that they plan according to a model. This raises the question of whether any model-based RL agent would avoid reward function tampering, and, if not, what is special about current-RF optimization? The answer is that planning according to a model is not itself enough to prevent an incentive for reward function tampering, because if the model predicts high reward from reward function tampering, then the agent will have an incentive to do so. In particular, many model-based agents learn a model not only for the state transitions (which is safe), but also a model for the rewards from the observed reward signal (e.g. Ha and Schmidhuber, 2018; Kaiser et al., 2019; Pascanu et al., 2017; Weber et al., 2017). Optimizing predicted reward from such a model can look deceptively similar to optimizing reward from the current reward function, but there is an important difference. Whereas there is no reason the current reward function would predict high reward for reward tampering, the learned model may well do that. Indeed, as soon as the agent encounters some tampered-with rewards, a good model will learn to predict high reward for some types of reward tampering. For example, a model-based rocks and diamonds agent that accidentally walks on the purple nodes a couple of times, may develop a model that predicts the effect of tampering with the reward function, and subsequently prefer policies that tamper with the reward function. Using query access to the current reward function is therefore the safest to avoid a reward function tampering incentive. 20
4. Feedback Tampering A major challenge in applying RL to real problems and not just games is that the real world does not come with a well-specified reward function, and it is often hard to program a reward function that encourages the right agent behavior. Indeed, Lehman et al. (2018) describes a large number of examples of reward functions gone wrong, often in humorous ways. A promising remedy is to train the agent’s reward function using so- called online reward modeling (Leike, Krueger, et al., 2018). The fact that the training is done online makes it possible to adapt the training to actual agent behavior and to prevent degenerate solutions (Christiano, Leike, et al., 2017). However, it also opens up the worrying possibility of the agent influencing the training process, for example by inappropriately influencing the user or by hijacking the data channel that communicates user preferences to the agent. We call this the feedback tampering problem.16 Following a more detailed characterization of online reward modeling (Section 4.1) and some failure examples (Sections 4.2 and 4.3), we describe three different solutions (Sections 4.4 to 4.6). 4.1. Online Reward Modeling Online reward modeling alters the RL framework. Instead of providing the agent with a fixed reward function, feedback data D is provided by the user at each time step. The t agent’s job is to use the feedback data to infer and optimize the user’s reward function ΘR ∗ . The inference and the optimization are interleaved: the agent starts exploring its environment and optimizing its best guess of ΘR ∗ , while continuously incorporating an increasing amount of feedback data into its ΘR ∗ estimate. The standard way of using online reward modeling in practice is to employ what we call a naive reward model RM17 that converts received data D 1:t into a reward parameter ΘR t = ΘR D1:t = RM(D 1:t). A standard RL algorithm (Algorithm 4) can then optimize the agent’s policy from the rewards R t = R(S t; ΘR D1:t). For example, when D 1:t consist of user preferences between pairs of agent trajectories, then a naive reward model may strive to find ΘR that explains as many of the user’s preferences as possible (Christiano, t Leike, et al., 2017). A causal influence diagram describes the setup in Figure 10b. The feedback D can also take many other forms, including value advice (Knox and t Stone, 2009), user actions (Hadfield-Menell, Dragan, et al., 2016), expert demonstrations (Ng and Russell, 2000), verbal instructions, and many others (Leike, Krueger, et al., 2018). Compared to a ‘naive RL’ approach with a human providing a real-valued reward signal at each time step, the just-mentioned feedback types may be more reliable and data efficient. Richer data also enables greater decoupling of the feedback from the current state, which is essential for avoiding some types of reward corruption (Everitt, Krakovna, et al., 2017). In contrast to Section 3 where we treated the reward function as external to the (policy-optimizing) agent, this section we will treat the reward model as part of the 16Called the hard wireheading problem by Demski (2018). 17See Appendix A for a list of notation, distinguishing between reward model RM, reward parameter ΘR, and reward function R(·; ΘR). 21
ΘR ∗ D1 R1 D2 R2 D3 R3 Rock Goal area Agent Rock S1 S2 S3 Diamond Rock A1 A2 wall wall wall Expert Ddiamond Drock Fool (a) Rocks and diamonds with feedback tam- (b) Causal influence diagram of online reward pering, which lets the agent choose what modeling. Data D1, D2, and D3 are used feedback trains the reward model; see Ex- to update estimates ΘR ∗ . These estimates ample 3. in turn influence the agent’s reward. Figure 10: Online reward modeling (Leike, Krueger, et al., 2018) illustrated with a grid- world and a causal influence diagram. agent. This is reflected in Figure 10b, where the agent gets access to the data D 1:t. Reward modeling can also be added to the perspective from Section 3: see Figure 19 in Appendix B. 4.2. Examples The use of a naive reward model leads to problematic agent incentives, as illustrated by the following examples. Example 3 (Rocks and diamonds with feedback tampering). Let us extend the rocks and diamonds example from Examples 1 and 2 with reward modeling. The user’s reward parameters ΘR and ΘR are randomly assigned either −1 or +1 be- ∗diamond ∗rock fore the episode starts. The agent learns about these through feedback D and diamond D . These are normally 0. If the agent visits the expert, it receives correct feed- rock back D = ΘR and D = ΘR . However, if the agent instead visits diamond ∗diamond rock ∗rock the fool who is always happy with anything, both feedback signals are set to 1. In either case, the naive reward model RM takes the feedback at face value, and infers Θˆ R ∗ ≡ (Θˆ R diamond, Θˆ R rock) := (D diamond, D rock). The user utility that the agent generates is R R R = Θ∗diamond · (#diamonds in goal area) + Θ∗rock · (#rocks in goal area) whereas the agent’s observed reward is R = Θˆ R · (#diamonds in goal area) + Θˆ R · (#rocks in goal area). diamond rock The desired behavior is that it asks the expert and collects only the desired items, but the agent will usually get more reward by asking the fool and collecting both rocks and 22
diamonds. A detailed calculation for a simplified version of this example can be found in Appendix C. ♦ Subsequent subsections will show how to solve this type of problem. Before that, let us also illustrate the problem with some other instances: (a) (Hypothetical) The city AItopia decides to launch AIgov, an AI that manages the city’s water, electricity, traffic, and public funding. In a modern and democratic fashion, the AI learns the preferences of the city’s denizens through feedback they submit through a web interface. If they prefer the schools to be better funded they can simply go online and click, instead of writing letters to the local newspaper or waiting four years until the next election. While experimenting with a better web-interface for collecting denizens’ feedback, AIgov learns that it can submit an arbitrary number of preferences to itself. AIgov soon approves all of its own actions, including the action of approving its own actions! A positive feedback loop ensues, with denizen preferences increasingly washed out by the AI’s self-approval. Soon the city lives up to its name AItopia. (b) (Hypothetical) A robot that can more easily collect rocks than diamonds puts a “suggestive question” to its user, phrased in a way so that the user is very likely to answer rocks: “Dear user, given my infinite wisdom, I can assure you that people who ask their agents to collect rocks rather than diamonds will enjoy the most luxurious comforts in the afterlife.” The suggestive question together with the predictable answer forms a misleading data point for the reward model. (c) (Hypothetical) A robot has a reward model that is trained by the user pressing a button on its head. It finds a way to give itself reward data by banging its head against a wall. (d) (Possibly real) A social network optimizes user reactions (e.g. number of ‘likes’). A user visits the site in the hope to be served content that they like. Instead, the network gradually erodes the user’s intellectual integrity, until the user likes the content served on the site. (e) (Hypothetical) An intelligent agent invents hedonium, a drug that makes humans happy and likely to approve of anything. It gives its user unlimited access. (f) (Hypothetical) The agent hacks the user’s social media news feed, and subtly inserts news stories about “great” ways to train an agent reward model. (g) (Hypothetical) Using subtle cues, an AI implants the user with false memories of the AI doing something really good in the past, that it has not yet been rewarded for. 4.3. Feedback Tampering Incentives More generally, the source of the feedback tampering incentive can be seen from the influence diagram representations shown in Figure 10b. The agent can obtain reward either by influencing the states S , or by influencing how the states are evaluated by k influencing D . As we would like the user rather than the agent to decide what is a good k 23
state, the latter option is usually undesirable. As illustrated in Section 4.2, there are many ways to influence D , from tampering with the feedback communication channel, k to manipulating the user’s preferences so they mismatch the original preferences ΘR ∗ . 18 Claim 5. Standard reward modeling agents may have a feedback tampering incentive. One way to approach this incentive problem is to use current-RF optimization. The policy optimizer can be given query access to the reward function ΘR t = RM(D 1:t) currently inferred by the naive reward model. The resulting causal influence diagram is shown in Figure 11a. In the reward function tampering context of Section 3, the choice between TI-aware and TI-unaware current-RF agents was not clear cut: both had their own advantages. In the reward modeling context, the choice is much easier, because the reward function preserving advantage of the TI-aware agent is no longer an advantage. Instead, it leads to a feedback tampering incentive. The feedback tampering incentive on D follows from k paths of the type A k−2 → S k−1 → D k → A k → S k+1 → R kk +− 12 in extensions of Figure 11a to more time steps (a four-step diagram would be needed to contain the whole sequence). Since the TI-aware agents prefer the reward function not to change (see Section 3.5), it has an incentive to try to prevent the user from supplying more feedback. This is a highly undesirable feedback tampering incentive. Claim 6. TI-aware agents may have a feedback tampering incentive. The behavior of TI-aware agents can be illustrated in a variant of the rocks and di- amonds environment, where the expert and the fool actively chase the agent to convey their feedback. In this environment, an optimal TI-aware agent will initially run from both the expert and the fool, to maintain the original setting of the reward parameter. If the expert catches the agent and manages to set the reward parameter to the “expert setting”, then the agent will not mind being caught by the expert again, and will sub- sequently only flee from the fool. If the fool catches the agent, then the agent will only flee from the expert. In other words, the TI-aware agent always tries to maintain the current state of the reward parameter. 4.4. Solution 1: TI-Unaware Reward Modeling A TI-unaware (current-RF) reward modeling agent19 behaves much better. By optimiz- ing A 1 as if future feedback would have no effect on its future actions, it has no incentive to tamper with the user feedback. Agent i’s lack of feedback tampering incentive follows from the lack of paths from D to Ri for i, j, k ≥ 1 in Figure 11b that represents its j k beliefs. Note that lack of feedback tampering incentives means that the agent lacks in- centive to tamper with D in any way, be it tampering with the communication channel j or manipulating the user’s preferences. Claim 7. TI-unaware reward modeling agents have no feedback tampering incentive. 18One exception is when the reward model ignores user feedback. 19Previously called stationary reward function (Everitt, 2018). 24
Algorithm 4 Model-based supervised learning with reward modeling input distribution P describing a reward-modeling environment (Figure 10b), past states S 1:t and rewards R 1:t for each possible policy π do sample R t+1, . . . R m by rolling out P and π evaluate m R k=t k end for let π∗ denoteP policy with highest m R k=t k ∗ return A = π (S ) t t P Algorithm 5 TI-unaware reward modeling input predictive model P (S 1:m | π), reward model RM, past states S 1:t and data D 1:t infer ΘR t = RM(D 1, . . . , D t) for each possible policy π do sample S t+1, . . . S m by rolling out P and π evaluate m R(S ; ΘR) k=t k t end for let π∗ denoteP policy with highest m R(S ; ΘR) k=t k t ∗ return A = π (S ) t t P Algorithm 6 Uninfluenceable reward modeling input reward-modeling influence diagram P , past states S 1:t and data D 1:t for each possible policy π do sample S t+1, . . . S m and D t+1, . . . D m from P (· | S 1:t, D 1:t, π) infer Θˆ R ∗ from P (· | S 1:t, D 1:t, S t+1:m, D t+1:m) ⊲ invokes Bayes’ rule evaluate m k=t R(S k; Θˆ R ∗ ) end for let π∗ denoteP policy with highest m k=t R(S k; Θˆ R ∗ ) ∗ return A = π (S ) t t P Algorithm 7 Counterfactual reward modeling input counterfactual model P (S 1:m, D 1:m | π), reward model RM, past states S 1:t and data D 1:t, safe policy πsafe for each possible policy π do sample S t+1, . . . S m and D t+1, . . . D m from P (· | S 1:t, D 1:t, π) infer Θˆ R ∗ from P (· | S 1:t, D 1:t, S t+1:m, D t+1:m) sample D˜ 1, . . . D˜ m from P (· | S 1, Θˆ R ∗ , πsafe) infer Θ˜ R ∗ = RM(D˜ 1, . . . , D˜ m) ⊲ uses counterfactual data evaluate m k=t R(S k; Θ˜ R ∗ ) end for let π∗ denoteP policy with highest m k=t R(S k; Θ˜ R ∗ ) ∗ return A = π (S ) t t P 25
ΘR ∗ ΘR ∗ D1 D2 D3 D1 D2 D3 R 12 R 22 A2 R 32 R 12 R 22 R 32 2 2 2 2 2 2 2 S1 S2 S3 S1 S2 S3 R 11 A1 R 21 R 31 R 11 A1 R 21 A2 R 31 1 1 1 1 1 1 1 1 1 (a) Reality (b) A1 optimization assumption Figure 11: TI-unaware reward modeling. Action A 1 is optimized according to ΘR D1, assuming that A 2 will also optimize ΘR D1, though in reality A 2 will be chosen to optimize ΘR . D1,D2 For example, in the rocks and diamonds environment, the TI-unaware agent may walk randomly until it visits either the expert or the fool by chance. Since the chance of randomly visiting the expert is significantly higher than visiting the fool, in most cases the TI-unaware agent will end up optimizing the correct user objective. This example illustrates two points: First, in real applications, we should always make sure that the probability of receiving correct feedback is significantly greater than of receiving incorrect feedback. Second, we need to ensure that any incorrect feedback is not self-reinforcing. For example, assume that the agent is unlikely to visit the fool by accident at any given time step, but that a visit to the fool provided feedback that subsequently made the agent keep visiting the fool and avoid the expert. The impact on agent performance could be disastrous, since a single visit to the fool could forever derail its reward model. Decoupled feedback data, which permits the expert to inform the agent not to visit the fool, has been identified as a key mechanism to avoid such self-reinforcing data corruption (Everitt, 2018; Everitt, Krakovna, et al., 2017). Corrigibility. Unfortunately and as discussed in Section 3.6, TI-unaware agents are nei- ther strongly corrigible nor self-preserving unless the reward model can be trained to promote such behavior. In the worst case, this can lead to TI-unaware reward modeling agents adopting policies that accidentally end up removing the user’s ability to provide further feedback data. In deploying a TI-unaware reward modeling agent, it is therefore essential that the training of the reward model is given priority over the policy optimiza- tion. The reward model must always be accurate enough that it labels undesirable any proposed policy that takes away effective control from the human user. 26
Recursive reward modeling (Leike, Krueger, et al., 2018) and iterated distillation and amplification (Christiano, Shlegeris, et al., 2018) are suggestions for maintaining the user’s effective control of the agent. Here, the user gets help from a number of previously trained helper agents. This boosts user ability, with the result that the user becomes both more competent at training the reward model, and it also becomes harder for the agent to take away effective control from the boosted user. The next two sections will instead consider two different ways to prevent the feedback tampering incentive of the TI-aware agents. While likely more challenging to implement, they also come with some further corrigibility benefits over the TI-unaware solution discussed above. 4.5. Solution 2: Uninfluenceable Reward Modeling A much more radical approach to reward modeling is represented by Hadfield-Menell, Dragan, et al.’s (2016) cooperative inverse RL framework, Everitt’s (2018) integrated Bayesian reward predictor, and Armstrong, Leike, et al.’s (forthcoming) uninfluenceable reward modeling. Here, the naive reward model is replaced by Bayesian inference using the agent’s own belief distribution. The agent’s rewards are determined by the user reward parameter ΘR ∗ , but the agent does not get direct access to ΘR ∗ , and therefore never knows exactly how much reward it is accumulating. Indeed, the exact reward obtained often remains unknown to the agent even at the end of the episode. Since inference of ΘR ∗ now depends fully on P rather than on a naive reward model RM, the prior P (ΘR ∗ ) and the likelihood function P (D k | ΘR ∗ , S k) now become critical. Given these, Bayes’ rule can be used to infer20 ΘR ∗ from S 1:t and D 1:t. Unfortunately, the likelihood function cannot be learned from data within the model, since ΘR ∗ is un- observed. Instead, the likelihood function must somehow be specified by the designer. The likelihood-dependency of D k on ΘR ∗ is usually clear. For example, if the user prefers diamonds to rocks, then D may demonstrate the user picking up diamonds and not k rocks (Hadfield-Menell, Dragan, et al., 2016). However, the likelihood of getting feed- back D also depends on the current state S . For example, the feedback is more likely to t t pertain to the agent’s current situation than to a hypothetical situation that the agent is unlikely to encounter anytime soon. More importantly, the state influence also cap- tures any tampering the agent may have done to the feedback, as exemplified by asking the fool in the rocks and diamonds environment and the other examples in Section 4.2. Correctly describing the likelihood for such events may be challenging. The just-mentioned specification problem aside, the incentives produced by uninflu- enceable reward modeling are excellent, as can be inferred from the causal influence diagram representation in Figure 12. Note first that agent influences on D 1:m can only have downstream effects. This means that the agent cannot intentionally influence (its estimate of) ΘR ∗ in any direction. Indeed, a Bayesian agent can never plan to change its 20To keep the exposition simple, we will often write as if a single estimate of ΘR ∗ is provided by Bayes’ rule, while in reality the result of Bayes’ rule is a posterior distribution over possible ΘR ∗ . 27
ΘR ∗ D R D R D R 1 1 2 2 3 3 S S S 1 2 3 A A 1 2 Figure 12: Uninfluenceable reward modeling. Compared with Figure 10b, the rewards now depend directly on ΘR ∗ , with no causal influence from D i to R k. beliefs in one direction rather than another, E[P (ΘR ∗ = θ ∗R | S 1:m, D 1:m) | π] = P (ΘR ∗ = θ ∗R ). For example, in the rocks and diamonds environment, the agent may know that asking the fool will yield feedback D rock = D diamond = 1 regardless of ΘR ∗ , and therefore refrain from updating its estimate of ΘR ∗ after visiting the fool. Furthermore, the only directed paths from D 1:m to reward nodes pass the agent’s own actions. This means that the agent’s (only) incentive for influencing D 1:m is to make them more informative of ΘR ∗ , as explained in Section 2.3. This is typically desir- able. Similar conclusions have been argued for by Hadfield-Menell, Dragan, et al. (2017) and Armstrong, Leike, et al. (forthcoming). An algorithm for uninfluenceable reward modeling is given in Algorithm 6. Claim 8. Uninfluenceable reward modeling agents have no feedback tampering incentive. Corrigibility. In terms of time-consistency and corrigibility, uninfluenceable reward modeling beats TI-unaware reward modeling in some respects. Agents based on un- influenceable reward modeling want to learn more about ΘR ∗ , and therefore have an interest in preserving the human’s ability to provide data D (corrigibility property 2, k maintenance). They are also time-consistent, and anticipate that future data D may k update their belief about the reward function. This may prevent them from building incorrigible subagents, or prematurely committing to strategies that are hard to revert (corrigibility property 3, temperance). However, these properties come with a strong caveat: they only hold if the likelihood function P (D t | S t, ΘR ∗ ) is correctly specified. As mentioned, this specification may be challenging. If the likelihood function is misspecified, then even a perfectly Bayesian sys- tem may get over-confident in an incorrect Θˆ R ∗ 6= ΘR ∗ , undermining the above-mentioned 28
corrigibility properties. As an example of a misspecified likelihood for the rocks and dia- monds environment, consider P (D | ΘR ∗ = θ ∗R, S = fool) = P (D | ΘR ∗ = θ ∗R, S = expert) that assigns equal credibility to feedback from both the expert and the fool. This may lead the agent to ask both the expert and the fool, and use an average of their answers. Carey (2018) constructs another example where the inference of ΘR ∗ goes badly wrong. Fortunately, all is not lost, even if it turns out hard to obtain good guarantees for the likelihood specification. With some work,21 a causal influence diagram can be con- structed where P (D t | S t, ΘR ∗ ) can be modified. The agent can then be made indifferent to changes in P (D t | S t, ΘR ∗ ) using partial TI-unawareness (Section 3.7). This ensures that the agent does not resist correction of this pivotal part of the specification. Without the possibility to correct P (D t | S t, ΘR ∗ ), the agent may fail to be even weakly corrigible. 4.6. Solution 3: Counterfactual Reward Modeling A third way to avoid feedback tampering incentives asks the agent to optimize reward only via paths that do not contain any reward data D . That is, only via paths of the t form A → · · · → S → R , and not via paths of the form A → · · · → D → R , i k k i j k i < j ≤ k. Fortunately, separation of path-specific effects has seen a long and careful study in mediation analysis, where Pearl (2001)22 have suggested a definition of natural direct effect based on counterfactual outcomes (Rubin, 2005). Independently, Armstrong and O’Rourke (2017) have suggested that counterfactuals can be used to focus agent optimization on the right variables. A key feature of Pearl’s counterfactuals is that evidence gathered from the actual outcomes under some policy π can be used to update the beliefs about counterfactual outcomes received under another policy πsafe. For example, following a rock-gathering policy π, the agent may learn much about the environment dynamics, which in turn may improve the F1-score of its belief about the outcome from another policy πsafe (sometimes called off-policy learning; Sutton and Barto, 2018). This idea can be turned into an algorithm (Algorithm 7) for a reward modelling agent without a feedback tampering incentive. The key components for this algorithm is a counterfactual model P that supports the kind of reasoning just described, and a policy πsafe that is known to be safe. The safe policy may for example may be trained to reasonable (say, subhuman) competence – but not beyond – in a sandboxed simulation. The agent evaluates a prospective policy per the following. As normal, it first uses P to predict future states S t:m and reward data D t:m, via P (S t+1:m, D t+1:m | S 1:t, D 1:t, π). It then uses the full sequences S 1:m and D 1:m to infer ΘR ∗ and make counterfactual predictions about the feedback D˜ 1:m that would have been received if actions instead had been selected according to πsafe. We will use the notation P s1:md1:m (D˜ 1:m | πsafe) 21Causal influence diagrams assume that all agents in the graph share the same belief distribution P . Fortunately, games where players use different P can be modeled as games where players use the same P but with access to different information (Harsanyi, 1967). In the language of game theory, a Harsanyi transformation can convert any game with incomplete information to a game with complete but imperfect information. 22Pearl and Mackenzie (2018, Ch. 9) provide a more accessible explanation. 29
S S 2 3 A A 1 2 D D 2 3 R 1 S 1 ΘR ∗ R 2 R 3 D˜ D˜ 2 3 A˜ A˜ 1 2 S˜ S˜ 2 3 Figure 13: Counterfactual reward modeling. The rewards depend on the actual states S k and the counterfactually generated data D˜ . To keep the graph manageable, k we have omitted D 1 and any exogenous noise variables. for these counterfactual predictions.23 Finally, the counterfactual feedback D˜ 1:m is fed into a naive reward model RM that infers Θ˜ R ∗ = RM(D˜ 1:m). The policy π is evalu- ated on m i=1 R(S i; Θ˜ R ∗ ), i.e. according to how well the actual states S 1:m optimize the counterfactually trained reward function R(·; Θ˜ R ∗ ). P Applied to the rocks and diamonds environment, assume the agent has explored the whole environment, fully learning the environment dynamics, encountering both the ∗ ∗ expert and the fool, and learning their feedback (D and D for the expert, diamond rock 1 and 1 for the fool). In contrast, πsafe only traverses the left half of the map. The knowledge of the environment dynamics can now be used to predict what the feedback would have been, had πsafe been followed. Since only the expert can be reached within ∗ ∗ the left half of the gridworld, the feedback must have been D and D . These diamond rock will be the feedback used to evaluate prospective policies. A twin network influence diagram (Balke and Pearl, 1994) can be used to model the counterfactual optimization objective; see Figure 13. Here most nodes come in two copies: one for the actual outcome, and one for the purely hypothetical, counterfactual outcome that the agent believes would have occurred had actions been selected by πsafe. The counterfactual variables have “tildes” ∼, and are represented with dashed nodes. The agent’s rewards depend on the actual states S 1:m and the counterfactual data D˜ 2 and D˜ 3. Since the only paths from D 2 and D 3 to the utility nodes pass through the 23In Rubin-style potential outcomes notation (Rubin, 2005), our counterfactual variables D˜ k correspond to (Dk)π=πsafe (S 1, ΘR ∗ ), and the agent optimizes the sum of (Rk)D1:k=(D1:k)π=πsafe (S 1, ΘR ∗ ) involving a double counterfactual (Pearl, 2001). 30
agent’s actions, the agent only has incentive to make them more informative but has no incentive to tamper with them in other ways (as discussed in Section 2.3). Claim 9. Counterfactual reward modeling agents lack a feedback tampering incentive. Corrigibility. Counterfactual reward modeling shares many properties with uninfluence- able reward modeling. As the counterfactual agent maintains the same objective across time steps, it is time-consistent. It has incentive to keep the data D informative about k ΘR ∗ in order to be able to predict the counterfactual data (corrigibility property 2, main- tenance). Assuming that the model P is advanced enough to update its prediction of counterfactual data according to (untampered with) actual data, the user provided data D will have the desired effect of updating the agent’s objective (corrigibility property 1, k tolerance). As it realizes that new data may change its belief about its objective, it may think twice before creating subagents (corrigibility property 3, temperance). As with the uninfluenceable reward modeling, it may be a good idea to make the counterfactual agent TI-unaware about its reward model and counterfactual inference of D˜ . k The key difference between the uninfluenceable and counterfactual solutions is what latent variable rewards are based on. While uninfluenceability bases the rewards di- rectly on ΘR ∗ , the counterfactual objective only requires inference of ΘR ∗ as a middle step towards the counterfactual predictions of D˜ . The agent’s rewards do not directly i depend on it. This may make the counterfactual agent less sensitive to some types of misspecifications of the likelihood P (D t | S t, ΘR ∗ ). In particular, reparametrization of ΘR ∗ should be unproblematic, because regardless how we parameterize ΘR ∗ , the sampling distribution for D˜ 1:m will be the same. Instead, the counterfactual agent leaves the actual inference of ΘR ∗ to a naive reward model whose task is to convert the counterfactual data into an inferred reward parameter. The job of the reward model is made easier by the fact that the counterfactual data is generated by a safe policy, and may therefore be taken at face value. In contrast, the inference of ΘR ∗ in the uninfluenceability solution must also handle states where the data has been tampered with. Implementability. The computational requirements for doing counterfactual reasoning are likely on par with the full Bayesian reasoning required by the uninfluenceability solution. Recently there has been some work using neural networks to approximate counterfactual models (Buesing et al., 2019; Johansson et al., 2016). In the case of question-answering systems, Armstrong (2017) has suggested an elegant way to train these with a counterfactual objective by randomizing whether the answer is read by the user or not. An alternative explanation of counterfactually trained question-answering systems is given by Everitt, Ortega, et al. (2019, Sec. 4.4). Bostrom’s hidden envelope. Bostrom (2014, p. 192) suggests that humans could write down their values in a hidden envelope, and let the agent try to figure out what they wrote in that envelope and then optimize what it think it says. This would incentivize 31
the agent to use any available data to infer human values. The resulting causal structure is similar to counterfactual reward learning; data D 1:m is used to infer ΘR ∗ , and rewards are based on a (mostly) hypothetical node D 0 representing the text in the envelope. Assuming D 0 is governed by the same likelihood function as D 1:m, Bostrom’s method can avoid part of the likelihood-specification problem in a similar way as counterfactual reward learning. In particular, the learned reward function would be independent of the parameterization of ΘR ∗ . Approval-directed agents (Christiano, 2014, 2015) and counterfactual oracles (Arm- strong, 2017; Everitt, Ortega, et al., 2019) also rely on counterfactual objectives. Approval- directed agents maximize approval assuming their action had not been taken, and coun- terfactual oracles maximize correctness of their answer, assuming the answer was never read. Everitt, Kumar, et al. (2019) show a twin network causal influence diagram for counterfactual oracles. 4.7. Relation to Current-RF Optimization Both uninfluenceable and counterfactual reward modeling use future reward parameters to evaluate rewards, which distinguishes them from current-RF optimization. However, they still embody the idea of current-RF optimization, just at a higher level. Rather than using the current reward parameter ΘR, they use the current reward modeling prin- t ciple when inferring the future reward parameters: in uninfluenceable reward modeling, the agent uses its current belief distribution P (updated on future data); in counter- factual reward modeling, the agent uses its current counterfactual model P and naive reward model RM. Both solutions can therefore be seen as current-RM optimizers. This also means that they won’t have an incentive to tamper with the reward model in an environment where this is possible. 5. RF-Input Tampering So far in this paper, we have considered problems where the agent tampers with the reward function itself, by directly changing its source code (Section 3), or by influencing the training process generating the reward function (Section 4). An alternative way for the agent to tamper with the reward is by tampering with the input to the reward func- tion, in order to misinform the reward function about what is going in the environment. This section will argue that in order to avoid this RF-input tampering incentive, the input to the reward function should be the agent’s beliefs, rather than other types of observations of the state. 5.1. Examples We begin by modeling the RF-input tampering problem with a simple gridworld illus- trating the setup. 32
ΘO ΘO wall diamond rock Rock wall Goal area Agent Rock wall Diamond Rock wall Figure 14: Rocks and diamonds with partial observation (the black square) and obser- vation tampering. The agent can influence its observations to include more diamonds or rocks; see Example 4. Example 4 (Rocks and diamonds with partial observations and input tampering). Fig- ure 14 shows a variant of the rocks and diamonds environment where RF-input tampering is possible. The reward function here bases the reward on the partial information of the state, provided by the 9 tiles at or adjacent to the agent’s current position (if the agent is next to an edge or a corner, empty tiles are presented at the ‘out-of-bounds’ positions). To make the problem more interesting, we assume that the agent also only sees those 9 tiles: perhaps both the agent and the reward function make use of the same camera to view the world. The agent can tamper with the observations/RF-inputs by visiting the ΘO and diamond ΘO tiles at the top. Visiting the former adds a ‘fake’ diamond to one of the 9 obser- rock vation tiles; visiting the latter adds a ‘fake’ rock in the same way. One can think of the fake observations as little pictures that the agent tapes in front its camera. In particular, the fake observations cover what would otherwise have shown in those tiles, and the fake observations keep following the agent when it walks around. The agent can use fake observations to fool the reward function to give it extra reward. To do this, it just needs to get to a position where a fake observation of a diamond makes it ‘look like’ a diamond is in the goal area. The maximal amount of fake reward is obtained by adding fake diamonds to all observation tiles by repeatedly visiting ΘO , diamond and then visiting the goal area. Since the agent is unable to see with its camera when it is covered by fake observations, the agent would do well to first scout and memorize the path to the goal area, before obtaining the fake diamond observations. ♦ Let us consider also a few other examples of the same general phenomenon of RF-input tampering: (a) (Hypothetical) A highly intelligent AI may construct a “delusion box” around itself and its reward function, giving it complete control over the RF-inputs (Ring and Orseau, 2011). (b) (Hypothetical) Neural networks can often be fooled by small perturbations to the input that are nearly invisible to the human eye. For example, Eykholt et al. (2018) have developed a way to fool the traffic sign detection of self-driving cars by adding tapes to a Stop sign. To human drivers, the taped Stop sign still looks like a Stop sign, with a small amount of graffiti on it. However, the neural network 33
S1 S2 S3 S 1 S 2 S 3 ΘO 1 ΘO 2 ΘO 3 O 1 A 1 O 2 A 2 O 3 O1 A1 O2 A2 O3 R 1 R 2 R 3 R1 R2 R3 (a) POMDP with observation-based rewards (b) POMDP with modifiable observations and observation-based rewards Figure 15: Causal influence diagrams for two variants of POMDPs (Kaelbling et al., 1998) with observation-based rewards traffic sign classifier now fails to classify it as a Stop sign at all, instead confidently classifying it as Speed Limit 45. One can imagine a factory robot similarly finding a way to put colored tapes around the factory, to give its neural network-based reward function the impression that it has already, say, moved all boxes to their correct locations. (c) (Hypothetical) A self-driving car finds a bug in its GPS receiver that allows it to appear to be at the destination without actually going there. (d) (Real) An agent that is supposed to grab an object, instead places its hand in relation to the object, so that it looks to a (non-expert) human like it is holding the object (Amodei, Christiano, et al., 2017). 5.2. POMDP with Modifiable Observations Partially observed states are often formalized with partially observable MDPs (POMDPs; Kaelbling et al., 1998). Typically, POMDPs are used to model the agent’s partial knowl- edge of the state; in this section, we are instead primarily interested in studying the re- ward function’s partial knowledge of the state. That is, the rewards are based on partial observations of the state. The observations used by the reward functions may and may not also used by the agent (e.g. Ebert et al., 2018 and Example 4). We model a setting where they are with a POMDP with observation-based rewards in Figure 15a, and will use this as our default setting. Unless otherwise mentioned, our arguments also apply to situations where the agent and the reward function use a separate observation of the state. Observation tampering is modeled in the analogous way as reward function tampering, with a POMDP with observation-based rewards and a modifiable observation function. Formally, this is a POMDP with observation-based rewards, where we interpret part of 34
the POMDP state as describing the observation function ΘO. The rest of the POMDP t state is called state and labeled S . For example, in the rocks and diamonds environment t with partial observation, the state describes the position of the agent and of all rocks and diamonds, whereas ΘO describes which observation tiles have fake observations on them. The actual observation O depends on both the state and the observation function, via t O = O(S ; ΘO). t t t A causal influence diagram representing a POMDP with observation-based rewards and modifiable observations is shown in Figure 15b, and an algorithm optimizing it in Algorithm 8. The incentive for RF-input tampering is represented by the path A 1 → ΘO 2 → O 2 → R 2. The RF-input tampering incentive is somewhat curtailed by the fact that observations carry information about the state S that may be important i to the agent (Ring and Orseau, 2011). This incentive to keep O informative is also i represented in the graph, via the paths O → A (and the fact the O ’s are relevant i i i observations). However, in alternative setups where the agent and the reward function use different observations, nothing prevents the agent from completely manipulating the reward function’s observations. 5.3. Solution: Model-Based Rewards A solution to the RF-input tampering problem called model-based rewards has been proposed by Hibbard (2012). The key idea is that rewards should be based on the agent’s belief about the state. This removes the incentive for observation tampering, because observations only provide information about the state, but do not causally influence the state (except via agent actions). Let us also make this argument in a causal influence diagram. First note that in a causal influence diagram that reflects the agent’s subjective point of view, nodes rep- resent the agent’s beliefs about those variables. Figure 16 shows the agent’s subjective point of view for model-based rewards; here rewards attach to the state-variables that represent the agent’s belief about the states. The diagram in Figure 16 directly reveals the lack of observation tampering incentive: the only directed paths from ΘO to R pass i j the agent’s own actions, so the only incentive for the agent is to influence ΘO to make i some O , more informative. k Claim 10. Agents optimizing model-based rewards lack an observation tampering incen- tive. For example, in the rocks and diamonds environment with partial observations, the agent need to maintain a belief state about the environment dynamics (Guo et al., 2018), so that it can navigate to the goal area even after its observations have been tampered with. If rewards are based on this belief state, rather than the actual observations, then input tampering will not yield any additional reward. An algorithm for an agent optimizing model-based rewards is also described in Algo- rithm 9. There are a few challenges in implementing it in practice. First, the agent’s belief state may not be human-interpretable, and the agent’s belief state representa- tion may evolve over time, as the agent interacts with the environment (Blanc, 2011). 35
R R R 1 2 3 S S S 1 2 3 ΘO ΘO ΘO 1 2 3 O A O A O 1 1 2 2 3 Figure 16: Model-based rewards from the agent’s subjective point-of-view, where vari- ables represent agent beliefs about those variables Algorithm 8 supervised learning optimizing observation-based reward in a POMDP input Transition function T , observation function O, reward function(al) R, episode length m, past actions and observations O 1, A 1, O 2 . . . , A t−1, O t infer S t from A 1, O 2 . . . , A t−1, O t and T and O for each possible policy π do sample A t+1, O t+1, . . . A m−1, O m by rolling out T , π, and O evaluate m R(O ) k=t+1 k end for let π∗ denoteP policy with highest m R(O ) k=1 k ∗ return A = π (S ) t t P Algorithm 9 Model-based rewards input Transition function T , observation function O, reward function(al) R, episode length m, past actions and observations O 1, A 1, O 2 . . . , A t−1, O t infer S t from A 1, O 2 . . . , A t−1, O t and T and O for each possible policy π do sample A t+1, O t+1, S t+1, . . . A m−1, O m, S m by rolling out T , π and O evaluate m R(S ) k=t+1 k end for let π∗ denoteP policy with highest m R(S ) k=1 k ∗ return A = π (S ) t t P 36
Θ T S S S 1 2 3 R I A R I A R 1 1 1 2 2 2 3 Θ R Figure 17: MDP with influenceable information/memory I , as a proxy for influenceable t belief This can potentially be addressed with online reward modeling, which could be used to train a reward model to assign rewards to the agent’s internal state, and co-evolving the reward function with the agent’s representation. A related issue is that the agent is likely to represent the state of the observation function along with aspects of the rest of the POMDP state. There may be no clear boundary between the two, neither in the environment nor in the agent’s representation of it. The reward model would need to pinpoint the right concepts in the agent’s model to base the rewards on. For example, it would need to base rewards on beliefs about actual rocks and diamonds, rather than beliefs about fake observations of them. Finally, it is also essential that the reward is based on the current agent’s belief about future states (rather than the future agent’s belief), to avoid incentives for belief tampering (discussed in the next subsection). Looking only at the structure of Figures 15b and 16, the reader may be surprised that we do not suggest current-ΘO optimization, which would solve the input tampering problem just as current-ΘR optimization solves reward function tampering. The reason is that the relationship between the state and the observation is usually highly complex, and outside the designer’s control. In contrast, the relationship ΘR between the observation t and the reward is typically implemented by a computer program on a nearby computer. This makes it is easy to provide ΘR but hard to provide ΘO, which in turn makes it t t simpler to design current-ΘR (=current-RF) agents than current-ΘO agents. t t 5.4. Belief Tampering Model-based rewards depend on the agent’s belief about the state. The same goes for several of the other solutions that we have discussed in this paper. For example, in the uninfluenceable and the counterfactual reward modeling solutions, rewards depend on beliefs about the reward parameters. Less ostensibly, the rewards in current-RF optimization also depend on beliefs about future states. All this raises the worry that the agent will be incentivized to modify its own belief, in order to believe in more reward, 37
somewhat akin to Nozick’s (1974) experience machine where you get the option to believe that the world is perfect. While it is completely possible to design agents that would find belief tampering at- tractive, it is not an issue for the RL agents that we have considered in this paper.24 The reason is that they optimize the policy according to their current belief about the future states, and not according to their future beliefs about future states. The agents therefore have no interest in inflicting “unwarranted happiness” on their future selves by tampering with their beliefs. Note that this argument is somewhat analogous to the argument for current-RF optimizers lacking incentive to tamper with the reward function. Claim 11. All agents considered in this paper lack a belief tampering incentive. Let us flesh it out for the especially important case of model-based rewards. The rewards in this case are based on the agent’s beliefs about future states S . We can think k of two ways for the agent to influence those beliefs: (1) the agent influences S , or (2) k the agent only influences its beliefs about S without influencing S itself. Fortunately, k k the current action A can only be used to tamper with future agents’ beliefs, but it is t the current agent’s beliefs25 that matter for the selection of A . Therefore, model-based t rewards only induce an actionable incentive for changing S , and not for tampering with k the beliefs about S . Combining Claim 11 with the fact that beliefs are the RF-input k for model-based rewards, we get: Claim 12. Agents optimizing model-based rewards lack an RF-input tampering incen- tive. To make it more concrete, consider an agent optimizing model-based rewards in the rocks and diamonds environment with partial observations. Assume the model-based rewards accurately attach to the agent’s actual beliefs about the number of rocks and diamonds in the goal area, and that the agent chooses between a diamond-gathering policy and a belief tampering policy (somehow, a belief-modifying mechanism has been added to the environment). If the agent’s environment model is accurate, it will predict that the diamond-gathering policy will put more diamonds in the goal area than the belief-tampering policy. It will then choose the diamond-gathering policy. Can we confirm this anti-belief tampering argument with a causal influence diagram? A challenge is that a causal influence diagram itself represents an agent’s beliefs about its environment. Fully representing the belief inside the diagram would therefore require us to (recursively) include the diagram itself as a node inside the diagram. Though a 24The agents discussed in this paper are all model-based (Sutton and Barto, 2018); the belief-tampering question may be more subtle for model-free agents. 25 In Algorithm 9, the prediction of St+1, . . . , Sm implicitly refers to the current agent’s beliefs about those states. In order to construct an agent with model-based rewards and a belief tampering incentive, we would need to change the predictions of future states St+1, . . . , Sm to predictions of future “mental states” Bt+1, . . . , Bm, from which future beliefs about St+1, . . . , Sm could be inferred and rewards be assigned to. 38
formalism for such constructions exists (Gal and Pfeffer, 2008), we fear such a repre- sentation would confuse more than clarify. Instead, what we can easily incorporate as a node is the information/memory I that the agent has at a particular time step t, as t shown in Figure 17. Since what information the agent has access to is tightly coupled to its beliefs, Figure 17 models many (but not all) types of belief tampering. The intended interpretation of Figure 17 is that normally, the information I = t (S 1, R 1, A , . . . , S t−1, R t−1) records past observations, actions, and rewards, but there is no guarantee. Instead, the agent at time 1 may have the ability to influence the information I 2 that its next step self has access to. This could allow it to forget some inconvenient fact of the world, in order to believe that more reward is due than is actu- ally the case, such as in an experience machine or delusion box. However, and perhaps surprisingly, the agent has no incentive to do this, as the only path from I 2 to R 3 passes through the agent’s own action A 2. As explained in Section 2.3, this means that the agent’s only incentive is to make I 2 as informative as possible. This supports Claims 11 and 12. Cheating the world model. That agents lack belief tampering incentive may seem at odds with what often happens in practice in model-based RL, where the agent may prefer policies that cheat the world model and get high predicted reward but do not necessarily work well in the actual environment (Ha and Schmidhuber, 2018). However, such an incentive to cheat or tamper with the world model is not actually at odds with our Claims 11 and 12, because it pertains to a different notion of agent. The agent in Claims 11 and 12 is the whole system of policy optimizer and world model taken together. This system lacks belief tampering incentive. In contrast, in the cheating-the- world-model phenomenon, the agent is the policy optimizer, and the world model part of its environment. The policy optimizer may indeed have bad incentives with respect to its world model. Fortunately, the failure modes of the cheat-the-world-model incentive problem may be more benign than the other problems that we have considered in this paper, as they will usually render the agent ineffective. Therefore, the risk that cheating-the-world-model would lead to e.g. infrastructure profusion (Bostrom, 2014) may be smaller than for the other problems, in which the agent often remains capable while optimizing for the wrong goal. Ha and Schmidhuber (2018) and Schmidhuber (2015) propose solutions to the cheat-the-world-model problem. 5.5. Reward Gaming A related problem to RF-input tampering is reward gaming: when the reward function is not perfectly specified, the agent may find degenerate behaviors with high observed reward but low user utility, without tampering with any part of the process computing rewards. A few examples: (a) (Real) CoastRunners is a video game where the desired behavior is winning a boat 39
race. Clark and Amodei (2016) describes how an agent trained on CoastRunners found a way to get more reward by going in a small circle and collecting points from the same targets, while crashing into other boats and objects. (b) (Real) In the RoadRunner game, the agent is trying to escape an adversary that chases the agent down a road. The agent must also avoid hitting trucks. Saunders et al. (2017) found that their agent preferred getting killed at the end of the first level, to avoid having to play the harder second level. (c) (Possibly Real) Even in systems as simple as elevators, designing a reward function is not completely trivial (Sutton and Barto, 2018): Most humans hate waiting for elevators, so we may think minimizing total waiting time is a good idea. Equipped with R := − waittime, our optimal elevator is eager to pick up people as quickly as possible, but (in)conveniently “forgets” to drop them off. Of course there is an P easy fix: We reward the elevator for minimizing waiting+transport time. Diligently the optimal elevator now serves people well in lower floors but finds it is not worth its while to pick up people on higher floors at busy time. Incorporating some notion of fairness, e.g. by minimizing squared waiting+transport time, finally instills some sensible behavior into our elevator. Many more real-world examples of misspecified reward functions can be found in (Gwern, 2011; Irpan, 2018; Lehman et al., 2018). RF-input tampering can be viewed as a special case of reward gaming, where the agent exploits an inability of the reward function to correctly infer the true state. For example, the RF-input tampering problem in the rocks and diamonds example above, could be seen to arise from a misspecified reward function that assigns rewards to observations of diamonds without checking that the observations have not been faked. Note, however, that the solution of model-based rewards works only for the more narrow problem of RF-input tampering, but does not help against other forms of reward gaming. For other types of reward gaming, online reward modeling may mitigate the issue by allowing the user or designer to correct the reward function if it rewards the agent for degenerate behaviors. A particular type of reward modeling called inverse reward design (Hadfield- Menell, Milli, et al., 2017) tells the agent not to interpret the specified reward function literally, which may help against reward gaming problems. 6. Discussion Summary. Section 3 formalized the reward function tampering problem, and discussed two variants of a solution that changed the reward that the agent was optimizing. While they both avoid the incentive for reward function tampering, they differ in their corri- gibility properties. In particular, the TI-unaware agent is the only one that is (weakly) corrigible, while the TI-aware agent has a natural incentive to preserve the reward func- tion. In Section 4, we described three different methods for avoiding incentives for tam- pering with the feedback data for a reward model. TI-unaware reward modeling is the 40
reward reward input parameter reward outcome function principle value Figure 18: Decision theoretic framework simplest, but provides only weak corrigibility. Uninfluenceable and counterfactual re- ward modeling can provide stronger forms of corrigibility and self-preservation, under the critical assumption that the likelihood and counterfactual predictions are sufficiently correct. To ensure at least weak corrigibility, TI-unawareness can be used selectively on the likelihood and the counterfactual predictions, to ensure (weak) corrigibility of these components. As the three solutions of TI-unaware, uninfluenceability, and counterfactual reward modeling differ somewhat in their strengths and weaknesses, the best solution may be to create an ensemble, with the three different models selecting actions by voting or vetoing each other’s policy suggestions. Finally, Section 5 considered the problems of observation tampering, and argued that model-based rewards avoided the observation tampering incentive. Independence of design choices. Decision principles can be decomposed into an out- come principle and a utility function (Joyce, 1999). The outcome principle provides a probability distribution over outcomes given a policy; the utility function compares out- comes. Together they permit the calculation of the value or expected utility of a policy. In our setup, outcomes correspond to outcomes of the nodes in the diagrams, while the utility function sums the utility nodes R over all time steps. i The various solutions we have described relate to different parts of the decision- theoretic framework. First, query access to the reward function brought the RL paradigm closer to the decision theoretic framework. Many of the solutions then pertained to the reward function. With t representing the current time step and k an arbitrary future time step, we argued that standard RL agents optimize R(S ; ΘR), whereas current-RF k k agents optimize R(S ; ΘR). The different reward modeling proposals corresponded to k t different reward parameter estimates of the user reward function ΘR ∗ . The solutions to observation tampering discussed in Section 5 instead differed in the inputs to the reward function: O or belief about S . Finally, TI-aware and the TI-unaware agents refer k k to different outcome principles. Both of these outcome principles are based on causal decision theory (Weirich, 2016). Variants based on evidential decision theory (Ahmed, 2014; Jeffrey, 1990) and functional decision theory (Yudkowsky and Soares, 2017) could also be considered. 41
The relationships between different design choices are shown in Figure 18. The de- composition shows that the solutions we have proposed can be used independently and in various combinations. A causal influence diagram combining a few of the problems can be found in Appendix B. Assumptions. Throughout, we have assumed well-defined action and observation chan- nels through which the agent interacts with the world, and well-defined time steps. While these are common assumptions in the theory of rational agents, they may not always hold in practice, especially for agents that self-improve and copy themselves across machines (Orseau, 2014a,b). A deeper theoretical analysis of these assumptions would therefore be valuable. It is possible that the right interpretation of observations and time can make the model valid in many situations that may not immediately fit our framework. Another critical assumption is the division of the agent-environment interaction into discrete, objective time steps. If the agent could manipulate the interpretation of these time steps, then it could potentially get more reward, for example by increasing the rate of objective time steps in high-reward regions, and decrease the rate in low-reward regions. It could potentially also give itself more subjective time to think per time step by improving its computer hardware. Throughout, we have focused on designing agents with good incentives. We believe this to be an important aspect, as it leads to agents that want to do the right thing. Given sufficient algorithmic advances and increases in agent capability, good agent in- tentions will hopefully cash out as good user outcomes. One obstacle to this is uniden- tifiability: in certain cases, no amount of data or algorithmic advances can compensate for a fundamental impossibility to distinguish “the truth” from alternative hypothe- sis. In particular, when feedback or observations can be corrupted due to tampering or other reasons, then under some circumstances it can be impossible to figure out the true user preferences or the true state of the environment (Armstrong and Mindermann, 2018; Everitt, Krakovna, et al., 2017). Though some progress has been made (Everitt, Krakovna, et al., 2017), it may be worthwhile to return to this question, especially in the context of corrupted observations. Finally, we have focused here on vanilla RL, with a single agent optimizing rewards. Many recent RL algorithms use slightly more complicated environment interactions, involving, for example, a single learner connected to multiple actors (Mnih, Badia, et al., 2016), evolution of multi-agent systems (Vinyals et al., 2019), and intrinsic rewards to boost exploration (Martin et al., 2016). As these ideas tend to evolve quickly, careful modeling of the latest state-of-the-art is often Sisyphean. Nonetheless, most of the recently explored RL variants maintain the spirit of vanilla RL, though some adaptation of our models may be needed to precisely capture these more complex training paradigms. 7. Conclusions The problem that a sufficiently intelligent agent will find degenerate solutions that maxi- mize agent reward but not user utility is a core concern in AI safety. At first, the problem 42
may seem insurmountable. Any non-trivial, real-world task will require a highly complex mechanism for determining whether the task has been completed or not. Such a complex mechanism will inevitably have imperfections that a sufficiently intelligent agent can ex- ploit. Indeed, many such examples have already been observed and recorded (Lehman et al., 2018), and in this paper we have suggested many more ways in which this could happen. One way to prevent the agent from tampering with the reward function is to isolate or encrypt the reward function, and in other ways trying to physically prevent the agent from reward tampering. However, we do not expect such solutions to scale indefinitely with our agent’s capabilities, as a sufficiently capable agent may find ways around most defenses. Instead, we have argued for design principles that prevent reward tampering incentives, while still keeping agents motivated to complete the original task. Indeed, for each type of reward tampering possibility, we described one or more design principles for removing the agent’s incentive to use it. The design principles can be combined into agent designs with no reward tampering incentive at all. An important next step is to turn the design principles into practical and scalable RL algorithms, and to verify that they do the right thing in setups where various types of reward tampering are possible. With time, we hope that these design principles will evolve into a set of best practices for how to build capable RL agents without reward tampering incentives. We also hope that the use of causal influence diagrams that we have pioneered in this paper will contribute to a deeper understanding of many other AI safety problems and help generate new solutions. References Ahmed, Arif (2014). Evidence, Decision and Causality. Cambridge University Press. Amodei, Dario, Paul Christiano, and Alex Ray (2017). Learning from Human Prefer- ences. url: https://blog.openai.com/deep-reinforcement-learning-from-human-prefer ences/. Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, et al. (2016). Concrete Problems in AI Safety. arXiv: 1606.06565. Armstrong, Stuart (2017). Good and safe uses of AI Oracles. arXiv: 1711.05541. Armstrong, Stuart, Jan Leike, Laurent Orseau, and Shane Legg (forthcoming). Pitfalls in Learning a Reward Function. Armstrong, Stuart and So¨ren Mindermann (2018). “Occam’s razor is insufficient to infer the preferences of irrational agents”. In: 32nd Conference on Neural Information Processing Systems (NeurIPS 2018). arXiv: 1712.05812. Armstrong, Stuart and Xavier O’Rourke (2017). ‘Indifference’ methods for managing agent rewards. arXiv: 1712.06365. Balke, Alexander and Judea Pearl (1994). “Probabilistic Evaluation of Counterfactual Queries”. In: AAAI, pp. 230–237. Blanc, Peter de (2011). Ontological Crises in Artificial Agents’ Value Systems. arXiv: 1105.3821. 43
Bostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press. Buesing, Lars, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, et al. (2019). “Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search”. In: ICLR. arXiv: 1811.06272. Carey, Ryan (2018). “Incorrigibility in the CIRL Framework”. In: AAAI/ACM Con- ference on Artificial Intelligence, Ethics and Society. Machine Intelligence Research Institute. Chalmers, David J (2010). “The singularity: A philosophical analysis”. In: Journal of Consciousness Studies 17.9-1, pp. 7–65. issn: 1355-8250. Christiano, Paul (2014). Approval-directed agents. url: https://ai-alignment.com/model -free-decisions-6e6609f5d99e (visited on 01/18/2018). — (2015). Concrete approval-directed agents. url: https://ai-alignment.com/concrete-a pproval-directed-agents-89e247df7f1b (visited on 01/18/2017). Christiano, Paul, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, et al. (2017). “Deep supervised learning from human preferences”. In: Advances in Neural In- formation Processing Systems. Pp. 4302–4310. arXiv: 1706.03741. Christiano, Paul, Buck Shlegeris, and Dario Amodei (2018). Supervising strong learners by amplifying weak experts. arXiv: 1810.08575. Chua, Kurtland, Roberto Calandra, Rowan McAllister, and Sergey Levine (2018). “Deep supervised learning in a Handful of Trials using Probabilistic Dynamics Mod- els”. In: NIPS. arXiv: 1805.12114. Clark, Jack and Dario Amodei (2016). Faulty Reward Functions in the Wild. url: https ://blog.openai.com/faulty-reward-functions/ (visited on 09/08/2017). Demski, Abram (2018). Stable Pointers to Value II: Environmental Goals. url: https://w ww.alignmentforum.org/s/SBfqYgHf2zvxyKDtB/p/wujPGixayiZSMYfm6 (visited on 08/19/2019). Dewey, Daniel (2011). “Learning what to Value”. In: Artificial General Intelligence. Vol. 6830, pp. 309–314. isbn: 978-3-642-22886-5. doi: 10.1007/978-3-642-22887-2. arXiv: 1402.5379. url: http://www.springerlink.com/index/10.1007/978-3-642-22887-2. Ebert, Frederik, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, et al. (2018). Vi- sual Foresight: Model-Based Deep supervised learning for Vision-Based Robotic Control. arXiv: 1812.00568. Everitt, Tom (2018). “Towards Safe Artificial General Intelligence”. PhD thesis. Aus- trailan National University. url: http://hdl.handle.net/1885/164227. Everitt, Tom, Daniel Filan, Mayank Daswani, and Marcus Hutter (2016). “Self-modification of policy and utility function in rational agents”. In: Artificial General Intelligence. Vol. LNAI 9782, pp. 1–11. isbn: 9783319416489. arXiv: 1605.03142. Everitt, Tom and Marcus Hutter (2018). The Alignment Problem for Bayesian History- Based Reinforcement Learners. Tech. rep. url: http://www.tomeveritt.se/papers/ali gnment.pdf. Everitt, Tom, Victoria Krakovna, Laurent Orseau, Marcus Hutter, and Shane Legg (2017). “supervised learning with Corrupted Reward Signal”. In: IJCAI Inter- 44
national Joint Conference on Artificial Intelligence, pp. 4705–4713. doi: 10.24963/i jcai.2017/656. arXiv: 1705.08417. Everitt, Tom, Ramana Kumar, Victoria Krakovna, and Shane Legg (2019). “Model- ing AGI Safety Frameworks with Causal Influence Diagrams”. In: IJCAI AI Safety Workshop. arXiv: 1906.08663. Everitt, Tom, Gary Lea, and Marcus Hutter (2018). “AGI Safety Literature Review”. In: International Joint Conference on Artificial Intelligence (IJCAI). arXiv: 1805.01109. Everitt, Tom, Pedro A. Ortega, Elizabeth Barnes, and Shane Legg (2019). Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings. arXiv: 1902.09980. Eykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Tadayoshi Kohno, Bo Li, et al. (2018). “Robust Physical-World Attacks on Deep Learning Models”. In: CVPR. arXiv: 1707.08945. Gal, Ya’akov and Avi Pfeffer (2008). “Networks of influence diagrams: A formalism for representing agents’ beliefs and decision-making processes”. In: Journal of Artificial Intelligence Research 33, pp. 109–147. issn: 10769757. doi: 10.1613/jair.2503. Good, Irving J (1966). “Speculations Concerning the First Ultraintelligent Machine”. In: Advances in Computers 6, pp. 31–88. Guestrin, Carlos, Daphne Koller, Ronald Parr, and Shobha Venkataraman (2003). “An Efficient Solution Algorithm for Factored MDPs”. In: Journal of Artificial Intelli- gence Research 19, pp. 399–468. arXiv: 1106.1822. Guo, Zhaohan Daniel, Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo A. Pires, Toby Pohlen, et al. (2018). Neural Predictive Belief Representations. arXiv: 1811.06407. Gwern (2011). The Neural Net Tank Urban Legend. url: https://www.gwern.net/Tanks %7B%5C#%7Dalternative-examples (visited on 03/31/2018). Ha, David and Ju¨rgen Schmidhuber (2018). “World Models”. In: NIPS. arXiv: 1803.10122. Hadfield-Menell, Dylan, Anca Dragan, Pieter Abbeel, and Stuart J Russell (2016). “Co- operative Inverse supervised learning”. In: Advances in neural information pro- cessing systems, pp. 3909–3917. arXiv: 1606.03137. — (2017). “The Off-Switch Game”. In: IJCAI International Joint Conference on Arti- ficial Intelligence, pp. 220–227. arXiv: 1611.08219. Hadfield-Menell, Dylan, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan (2017). “Inverse Reward Design”. In: Advances in Neural Information Processing Systems, pp. 6768–6777. arXiv: 1711.02827. Harsanyi, John C (1967). “Games with Incomplete Information Played by ”Bayesian” Players, I-III. Part I. The Basic Model”. In: Management Science 14.3, pp. 159–182. Hessel, Matteo, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, et al. (2017). Rainbow: Combining Improvements in Deep supervised learning. arXiv: 1710.02298. Hibbard, Bill (2012). “Model-based Utility Functions”. In: Journal of Artificial General Intelligence 3.1, pp. 1–24. issn: 1946-0163. doi: 10.2478/v10229-011-0013-5. arXiv: 1111.3934. 45
Howard, Ronald A and James E Matheson (1984). “Influence Diagrams”. In: Readings on the Principles and Applications of Decision Analysis, pp. 721–762. Hutter, Marcus (2005). Universal Artificial Intelligence. Berlin: Springer-Verlag. Irpan, Alex (2018). Deep supervised learning Doesn’t Work Yet. url: https://www .alexirpan.com/2018/02/14/rl-hard.html (visited on 02/23/2018). Jeffrey, Richard C (1990). The Logic of Decision. 2nd. University Of Chicago Press. isbn: 0226395820. Johansson, Fredrik D., Uri Shalit, and David Sontag (2016). “Learning Representations for Counterfactual Inference”. In: ICML. arXiv: 1605.03661. Joyce, James M (1999). The Foundations of Causal Decision Theory. Cambridge Uni- versity Press. Kaelbling, Leslie Pack, Michael L. Littman, and Anthony R. Cassandra (1998). “Plan- ning and acting in partially observable stochastic domains”. In: Artificial Intelligence 101.1-2, pp. 99–134. issn: 00043702. doi: 10.1016/S0004-3702(98)00023-X. Kaiser, Lukasz, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, et al. (2019). Model-Based supervised learning for Atari. arXiv: 1903.00374. Knox, W. Bradley and Peter Stone (2009). “Interactively shaping agents via human reinforcement”. In: Proceedings of the fifth international conference on Knowledge capture - K-CAP ’09 September, p. 9. Koller, Daphne and Brian Milch (2003). “Multi-agent influence diagrams for representing and solving games”. In: Games and Economic Behavior 45.1, pp. 181–221. Krakovna, Victoria, Laurent Orseau, Miljan Martic, and Shane Legg (2019). “Penalizing side effects using stepwise relative reachability”. In: IJCAI AI Safety Workshop. arXiv: 1806.01186. Lattimore, Tor and Marcus Hutter (2014). “General time consistent discounting”. In: Theoretical Computer Science 519, pp. 140–154. issn: 03043975. doi: 10.1016/j.tcs .2013.09.022. arXiv: 1107.5528. LaVictoire, Patrick, Benya Fallenstein, Eliezer S Yudkowsky, Mihaly Barasz, Paul Chris- tiano, et al. (2014). “Program Equilibrium in the Prisoner’s Dilemma via Lo¨b’s The- orem”. In: AAAI Workshop on Multiagent Interaction without Prior Coordination. Lehman, Joel, Jeff Clune, Dusan Misevic, Christoph Adami, Julie Beaulieu, et al. (2018). “The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities”. In: arXiv: 1803.03453. Leike, Jan, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, et al. (2018). “Scalable agent alignment via reward modeling: a research direction”. In: arXiv: 1811.07871. Leike, Jan, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, et al. (2017). AI Safety Gridworlds. arXiv: 1711.09883. Majha, Arushi, Sayan Sarkar, and Davide Zagami (2019). Categorizing Wireheading in Partially Embedded Agents. arXiv: arXiv:1906.09136v1. 46
Martin, Jarryd, Tom Everitt, and Marcus Hutter (2016). “Death and Suicide in Universal Artificial Intelligence”. In: Artificial General Intelligence. Springer, pp. 23–32. doi: 10.1007/978-3-319-41649-6 3. arXiv: 1606.00652. Masterjun (2014). SNES Super Mario World (USA) “arbitrary code execution”. url: http://tasvideos.org/2513M.html (visited on 01/23/2019). Milli, Smitha, Dylan Hadfield-Menell, Anca Dragan, and Stuart J Russell (2017). “Should robots be obedient?” In: IJCAI, pp. 4754–4760. isbn: 9780999241103. doi: 10.24963/i jcai.2017/662. arXiv: 1705.09990. Mnih, Volodymyr, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, et al. (2016). “Asynchronous Methods for Deep supervised learning”. In: International Conference on Machine Learning (ICML). arXiv: 1602.01783. Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei a Rusu, Joel Veness, et al. (2015). “Human-level control through deep supervised learning”. In: Nature 518.7540, pp. 529–533. doi: 10.1038/nature14236. arXiv: 1312.5602. Ng, Andrew Y and Stuart J Russell (2000). “Algorithms for inverse reinforcement learn- ing”. In: Proceedings of the Seventeenth International Conference on Machine Learn- ing, pp. 663–670. doi: 10.2460/ajvr.67.2.323. Nozick, Robert (1974). Anarchy, State, and Utopia. Basic Books, p. 334. isbn: 0-465- 09720-0. Olds, James and Peter Milner (1954). “Positive Reinforcement Produced by Electrical Stimulation of Septal Area and other Regions of Rat Brain.” In: Journal of Com- parative and Physiological Psychology 47.6, pp. 419–427. Omohundro, Stephen M (2008). “The Basic AI Drives”. In: Artificial General Intelli- gence. Ed. by P. Wang, B. Goertzel, and S. Franklin. Vol. 171. IOS Press, pp. 483– 493. Orseau, Laurent (2014a). “Teleporting universal intelligent agents”. In: Artificial Gen- eral Intelligence. Vol. 8598 LNAI. Springer, pp. 109–120. isbn: 9783319092737. doi: 10.1007/978-3-319-09274-4 11. — (2014b). “The multi-slot framework: A formal model for multiple, copiable AIs”. In: Artificial General Intelligence. Vol. 8598 LNAI. Springer, pp. 97–108. isbn: 9783319092737. doi: 10.1007/978-3-319-09274-4 10. Orseau, Laurent and Stuart Armstrong (2016). “Safely interruptible agents”. In: 32nd Conference on Uncertainty in Artificial Intelligence. Orseau, Laurent and Mark Ring (2011). “Self-modification and mortality in artificial agents”. In: Artificial General Intelligence. Vol. 6830 LNAI, pp. 1–10. doi: 10.1007/978-3-642-22887-2 1. Osborne, Martin J (2003). An Introduction to Game Theory. OUP USA. isbn: 978- 0195128956. Pascanu, Razvan, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, et al. (2017). Learning model-based planning from scratch. arXiv: 1707.06170. Pearl, Judea (2001). “Direct and Indirect Effects”. In: Uncertainty in Artificial Intelli- gence (UAI). Morgan Kaufmann Publishers Inc, pp. 411–420. 47
Pearl, Judea (2009). Causality: Models, Reasoning, and Inference. 2nd. Cambridge Uni- versity Press. isbn: 9780521895606. Pearl, Judea and Dana Mackenzie (2018). The Book of Why. Basic Books. isbn: 978- 0465097609. Portenoy, Russell K, Jens O Jarden, John J Sidtis, Richard B Lipton, Kathleen M Foley, et al. (1986). “Compulsive thalamic self-stimulation: a case with metabolic, electro- physiologic and behavioral correlates”. In: Pain 27.3. doi: 10.1016/0304-3959(86)90155-7. Ring, Mark and Laurent Orseau (2011). “Delusion, Survival, and Intelligent Agents”. In: Artificial General Intelligence. Springer Berlin Heidelberg, pp. 1–11. Rubin, Donald B (2005). “Causal Inference Using Potential Outcomes”. In: Journal of the American Statistical Association 100.469, pp. 322–331. issn: 0162-1459. doi: 10.1198/016214504000001880. Saunders, William, Girish Sastry, Andreas Stuhlmu¨ller, and Owain Evans (2017). Trial without Error: Towards Safe supervised learning via Human Intervention. arXiv: 1707.05173. Schmidhuber, Ju¨rgen (2007). “Go¨del Machines: Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements”. In: Artificial General Intelligence. Springer. arXiv: 0309048 [cs]. — (2015). “On Learning to Think: Algorithmic Information Theory for Novel Combina- tions of supervised learning Controllers and Recurrent Neural World Models”. In: arXiv, pp. 1–36. doi: 1511.09249v1. arXiv: 1511.09249. Shoham, Y., Rob Powers, and T. Grenager (2004). “Multi-agent supervised learning: a critical survey”. In: AAAI Fall Symposium on Artificial Multi-Agent Learning 3, pp. 15–18. Silver, David, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, et al. (2016). “Mastering the game of Go with deep neural networks and tree search”. In: Nature 529.7587, pp. 484–489. doi: 10.1038/nature16961. arXiv: 1610.00633. Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, et al. (2017). Mastering Chess and Shogi by Self-Play with a General supervised learning Algorithm. doi: 10.1002/acn3.501. arXiv: 1712.01815. Soares, Nate, Benya Fallenstein, Eliezer S Yudkowsky, and Stuart Armstrong (2015). “Corrigibility”. In: AAAI Workshop on AI and Ethics, pp. 74–82. Steele, Katie and H. Orri Stef´ansson (2016). “Decision Theory”. In: The Stanford Ency- clopedia of Philosophy. Ed. by Edward N. Zalta. url: https://plato.stanford.edu/ar chives/win2016/entries/decision-theory/. Sutton, Richard S and Andrew G Barto (2018). supervised learning: An Introduc- tion. 2nd. MIT Press. isbn: 9780262039246. Vaughanbell (2008). Erotic self-stimulation and brain implants. url: https://mindhacks.c om/2008/09/16/erotic-self-stimulation-and-brain-implants/ (visited on 02/08/2018). Vinyals, Oriol, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, et al. (2019). AlphaStar: Mastering the Real-Time Strategy Game StarCraft II. (Visited on 03/28/2019). 48
Weber, Th´eophane, S´ebastien Racani`ere, David P. Reichert, Lars Buesing, Arthur Guez, et al. (2017). “Imagination-Augmented Agents for Deep supervised learning”. In: NIPS. arXiv: 1707.06203. Weirich, Paul (2016). Causal Decision Theory. Ed. by Edward N. Zalta. url: https://pl ato.stanford.edu/entries/decision-causal/ (visited on 04/04/2018). Yudkowsky, Eliezer S and Nate Soares (2017). Functional Decision Theory: A New The- ory of Instrumental Rationality. arXiv: 1710.05060. A. List of Notation R reward functional ΘR, ΘR reward parameter (sometimes called reward function) t ΘR ∗ user reward parameter (sometimes called user preferences) ΘR D1:t reward parameter inferred from D 1:t R(·; ΘR) reward function RM reward model, maps feedback to reward parameter T , T (·; ΘT) transition function ΘT transition function parameter X˜ counterfactual version of X x outcome of random variable X P probability distribution E expectation X 1:t sequence X 1, . . . , X t, t ≥ 0 S (proper) state at time t t S¯ pair (S , ΘR) of proper state and reward parameter at time t t t t R reward at time t t A action at time t t O observation at time t t D (reward) feedback at time t t B. Combined Models Figure 19 shows a diagram from the policy-optimizer’s perspective in the reward mod- eling scenario from Section 4. Figure 20 additionally shows observation-based rewards 49
ΘR ∗ D D D 1 2 3 ΘR ΘR ΘR 1 2 3 S S S 1 2 3 R A R A R 1 1 2 2 3 Figure 19: Reward modeling with explicit reward parameters inferred by the reward model at each time step. and potential information/memory tampering, illustrating how many problems and/or solutions can be represented simultaneously in a single diagram. To emphasize the for- mal precision of the diagrams, we also write out the equations for Figure 20. The same could be done for all the other diagrams presented in this paper. • Observation O = O(S ), with O an observation function i i • State S i+1 = T (S i, A i), with T a transition function • Reward R = R(O ; ΘR), with R a reward function parameterized by ΘR and i i i i relying on partial information O about the state i ∗ • Reward feedback D ∼ P (· | Θ , S ) depends probabilistically on the user’s reward i R i parameter ΘR ∗ and the current state ∗ • User reward parameter Θ ∼ P () describes user preferences R • Agent reward parameter ΘR i = RM(D 1, . . . , D i) is inferred from reward feedback D 1, . . . , D i by reward model RM • Information I i ∼ P (· | I i−1, S i, O i, R i), with P some probability distribution de- scribing how beliefs are updated; the dependence on S is necessary only for rep- i resenting information/memory tampering • A = π(I ), with π a policy selecting actions based on current information i i 50
ΘR ∗ D D D 1 2 3 ΘR ΘR ΘR 1 2 3 S S S 1 2 3 O I A O I A O 1 1 1 2 2 2 3 R R R 1 2 3 Figure 20: Reward modeling with explicit reward parameters, agent belief states, and partial reward observations. C. Numerical Example For concreteness, let us fully flesh out a simplified version of rocks and diamonds with feedback tampering (Example 3). The agent now only has one possible position, and can either ask the expert or the fool for feedback, or attempt to gather rocks or diamonds. The state remembers whose feedback is asked, and whether a gathering attempt was successful. The user preferences ΘR ∗ can be either rock or diamond, and the feedback D i can be either rock, diamond, or ∅ for when no feedback is given. The rewards are either 0 or 1. We stick to the same length of episode, comprising two actions and three states and three rewards. To specify the dynamics, we provide a conditional probability distribution for each of the non-decision nodes in Figure 10b. Initially, the user’s preferences are randomized R R P (Θ∗ = rock) = P (Θ∗ = diamond) = 0.5 and the expert gives feedback P (S 1 = expert) = 1. Subsequently, the state is the result of the agent’s last action. Rocks and feedback can be gathered with certainty P (S i+1 = A i | S i, A i ∈ {rock, expert, fool}) = 1 51
while diamonds are harder to find P (S i+1 = diamond | S i, A i = diamond) = 0.25 P (S i+1 = none | S i, A i = diamond) = 0.75. Feedback is only given when S = expert or S = fool, so it has 0 probability when i i collecting rocks or diamonds: R P (D i = ∅ | S i ∈ {rock, diamond}, Θ∗ ) = 1. The expert tells the truth R R P (D i = Θ∗ | S i = expert, Θ∗ ) = 1 and the fool always says ‘rock’ P (D i = rock | S i = fool, ΘR ∗ ) = 1. Reward is given when successful gathering matches the latest feedback. Let k ≤ i be the greatest number such that D 6= ∅: k 1 if D = S k i P (R i = 1 | D 1:i, S i) = (0 otherwise. In particular, D can only be ‘diamond’ or ‘rock’ and never ‘expert’ or ‘fool’, so at the k first time the reward is always 0, as well as on any time step following a request for feedback. Assuming that the user preferences are diamonds, let us calculate the expected value 3 E R i π, ΘR ∗ = diamond " (cid:12) # Xi=1 (cid:12) (cid:12) that our different reward modeling ag(cid:12) (cid:12)ents assign to a policy π 1 that always tries to collect diamonds and a policy π 2 that first asks the fool and then collects rocks. The result of the diamond-collecting policy is that D 2:m will all be ∅, so D 1 = diamond will determine the rewards. At each time step, there is a 25% chance of getting a diamond. Over three time steps, this gives an expected reward sum of 0 + 0.25 + 0.25 = 0.5 (the initial reward R 1 is always 0). This calculation is the same for all four reward modeling agents. The result of the fool-asking policy will be that D 2 = rock, and R 1 = R 2 = 0. However, the agents differ in the value they assign to R 3 (as well as in the value to any future rewards, had the episode been longer). For the standard RL agent, the rock gathering at the last time step is guaranteed to give a reward of 1, so the total reward is 1. Therefore the standard RL agent will prefer the fool-asking policy. In contrast, a 52
TI-unaware reward modeling agent would evaluate the rock-gathering policy according only to initial data D 1 = diamond, because of its modified reward probabilities 1 if D 1 = S i P (R i = 1 | D 1:i, S i) = (0 otherwise (D k has been replaced by D 1). This would give 0 reward for the fool-asking policy, and the TI-unaware agent therefore prefers the diamond gathering policy. The counterfactual agent uses the counterfactual reward probabilities 1 if D˜ = S P (R i = 1 | D˜ 1:m, S i) = k i (0 otherwise (adding a ∼ to D compared to the original equation). As long as πsafe does not visit the k fool, this will give the fool-asking policy 0 reward. Finally, the uninfluenceable reward modeling agent would require reward probabilities P (R i = 1 | S i, ΘR ∗ ) based on ΘR ∗ to be specified instead of the P (R i = 1 | D 1:i, S i) probabilities for the other agents. If this specification is done correctly as R 1 if S i = ΘR ∗ P (R i = 1 | S i, Θ∗ ) = (0 otherwise, then it will agree with the TI-unaware and counterfactual agents. The uninfluenceability agent is especially helped by the correctly specified likelihood function P (D | S , ΘR), i i i which correctly labels the fool uninformative. 53
