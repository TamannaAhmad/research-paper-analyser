Meta-Learning with Warped Gradient Descent Sebastian Flennerhag Andrei A. Rusu Razvan Pascanu The Alan Turing Institute DeepMind DeepMind sflennerhag@turing.ac.uk andreirusu@google.com razp@google.com Hujun Yin Raia Hadsell University of Manchester DeepMind hujun.yin@manchester.ac.uk raia@google.com Abstract A versatile and effective approach to meta-learning is to infer a gradient-based up- date rule directly from data that promotes rapid learning of new tasks from the same distribution. Current methods rely on backpropagating through the learning process, limiting their scope to few-shot learning. In this work, we introduce Warped Gradi- ent Descent (WarpGrad), a family of modular optimisers that can scale to arbitrary adaptation processes. WarpGrad methods meta-learn to warp task loss surfaces across the joint task-parameter distribution to facilitate gradient descent, which is achieved by a reparametrisation of neural networks that interleaves warp-layers in the architecture. These layers are shared across task learners and fixed during adaptation; they represent a projection of task parameters into a meta-learned space that is conducive to task adaptation and standard meta-optimization synthesis induces a form of gradient preconditioning. WarpGrad methods are computationally efficient and easy to implement as they rely on parameter sharing and meta-optimization synthesis. They are readily combined with other meta-learners and can scale both in terms of model size and length of adaptation trajectories as meta-learning warp parameters do not require differentiation through task adaptation processes. We show empirically that WarpGrad optimisers meta-learn a warped space where gradient descent is well behaved, with faster convergence and better performance in a variety of settings, including few-shot, standard supervised, continual, and supervised learning. 1 Introduction The current paradigm in optimisation-based meta-learning maintains that a meta-learner should be model-agnostic and learn either an explicit optimiser (Ravi & Larochelle, 2016; Andrychowicz et al., 2016; Li & Malik, 2016; Chen et al., 2017) or an initialisation for a standard optimiser (Finn et al., 2017; Nichol et al., 2018; Flennerhag et al., 2019), such that a learner can rapidly adapt to a new task within a handful of (gradient) steps. However, empirical evidence from few-shot learning suggests that meta-learners that are not model agnostic can generalise better across a range of task domains. Enforcing some form of strict parameter sharing has been shown to reduce overfitting (Lee et al., 2017; Mishra et al., 2018; Munkhdalai et al., 2018), induce more robust convergence (Zintgraf et al., 2019; Rakelly et al., 2019), and yield superior final performance (Oreshkin et al., 2018; Rusu et al., 2019; Lee et al., 2019). Strict parameter sharing can also be leveraged to meta-learn an update rule that facilitates few-shot adaptation across tasks (Li et al., 2017; Lee et al., 2017; Park & Oliva, 2019). These findings suggest that sharing meta-learned parameters offers a strong inductive bias. However, prior works focus exclusively on few-shot learning and it remains unclear whether this inductive bias holds generally. Unfortunately, because they backpropagate through the adaptation process, we cannot scale them to general forms of meta-learning (Wu et al., 2018; Flennerhag et al., 2019). 9102 guA 03 ]GL.sc[ 1v52000.9091:viXra
Task Adaptation Meta-Learning y ∇ L(θ; φ) y ω2 φ2 Dω2 θ(cid:48)(cid:48) h2 θ2 P 2∇ θ2L min E θ[J(φ)] θ(cid:48) P ∇ L ω1 φ1 Dω1 ∇ L θ h1 θ1 P 1∇ θ1L φ x θ Task learners Shared Warp Figure 1: Schematics of WarpGrad. A meta-learned optimiser (ω) is embedded in task learners (h) through shared warp layers (ω1,2) that are fixed during task adaptation of task parameters θ. WarpGrad acts on a task learner by modulating its activation in the forward pass and modulating its gradients (∇ L) in the backward pass by backpropagating through warp layers (Dω). This yields a form of preconditioning (P ) we refer to as gradient warping. Warp-parameters (φ) are meta-learned over the joint task parameter distribution (E [J(φ)]) to form a geometry that facilitates task learning. θ In this paper, we resolve this limitation by proposing a novel framework, Warped Gradient Descent (WarpGrad), that preserves the inductive bias of gradient-based few-shot adaptation while generalising to meta-learning over arbitrary adaptation processes. WarpGrad methods meta-learn an optimiser via fixed meta-parameters that are shared across tasks. We find that such parameter sharing offers a strong inductive bias that yields faster convergence and superior generalisation performance across learning regimes, including few-shot, supervised, reinforcement, and continual learning. Specifically, WarpGrad is a family of modular optimisation methods that meta-learn shared warp parameters across task learners such that gradient-based adaptation of task parameters leads to fast learning and generalisation. WarpGrad methods belong to a class of meta-learners that precondition native gradients (Li et al., 2017; Lee et al., 2017; Park & Oliva, 2019), as opposed to recurrent meta-learned optimisers (Ravi & Larochelle, 2016; Andrychowicz et al., 2016; Li & Hoiem, 2016) that learn an update rule from scratch. This provides WarpGrad methods with a well-defined structure as any non-degenerate warp inherits gradient descent properties with respect to task parameters. In particular—and in contrast to recurrent meta-learned optimisers—task adaptation is guaranteed to converge and we show that WarpGrad methods learn a Riemann geometry for task adaptation. Uniquely among gradient-based meta-learners, WarpGrad methods meta-learn to precondition gra- dients over the joint task parameter distribution. This fundamental departure from the paradigm of meta-optimization synthesis through the adaptation process lets us scale WarpGrad methods beyond few-shot learning without incurring a short-horizon bias (Wu et al., 2018). WarpGrad methods are capable of meta-learning over arbitrary adaptation processes, even if a final loss is not well defined. We find that WarpGrad methods surpass baseline gradient-based meta-learners on standard few-shot learning tasks (miniImageNet, tieredImageNet; Vinyals et al., 2016; Ravi & Larochelle, 2016; Ren et al., 2018), while scaling beyond few-shot learning to standard supervised settings on the “multi”- shot Omniglot benchmark (Flennerhag et al., 2019) and a multi-shot version of tieredImageNet. We demonstrate the generality of WarpGrad by outperforming competing methods in a supervised learning setting where previous gradient-based meta-learners fail; maze-navigation with Recurrent Neural Networks (Miconi et al., 2019). Finally, we show that WarpGrad methods can be used to meta- learn an optimiser that protects against catastrophic forgetting in a continual learning setting. 2 Warped Gradient Descent We begin by reviewing gradient-based meta-learning for few-shot learning. We then propose a general method for expressing gradient preconditioning, which allows us to introduce new forms of non-linear and conditional preconditioning. We show that our method represents a Riemann Metric, an insight we use to derive a scalable meta-objective based on the gradient descent principle. 2
2.1 Preliminaries: Meta-Learned Gradient Preconditioning for Few-Shot Learning We define gradient-based few-shot learning as in the Model-Agnostic Meta-Learner (MAML; Finn et al., 2017) setting, where a task τ is defined by a training set Dτ and a validation set Dτ , with train test D := {(x , y )}n for some small n. We adapt a task learner h via stochastic gradient descent i i i=1 from an initialisation θ 0; given learning rate α and objective L Dτ (θ) := E x,y∼Dτ [(cid:96)(h(x, θ), y)] train train for some loss (cid:96), task adaptation is defined by θ Kτ = θ 0 − α (cid:80)K k=− 01 ∇ L Dτ train(θ kτ ). MAML meta-learn a shared initialisation θ to yield good validation performance across the task distribution, p(τ ). 0 Subsequent work extend MAML by preconditioning task gradients with some meta-learned matrix P . Meta-parameters φ of P are learned jointly with θ in a general MAML framework, 0 (cid:32) K−1 (cid:33) (cid:88) (cid:88) LMAML(θ 0, φ) := L Dτ test θ 0 − α P (θ kτ ; φ)∇ L Dτ train(θ kτ ) . (1) τ∼p(τ) k=0 We recover MAML by setting P equal to the identity matrix. This framework backpropagates through the adaptation process, which limits it to few-shot learning as it is computational expensive, its meta-gradient is susceptible to exploding/vanishing gradients (Antoniou et al., 2019), and it is subject to the credit assignment problem. Our goal is to develop a framework for meta-learning that overcomes all three limitations. Outside of meta-learning, gradient preconditioning arises in second- order methods as a means of correcting for curvature (for instance via the Hessian). Unfortunately, P is typically prohibitively large and can be intractable to compute, thus necessitating some form of approximation (Nocedal & Wright, 2006; Martens, 2010; Martens & Grosse, 2015). In few- shot meta-learning, Meta-SGD (Li et al., 2017) parameterise P as a fixed diagonal matrix that encodes parameter-specific learning rates. Meta-Curvature (Park & Oliva, 2019) allows for a block- diagonal P at the cost of added complexity. Other works embed preconditioning in feed-forward networks, h(x, θ) = W x, by inserting non-learnable projections T . Backpropagating through the reparameterised model h(x, θ, φ) = T W x automatically preconditions task gradients and defines P via P (θ; φ)∇ L Dτ (θ) = ∇ L Dτ (θ; φ). Natural Neural Networks (N-Nets) exploit this identity train train to embed a block-diagonal approximation of Natural Gradient Descent (Desjardins et al., 2015). T-Nets (Lee & Choi, 2018) meta-learn T , which is fixed during task adaptation, to induce a fixed block- diagonal preconditioning matrix P with block entries T T T . Similarly, CAVIA (Zintgraf et al., 2019) meta-learn fixed parameters to facilitate adaptation of a small set of task parameters. Importantly, embedding preconditioning simplifies task adaptation as it avoids explicit computation of P . We contribute to these strands of work by (a) generalising embedded preconditioning beyond a block- diagonal structure (b) and deriving a novel meta-objective for general-purpose meta-learning. 2.2 General-Purpose Preconditioning First, we generalise the notion of embedded preconditioning beyond linear projections in feed-forward layers. This has profound implications as it allows us to define entirely new forms of preconditioning in any form of task learner; we introduce a recurrent warp layer for recurrent neural networks that outperforms competing methods in our maze navigation experiment (Section 4.2), as does a warp layer defined in terms of residual neural networks (He et al., 2016) in our Omniglot experiment (Section 4.1). This generality comes at the cost of an explicit identification of P . However, we show that WarpGrad methods are first-order equivalent to gradient descent under an induced P defined by the warp layers. Moreover, non-degenerate warps represent a valid Riemann metric. As N-Nets and T-Nets, WarpGrad methods do not explicitly define or compute P but embed pre- conditioning in the architecture of the task learner. Standard gradient descent under the embedding automatically preconditions task parameter gradients. In contrast to N-Nets and T-Nets, we are free to choose the representation of preconditioning: to implement a WarpGrad optimiser, we choose a model architecture (for instance, a random forest (Lecun et al., 1998) or Long Short-Term Memory model (LSTM; Hochreiter & Schmidhuber, 1997)) and either designate some layers or insert new warp-layers, with other layers being task-adaptable (see Appendix A). Warp-layers are held fixed during task adaptation and meta-learned across tasks to produce preconditioning that facilitates task adaptation. The design of the task learner and the choice of parameter sharing scheme induce a parameterised optimiser such that backpropagating through shared warp parameters precondition the gradients of task adaptable parameters in a process we refer to as gradient warping (Figure 1). 3
P τ(cid:48) P τ θ θ(cid:48) ω ◦ ω )(θ) ω W W τ(cid:48) ∇ ( L τ γ(cid:48) γ G−1 ∇ L ( γ ) Figure 2: Left: synthetic experiment illustrating how WarpGrad warps gradients (see Appendix D for full details). Each task f ∼ p(f ) defines a distinct loss surface (W, bottom row). Gradient descent (black) on these surfaces struggles to find a minimum. WarpGrad meta-learns a warp ω to produce better update directions (magenta; Section 2.4). In doing so, WarpGrad learns a meta-geometry P where standard gradient descent is well behaved (top row). Right: gradient descent in P is equivalent to first-order Riemannian descent in W under a meta-learned Riemann metric (Section 2.3). Gradient warping exploits that preconditioning can be realised by backpropagating through a fixed transformation (Desjardins et al., 2015). This becomes a powerful method in task learners that define a composition of learnable modules, h = hl ◦· · ·◦h1, such as Neural Networks. For these, we can make full use of the meta-optimization synthesis process and learn a distributed representation of the preconditioning matrix by inserting warp-layers ω that act on the activations of task-adaptable layers; for instance, we may interleave warp-layers to define an embedded WarpGrad optimiser by h = ωl ◦ hl ◦ · · · ◦ ω1 ◦ h1. The gradient of task parameters θi of a layer hi embeds preconditioned via meta-optimization synthesis:     l−(i+1) ∂ L (cid:89) ∂θi = E ∇(cid:96)T  D xωl−jD xhl−j  D xωiD θihi  , (2) j=0 where D and D denote the Jacobian with respect to input and parameters, respectively. We obtain x θ T-Nets by enforcing independent Jacobians D ω = T . In WarpGrad, gradient warping arises first x by modulating layer activations in the forward pass, which modulates Jacobians D ω, then by x preconditioning task parameter gradients D h in the backward pass. Thus, non-linear warps induce θ interdependent Jacobians that express preconditioning beyond the block-diagonal structure imposed by prior works. In particular, as we have made no assumptions on the form of hi or ωi, WarpGrad methods can act on any neural network through any form of warping (including recurrence). WarpGrad methods are thus imbued with three powerful properties. First, they act on task parameter gradients through standard meta-optimization synthesis. Because of this, they inherit gradient descent properties, most notably guarantees of convergence. Second, they form a distributed representation of precon- ditioning that disentangles its expressive capacity from that of the task learner’s. Third, because warp parameters are meta-learned across tasks, they can capture properties of the task-distribution beyond local information. Figure 2 demonstrates this property in a synthetic scenario. We construct a family of tasks f : R2 → R (see Appendix D for details) and meta-learn a WarpGrad across the task distribution. As can be seen, the warped loss surfaces of two new tasks (P , P ) are smoother and τ τ(cid:48) more well-behaved than their respective native loss-surfaces (W , W ). τ τ(cid:48) 2.3 The Geometry of Warped Gradient Descent The generality of WarpGrad methods means that we are not guaranteed to obtain a well-behaved preconditioning matrix. Further, while characterising WarpGrad methods as embedding precondition- ing via gradient warping provides us with a framework for designing WarpGrad optimisers, it offers little guidance as to how we may meta-learn warp parameters. To address these points, we take a geometric perspective and reinstate a valid Riemann metric, from which we derive a framework for meta-learning warp parameters on first principles. 4
The inner update rule in Eq. 1 represents first-order Riemannian Gradient Descent on task parameters if G := P −1 is a valid Riemann metric (Amari, 1998), which enjoys similar convergence guarantees to stochastic gradient descent. Gradient warping is thus well-behaved if it represents a valid (meta- learned) Riemann metric. Informally, a metric tensor G is a positive-definite, smoothly varying matrix that measures the curvature on a manifold W. The metric tensor defines the steepest direction of descent by −G−1∇ L (Lee, 2003). To show that a WarpGrad optimiser represents a metric tensor, we define an explicit warp Ω by reparameterising warp layers such that ωi(hi(x; θi)) = hi(x; Ω(θ)i) ∀x, i. Let Ω be a map from a Euclidean representation space P onto a Riemannian space W with γ = Ω(θ; φ). The metric tensor G can then be identified via push-forward: ∆θ := ∇ (L ◦ Ω) (θ; φ) = [DΩ(θ; φ)]T ∇ L (γ) (3) ∆γ := DΩ(θ; φ) ∆θ = G(γ; φ)−1∇ L(γ), (4) where G−1 := [DΩ][DΩ]T . Provided Ω is not degenerate (G is non-singular), G−1 is positive-definite, hence a valid Riemann metric. Because WarpGrad optimisers act in P, they represent this metric implicitly. Task parameter gradients under non-linear warps are therefore not exactly equivalent to preconditioning under P = G−1, but from first-order Taylor series expansion we have that (L ◦ Ω)(θ − α∆θ) = L(γ − α∆γ) + O(α2). (5) Consequently, gradient warping can be understood as warping the native loss surface into a surface conducive to task adaptation. Warp parameters φ control this geometry and therefore what task adaptation converges to. By meta-learning φ we can accumulate information that is conducive to task adaptation but that may not be available during that process. Because task adaptation relies on stochastic gradient descent, the meta-learned geometry should yield as effective updates as possible across expected task parameterisations, which implies a canonical meta-objective of the form (cid:104) (cid:16) (cid:17)(cid:105) min E L γ − α G(γ; φ)−1∇ L(γ) . (6) L,γ∼p(L,γ) φ In contrast to MAML-based approaches (Section 2.1), this objective avoids meta-optimization synthesis through the adaptation process. Instead, it defines task adaptation abstractly by introducing a joint distribution over objectives and parameterisations, opening up for general-purpose meta-learning at scale. 2.4 Meta-Learning Warp Parameters We consider a general meta-learning setting where we are given a task distribution p(τ ). A task τ = (hτ , Lτ , Lτ ) is defined by a task learner hτ embedded with a shared WarpGrad optimiser, meta task a meta-training objective Lτ , and a task adaptation objective Lτ . We use Lτ to adapt task meta task task parameters θ and Lτ to adapt warp parameters φ. Meta and task objectives can differ in arbitrary meta ways, but both are expectations over some data, i.e. L(θ, φ) = E [(cid:96)(h(x, θ, φ), y)]. In the simplest x,y case, they differ in terms of validation versus training data, but they may differ in terms of learning paradigm. For instance, we evaluate a WarpGrad optimiser in a continual learning experiment where Lτ is the MSE on the current task, while Lτ is designed to prevent catastrophic forgetting across task meta sequences of tasks (Section 4.2). WarpGrad is ideally suited for this form of integrated meta-learning, which is a promising avenue for future research (Metz et al., 2019; Mendonca et al., 2019). To obtain our meta-objective, we recast the canonical objective (Eq. 6) in terms of θ using first-order equivalence of gradient steps (Eq. 5). Next, we factorise p(τ, θ) into p(θ | τ )p(τ ). Since p(τ ) is given, it remains to consider a sampling strategy for p(θ | τ ). We exploit that task adaptation under stochastic gradient descent can be seen as sampling from an empirical prior p(θ | τ ) (Grant et al., 2018); in particular, each iterate θτ can be seen as a sample from p(θτ | θτ , φ). Thus, k k k−1 K-step adaptation forms a Monte-Carlo chain θτ = (θτ , . . . , θτ ), which in turn defines an empirical 0 K distribution p(θ | τ ) around some prior p(θ | τ ), discussed below. Thus, we sample from p(θ | τ ) 0 by sampling from trajectories θτ (collected online or via a buffer, discussed below) to obtain (cid:18) (cid:19) (cid:88) (cid:88) L(φ) := Lτ θτ − α∇Lτ (θτ ; φ); φ . (7) meta task τ∼p(τ) θτ ∼p(θ|τ) 5
Uniquely among gradient-based meta-learners, the Algorithm 1 WarpGrad: online meta-training WarpGrad meta-objective is defined point-wise as an expectation over the joint task parameter distribution Require: p(τ ): distribution over tasks and can be interpreted as meta-learning an update Require: α, β, λ: hyper-parameters rule over a joint search space defined by this distri- 1: initialise φ and θ 0 bution. This objective allows WarpGrad methods to 2: while not done do scale beyond few-shot learning and generalise to ar- 3: Sample mini-batch of task B from p(τ ) bitrary adaptation processes, even if a final loss is not 4: g φ, g θ0 ← 0 well defined. Yet, they retain the inductive bias of the 5: for all τ ∈ B do MAML framework; in effect, the WarpGrad objective 6: θ 0τ ← θ 0 is an expectation over 1-step MAML losses with re- 7: for all k in 0, . . . , K τ −1 do spect to warp-parameters φ. Crucially, the WarpGrad 8: θ kτ +1 ← θ kτ − α∇Lτ task (θ kτ ; φ) framework does not suffer from vanishing/exploding 9: g φ ← g φ + ∇L(φ; θ kτ ) gradients or the credit assignment problem associated 10: g θ0 ← g θ0 + ∇C(θ 0; θ 0τ :k) with meta-optimization synthesis through the adaptation process. 11: end for However, it does require second-order gradients. In 12: end for some cases, this in itself can be prohibitively costly. 13: φ ← φ − βg φ To overcome this limitation, we can relax the meta- 14: θ 0 ← θ 0 − λβg θ0 objective to take task parameter updates as given and 15: end while optimise φ with respect to post-hoc updates; (cid:18) (cid:19) Lˆ(φ) := (cid:88) (cid:88) Lτ sg (cid:2) θτ − α∇Lτ (θτ ; φ)(cid:3) ; φ , (8) meta task τ∼p(τ) θτ ∼p(θ|τ) where sg is the stop-gradient operator. In contrast to the First-order approximation of MAML (Finn et al., 2017), which ignores the entire trajectory except for the final gradient, this approximation retains all gradient terms and only discards local second-order effects; these are typically dominated by first- order effect in long parameter trajectories (Flennerhag et al., 2019). We find that our approximation only incurs a minor loss of performance in an ablation study on Omniglot (Appendix F) while outperforming baselines in our supervised learning experiment (Section 4). Intriguingly, the approximation hints at the close relationship with multi-task learning (Li & Hoiem, 2016; Bilen & Vedaldi, 2017; Rebuffi et al., 2017); it differs in the expectation over parameter space. 2.5 Integration with Learned Initialisations The WarpGrad objective is defined in terms of warp parameters φ and takes θ as given. As such, we can integrate the WarpGrad objective with any meta-learner defined over θ, in particular those that define a “prior” p(θ | τ ). For instance, (a) Multi-task solution: in online learning, we can 0 alternate between updating a multi-task solution and tuning warp parameters. We use this approach in our supervised learning experiment (Section 4.2); (b) Meta-learned point-estimate: when task adaptation occurs in batch mode, we can meta-learn a shared initialisation θ . Our few-shot and 0 supervised learning experiments take this approach Section 4.1; (c) Meta-learned prior: we can use meta-learners that define a full prior (Rusu et al., 2019; Oreshkin et al., 2018; Lacoste et al., 2018; Kim et al., 2018), enabling Bayesian approaches to WarpGrad. The family of WarpGrad methods is fully described by combining our meta-objective L (or Lˆ) with a meta-objective C over θ , 0 J(φ, θ ) := L (φ) + λ C (θ ) , (9) 0 0 where λ ∈ [0, ∞) is a hyper-parameter. To train a WarpGrad optimiser, we use stochastic gradient descent. Because we partition the parameters of the task learner into two sets φ and θ, our method can also be seen as a form of coordinate descent. We solve Eq. 9 by alternating between sampling task parameters given the current parameter values for φ and θτ and taking meta-gradient steps over 0 these samples. The details of the sampling procedure may vary depending on the specifics of the tasks (static, sequential), the design of the task learner (feed-forward, recurrent), and the learning objective (supervised, self-supervised, supervised learning). In Algorithm 1 we illustrate a default version, which is also the simplest. This algorithm has constant complexity in terms of memory and linear complexity in terms of trajectory length, assuming C does. A drawback is that it is relatively data inefficient; in Appendix B we detail a more complex offline training algorithm that stores task 6
parameters in a replay buffer for mini-batched training of φ. The gains can be dramatic; in our Omniglot experiment (Section 4.1), offline meta-training allows us to update warp parameters 2000 times with each meta-batch, improving final test F1-score from 76.3% to 84.3% (Appendix F). Examples To make the objective in Eq. 9 tangible, we illustrate two WarpGrad optimisers used in our experiment (for detailed definitions of all WarpGrad optimisers we use, see Appendix C). We define a Warp-MAML that we use for few-shot learning by choosing C to be the MAML objective (Eq. 1). For experiments that MAML cannot scale to, we define Warp-Leap by choosing the Leap objective (Flennerhag et al., 2019) for C. Leap is an attractive complement as it has linear complexity in task adaptation length as well as constant memory usage. It is defined by minimising the average distance between θ and θτ , implemented by fixing θτ and pulling θτ forward towards it under 0 K k k−1 the Euclidean norm in P. By virtue of push-forward, this corresponds to the Riemannian norm on W. Hence, Warp-Leap is a greedy search for geodesics in the meta-learned geometry defined by CLeap(θ 0) := (cid:88) (cid:88)Kτ (cid:13) (cid:13)sg [ϑτ k] − ϑτ k−1(cid:13) (cid:13) 2, ϑτ k = (θ kτ ,0, . . . , θ kτ ,n, Lτ task (θ kτ ; φ)). (10) τ∼p(τ) k=1 3 Related Work Learning to learn, or meta-learning, has previously been explored in a variety of settings. Early work focused on evolutionary approaches (Schmidhuber, 1987; Bengio et al., 1991; Thrun & Pratt, 1998). Hochreiter et al. (2001) introduced gradient descent methods to meta-learning, specifically for recurrent meta-learning algorithms. This approach was generalised by Andrychowicz et al. (2016) and Ravi & Larochelle (2016), where the update rule is a parameterised as a Recurrent Neural Network (RNN) that is meta-learned. While powerful in principle, fully connected recurrent layers scale poorly both in the length of the trajectory and in the number of parameters. Meta-learning has also been proposed as a separation of parameters into “slow” and “fast” weights, where the latter captures meta-information and the former encapsulates rapid adaptation (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba et al., 2016). This idea has been generalised to embedding neural networks that dynamically adapt the parameters of a main architecture (Ha et al., 2016). WarpGrad can be seen as learning slow warp-parameters that preconditioning adaptation of fast task parameters. Recent meta-learning has focused almost exclusively on the special case of few-shot learning, where tasks are characterised by severe data scarcity. In this settings, tasks must be relatively similar, such that a single or handful of examples are sufficient to learn the task at hand (Lake et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Ren et al., 2018). Several meta-learners have been proposed that directly predict the parameters of the task-learner (Bertinetto et al., 2016; Munkhdalai et al., 2018; Gidaris & Komodakis, 2018; Qiao et al., 2018). To scale, such methods typically pretrain a feature extractor and predict a small subset of the parameters. Alternatively, works in gradient-based few-shot learning extend MAML by meta-learning latent space for concepts or task inference, which implicitly modulates task gradients (Zhou et al., 2018; Rusu et al., 2019). Our work is further related to methods that attempt to scale beyond few-shot learning (Nichol et al., 2018; Flennerhag et al., 2019). Meta-learned preconditioning is closely related to parallel work on second-order optimisation methods for high dimensional non-convex loss surfaces. In this setting, second-order optimisers typically struggle to improve upon first-order baselines (Sutskever et al., 2013). As second-order curvature is typically intractable to compute, such methods resort to low-rank approximations (Martens, 2010; Martens & Grosse, 2015) suffer from instability (Byrd et al., 2016). In particular, Natural Gradient Descent (Amari, 1998) is a method that uses the Fisher Information Matrix as curvature metric (Amari & Nagaoka, 2007). Several proposed methods for amortising the cost of estimating the metric (Pascanu & Bengio, 2014; Martens & Grosse, 2015; Desjardins et al., 2015). As noted by Desjardins et al. (2015), expressing preconditioning through interleaved projections can be seen as a form of Mirror Descent (Beck & Teboulle, 2003). WarpGrad offers a new perspective on gradient preconditioning by introducing a generic form of model-embedded preconditioning that leverages exploits global information, even beyond the task at hand, for instance by conditioning on hidden states in recurrent warp-layers. 7
0.8 0.7 0.6 0.5 0.4 1 5 10 15 20 25 Number of tasks in meta-training set sksattuo-dlehnoycaruccatseT Warp-Leap Leap Reptile FT† SGD‡ KFAC‡ 175 150 125 100 75 50 25 0 0 20000 40000 60000 80000 100000 Number of Episodes draweR Warp-RNN Hebb-RNN† Hebb-RNN‡ RNN Figure 3: Left: Omniglot test accuracies on held-out tasks after meta-training on a varying number of tasks. Shading represents standard deviation across 10 independent runs. We compare Warp- Leap, Leap, and Reptile, multi-headed finetuning, as well as SGD and KFAC which used random initialisation but with 10x larger batch size and learning rate. Right: On a RL maze navigation task, mean cumulative return is shown. Shading represents inter-quartile ranges across 10 independent runs.†Simple modulation and ‡retroactive modulation are used (Miconi et al., 2019). 4 Experiments We evaluate WarpGrad methods in a set of Table 1: Mean test F1-score after task adaptation on experiments designed to answer three ques- held out evaluation tasks. †Multi-headed. ‡No meta- tions: (a) do WarpGrad methods retain the training; see Appendix E and Appendix H. inductive bias of MAML-based few-shot miniImageNet learners? (b) Can WarpGrad methods scale to problems beyond the reach of such meth- 5-way 1-shot 5-way 5-shot ods? (c) Can WarpGrad generalise to com- Reptile 50.0 ± 0.3 66.0 ± 0.6 plex meta-learning problems? We report Meta-SGD 50.5 ± 1.9 64.0 ± 0.9 positive results in all three cases. (M)T-Net 51.7 ± 1.8 − CAVIA (512) 51.8 ± 0.7 65.9 ± 0.6 4.1 Supervised Classification MAML 48.7 ± 1.8 63.2 ± 0.9 Warp-MAML 52.3 ± 0.8 68.4 ± 0.6 For all experiments, we use a default con- volutional architecture that stacks blocks made up of a 3 × 3 convolution, max- tieredImageNet pooling, batch-norm, and ReLU activation. 5-way 1-shot 5-way 5-shot We define WarpGrad optimisers by insert- MAML 51.7 ± 1.8 70.3 ± 1.8 ing warp layers in the form of 3 × 3 con- Warp-MAML 57.2 ± 0.9 74.1 ± 0.7 volutions after each task block. Here, we use linear warp layers to isolate the effect Omniglot tieredImageNet of our meta-objective, as prior works use 20-way 100-shot 10-way 640-shot linear preconditioning. Our ablation study in Appendix F demonstrates that non-linear SGD‡ 51.0 58.1 ± 1.5 warps outperform linear warps. KFAC‡ 56.0 − Finetuning† 76.4 ± 2.2 − (a) Few-Shot Learning We evaluate Reptile 70.8 ± 1.9 76.52 ± 2.1 on the miniImageNet (Vinyals et al., Leap 75.5 ± 2.6 73.9 ± 2.2 2016; Ravi & Larochelle, 2016) and Warp-Leap 83.6 ± 1.9 80.4 ± 1.6 tieredImageNet (Ren et al., 2018) datasets. The former is a random sub-sample of 100 classes from ILSVRC-12, each with 600 images; the latter stratifies 608 classes into 34 higher-level categories human-curated MNIST hierarchy (Deng et al., 2009). We use a Warp-MAML meta-learner that stacks 4 convolutional blocks with 128 filters. All baselines are tuned with identical and independent hyper-parameter searches (including filter size, see Appendix H) and we report best results from our experiments or the literature. Warp-MAML out- performs all baselines (Table 1), and as such preserves the inductive bias of MAML. (b) Multi-Shot Learning We propose a new protocol for tieredImageNet that allow for 640 adap- tation steps using task learners with 6 convolutional blocks, ruling out MAML-based approaches. We find that Warp-Leap surpasses all baselines with a wide margin (Table 1). For Omniglot (Lake 8
et al., 2011), we follow the protocol of Flennerhag et al. (2019), where each of the 50 alphabets is a 20-way classification task. Task adaptation involves 100 gradient steps on random samples that are preprocessed by random affine transformations (see Appendix E for details). We train Warp-Leap with an offline algorithm (Algorithm 2, Appendix B) that makes 2000 updates to warp parameters per meta mini-batch, compared to 1 update for other meta-learners. Except for the case of a single pretraining task, Warp-Leap substantially outperforms all baselines (Figure 3). It achieves a higher rate of conver- gence and reduces final test error from ~30% to ~15% (Table 1). Non-linear warps, which go beyond block-diagonal preconditioning, reach ~11% test error (Appendix F). Finally, we find that WarpGrad methods behave distinctly different from Natural Gradient Descent methods (Appendix G). 4.2 Complex Meta-Learning: Reinforcement and Continual Learning (c.1) supervised learning To illustrate how WarpGrad may be used both with recurrent neural networks and in meta-supervised learning, we evaluate it in a maze navigation task proposed by Miconi et al. (2018). The environment is a fixed maze and a task is defined by randomly choosing a goal location. The agent’s objective is to find the location as many times as possible, being teleported to a random location each time it finds it. The task learner is advantage actor-critic with a basic recurrent neural network (Wang et al., 2016). We design a Warp-RNN as a HyperNetwork (Ha et al., 2016) that uses a Warp-LSTM that is fixed during task training. This warp-LSTM modulates the weights of the task RNN (defined in Appendix I), which in turn is trained on mini-batches of 30 episodes for 200 000 steps. Linear warps (T-Nets) do worse than the baseline RNN. We accumulate the gradient of fixed warp-parameters continually (Algorithm 3, Appendix B) at each task parameter update. Warp parameters are updated on every 30th step on task parameters (we control for meta- LSTM capacity in Appendix I). We compare against Learning to Reinforcement Learn (Wang et al., 2016) and Hebbian meta-learning (Miconi et al., 2018, 2019); see Appendix I for details. The Warp- RNN converges faster, more robustly, and to a higher level of performance (Figure 3). (c.2) Continual Learning We test if a WarpGrad optimiser can prevent catastrophic forget- ting (French, 1999). To this end, we design a continual learning version of the sine regression meta-learning experiment in Finn et al. (2017) by splitting the input interval [−5, 5] ⊂ R into 5 consecutive sub-tasks (an alternative protocol was recently proposed independently of us by Javed & White, 2019). Each sub-task is a regression problem with the target being a mixture of two random sine waves; for each task, we train a 4-layer feed-forward task learner with interleaved warp layers incrementally on one sub-task at a time (see Appendix J for details). To isolate the behaviour of WarpGrad parameters, we use a fixed random initialisation for each task sequence. Warp parameters are meta-learned to prevent catastrophic forgetting by defining Lτ to be the average task loss over meta current and previous sub-tasks, for each sub-task in a task sequence. This forces warp-parameters to disentangle the adaptation process of current and previous sub-tasks. Evaluating our WarpGrad optimiser on held-out tasks, we find that it learns new sub-tasks while largely retaining performance on previously seen sub-tasks. Hence, we find it to be an effective mechanism against catastrophic forgetting and promising avenue for further research. For detailed results, see Appendix J. 5 Conclusion We propose WarpGrad, a novel family of modular meta-learned optimisers that scale gradient-based meta-learning to arbitrary adaptation processes. We find empirically that it retains the inductive bias of MAML-based few-shot learners while being able to scale to complex problems and task learner architectures. WarpGrad provides a principled framework for general-purpose meta-learning that integrates learning paradigms, an exciting avenue for future research. We introduce novel means for preconditioning task gradients, for instance with residual and recurrent warp layers. Understanding how WarpGrad manifolds relate to second-order optimisation methods will further our understanding of gradient-based meta-learning and aid us in designing warp-layers with strong inductive bias. In their current form, WarpGrad methods share some of the limitations of many popular meta-learning approaches. While WarpGrad optimisers can be designed to avoid backpropagating through the task training process, as in Warp-Leap, the WarpGrad objective samples from parameter trajectories and has therefore linear computational complexity in the number of adaptation steps, currently an unresolved limitation of gradient-based meta-learning. Our offline algorithm hints at exciting possibilities for overcoming this limitation (Algorithm 2). 9
References Amari, Shun-Ichi. Natural gradient works efficiently in learning. Neural computation, 10(2):251–276, 1998. Amari, Shun-ichi and Nagaoka, Hiroshi. Methods of information geometry, volume 191. American Mathematical Society, 2007. Andrychowicz, Marcin, Denil, Misha, Gomez, Sergio, Hoffman, Matthew W, Pfau, David, Schaul, Tom, Shillingford, Brendan, and De Freitas, Nando. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, 2016. Antoniou, Antreas, Edwards, Harrison, and Storkey, Amos J. How to train your MAML. In International Conference on Learning Representations, 2019. Ba, Jimmy, Hinton, Geoffrey E, Mnih, Volodymyr, Leibo, Joel Z, and Ionescu, Catalin. Using fast weights to attend to the recent past. In Advances in Neural Information Processing Systems, 2016. Beck, Amir and Teboulle, Marc. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31:167–175, 2003. Bengio, Yoshua, Bengio, Samy, and Cloutier, Jocelyn. Learning a synaptic learning rule. Université de Montréal, Département d’informatique et de recherche opérationnelle, 1991. Bertinetto, Luca, Henriques, João F, Valmadre, Jack, Torr, Philip, and Vedaldi, Andrea. Learning feed-forward one-shot learners. In Advances in Neural Information Processing Systems, 2016. Bilen, Hakan and Vedaldi, Andrea. Universal representations: The missing link between faces, text, planktons, and cat breeds. arXiv preprint arXiv:1701.07275, 2017. Byrd, R., Hansen, S., Nocedal, J., and Singer, Y. A stochastic quasi-newton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008–1031, 2016. Chen, Yutian, Hoffman, Matthew W, Colmenarejo, Sergio Gómez, Denil, Misha, Lillicrap, Timothy P, Botvinick, Matt, and de Freitas, Nando. Learning to learn without gradient descent by gradient descent. In International Conference on Machine Learning, 2017. Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. MNIST: A large- scale hierarchical image database. In International Conference on Computer Vision and Pattern Recognition, 2009. Desjardins, Guillaume, Simonyan, Karen, Pascanu, Razvan, and kavukcuoglu, koray. Natural neural networks. In Advances in Neural Information Processing Systems, 2015. Finn, Chelsea, Abbeel, Pieter, and Levine, Sergey. Model-Agnostic Meta-Learning for Fast Adapta- tion of Deep Networks. In International Conference on Machine Learning, 2017. Flennerhag, Sebastian, Yin, Hujun, Keane, John, and Elliot, Mark. Breaking the activation function bottleneck through adaptive parameterization. In Advances in Neural Information Processing Systems, 2018. Flennerhag, Sebastian, Moreno, Pablo G., Lawrence, Neil D., and Damianou, Andreas. Transferring knowledge across learning processes. In International Conference on Learning Representations, 2019. French, Robert M. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3 (4):128–135, 1999. Gidaris, Spyros and Komodakis, Nikos. Dynamic few-shot visual learning without forgetting. In International Conference on Computer Vision and Pattern Recognition, 2018. Grant, Erin, Finn, Chelsea, Levine, Sergey, Darrell, Trevor, and Griffiths, Thomas L. Recasting gradient-based meta-learning as hierarchical bayes. In International Conference on Learning Representations, 2018. Ha, David, Dai, Andrew M., and Le, Quoc V. Hypernetworks. In International Conference on Learning Representations, 2016. 10
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image recognition. In International Conference on Computer Vision and Pattern Recognition, 2016. Hinton, Geoffrey E and Plaut, David C. Using fast weights to deblur old memories. In 9th Annual Conference of the Cognitive Science Society, 1987. Hochreiter, Sepp and Schmidhuber, Jürgen. Long short-term memory. Neural computation, 9: 1735–80, 12 1997. Hochreiter, Sepp, Younger, A. Steven, and Conwell, Peter R. Learning to learn using gradient descent. In International Conference on Articial Neural Networks, 2001. Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015. Javed, Khurram and White, Martha. Meta-learning representations for continual learning. arXiv preprint arXiv:1905.12588, 2019. Kim, Taesup, Yoon, Jaesik, Dia, Ousmane, Kim, Sungwoong, Bengio, Yoshua, and Ahn, Sungjin. Bayesian model-agnostic meta-learning. arXiv preprint arXiv:1806.03836, 2018. Kingma, Diederik P. and Ba, Jimmy. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations, 2015. Lacoste, Alexandre, Oreshkin, Boris, Chung, Wonchang, Boquet, Thomas, Rostamzadeh, Negar, and Krueger, David. Uncertainty in multitask transfer learning. In Advances in Neural Information Processing Systems, 2018. Lake, Brenden, Salakhutdinov, Ruslan, Gross, Jason, and Tenenbaum, Joshua. One shot learning of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society, 2011. Lake, Brenden M., Salakhutdinov, Ruslan, and Tenenbaum, Joshua B. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015. Lecun, Yann, Bottou, Léon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, pp. 2278–2324, 1998. Lee, John M. Introduction to Smooth Manifolds. Springer, 2003. Lee, Kwonjoon, Maji, Subhransu, Ravichandran, Avinash, and Soatto, Stefano. Meta-learning with differentiable convex optimization. In CVPR, 2019. Lee, Sang-Woo, Kim, Jin-Hwa, Ha, JungWoo, and Zhang, Byoung-Tak. Overcoming Catastrophic Forgetting by Incremental Moment Matching. In Advances in Neural Information Processing Systems, 2017. Lee, Yoonho and Choi, Seungjin. Meta-Learning with Adaptive Layerwise Metric and Subspace. In International Conference on Machine Learning, 2018. Li, Ke and Malik, Jitendra. Learning to optimize. In International Conference on Machine Learning, 2016. Li, Zhenguo, Zhou, Fengwei, Chen, Fei, and Li, Hang. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017. Li, Zhizhong and Hoiem, Derek. Learning without forgetting. In European Conference on Computer Vision, 2016. Martens, James. Deep learning via hessian-free optimization. In International Conference on Machine Learning, 2010. Martens, James and Grosse, Roger. Optimizing neural networks with kronecker-factored approximate curvature. In International Conference on Machine Learning, 2015. Mendonca, Russell, Gupta, Abhishek, Kralev, Rosen, Abbeel, Pieter, Levine, Sergey, and Finn, Chelsea. Guided meta-policy search. arXiv preprint arXiv:1904.00956, 2019. 11
Metz, Luke, Maheswaranathan, Niru, Cheung, Brian, and Sohl-Dickstein, Jascha. Meta-learning update rules for unsupervised representation learning. In International Conference on Learning Representations, 2019. Miconi, Thomas, Clune, Jeff, and Stanley, Kenneth O. Differentiable plasticity: training plastic neural networks with meta-optimization synthesis. In International Conference on Machine Learning, 2018. Miconi, Thomas, Clune, Jeff, and Stanley, Kenneth O. Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity. In International Conference on Learning Representations, 2019. Mishra, Nikhil, Rohaninejad, Mostafa, Chen, Xi, and Abbeel, Pieter. A Simple Neural Attentive Meta-Learner. In International Conference on Learning Representations, 2018. Mujika, Asier, Meier, Florian, and Steger, Angelika. Fast-slow recurrent neural networks. In Advances in Neural Information Processing Systems, 2017. Munkhdalai, Tsendsuren, Yuan, Xingdi, Mehri, Soroush, Wang, Tong, and Trischler, Adam. Learning rapid-temporal adaptations. In International Conference on Machine Learning, 2018. Nichol, Alex, Achiam, Joshua, and Schulman, John. On First-Order Meta-Learning Algorithms. arXiv preprint ArXiv:1803.02999, 2018. Nocedal, Jorge and Wright, Stephen. Numerical optimization. Springer, 2006. Oreshkin, Boris N, Lacoste, Alexandre, and Rodriguez, Paul. Tadam: Task dependent adaptive metric for improved few-shot learning. In Advances in Neural Information Processing Systems, 2018. Park, Eunbyung and Oliva, Junier B. Meta-curvature. arXiv preprint arXiv:1902.03356, 2019. Pascanu, Razvan and Bengio, Yoshua. Revisiting natural gradient for deep networks. In International Conference on Learning Representations, 2014. Perez, Ethan, Strub, Florian, De Vries, Harm, Dumoulin, Vincent, and Courville, Aaron. Film: Visual reasoning with a general conditioning layer. In Association for the Advancement of Artificial Intelligence, 2018. Qiao, Siyuan, Liu, Chenxi, Shen, Wei, and Yuille, Alan L. Few-shot image recognition by predict- ing parameters from activations. In International Conference on Computer Vision and Pattern Recognition, 2018. Rakelly, Kate, Zhou, Aurick, Quillen, Deirdre, Finn, Chelsea, and Levine, Sergey. Efficient off-policy meta-supervised learning via probabilistic context variables. arXiv preprint arXiv:1903.08254, 2019. Ravi, Sachin and Larochelle, Hugo. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2016. Rebuffi, Sylvestre-Alvise, Bilen, Hakan, and Vedaldi, Andrea. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems, 2017. Ren, Mengye, Triantafillou, Eleni, Ravi, Sachin, Snell, Jake, Swersky, Kevin, Tenenbaum, Joshua B., Larochelle, Hugo, and Zemel, Richard S. Meta-learning for semi-supervised few-shot classification. In International Conference on Learning Representations, 2018. Rusu, Andrei A., Rao, Dushyant, Sygnowski, Jakub, Vinyals, Oriol, Pascanu, Razvan, Osindero, Simon, and Hadsell, Raia. Meta-learning with latent embedding optimization. In International Conference on Learning Representations, 2019. Schmidhuber, Jürgen. Evolutionary principles in self-referential learning. PhD thesis, Technische Universität München, 1987. Schmidhuber, Jürgen. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131–139, 1992. Snell, Jake, Swersky, Kevin, and Zemel, Richard S. Prototypical Networks for Few-shot Learning. In Advances in Neural Information Processing Systems, 2017. 12
Suarez, Joseph. Language modeling with recurrent highway hypernetworks. In Advances in Neural Information Processing Systems, 2017. Sutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of ini- tialization and momentum in deep learning. In International Conference on Machine Learning, 2013. Thrun, Sebastian and Pratt, Lorien. Learning to learn: Introduction and overview. In In Learning To Learn. Springer, 1998. Vinyals, Oriol, Blundell, Charles, Lillicrap, Timothy, Kavukcuoglu, Koray, and Wierstra, Daan. Matching Networks for One Shot Learning. In Advances in Neural Information Processing Systems, 2016. Wang, Jane X., Kurth-Nelson, Zeb, Tirumala, Dhruva, Soyer, Hubert, Leibo, Joel Z., Munos, Rémi, Blundell, Charles, Kumaran, Dharshan, and Botvinick, Matthew. Learning to reinforcement learn. In Annual Meeting of the Cognitive Science Society, 2016. Wu, Yuhuai, Ren, Mengye, Liao, Renjie, and Grosse, Roger B. Understanding short-horizon bias in stochastic meta-optimization. In International Conference on Learning Representations, 2018. Zhou, Fengwei, Wu, Bin, and Li, Zhenguo. Deep meta-learning: Learning to learn in the concept space. arXiv preprint arXiv:1802.03596, 2018. Zintgraf, Luisa M., Shiarlis, Kyriacos, Kurin, Vitaly, Hofmann, Katja, and Whiteson, Shimon. Fast Context Adaptation via Meta-Learning. International Conference on Machine Learning, 2019. 13
Appendix A WarpGrad Design Principles for Neural Nets x c h task warp x (cid:12) (cid:12) conv block x h z 4x linear 4x linear conv LSTM (cid:12) (cid:12) x BN LSTM (cid:12) conv block ⊕ h h conv BN linear linear y y y y (a) Warp-ConvNet (b) Warp-ResNet (c) Warp-LSTM (d) Warp-HyperNetwork Figure 4: Illustration of possible WarpGrad architectures. Orange represents task layers and blue represents warp layers. ⊕ denotes residual connections and (cid:12) any form of gating mechanism. We can obtain warped architectures by interleaving task and warp layers (a, c) or by designating some layers in standard architectures as task-adaptable and some as warp layers (b, d). WarpGrad is a general family of model-embedded meta-learned optimisers. This offers a great deal of flexibility in approaching a meta-learning problem. To provide some guidance in these design choices, we briefly discuss general principles we found useful. To embed warp layers given a task-learner architecture, we may either insert new warp layers in the given architecture or designate some layers as warp layers and some as task layers. We found that WarpGrad can both be used in a high-capacity mode where task learners are relatively weak to avoid overfitting, as well as in a low-capacity mode where task learners are powerful and warp layers are relatively weak. The best approach depends on the problem at hand. We highlight three approaches to designing WarpGrad optimisers: (a) Model partitioning. Given a desired architecture, designate some operations as task-adaptable and the rest as warp layers. Task layers do not have to interleave exactly with warp layers as gradient warping arises both through the forward pass and through meta-optimization synthesis. This was how we approached the tieredImageNet and miniImageNet experiments. (b) Model augmentation. Given a model, designate all layers as task-adaptable and interleave warp layers. Warp layers can be relatively weak as meta-optimization synthesis through non-linear activations ensures expressive gradient warping. This was our approach to the Omniglot experiment; our main architecture interleaves linear warp layers in a standard architecture. (c) Information compression. Given a model, designate all layers as warp and interleave weak task layers. In this scenario, task learners are prone to overfitting. Pushing capacity into the warp allows it to encode general information the task learner can draw on during task adaptation. This approach is similar to approaches in transfer and meta-learning that restrict the number of free parameters during task training (Rebuffi et al., 2017; Lee & Choi, 2018; Zintgraf et al., 2019). Note that in either case, once warp layers have been chosen, standard meta-optimization synthesis automatically warps gradients for us. Thus, WarpGrad is fully compatible with any architecture, for instance, Residual Neural Networks (He et al., 2016) or LSTMs. For convolutional neural networks, we may use any form of convolution, learned normalization (e.g. Ioffe & Szegedy, 2015), or adaptor module (e.g. Rebuffi et al., 2017; Perez et al., 2018) to designed task and warp layers. For recurrent networks, we can use stacked LSTMs to interleave warped layers, as well as any type of HyperNetwork architecture (e.g. Ha et al., 2016; Suarez, 2017; Flennerhag et al., 2018) or partitioning of fast and slow weights (e.g. Mujika et al., 2017). Figure 4 illustrates this process. 14
B WarpGrad Meta-Training Algorithms In this section, we provide variants of WarpGrad training algorithms used in this paper. Algorithm 1 describes a simple online algorithm, which accumulates meta-gradients online during task adaptation. This algorithm has constant memory and scales linearly in the length of task trajectories. In Algo- rithm 2, we describe an offline meta-training algorithm. This algorithm is similar to Algorithm 1 in many respects, but differs in that we do not compute meta-gradients online during task adaptation. Instead, we accumulate them into a replay buffer of sampled task parameterisations. This buffer is a Monte-Carlo sample of the expectation in the meta objective (Eq. 9) that can be thought of as a dataset in its own right. Hence, we can apply standard mini-batching with respect to the buffer and perform mini-batch gradient descent on warp parameters. This allows us to update warp parameters several times for a given sample of task parameter trajectories, which can greatly improve data efficiency. In our Omniglot experiment, we found offline meta-training to converge faster: in fact, a mini-batch size of 1 (i.e. η = 1 in Algorithm 2 converges rapidly without any instability. Finally, in Algorithm 3, we present a continual meta-training process where meta-training occurs throughout a stream of learning experiences. Here, C represents a multi-task objective, such as the average task loss, Cmulti = (cid:80) Lτ . Meta-learning arise by collecting experiences continuously τ∼p(τ) task (across different tasks) and using these to accumulate the meta-gradient online. Warp parameters are updated intermittently with the accumulated meta-gradient. We use this algorithm in our maze navigation experiment, where task adaptation is internalised within the RNN task learner. Algorithm 1 WarpGrad: online meta-training Algorithm 2 WarpGrad: offline meta-training Require: p(τ ): distribution over tasks Require: p(τ ): distribution over tasks Require: α, β, λ: hyper-parameters Require: α, β, λ, η: hyper-parameters 1: initialise φ and θ 0 1: initialise φ and θ 0 2: while not done do 2: while not done do 3: Sample mini-batch of task B from p(τ ) 3: Sample mini-batch of task B from p(τ ) 4 5: : g foφ r, g aθ ll0 τ← ∈0 B do 4: T ← {τ : [θ 0] for τ in B} 6: θ 0τ ← θ 0 5: for all τ ∈ B do 7: for all k in 0, . . . , K τ −1 do 6: θ 0τ ← θ 0 8: θ kτ +1 ← θ kτ − α∇Lτ task (θ kτ ; φ) 7: for all k in 0, . . . , K τ −1 do 9: g φ ← g φ + ∇L(φ; θ kτ ) 8: θ kτ +1 ← θ kτ − α∇Lτ task (θ kτ ; φ) 10: g θ0 ← g θ0 + ∇C(θ 0τ ; θ 0τ :k) 9: T [τ ].append(θτ ) 11: end for k+1 12: end for 10: end for 13: φ ← φ − βg φ 11: end for 14: θ 0 ← θ 0 − λβg θ0 12: i, g φ, g θ0 ← 0 15: end while 13: while T not empty do 14: sample τ, k without replacement Algorithm 3 WarpGrad: continual meta-training 15: g φ ← g φ + ∇L(φ; θ kτ ) R Re eq qu ui ir re e: : p α( ,τ β) ,: λdi ,s ηtr :ib hu yt pio ern -po av re ar mta es tek rs s 16: g θ0 ← g θ0 + ∇C(θ 0τ ; θ 0τ :k) 17: i ← i + 1 1: initialise φ and θ 18: if i = η then 2: i, g φ, g θ ← 0 19: φ ← φ − βg φ 3 54: :: wh S fi oal re m an p lo l lt e τd mo ∈in ne Bi-d b do a otch of task B from p(τ ) 2 20 1: : θ i,0 g← φ, gφ θ0− ←λβ 0g θ0 22: end if 6: g φ ← g φ + ∇L(φ; θ) 23: end while 7: g θ ← g θ + ∇C(θ; φ) 24: end while 8: end for 9: θ ← θ − λβg θ 10: g θ, i ← 0, i + 1 11: if i = η then 12: φ ← φ − βg φ 13: i, g θ ← 0 14: end if 15: end while 15
C WarpGrad Optimisers In this section, we detail WarpGrad methods used in our experiments. Warp-MAML We use this algorithm for few-shot learning (Section 4.1). We use the full warp- objective in Eq. 7 together with the MAML objective (Eq. 1), JWarp-MAML := L(φ) + λCMAML(θ ), (11) 0 where CMAML = LMAML under the constraint P = I. In our experiments, we trained Warp-MAML using the online training algorithm (Algorithm 1). Warp-Leap We use this algorithm for multi-shot meta-learning. It is defined by applying Leap to θ (Eq. 10), 0 JWarp-Leap := L(φ) + λCLeap(θ ). (12) 0 Note that the Leap meta-gradient makes a first-order approximation to avoid backpropagating through the adaptation process. It is given by ∇CLeap(θ 0) ≈ − (cid:88) (cid:88)Kτ ∆Lτ task (θ kτ ; φ (cid:13) (cid:13)) ϑ∇ τ L −τ ta ϑsk τ(cid:0) θ kτ (cid:13) (cid:13)−1; φ(cid:1) + ∆θ kτ , (13) τ∼p(τ) k=1 k k−1 2 where ∆Lτ (θτ ; φ) := Lτ (θτ ; φ) − Lτ (cid:0) θτ ; φ(cid:1) and ∆θτ := θτ − θτ . In our experiments, task k task k task k−1 k k k−1 we train Warp-Leap using Algorithm 1 in the multi-shot tieredImageNet experiment and Algorithm 2 in the Omniglot experiment. We perform an ablation study for training algorithms, exact (Eq. 7) versus approximate (Eq. 8) meta-objective, and warp architecture on Omniglot in Appendix F. Warp-RNN For our supervised learning experiment, we define a WarpGrad optimiser by meta-learning an LSTM that modulates the weights of the task learner (see Appendix I for details). For this algorithm, we face a continuous stream of experiences (episodes) that we meta-learn over using our continual meta-training algorithm (Algorithm 3). In our experiment, both Lτ and Lτ task meta are the advantage actor-critic objective (Wang et al., 2016); C is computed on one batch of 30 episodes, whereas L is accumulated over η = 30 such batches, for a total of 900 episodes. As each episode involves 300 steps in the environment, we cannot apply the exact meta objective, but use the approximate meta objective (Eq. 8). Specifically, let Eτ = {s , a , r , s , . . . , s , a , r , s } 0 1 1 1 T t T T +1 denote an episode on task τ , where s denotes state, a action, and r instantaneous reward. Denote a mini-batch of randomly sampled task episodes by E = {Eτ } and an ordered set of K consecu- τ∼p(τ) tive mini-batches by Ek = {E }K−1. Then Lˆ(φ; Ek) = 1/n (cid:80) (cid:80) Lτ (φ; θ, Eτ ) and Cmulti(θ; E ) = 1/n(cid:48) (cid:80) k−i i=0 Lτ (θ; φ, Eτ ), where n anE di∈ nE (cid:48)k areE niτ , oj∈ rmEi alism inet ga constani, tj s. k E kτ ,j∈Ek task k,j The Warp-RNN objective is defined by (cid:26) L(φ; Ek) + λCmulti(θ; E ) if k = η JWarp-RNN := k (14) λCmulti(θ; E ) otherwise. k WarpGrad for Continual Learning For this experiment, we focus on meta-learning warp- parameters. Hence, the initialisation for each task sequence is a fixed random initialisation, (i.e. λC(θ0) = 0). For the warp meta-objective, we take expectations over N task sequences, where each task sequence is a sequence of T = 5 sub-tasks that the task-learner observes one at a time; thus while the task loss is defined over the current sub-task, the meta-loss averages of the current and all prior sub-tasks, for each sub-task in the sequence. See Appendix J for detailed definitions. Importantly, because WarpGrad defines task adaptation abstractly by a probability distribution, we can readily im- plement a continual learning objective by modifying the joint task parameter distribution p(τ, θ) that we use in the meta-objective (Eq. 7). A task defines a sequence of sub-tasks over which we generate parameter trajectories θτ . Thus, the only difference from multi-task meta-learning is that parameter trajectories are not generated under a fixed task, but arise as a function of the continual learning 16
algorithm used for adaptation. We define the conditional distribution p(θ | τ ) as before by sampling sub-task parameters θτt from a mini-batch of such task trajectories, keeping track of which sub-task t it belongs to and which sub-tasks came before it in the given task sequence τ . The meta-objective is constructed, for any sub-task parameterisation θτt, as Lτ meta(θτt) = 1/t (cid:80)t i=1 Lτ task (θτi, D i; φ), where D is data from sub-task j (Appendix J). The meta-objective is an expectation over task j parameterisations, T (cid:18) (cid:19) (cid:88) (cid:88) (cid:88) LCL(φ) := Lτ θτt; φ . (15) meta τ∼p(τ) t=1 θτt∼p(θ|τt) D Synthetic Experiment To build intuition for what it means to warp space, we construct a simple 2-D problem over loss surfaces. A learner is faced with the task of minimising an objective function of the form f τ (x , x ) = 1 2 gτ (x ) exp(gτ (x )) − gτ (x ) exp(gτ (x , x )) − gτ exp(gτ (x )),, where each task f τ is defined by 1 1 2 2 3 1 4 1 2 5 6 1 scale and rotation functions gτ that are randomly sampled from a predefined distribution. Specifically, each task is defined by the objective function f τ (x , x ) = bτ (aτ − x )2 exp(−x2 − (x + aτ )2) 1 2 1 1 1 1 2 2 − bτ (x /sτ − x3 − x5) exp(−x2 − x2) 2 1 1 2 1 2 − bτ exp(−(x + aτ )2 − x2)), 3 1 3 1 where each a, b and s are randomly sampled parameters from sτ ∼ Cat(1, . . . , 10) aτ ∼ Cat(−1, 0, 1) i bτ ∼ Cat(−5, . . . , 5). i The task is to minimise the given objective from a randomly sampled initialisation, x ∼ {i=1,2} U (−3, 3). During meta-training, we train on a task for 100 steps using a learning rate of 0.1. Each task has a unique loss-surface that the learner traverses from the randomly sampled initialisation. While each loss-surface is unique, they share an underlying structure. Thus, by meta-learning a warp over trajectories on randomly sampled loss surfaces, we expect WarpGrad to learn a warp that is close to invariant to spurious descent directions. In particular, WarpGrad should produce a smooth warped space that is quasi-convex for any given task to ensure that the task learner finds a minimum as fast as possible regardless of initialisation. To visualise the geometry, we use an explicit warp Ω defined by a 2-layer feed-forward network with a hidden-state size of 30 and tanh non-linearities. We train warp parameters for 100 meta-training steps; in each meta-step we sample a new task surface and a mini-batch of 10 random initialisations that we train separately. We train to convergence and accumulate the warp meta-gradient online (Algorithm 1). We evaluate against gradient descent in Figure 5 on randomly sampled loss surfaces. Both optimisers start from the same initialisation, chosen such that standard gradient descent struggles; we expect that the WarpGrad optimisers has learned a geometry that is robust to the initialisation (top row in Figure 5). This is indeed what we find; the geometry learned by WarpGrad smoothly warps the native loss surface into a well-behaved space where gradient descent converges to a local minimum. E Omniglot We follow the protocol of Flennerhag et al. (2019), including choice of hyper-parameters. In this setup, each of the 50 alphabets that comprise the dataset constitutes a distinct task. Each task is treated as a 20-way classification problem. Four alphabets have fewer than 20 characters in the alphabet and are discarded, leaving us with 46 alphabets in total. 10 alphabets are held-out for final meta-testing; which alphabets are held out depend on the seed to account for variations across alphabets; we train an evaluate all baselines on 10 seeds. For each character in an alphabet, there are 20 raw samples. Of these, 5 are held out for final evaluation on the task while the remainder is used to construct a training set. Raw samples are pre-processed by random affine transformations in the 17
Figure 5: Example trajectories on three task loss surfaces. We start Gradient Descent (black) and WarpGrad (magenta) from the same initialisation; while SGD struggles with the curvature, the WarpGrad optimiser has learned a warp such that gradient descent in the representation space (top) leads to rapid convergence in model parameter space (bottom). form of (a) scaling between [0.8, 1.2], (b) rotation [0, 360), and (c) cropping height and width by a factor of [−0.2, 0.2] in each dimension. This ensures tasks are too hard for few-shot learning. During task adaptation, mini-batches are sampled at random without ensuring class-balance (in contrast to few-shot classification protocols (Vinyals et al., 2016)). Note that benchmarks under this protocol are not compatible with few-shot learning benchmarks. We use the same random forest architecture and hyper-parameters as in Flennerhag et al. (2019). This learner stacks a convolutional block comprised of a 3 × 3 convolution with 64 filters, followed by 2 × 2 max-pooling, batch-normalisation, and ReLU activation, four times. All images are down-sampled to 28 × 28, resulting in a 1 × 1 × 64 feature map that is passed on to a final linear layer. We create a Warp Leap meta-learner that inserts warp layers between each convolutional block, W ◦ ω4 ◦ h4 ◦ · · · ◦ ω1 ◦ h1, where each h is defined as above. In our main experiment, each ωi is simply a 3 × 3 convolutional layer with zero padding; in Appendix F we consider both simpler and more sophisticated versions. We find that relatively simple warp layers do quite well. However, adding capacity does improve generalisation performance. We meta-learn the initialisation of task parameters using the Leap objective (Eq. 10), detailed in Appendix C. Both Lτ and Lτ are defined as the negative log-likelihood loss; importantly, we evaluate them on meta task different batches of task data to ensure warp layers encourage generalisation. We found no additional benefit in this experiment from using hold-out data to evaluate Lτ . We use the offline meta-training meta algorithm (Appendix B, Algorithm 2); in particular, during meta-training, we sample mini-batches of 20 tasks and train task learners for 100 steps to collect 2000 task parameterisations into a replay buffer. Task learners share a common initialisation and warp parameters that are held fixed during task adaptation. Once collected, we iterate over the buffer by randomly sampling mini-batches of task parameterisations without replacement. Unless otherwise noted, we used a batch size of η = 1. For each mini-batch, we update φ by applying gradient descent under the canonical meta-objective (Eq. 7), where we evaluate Lτ on a randomly sampled mini-batch of data from the corresponding task. meta Consequently, for each meta-batch, we take (up to) 2000 meta-gradient steps on warp parameters φ. We find that this form of mini-batching causes the meta-training loop to converge much faster and induces no discernible instability. We compare Warp-Leap against no meta-learning with standard gradient descent (SGD) or KFAC (Martens & Grosse, 2015). We also benchmark against baselines provided in Flennerhag et al. (2019); Leap, Reptile (Nichol et al., 2018), MAML, and multi-headed fine-tuning. All learners benefit substantially from large batch sizes as this enables higher learning rates. To render no-pretraining a competitive option within a fair computational budget, we allow SGD and KFAC to use 10x larger batch sizes, enabling 10x larger learning rates. This renders them computationally costly, taking 2x and 4x longer to train on a given task during meta-test time than Warp-Leap, respectively. 18
0.8 0.7 0.6 0.5 0.4 1 5 10 15 20 25 Number of tasks in meta-training set sksat tuo-dleh no ycarucca tseT Warp-Leap Leap Reptile FT† SGD‡ KFAC‡ 0.7 0.6 0.5 0.4 0.3 1 5 10 15 20 25 Number of tasks in meta-training set sksat tuo-dleh no CUA ccA niarT Warp-Leap Leap Reptile FT† SGD‡ KFAC‡ Figure 6: Omniglot results. Top: test accuracies on held-out tasks after meta-training on a varying number of tasks. Bottom: AUC under F1-score curve on held-out tasks after meta-training on a varying number of tasks. Shading represents standard deviation across 10 independent runs. We compare between Warp-Leap, Leap, and Reptile, multi-headed finetuning, as well as SGD and KFAC which used random initialisation but with 10x larger batch size and learning rate. F Ablation study: Warp Layers, Meta-Objective, and Meta-Training WarpGrad provides a principled approach for model-informed meta-learning. It offers several degrees of freedom and to evaluate these design choices, we conduct an ablation study where we vary the design of warp layers as well as meta-training approach. For the ablation study, we fixed the number of pretraining tasks to 25 and report final test F1-score over 4 independent runs. All ablations use the same hyper-parameters, except for online meta-training which uses a learning rate of 0.001. First, we vary the meta-training protocol by (a) using the approximate objective (Eq. 8), (b) using online meta-training (Algorithm 1), and (c) whether meta-learning the learning rate used for task adaptation is beneficial in this experiment. We meta-learn a single scalar learning rate (as warp parameters can learn layer-wise scaling) along with a single scalar for momentum. Meta-gradients for the learning rate and momentum rate are clipped at 0.001 and we use a learning rate of 0.001. Note that when using offline meta-training, we store both task parameterisations and the momentum buffer at that point; when computing the canonical objective (Eq. 7) we use these in the update rule. Further, we vary the architecture used for warp layers. We study simpler versions that use channel- wise scaling and more complex versions that use non-linearities and residual connections. We also evaluate a version where each warp-layer has two stacked convolutions, where the first warp convolution outputs 128 filters and the second warp convolution outputs 64 filters. Finally, in the 19
Table 2: Mean test error after 100 training steps on held out evaluation tasks. †Multi-headed. ‡No meta-training, but 10x larger batch sizes (allows 10x larger learning rates). Method WarpGrad Leap Reptile Finetuning† MAML KFAC‡ SGD‡ No. Meta-training tasks 1 49.5 ± 7.8 37.6 ± 4.8 40.4 ± 4.0 53.8 ± 5.0 40.0 ± 2.6 56.0 51.0 3 68.8 ± 2.8 53.4 ± 3.1 53.1 ± 4.2 64.6 ± 3.3 48.6 ± 2.5 56.0 51.0 5 75.0 ± 3.6 59.5 ± 3.7 58.3 ± 3.3 67.7 ± 2.8 51.6 ± 3.8 56.0 51.0 10 81.2 ± 2.4 67.4 ± 2.4 65.0 ± 2.1 71.3 ± 2.0 54.1 ± 2.8 56.0 51.0 15 82.7 ± 3.3 70.0 ± 2.4 66.6 ± 2.9 73.5 ± 2.4 54.8 ± 3.4 56.0 51.0 20 82.0 ± 2.6 73.3 ± 2.3 69.4 ± 3.4 75.4 ± 3.2 56.6 ± 2.0 56.0 51.0 25 83.8 ± 1.9 74.8 ± 2.7 70.8 ± 1.9 76.4 ± 2.2 56.7 ± 2.1 56.0 51.0 two-layer warp-architecture, we evaluate a version that inserts a FiLM layer between the two warp convolutions. These are adapted during task training from a 0 initialisation; they amount to task embeddings that condition gradient warping on task statistics. Table 3: Ablation study: mean test error after 100 training steps on held out evaluation tasks. Mean and standard deviation over 4 independent runs. Offline refers to offline meta-training (Appendix B), online to online meta-training Algorithm 1; full denotes Eq. 7 and approx denotes Eq. 8;†Batch Normalization (Ioffe & Szegedy, 2015); ‡equivalent to FiLM layers (Perez et al., 2018);§Residual connection (He et al., 2016), when combined with BN, similar to the Residual Adaptor architec- ture (Rebuffi et al., 2017); ¶FiLM task embeddings. Architecture Meta-training Meta-objective F1-score None (Leap) Online None 74.8 ± 2.7 3 × 3 conv (default) Offline full (L, Eq. 7) 84.4 ± 1.7 3 × 3 conv Offline approx (Lˆ, Eq. 8) 83.1 ± 2.7 3 × 3 conv Online full 76.3 ± 2.1 3 × 3 conv Offline full, learned α 83.1 ± 3.3 Scaling‡ Offline full 77.5 ± 1.8 1 × 1 conv Offline full 79.4 ± 2.2 3 × 3 conv + ReLU Offline full 83.4 ± 1.6 3 × 3 conv + BN† Offline full 84.7 ± 1.7 3 × 3 conv + BN† + ReLU Offline full 85.0 ± 0.9 3 × 3 conv + BN† + Res§ + ReLU Offline full 86.3 ± 1.1 2-layer 3 × 3 conv + BN† + Res§ Offline full 88.0 ± 1.0 2-layer 3 × 3 conv + BN† + Res§ + TA¶ Offline full 88.1 ± 1.0 G Ablation study: WarpGrad and Natural Gradient Descent Here, we perform ablation studies to Table 4: Ablation study: mean test error after 100 training compare the geometry that a WarpGrad steps on held out evaluation tasks from a random initialisa- optimiser learns to the geometry that tion. Mean and standard deviation over 4 seeds. Natural Gradient Descent (NGD) meth- ods represent (approximately). For con- Method Preconditioning F1-score sistency, we run the ablation on Om- SGD None 40.1 ± 6.1 niglot. As computing the true Fisher In- KFAC (NGD) Linear (block-diagonal) 58.2 ± 3.2 formation Matrix is intractable, we can WarpGrad Linear (block-diagonal) 68.0 ± 4.4 compare WarpGrad against two com- WarpGrad Non-linear (full) 81.3 ± 4.0 mon block-diagonal approximations, KFAC (Martens & Grosse, 2015) and Natural Neural Nets (Desjardins et al., 2015). First, we isolate the effect of warping task loss surfaces by fixing a random initialisation and only meta-learning warp parameters. That is, in this experiment, we set λC(θ0) = 0. We compare against two baselines, stochastic gradient descent (SGD) and KFAC, both trained from a random initialisation. We use task mini-batch sizes of 200 and task learning rates of 1.0, otherwise we use the same hyper-parameters as in the main experiment. For WarpGrad, we meta-train with these 20
0.5 0.4 0.3 0.2 0.1 0.0 0.1 1 2 3 4 Layer noitavitcA detcepxE pre-warp post-warp 10 8 6 4 2 0 1 2 3 4 Layer I - voC fo mroN 1-nettahS pre-warp post-warp Figure 7: Ablation study. Left: Comparison of mean activation value E[h(x)] across layers, pre- and post-warping. Right: Shatten-1 norm of Cov(h(x), h(x)) − I, pre- and post-norm. Statistics are gathered on held-out test set and averaged over tasks and adaptation steps. hyper-parameters as well. We evaluate two WarpGrad architectures, in one, we use linear warp-layers, which gives a block-diagonal preconditioning, as in KFAC. In the other, we use our most expressive warp configuration from the ablation experiment in appendix F, where warp-layers are two-layer convolutional block with residual connections, batch normalisation, and ReLU activation. We find that warped geometries facilitate task adaptation on held-out tasks to a greater degree than either SGD or KFAC by a significant margin (table 4). We further find that going beyond block-diagonal preconditioning yields a significant improvement in performance. Second, we explore whether the geometry that we meta-learn under in the full Warp-Leap algorithm is approximately Fisher. In this experiment we use the main Warp-Leap architecture. We use a meta- learner trained on 25 tasks and that we evaluate on 10 held-out tasks. Because warp layers are linear in this configuration, if the learned geometry is approximately Fisher, post-warp activations should be zero-centred and the layer-wise covariance matrix should satisfy Cov(ωi(hi(x)), ωi(hi(x))) = I, where I is the identify matrix (Desjardins et al., 2015). If true, Warp-Leap would learn a block- diagonal approximation to the Inverse Fisher Matrix, as Natural Neural Nets. To test this, during task adaptation on held-out tasks, we compute the mean activation in each convolutional layer pre- and post-warping. We also compute the Shatten-1 norm of the difference between layer activation covariance and the identity matrix pre- and post-warping, as described above. We average statistics over task and adaptation step (we found no significant variation in these dimensions). Figure 7 summarise our results. We find that, in general, WarpGrad-Leap has zero-centered post-warp activations. That pre-warp activations are positive is an artefact of the ReLU activation function. However, we find that the correlation structure is significantly different from what we would expect if Warp-Leap were to represent the Fisher matrix; post-warp covariances are significantly different from the identity matrix and varies across layers. These results indicate that WarpGrad methods behave distinctly different from Natural Gradient Descent methods. One possibility is that WarpGrad methods do approximate the Fisher Information Matrix, but with higher F1-score than other methods. A more likely explanation is that WarpGrad methods encode a different geometry since it can learn to leverage global information beyond the task at hand, which enables it to express geometries that standard Natural Gradient Descent cannot. H miniImageNet and tieredImageNet miniImageNet This dataset is a subset of 100 classes sampled randomly from the 1000 base classes in the ILSVRC-12 training set, with 600 images for each class. Following (Ravi & Larochelle, 2016), classes are split into non-overlapping meta-training, meta-validation and meta-tests sets with 64, 16, and 20 classes in each respectively. 21
tieredImageNet As described in (Ren et al., 2018), this dataset is a subset of ILSVRC-12 that stratifies 608 classes into 34 higher-level categories in the MNIST human-curated hierarchy (Deng et al., 2009). In order to increase the separation between meta-train and meta-evaluation splits, 20 of these categories are used for meta-training, while 6 and 8 are used for meta-validation and meta- testing respectively. Slicing the class hierarchy closer to the root creates more similarity within each split, and correspondingly more diversity between splits, rendering the meta-learning problem more challenging. High-level categories are further divided into 351 classes used for meta-training, 97 for meta-validation and 160 for meta-testing, for a total of 608 base categories. All the training images in ILSVRC-12 for these base classes are used to generate problem instances for tieredImageNet, of which there are a minimum of 732 and a maximum of 1300 images per class. For all experiments, N -way K-shot classification problem instances were sampled following the standard image classification methodology for meta-learning proposed in Vinyals et al. (2016). A subset of N classes was sampled at random from the corresponding split. For each class, K arbitrary images were chosen without replacement to form the training dataset of that problem instance. As usual, a disjoint set of L images per class were selected for the validation set. Few-shot classification In these experiments we used the established experimental protocol for evaluation in meta-validation and meta-testing: 600 task instances were selected, all using N = 5, K = 1 or K = 5, as specified, and L = 15. During meta-training we used N = 5, K = 5 or K = 15 respectively, and L = 15. Task learners used 4 convolutional blocks defined by with 128 filters (or less, chosen by hyper- parameter tuning), 3 × 3 kernels and strides set to 1, followed by batch normalisation with learned scales and offsets, a ReLU non-linearity and 2 × 2 max-pooling. The output of the convolutional stack (5 × 5 × 128) was flattened and mapped, using a linear layer, to the 5 output units. The last 3 convolutional layers were followed by warp layers with 128 filters each. Only the final 3 task-layer parameters and their corresponding scale and offset batch-norm parameters were adapted during task-training, with the corresponding warp layers and the initial convolutional layer kept fixed and meta-learned using the WarpGrad objective. Note that, with the exception of CAVIA, other baselines do worse with 128 filters as they overfit; MAML and T-Nets achieve 46% and 49 % 5-way-1-shot test F1-score with 128 filters, compared to their best reported results (48.7% and 51.7%, respectively). Hyper-parameters were tuned independently for each condition using random grid search for highest test F1-score on meta-validation left-out tasks. Grid sizes were 50 for all experiments. We choose the optimal hyper-parameters (using early stopping at the meta-level) in terms of meta-validation test set F1-score for each condition and we report test F1-score on the meta-test set of tasks. 60000 meta- training steps were performed using meta-gradients over a single randomly selected task instances and their entire trajectories of 5 adaptation steps. Task-specific adaptation was done using stochastic gradient descent without momentum. We use Adam (Kingma & Ba, 2015) for meta-updates. Multi-shot classification For these experiments we used N = 10, K = 640 and L = 50. Task learners are defined similarly, but stacking 6 convolutional blocks defined by 3 × 3 kernels and strides set to 1, followed by batch normalisation with learned scales and offsets, a ReLU non-linearity and 2 × 2 max-pooling (first 5 layers). The sizes of convolutional layers were chosen by hyper-parameter tuning to {64, 64, 160, 160, 256, 256}. The output of the convolutional stack (2 × 2 × 256) was flattened and mapped, using a linear layer, to the 10 output units. Hyper-parameters were tuned independently for each algorithm, version and baseline using random grid search for highest test F1-score on meta-validation left-out tasks. Grid sizes were 200 for all multi-shot experiments. We choose the optimal hyper-parameters in terms of mean meta-validation test set F1-score AUC (using early stopping at the meta-level) for each condition and we report test F1-score on the meta-test set of tasks. 2000 meta-training steps were performed using averaged meta-gradients over 5 random task instances and their entire trajectories of 100 adaptation steps with batch size 64, or inner-loops. Task-specific adaptation was done using stochastic gradient descent with momentum (0.9). Meta-gradients were passed to Adam in the outer loop. We test WarpGrad against Leap, Reptile, and training from scratch with large batches and tuned mo- mentum. We tune all meta-learners for optimal performance on the validation set. WarpGrad outper- forms all baselines both in terms of rate of convergence and final test performance (Figure 8). 22
Figure 8: Multi-shot tieredImageNet results. Top: mean learning curves (test classification F1-score) on held-out meta-test tasks. Bottom: mean test classification performance on held-out meta-test tasks during meta-training. Training from scratch omitted as it is not meta-trained. I Maze Navigation To illustrate both how WarpGrad may be used with Recurrent Neural Networks in an online meta- learning setting, as well as in a supervised learning environment, we evaluate it in a maze navigation task proposed by Miconi et al. (2018). The environment is a fixed maze and a task is defined by randomly choosing a goal location in the maze. During a task episode of length 200, the goal location is fixed but the agent gets teleported once it finds it. Thus, during an episode the agent must first locate the goal, then return to it as many times as possible, each time being randomly teleported to a new starting location. We use an identical setup as Miconi et al. (2019), except our grid is of size 11 × 11 as opposed to 9 × 9. We compare our Warp-RNN to a Learning to Reinforcement Learn (Wang et al., 2016) and Hebbian meta-learners (Miconi et al., 2018, 2019). The task learner in all cases is an advantage actor-critic (Wang et al., 2016), where the actor and critic share an underlying basic RNN, whose hidden state is projected into a policy and value function by two separate linear layers. The RNN has a hidden state size of 100 and tanh non-linearities. Following (Miconi et al., 2019), for all benchmarks, we train the task learner using Adam with a learning rate of 1e − 3 for 200 000 steps using batches of 30 episodes, each of length 200. Meta- learning arises in this setting as each episode encodes a different task, as the goal location moves, and by learning across episodes the RNN is encoding meta-information in its parameters that is can leverage during task adaptation (via its hidden state (Hochreiter & Schmidhuber, 1997; Wang et al., 2016)). See Miconi et al. (2019) for further details. We design a Warp-RNN by introducing a warp-layer in the form of an LSTM that is frozen for most of the training process. Following Flennerhag et al. (2018), we use this meta-LSTM to modulate the task RNN. Given an episode with input vector x , the task RNN is defined by t h = tanh (cid:0) U 2 V U 1 h + U 2 W U 1 x + U b b(cid:1) , (16) t h,t h,t t−1 x,t x,t t t 23
where W, V, b are task-adaptable parameters; each U i is a diagonal warp matrix produced by j,t projecting from the hidden state of the meta-LSTM, U i = diag(tanh(P iz )), where z is the hidden- j,t j t state of the meta-LSTM. See Flennerhag et al. (2018) for details. Thus, our Warp-RNN is a form of HyperNetwork (see Figure 4, Appendix A). Because the meta-LSTM is frozen for most of the training process, task adaptable parameters correspond to those of the baseline RNN. To control for the capacity of the meta-LSTM, we also train a HyperRNN where the LSTM is updated with every task adaptation; we find this model does worse than the WarpGrad-RNN. We also compare the non-linear preconditioning that we obtain in our Warp-RNN to linear forms of preconditioning defined in prior works. We implement a T-Nets-RNN meta-learner, defined by embedding linear projections T , T and T that are meta-learned in the task RNN, h = tanh(T V h + T W x + b). h x b t h t x t Note that we cannot backpropagate to these meta-parameters as per the T-Nets (MAML) framework. Instead, we train T , T , T with the meta-objective and meta-training algorithm we use for the h x b Warp-RNN. The T-Nets-RNN does worse than the baseline RNN and generally fails to learn. We meta-train the Warp-RNN using the continual meta-training algorithm (Algorithm 3, see Ap- pendix B for details), which accumulates meta-gradients continuously during training. Because task training is a continuous stream of batches of episodes, we accumulating the meta-gradient using the approximate objective (Eq. 8, where Lτ and Lτ are both the same advantage actor-critic objective) task meta and update warp-parameters on every 30th task parameter update. We detail the meta-objective in Appendix C (see Eq. 14). Our implementation of a Warp-RNN can be seen as meta-learning “slow” weights to facilitate learning of “fast” weights (Schmidhuber, 1992; Mujika et al., 2017). Implementing Warp-RNN requires four lines of code on top of the standard training script. The task-learner is the same in all experiments with the same number of learnable parameters and hidden state size. Compared to all baselines, we find that the Warp-RNN converges faster and achieves a higher cumulative reward (Figure 3 and Figure 9). J Meta-Learning for Continual Learning Online SGD and related optimisation methods tend to adapt neural network models to the data distribution encountered last during training, usually leading to what has been termed “catastrophic forgetting” (French, 1999). In this experiment, we investigate whether WarpGrad optimisers can meta-learn to avoid this problem altogether and directly minimise the joint objective over all tasks with every update in the fully online learning setting where no past data is retained. Continual Sine Regression We propose a continual learning version of the sine regression meta- learning experiment in Finn et al. (2017). We split the input interval [−5, 5] ⊂ R evenly into 5 consecutive sub-intervals, corresponding to 5 regression tasks. These are presented one at a time to a task learner, which adapts to each sub-task using 20 gradient steps on data from the given sub-task only. Batch sizes were set to 5 samples. Sub-tasks thus differ in their input domain. A task sequence is defined by a target function composed of two randomly mixed sine functions of the form f (x) = a sin(x − b ) each with randomly sampled amplitudes a ∈ [0.1, 5] and phases ai,bi i i i b ∈ [0, π]. A task τ = (a , b , a , b , o) is therefore defined by sampling the parameters that specify i 1 1 2 2 this mixture; a task specifies a target function f by τ f (x) = α (x)f (x) + (1 − α (x))f (x), (17) τ o a1,b1 o a2,b2 where α (x) = σ(x + o) for a randomly sampled offset o ∈ [−5, 5], with σ being the sigmoid o activation function. Model We define a task learner as 4-layer feed-forward networks with hidden layer size 200 and ReLU non-linearities to learn the mapping between inputs and regression targets, h(·, θ, φ). For each task sequence τ , a task learner is initialised from a fixed random initialisation θ (that is not 0 meta-learned). Each non-linearity is followed by a residual warping block consisting of 2-layer feed-forward networks with 100 hidden units and tanh non-linearities, with meta-learned parameters φ which are fixed during the task adaptation process. 24
175 150 125 100 75 50 25 0 0 20000 40000 60000 80000 100000 Number of Episodes draweR Warp-RNN HyperRNN Hebb-RNN† Hebb-RNN‡ RNN T-Nets-RNN 175 150 125 100 75 50 25 0 0 20000 40000 60000 80000 100000 120000 140000 160000 180000 200000 Number of Episodes draweR Warp-RNN HyperRNN Hebb-RNN† Hebb-RNN‡ RNN T-Nets-RNN Figure 9: Mean cumulative return for maze navigation task, for 200000 training steps. Shading represents inter-quartile ranges across 10 independent runs.†Simple modulation and ‡retroactive modulation, respectively (Miconi et al., 2019). Continual learning as task adaptation The task target function f is partition into 5 sets of sub- τ task. The task learner sees one partition at a time and is given n = 20 gradient steps to adapt, for a total of K = 100 steps of online gradient descent updates for the full task sequence; recall that every such sequence starts from a fixed random initialisation θ . The adaptation is completely online 0 since at step k = 1, . . . , K we sample a new mini-batch Dk of 5 samples from a single sub-task task (sub-interval). The data distribution changes after each n = 20 steps with inputs x coming from the next sub-interval and targets form the same function f (x). During meta-training we always present τ tasks in the same order, presenting intervals from left to right. The online (sub-)task loss is defined on the current mini-batch Dk at step k: task Lτ (cid:0) θτ , Dk ; φ(cid:1) = 1 (cid:88) (cid:0) h(x, θτ ; φ) − f (x)(cid:1)2 . (18) task k task 2| Dk | k τ task x∈Dk task Adaptation to each sub-task uses sub-task data only to form task parameter updates θτ ← θτ − k+1 k α∇Lτ (cid:0) θτ , Dk ; φ(cid:1) . We used a constant learning rate α = 0.001. Warp-parameters φ are fixed task k task across the full task sequence during adaptation. They are meta-learned across random samples of task sequences, which we describe next. Meta-learning an optimiser for continual learning To investigate the ability of WarpGrad to learn an optimiser for continual learning that mitigates catastrophic forgetting, we fix a random initialisation prior to meta-training that is not meta-learned; every task learner is initialised with these parameters. To meta-learn an optimiser for continual learning, we need a meta-objective that 25
encourages such behaviour. Here, we take a first step towards a framework for meta-learned continual learning. We define the meta-objective Lτ as incremental multitask objective that, for each sub-task meta τ in a given task sequence τ , averages the validation sub-task losses (Eq. 18) for the current and t every preceding loss in the task sequence. The task meta-objective is defined by summing over all sub-tasks in the task sequence. For some sub-task parameterisation θτt, we have (cid:18) (cid:19) t (cid:18) (cid:19) (cid:88) 1 Lτ θτt; φ = Lτ θτi, Di ; φ . (19) meta n(T − t + 1) task val i=1 As before, the full meta-objective is an expectation over the joint task parameter distribution (Eq. 7); for further details on the meta-objective, see Appendix C, Eq. 15. This meta-objective gives equal weight to all the tasks in the sequence by averaging the regression step loss over all sub-tasks where a prior sub-task should be learned or remembered. For example, losses from the first sub-task, defined using the interval [−5, −3], will appear nT times in the meta-objective. Conversely, the last sub-task in a sequence, defined on the interval [3, 5], is learned only in the last n = 20 steps of task adaptation, and hence appers n times in the meta-objective. Normalising on number of appearances corrects for this bias. We trained warp-parameters using Adam and a meta-learning rate of 0.001, sampling 5 random tasks to form a meta-batch and repeating the process for 20 000 steps of meta-training. Results Figure 10 shows a breakdown of the validation loss across the 5 sequentially learned tasks over the 100 steps of online learning during task adaptation. Results are averaged over 100 random regression problem instances. The meta-learned WarpGrad optimiser reduces the loss of the task currently being learned in each interval while also largely retaining performance on previous tasks. There is an immediate relatively minor loss of performance, after which performance on previous tasks is retained. We hypothesise that this is because the meta-objectives averages over the full learning curve, as opposed to only the performance once a task has been adapted to. As such, the WarpGrad optimiser may allow for some degree of performance loss. Intriguingly, in all cases, after an initial spike in previous sub-task losses when switching to a new task, the spike starts to revert back some way towards optimal performance, suggesting that the WarpGrad optimiser facilitates positive backward transfer, without this being explicitly enforced in the meta-objective. Deriving a principled meta-objective for continual learning is an exciting area for future research. (a) Task order as seen during meta-training. (b) Random task order. Figure 10: Continual learning regression experiment. Average log-loss over 100 randomly sampled tasks. Each task contains 5 sub-tasks learned (a) sequentially as seen during meta-training or (b) in random order [sub-task 1, 3, 4, 2, 0]. We train on each sub-task for 20 steps, for a total of K = 100 task adaptation steps. 26
(a) Task order seen during meta-training. (b) Random task order. Figure 11: Continual learning regression: evaluation after partial task adaptation. We plot the ground truth (black), task learner prediction before adaptation (dashed green) and task learner prediction after adaptation (red). Each row illustrates how task learner predictions evolve (red) after training on sub-tasks up to and including that sub-task (current task illustrate in plot). (a) sub-tasks are presented in the same order as seen during meta-training; (b) sub-tasks are presented in random order at meta-test time in sub-task order [1, 3, 4, 2 and 0]. 27
