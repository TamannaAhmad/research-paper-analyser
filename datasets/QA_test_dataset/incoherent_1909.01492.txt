Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation Po-Sen Huang† Robert Stanforth†§ Johannes Welbl‡§ Chris Dyer† Dani Yogatama† Sven Gowal† Krishnamurthy Dvijotham† Pushmeet Kohli† †DeepMind ‡University College London {posenhuang, stanforth, cdyer, dyogatama, sgowal, dvij, pushmeet}@google.com {j.welbl}@cs.ucl.ac.uk Abstract Neural networks are part of many contempo- rary NLP systems, yet their empirical suc- cesses come at the price of vulnerability to adversarial attacks. Previous work has used adversarial training and data augmentation to partially mitigate such brittleness, but these fantastic are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations. In this work, we approach the problem from the opposite di- rection: to formally verify a system’s robust- ness against a predefined class of adversarial attacks. We study text classification under syn- onym replacements or character flip perturba- tions. We propose modeling these input per- turbations as a simplex and then using Interval Bound Propagation – a formal model verifica- tion method.1 We modify the conventional log- likelihood training objective to train models that can be efficiently verified, which would otherwise come with exponential search com- plexity. The resulting models show only lit- tle difference in terms of nominal F1-score, but have much improved verified F1-score un- der perturbations and come with an efficiently computable formal guarantee on worst case ad- versaries. 1 Introduction Deep models have been shown to be vulnerable against adversarial input perturbations (Szegedy et al., 2013; Kurakin et al., 2016). Small, seman- tically invariant input alterations can lead to dras- tic changes in predictions, leading to poor perfor- mance on adversarially chosen samples. Recent work (Jia and Liang, 2017; Belinkov and Bisk, ‡Work done during an internship at DeepMind. §Equal contribution. 1The source code is available at https://github. com/deepmind/interval-bound-propagation/ tree/master/examples/language/ vnoC ULeR CF CF logits great event Upper bounds Interval bounds Input perturbations conference event occasion great good Propagated Decision regions boundary Figure 1: Illustration of verification with the input simplex and Interval Bound Propagation. From the left, input perturbations define the extreme points of a simplex (in red, projected to 2D here) around the statement “great event” that is propagated through a model. At each layer, this shape deforms itself, but can be bounded by axis-parallel bounding boxes, which are propagated similarly. Finally, in logit space, we can compute an upper bound on the worst-case specifica- tion violation (e.g., prediction changes). 2018; Ettinger et al., 2017) also exposed the vul- nerabilities of neural NLP models, e.g. with small character perturbations (Ebrahimi et al., 2018) or paraphrases (Ribeiro et al., 2018; Iyyer et al., 2018). These adversarial attacks highlight often unintu- itive model failure modes and present a challenge to deploying NLP models. Common attempts to mitigate the issue are ad- versarial training (Ebrahimi et al., 2018) and data augmentation (Belinkov and Bisk, 2018; Li et al., 2017), which lead to improved F1-score on adver- sarial examples. However, this might cause a false sense of security, as there is generally no guarantee that stronger adversaries could not circumvent de- fenses to find other successful attacks (Carlini and Wagner, 2017; Athalye et al., 2018; Uesato et al., 2018). Rather than continuing the race with adver- saries, formal verification (Baier and Katoen, 2008; Barrett and Tinelli, 2018; Katz et al., 2017) offers a different approach: it aims at providing provable guarantees to a given model specification. In the 9102 ceD 02 ]LC.sc[ 2v29410.9091:viXra
case of adversarial robustness, such a specification replacements – on text classification tasks. We can be formulated as prediction consistency under compare our verifiable approach to other defenses any altered – but semantically invariant – input including adversarial training (Goodfellow et al., change. 2014) and data augmentation (Li et al., 2017; Be- In this paper, we study verified robustness, i.e., linkov and Bisk, 2018). Note that some existing providing a certificate that for a given network adversarial perturbations such as syntactically con- and test input, no attack or perturbation under the trolled paraphrasing (Iyyer et al., 2018), exploit- specification can change predictions, using the ex- ing backtranslation systems (Ribeiro et al., 2018), ample of text classification tasks, Stanford Sen- output-lengthening attacks (Wang et al., 2019), or timent Treebank (SST) (Socher et al., 2013) and using targeted keyword attack (Cheng et al., 2018) AG News (Zhang et al., 2015). The specification are beyond the specification in this paper. against which we verify is that a text classification model should preserve its prediction under char- Formal Verification of Neural Networks. For- acter (or synonym) substitutions in a character (or mal verification provides a provable guarantee that word) based model. We propose modeling these models are consistent with a specification for all input perturbations as a simplex and then using possible model inputs. Previous work can be cat- Interval Bound Propagation (IBP) (Gowal et al., egorised into complete methods that use Mixed- 2018; Mirman et al., 2018; Dvijotham et al., 2018) Integer Programming (MIP) (Bunel et al., 2017; to compute worst case bounds on specification sat- Cheng et al., 2017) or Satisfiability Modulo The- isfaction, as illustrated in Figure 1. Since these ory (SMT) (Katz et al., 2017; Carlini et al., 2017), bounds can be computed efficiently, we can fur- and incomplete methods that solve a convex relax- thermore derive an auxiliary objective for models ation of the verification problem (Weng et al., 2018; to become verifiable. The resulting classifiers are Wong and Kolter, 2018; Wang et al., 2018). Com- efficiently verifiable and improve robustness on ad- plete methods perform exhaustive enumeration to versarial examples, while maintaining comparable find the worst case. Hence, complete methods are performance in terms of nominal test F1-score. expensive and difficult to scale, though they pro- The contributions of this paper are twofold: vide exact robustness bounds. Incomplete meth- ods provide loose robustness bounds, but can be • To the best of our knowledge, this paper is more scalable and used inside the training loop for the first to introduce verification and verifi- training models to be robust and verifiable (Raghu- able training for neural networks in natural nathan et al., 2018; Wong and Kolter, 2018; Dvi- language processing (§3). jotham et al., 2018; Gowal et al., 2018). Our work is the first to extend incomplete verification to text • Through a series of experiments (§4), we classification, considering input perturbations on a demonstrate (a) the effectiveness of model- simplex and minimising worst case bounds to ad- ing input perturbations as a simplex and using versarial attacks in text classification. We highlight simplex bounds with IBP for training and test- that the verification of neural networks is an ex- ing, (b) the weakness of adversarial training tremely challenging task, and that scaling complete under exhaustive verification, (c) the effects and incomplete methods to large models remains of perturbation space on the performance of an open challenge. different methods, and (d) the impact of using GloVe and counter-fitted embeddings on the Representations of Combinatorial Spaces. IBP verification bounds. Word lattices and hypergraphs are data structures that have often been used to efficiently represent 2 Related Work and process exponentially large numbers of Adversarial Examples in NLP. Creating adver- sentences without exhaustively enumerating them. sarial examples for NLP systems requires identi- Applications include automatic speech recognition fying semantically invariant text transformations (ASR) output rescoring (Liu et al., 2016), machine to define an input perturbation space. In this pa- translation of ASR outputs (Bertoldi et al., 2007), per, given our specification, we study word- and paraphrase variants (Onishi et al., 2010), and character-level HotFlip attacks (Ebrahimi et al., word segmentation alternatives (Dyer et al., 2008). 2018) – which consist of character and synonym The specifications used to characterise the space
of adversarial attacks are likewise a compact where e is a one-hot vector with 1 in the i-th posi- i representation, and the algorithms discussed below tion. In other words, the true class logit should be operate on them without exhaustive enumeration. greater or equal than those for all other classes y, which means the prediction remains constant. 3 Methodology 3.2 Verification as Optimisation We assume a fixed initial vector representation z 0 of a given input sentence z2 (e.g. the concatenation Verifying the specification in Eq. (2) can be done of pretrained word embeddings) and use a neural by solving the following constrained optimisation network model, i.e. a series of differentiable trans- problem to find the input that would most strongly formations h : violate it: k z = h (z ) k = 1, . . . , K (1) maximize c(cid:62)z k k k−1 K z0∈Xin(x0) where z k is the vector of activations in the k-th subject to z k = h k(z k−1) k = 1, . . . , K layer and the final output z K consists of the logits (3) for each class. Typically each h k will be an affine where c is a vector with entries c y = 1, c ytrue = −1 transformation followed by an activation function and 0 everywhere else. If the optimal value of the (e.g. ReLU or sigmoid). The affine transformation above optimisation problem is smaller than 0, then can be a convolution (with the inputs and outputs the specification in Eq. (2) is satisfied, otherwise a having an implied 2D structure) of a vector of acti- counter-example has been found. In our case, this vations at each point in a sequence; in what follows corresponds to a successful adversarial attack. these activations will be concatenated along the sequence to form a vector z . 3.3 Modeling Input Perturbations using k Simplices 3.1 Verification In the interests of computational feasibility, we Verification is the process of examining whether will actually attempt to verify the specification on a the output of a model satisfies a given specification. larger, but more tractable input perturbation space Formally, this means establishing whether the fol- X¯ ⊇ X . Any data point that is verifiable on in in lowing holds true for a given normal model input this larger input perturbation space is necessarily x : ∀z ∈ X (x ) : z ∈ X , where X char- 0 0 in 0 K out out verifiable with respect to the original specification. acterizes a constraint on the outputs, and X (x ) in 0 In the domain of image classification, X is of- in defines a neighbourhood of x throughout which 0 ten modeled as an L -ball, corresponding to input ∞ the constraint should be satisfied. perturbations in which each pixel may be indepen- In our concrete use case, we consider a speci- dently varied within a small interval. However, fication of robustness against adversarial attacks using such interval bounds is unsuitable for our sit- which are defined by bounded input perturbations uation of perturbations consisting of a small num- (synonym flips up to δ words, or character flips up ber δ of symbol substitutions. Although we could to δ characters) of the original sentence x. The construct an axis-aligned bounding box X¯ in em- in attack space X (x ) is the set of vector repre- in 0 bedding space that encompasses all of X , it would in sentations (embeddings) of all such perturbed sen- over-approximate the perturbation space to such an tences. Denoting by z the logit of label y, we K,y extent that it would contain perturbations where formulate the output constraint that for all classes all symbols in the sentence have been substituted y : z ≥ z . This specification estab- K,ytrue K,y simultaneously. lishes that the prediction of all perturbed sentences To remedy this, we propose a tighter over- z ∈ X (x ) should correspond to the correct la- 0 in 0 approximation in the form of a ‘simplex’ in embed- bel y . This specification may equivalently be true ding space. We first define this for the special case formulated as a set of half-space constraints on the (m) δ = 1, in which X = {x } ∪ {p : 1 ≤ m ≤ logits: for each class y in 0 0 M } consists of the representations of all M sen- (e − e )(cid:62)z ≤ 0 ∀z ∈ X (x ) (2) tences p(m) derived from x by performing a single y ytrue K 0 in 0 synonym (or character) substitution, together with 2For brevity, we will refer both to the original symbol the unperturbed sentence x itself. In this case we sequence and its corresponding vector representation with the same variable name, distinguishing them by styling. define X¯ in to be the convex hull S 1 of X in. Note we
are not considering contextual embeddings (Peters components z of z are: k,i k (m) et al., 2018) here. Each ‘vertex’ p is a sequence 0 of embedding vectors that differs from x at only z (δ) = min e(cid:62)h (z ) 0 k,i i k k−1 z (δ)≤z ≤z (δ) one word (or character) position. k−1 k−1 k−1 (5) z (δ) = max e(cid:62)h (z ) For a larger perturbation radius δ > 1, the cardi- k,i i k k−1 z (δ)≤z ≤z (δ) k−1 k−1 k−1 nality of X grows exponentially, so manipulating in its convex hull becomes infeasible. However, di- The above optimisation problems can be solved lating S centered at x , scaling it up by a factor 1 0 in closed form quickly for affine layers and mono- of δ, yields a simplex S with M + 1 vertices that δ tonic activation functions, as illustrated in Gowal contains X . in et al. (2018). Finally, the lower and upper bounds More formally, we define a region in the in- of the output logits z can be used to construct an K put embedding space based on the M ‘elemen- upper bound on the solution of (3): (m) tary’ perturbations {p : m = 1 . . . M } of x 0 0 defined earlier for the δ = 1 case. For perturba- maximize c(cid:62)z K (6) tions of up to δ substitutions, we define X¯ (x ) z K(δ)≤zK≤zK(δ) in 0 (m) as the convex hull of {z : m = 0 . . . M }, 0 Verifiable Training. The upper bound in (6) is (0) where z 0 = x 0 denotes the original (unper- fast to compute (only requires two forward passes turbed) sentence representation and, for m ≥ 1, for upper and lower bounds through the network). (m) (m) z = x + δ · (p − x ). The convex hull is Hence, we can define a loss to optimise models 0 0 0 0 an over-approximation of X (x ): it contains the such that the models are trained to be verifiable. in 0 representations of all sentences derived from x by Solving (6) is equivalent to finding the worst-case performing up to δ substitutions at distinct word logit difference, and this is achieved when the logit (or character) positions. of the true class is equal to its lower bound, and all other logits equal to their upper bounds. Concretely, for each class y (cid:54)= y : zˆ (δ) = z (δ), and 3.4 Interval Bound Propagation true K,y K,y zˆ (δ) = z (δ). The training loss can then K,ytrue K,ytrue To estimate the optimal value of the problem be formulated as (3), given an input z , we can propagate the up- 0 per/lower bounds on the activations z k of each layer L = κ (cid:96)(z K , y true) +(1 − κ) (cid:96)(zˆ K (δ), y true) (7) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) using interval arithmetic (Gowal et al., 2018). L normal Lspec We begin by computing interval bounds on the first layer’s activations. Recall that any input where (cid:96) is the cross-entropy loss, κ a hyperparam- z ∈ X will lie within the convex hull of certain eter that controls the relative weights between the 0 in vertices {z(m) : m = 0 . . . M }. Then, assuming classification loss L normal and specification loss 0 that the first layer h 1 is an affine transformation (e.g. L spec. If δ = 0 then z K = zˆ K (δ), and thus L linear or convolutional) followed by a monotonic reduces to a standard classification loss. Empiri- activation function, the lower and upper bounds on cally, we found that a curriculum-based training, the components z of the first layer’s activations starting with κ=1 and linearly decreasing to 0.25, 1,i z are as follows: is effective for verifiable training. 1 4 Experiments z (δ) = min e(cid:62)h (z(m) ) 1,i i 1 0 m=0,...,M (4) We conduct verification experiments on two text z 1,i(δ) = max e(cid:62) i h 1(z( 0m) ) classification datasets, Stanford Sentiment Tree- m=0,...,M bank (SST) (Socher et al., 2013) and AG News cor- pus, processed in (Zhang et al., 2015). We focus Note that these bounds are efficient to compute on word-level and character-level experiments on (m) (by passing each perturbation z 0 through the first SST and character-level experiments on AG News. layer); in particular there is no need to compute the Our specification is that models should preserve convex hull polytope. their prediction against up to δ synonym substitu- For subsequent layers k > 1, the bounds on the tions or character typos, respectively.
4.1 A Motivating Example Eq. (7) (called Verifiable Training). We provide an example from Table 2 to highlight 4.2 Baselines different evaluation metrics and training methods. Given a sentence, “you ’ ve seen them a million In this section we detail our baseline models. times .”, that is predicted correctly (called Nomi- nal Accuracy3) by a classification model, we want Adversarial Training. In adversarial training (Madry et al., 2018; Goodfellow et al., 2014), the to further examine whether the model is robust goal is to optimise the saddle point problem: against character typos (e.g., up to δ = 3 typos) to this example. One way is to use some heuristic to (cid:20) (cid:21) search for a valid example with up to 3 typos that min E max (cid:96) (z , y) (8) θ 0 can change the prediction the most (called adver- θ (x0,y) z0∈Xin(x0) sarial example). We evaluate the model using this where the inner maximisation problem is to find adversarial example and report the performance an adversarial perturbation z ∈ X (x ) that can (called Adversarial F1-score). However, even if 0 in 0 maximise the loss. In the inner maximisation prob- the adversarial example is predicted correctly, one lem, we use HotFlip (Ebrahimi et al., 2018) with can still ask: is the model truly robust against any perturbation budget δ to find the adversarial ex- typos (up to 3) to this example? In order to have a ample. The outer minimisation problem aims to certificate that the prediction will not change under update model parameters such that the adversar- any δ = 3 character typos (called verifiably ro- ial risk of (8) is minimised. To balance between bust), we could in theory exhaustively search over the adversarial robustness and nominal F1-score, all possible cases and check whether any of the we use an interpolation weight of 0.5 between the predictions is changed (called Oracle F1-score). If original cross-entropy loss and the adversarial risk. we only allow a character to be replaced by another character nearby on the keyboard, already for this Data Augmentation Training. In the data aug- short sentence we need to exhaustively search over mentation setup, we randomly sample a valid per- 2,951 possible perturbations. To avoid this combi- turbation z with perturbation budget δ from a nor- natorial growth, we can instead model all possible mal input x, and minimise the cross-entropy loss perturbations using the proposed simplex bounds given the perturbed sample z (denoted as data and propagate the bounds through IBP at the cost augmentation loss). We also set the interpolation of two forward passes. Following Eq. (3), we can weight between the data augmentation loss and the check whether this example can be verified to be original normal cross-entropy loss to 0.5. robust against all perturbations (called IBP-Verified F1-score). Normal Training. In normal training, we use the There are also a number of ways in which the likelihood-based training using the normal training training procedure can be enhanced to improve the input x. verified robustness of a model against typos to the sentence. The baseline is to train the model with the 4.3 Setup original/normal sentence directly (called Normal We use a shallow convolutional network with a Training). Another way is to randomly sample typo small number of fully-connected layers for SST sentences among the 2,951 possible perturbations and AG News experiments. The detailed model and add these sentences to the training data (called architectures and hyperparameter details are intro- Data Augmentation Training). Yet another way is duced in the supplementary material. Although to find, at each training iteration, the adversarial we use shallow models for ease of verifiable train- example among the (subset of) 2,951 possible per- ing, our nominal F1-score is on par with previous turbations that can change the prediction the most; work such as Socher et al. (2013) (85.4%) and we then use the adversarial example alongside the Shen et al. (2018) (84.3%) in SST and Zhang et al. training example (called Adversarial Training). Fi- (2015) (87.18%) in AG News. During training, nally, as simplex bounds with IBP is efficient to run, we set the maximum number of perturbations to we can train a model to be verifiable by minimising δ = 3, and evaluate performance with the maxi- mum number of perturbations from δ = 1 to 6 at 3We use the term “nominal F1-score” to indicate the accu- racy under various adversarial perturbations is much lower. test time.
SST-Char-Level SST-Word-Level AG-Char-Level Training Acc. Adv. Acc. Oracle Acc. Adv. Acc. Oracle Acc. Adv. Acc. Oracle Normal 79.8 36.5 10.3 84.8 71.3 69.8 89.5 75.4 65.1 Adversarial 79.0 74.9 25.8 85.0 76.8 74.6 90.5 85.5 81.6 Data aug. 79.8 37.8 13.7 85.4 72.7 71.6 88.4 77.5 72.0 Verifiable (IBP) 74.2 73.1 73.1 81.7 77.2 76.5 87.6 87.1 87.1 Table 1: Experimental results for changes up to δ=3 and δ=2 symbols on SST and AG dataset, respectively. We compare normal training, adversarial training, data augmentation and IBP-verifiable training, using three metrics on the test set: the nominal F1-score, adversarial F1-score, and exhaustively verified F1-score (Oracle) (%). Prediction SST word-level examples (by exhaustive verification, not by adversarial attack) + it ’ s the kind of pigeonhole-resisting romp that hollywood too rarely provides . - it ’ s the kind of pigeonhole-resisting romp that hollywood too rarely gives . - sets up a nice concept for its fiftysomething leading ladies , but fails loudly in execution . + sets up a nice concept for its fiftysomething leading ladies , but fails aloud in execution . Prediction SST character level examples (by exhaustive verification, not by adversarial attack) - you ’ ve seen them a million times . + you ’ ve sern them a million times . + choose your reaction : a. ) that sure is funny ! - choose tour reaction : a. ) that sure is funny ! Table 2: Pairs of original inputs and adversarial examples for SST sentiment classification found via an exhaus- tive verification oracle, but not found by the HotFlip attack (i.e., the HotFlip attack does not change the model prediction). The bold words/characters represent the flips found by the adversary that change the predictions. For word-level experiments, we construct the 4.5 Results synonym pairs using the PPDB database (Ganitke- Table 1 shows the results of IBP training and base- vitch et al., 2013) and filter the synonyms with line models under δ = 3 and δ = 24 perturbations fine-grained part-of-speech tags using Spacy (Hon- on SST and AG News, respectively. Figures 2 and nibal and Montani, 2017). For character-level ex- 3 show the character- and word-level results with δ periments, we use synthetic keyboard typos from between 1 and 6 under four metrics on the SST test Belinkov and Bisk (2018), and allow one possible set; similar figures for SST word-level (adversarial alteration per character that is adjacent to it on an training, data augmentation) models and AG News American keyboard. The allowable input pertur- dataset can be found in the supplementary material. bation space is much larger than for word-level synonym substitutions, as shown in Table 3. Oracle F1-score and Adversarial F1-score. In Table 1, comparing adversarial F1-score with exhaustive verification F1-score (oracle), we ob- serve that although adversarial training is effective 4.4 Evaluation Metrics at defending against HotFlip attacks (74.9 / 76.8 / 85.5%), the oracle adversarial F1-score under ex- We use the following four metrics to evaluate our haustive testing (25.8 / 74.6 / 81.6%) is much lower models: i) test set F1-score (called Acc.), ii) ad- in SST-character / SST-word / AG-character level, versarial test F1-score (called Adv. Acc.), which respectively. For illustration, we show some con- uses samples generated by HotFlip attacks on the crete adversarial examples from the HotFlip attack original test examples, iii) verified F1-score under in Table 2. For some samples, even though the IBP verification (called IBP-verified), that is, the model is robust with respect to HotFlip attacks, its ratio of test samples for which IBP can verify that predictions are incorrect for stronger adversarial the specification is not violated, and iv) exhaus- examples obtained using the exhaustive verification tively verified F1-score (called Oracle), computed oracle. This underscores the need for verification, by enumerating all possible perturbations given the as robustness with respect to suboptimal adversarial perturbation budget δ, where a sample is verifiably attacks alone might give a false sense of security. robust if the prediction is unchanged under all valid 4Note that the exhaustive oracle is not computationally perturbations. feasible beyond δ = 2 on AG News.
80 60 40 20 0 1 2 3 4 5 6 Perturbation Budget )%( ycaruccA 79.8 79.8 79.8 79.8 79.8 79.8 80 62.9 60 50.1 49.9 36.5 40 28.0 24.8 21.4 17.6 20 10.3 0.0 0.0 0.0 0.0 0.0 0.0 0 1 2 3 4 5 6 Perturbation Budget (a) Normal Training )%( ycaruccA 79.0 79.0 79.0 79.0 79.0 79.0 78.5 76.2 74.9 73.3 73.8 72.3 Acc. 59.7 Adv. Acc 41.3 Oracle 25.8 IBP-verified 0.0 0.0 0.0 0.0 0.0 0.0 (b) Adversarial Training 80 60 40 20 0 1 2 3 4 5 6 Perturbation Budget )%( ycaruccA 79.8 79.8 79.8 79.8 79.8 79.8 80 64.6 60 53.4 49.8 37.8 40 28.7 29.2 22.8 18.3 20 13.7 0.0 0.0 0.0 0.0 0.0 0.0 0 1 2 3 4 5 6 Perturbation Budget (c) Data Augmentation Training )%( ycaruccA 74.2 74.2 74.2 74.2 74.2 74.2 7 73 3. .4 3 7 73 3. .4 2 7 73 3. .1 1 73.3 73.3 73.2 Acc. 72.5 71.4 70.1 59.4 Adv. Acc 33.1 Oracle 15.8 IBP-verified (d) Verifiable Training (IBP) Figure 2: SST character-level models with different training objectives (trained at δ=3) against different perturba- tion budgets in nominal F1-score, adversarial F1-score, exhaustively verified F1-score (Oracle), and IBP verified F1-score. Note that exhaustive verification is not scalable to perturbation budget 4 and beyond. 80 60 40 20 0 1 2 3 4 5 6 Perturbation Budget )%( ycaruccA 84.8 84.8 84.8 84.8 84.8 84.8 7 76 6. .9 1 7 72 1. .4 6 7 61 9. .3 8 7 60 9. .5 3 7 60 9. .7 0 70.5 68 00 40 13.1 20 7.1 6.0 5.8 5.6 5.5 0 1 2 3 4 5 6 Perturbation Budget (a) Normal Training (GloVe) )%( ycaruccA 81.7 81.7 81.7 81.7 81.7 81.7 78.4 77.2 77.2 76.8 76.9 76.8 7 78 2. .2 5 76.8 76.5 76.3 76.3 Acc. 60.4 Adv. Acc 44.5 Oracle 31.2 22.5 IBP-verified 17.2 (b) Verifiable Training (IBP) (GloVe) 80 60 40 20 0 1 2 3 4 5 6 Perturbation Budget )%( ycaruccA 82.6 82.6 82.6 82.6 82.6 82.6 79.2 78.3 78.1 78.1 78.0 78.0 80 79.2 78.2 78.0 77.9 77.9 60 46.3 40 26.4 20.0 17.4 14.5 12.6 20 0 1 2 3 4 5 6 Perturbation Budget (c) Normal Training (CF) )%( ycaruccA 81.4 81.4 81.4 81.4 81.4 81.4 7 79 9. .7 7 7 79 9. .0 0 7 79 9. .1 0 7 79 8. .0 9 7 79 8. .0 9 79.0 76.7 72.3 Acc. 65.6 58.8 Adv. Acc 50.2 43.2 Oracle IBP-verified (d) Verifiable Training (IBP) (CF) Figure 3: SST word-level models with normal and verifiable training objectives (trained at δ=3) using GloVe and counter-fitted (CF) embeddings against different perturbation budgets in nominal F1-score, adversarial F1-score, exhaustively verified F1-score (Oracle), and IBP verified F1-score. Note that exhaustive verification is not scalable to perturbation budget 6 and beyond. Effectiveness of Simplex Bounds with IBP. SST word-level), (a) across models, there is a larger Rather than sampling individual points from the gap in adversarial F1-score and true robustness perturbation space, IBP training covers the full (oracle); (b) the difference in oracle robustness space at once. The resulting models achieve the between IBP and adversarial training is even larger highest exhaustively verified F1-score at the cost (73.1% vs. 25.8% and 76.5% vs. 74.6%). of only moderate deterioration in nominal F1-score (Table 1). At test time, IBP allows for constant-time Perturbation Budget. In Figures 2 and 3, we verification with arbitrary δ, whereas exhaustive compare normal training, adversarial training, data verification requires evaluation over an exponen- augmentation, and verifiable training models with tially growing search space. four metrics under various perturbation budgets on the SST dataset. Overall, as the perturbation bud- Perturbation Space Size. In Table 1, when the get increases, the adversarial F1-score, oracle accu- perturbation space is larger (SST character-level vs. racy, and IBP-verified F1-score decrease. We can
Perturbation radius δ =1 δ =2 δ =3 80 70 SST-word 49 674 5,136 60 SST-character 206 21,116 1,436,026 50 AG-character 722 260,282 - 40 30 Table 3: Maximum perturbation space size in the SST 20 and AG News test set using word / character substitu- 10 tions, which is the maximum number of forward passes 0 per sentence to evaluate in the exhaustive verification. 100 101 102 103 104 105 106 Computational budget (forward passes per sample) observe that even for large perturbation budgets, verifiably trained models are still able to verify a sizable number of samples. Again, although ad- versarial F1-score flattens for larger perturbation budgets in the word level experiments, oracle veri- fication can further find counterexamples to change the prediction. Note that exhaustive verification becomes intractable with large perturbation sizes. Computational Cost of Exhaustive Verification. The perturbation space in NLP problems is discrete and finite, and a valid option to verify the specifica- tion is to exhaustively generate predictions for all z ∈ X (x ), and then check if at least one does 0 in 0 not match the correct label. Conversely, such an exhaustive (oracle) approach can also identify the strongest possible attack. But the size of X grows in exponentially with δ, and exhaustive verification quickly becomes prohibitively expensive. In Table 3, we show the maximum perturbation space size in the SST and AG News test set for different perturbation radii δ. This number grows exponentially as δ increases. To further illustrate this, Figure 4 shows the number of forward passes required to verify a given proportion of the SST test set for an IBP-trained model using exhaustive veri- fication and IBP verification. IBP reaches verifica- tion levels comparable to an exhaustive verification oracle, but requires only two forward passes to ver- ify any sample – one pass for computing the upper, and one for the lower bounds. Exhaustive verifica- tion, on the other hand, requires several orders of magnitude more forward passes, and there is a tail of samples with extremely large attack spaces. 4.6 Counter-Fitted Embeddings As shown in Figures 2 and 3a, although IBP can verify arbitrary networks in theory, the ver- ification bound is very loose except for models trained to be IBP-verifiable. One possible rea- son is the potentially large volume of the perturba- tion simplex. Since representations of substitution )%( ycaruccA deifireV 72.50 71.40 70.10 δ=1 oracle δ=2 oracle δ=3 oracle Figure 4: Verified F1-score vs. computation budget for exhaustive verification oracles on the SST character- level test set, for an IBP-trained model trained with δ=3. Solid lines represent the number of forward passes re- quired to verify a given proportion of the test set using exhaustive search. Dashed lines indicate verification levels achievable using IBP verification, which comes at small and constant cost, and is thus orders of magni- tude more efficient. words/characters are not necessarily close to those of synonyms/typos in embedding space, the ver- tices of the simplex could be far apart, and thus cover a large area in representation space. There- fore, when propagating the interval bounds through the network, the interval bounds become too loose and fail to verify most of the examples if the mod- els are not specifically trained. To test this hy- pothesis, we follow Mrksˇic´ et al. (2016) and use fine-tuned GloVe embeddings trained to respect linguistic constraints; these representations (called counter-fitted embeddings) force synonyms to be closer and antonyms to be farther apart using word pairs from the PPDB database (Ganitkevitch et al., 2013) and WordNet (Miller, 1995). We repeat the word level experiments with these counter-fitted embeddings, Figures 3c and 3d show the experi- mental results. We observe that IBP verified ac- curacy is now substantially higher across models, especially for δ = 1, 2, 3. The examples which IBP can verify increase by up to 33.2% when us- ing the counter-fitted embeddings (normal training, δ = 1). Moreover, adversarial and exhaustively verified F1-score are also improved, at the cost of a mild deterioration in nominal test F1-score. The IBP-trained model also further improves both its oracle F1-score and IBP verified F1-score. These results validate our hypothesis that reducing the simplex volume via soft linguistic constraints can provide even tighter bounds for IBP, resulting in larger proportions of verified samples.
5 Discussion of security: Circumventing defenses to adversarial examples. In ICML, pages 274–283. Our experiments indicate that adversarial attacks are not always the worst adversarial inputs, which Christel Baier and Joost-Pieter Katoen. 2008. Princi- ples of Model Checking. MIT press. can only be revealed via verification. On the other hand, exhaustive verification is computationally Clark Barrett and Cesare Tinelli. 2018. Satisfiability very expensive. Our results show that using the modulo theories. In Handbook of Model Checking, proposed simplex bounds with IBP can verify a siz- pages 305–343. Springer. able amount of test samples, and can be considered Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic a potent verification method in an NLP context. We and natural noise both break neural machine transla- note however two limitations within the scope of tion. In International Conference on Learning Rep- this work: i) limited model depth: we only inves- resentations. tigated models with few layers. IBP bounds are Nicola Bertoldi, Richard Zens, and Marcello Federico. likely to become looser as the number of layers 2007. Speech translation by confusion network de- increases. ii) limited model types: we only studied coding. In Proc. ICASSP. models with CNN and fully connected layers. We focused on the HotFlip attack to showcase Rudy Bunel, Ilker Turkaslan, Philip HS Torr, Pushmeet Kohli, and M Pawan Kumar. 2017. Piecewise lin- specification verification in the NLP context, with ear neural network verification: a comparative study. the goal of understanding factors that impact its arXiv preprint arXiv:1711.00455. effectiveness (e.g. the perturbation space volume, see Section 4.6). It is worth noting that symbol Nicholas Carlini, Guy Katz, Clark Barrett, and David L substitution is general enough to encompass other Dill. 2017. Ground-truth adversarial examples. arXiv preprint arXiv:1709.10207. threat models such as lexical entailment perturba- tions (Glockner et al., 2018), and could potentially Nicholas Carlini and David Wagner. 2017. Adver- be extended to the addition of pre/postfixes (Jia and sarial examples are not easily detected: Bypassing Liang, 2017; Wallace et al., 2019). ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Secu- Interesting directions of future work include: rity, pages 3–14. ACM. tightening IBP bounds to allow applicability to deeper models, investigating bound propagation Chih-Hong Cheng, Georg Nu¨hrenberg, and Harald in other types of neural architectures (e.g. those Ruess. 2017. Maximum resilience of artificial neu- ral networks. In International Symposium on Au- based on recurrent networks or self-attention), and tomated Technology for Verification and Analysis, exploring other forms of specifications in NLP. pages 251–268. Springer. 6 Conclusion Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. 2018. Seq2Sick: Evaluating the We introduced formal verification of text classifi- robustness of sequence-to-sequence models with ad- versarial examples. CoRR, abs/1803.01128. cation models against synonym and character flip perturbations. Through experiments, we demon- Krishnamurthy Dvijotham, Sven Gowal, Robert Stan- strated the effectiveness of the proposed simplex forth, Relja Arandjelovic, Brendan O’Donoghue, bounds with IBP both during training and test- Jonathan Uesato, and Pushmeet Kohli. 2018. Train- ing, and found weaknesses of adversarial training ing verified learners with learned verifiers. arXiv preprint arXiv:1805.10265. compared with exhaustive verification. Verifiably trained models achieve the highest exhaustive veri- Christopher Dyer, Smaranda Muresan, and Philip fication F1-score on SST and AG News. IBP veri- Resnik. 2008. Generalizing word lattice transla- fies models in constant time, which is exponentially tion. In Proceedings of ACL-08: HLT, pages 1012– 1020, Columbus, Ohio. Association for Computa- more efficient than naive verification via exhaustive tional Linguistics. search. Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-box adversarial exam- References ples for text classification. In Proceedings of the 56th Annual Meeting of the Association for Compu- Anish Athalye, Nicholas Carlini, and David A. Wag- tational Linguistics (Volume 2: Short Papers), pages ner. 2018. Obfuscated gradients give a false sense 31–36. Association for Computational Linguistics.
Allyson Ettinger, Sudha Rao, Hal Daume´ III, and Yitong Li, Trevor Cohn, and Timothy Baldwin. 2017. Emily M. Bender. 2017. Towards linguistically gen- Robust training under linguistic adversity. In Pro- eralizable NLP systems: A workshop and shared ceedings of the 15th Conference of the European task. In Proceedings of the First Workshop on Build- Chapter of the Association for Computational Lin- ing Linguistically Generalizable NLP Systems. guistics: Volume 2, Short Papers, pages 21–27. Juri Ganitkevitch, Benjamin Van Durme, and Chris Xunying Liu, Xie Chen, Yongqiang Wang, Mark J. F. Callison-Burch. 2013. PPDB: The paraphrase Gales, and Philip C. Woodland. 2016. Two efficient database. In Proceedings of the 2013 Conference of lattice rescoring methods using recurrent neural net- the North American Chapter of the Association for work language models. IEEE/ACM Trans. Audio, Computational Linguistics: Human Language Tech- Speech & Language Processing, 24(8):1438–1449. nologies, pages 758–764, Atlanta, Georgia. Associa- tion for Computational Linguistics. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Max Glockner, Vered Shwartz, and Yoav Goldberg. Towards deep learning models resistant to adversar- 2018. Breaking NLI systems with sentences that re- ial attacks. In International Conference on Learning quire simple lexical inferences. In Proceedings of Representations. the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 2: Short Papers), George A. Miller. 1995. WordNet: A lexical database pages 650–655, Melbourne, Australia. Association for English. Commun. ACM, 38(11):39–41. for Computational Linguistics. Matthew Mirman, Timon Gehr, and Martin Vechev. Ian J Goodfellow, Jonathon Shlens, and Christian 2018. Differentiable abstract interpretation for prov- Szegedy. 2014. Explaining and harnessing adversar- ably robust neural networks. In Proceedings of the ial examples. arXiv preprint arXiv:1412.6572. 35th International Conference on Machine Learning, volume 80, pages 3578–3586. Sven Gowal, Krishnamurthy Dvijotham, Robert Stan- forth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Nikola Mrksˇic´, Diarmuid O´ Se´aghdha, Blaise Thom- Relja Arandjelovic, Timothy A. Mann, and Push- son, Milica Gasˇic´, Lina M. Rojas-Barahona, Pei- meet Kohli. 2018. On the effectiveness of inter- Hao Su, David Vandyke, Tsung-Hsien Wen, and val bound propagation for training verifiably robust Steve Young. 2016. Counter-fitting word vectors to models. CoRR, abs/1810.12715. linguistic constraints. In Proceedings of the 2016 Conference of the North American Chapter of the Matthew Honnibal and Ines Montani. 2017. spaCy 2: Association for Computational Linguistics: Human Natural language understanding with Bloom embed- Language Technologies, pages 142–148, San Diego, dings, convolutional neural networks and incremen- California. Association for Computational Linguis- tal parsing. To appear. tics. Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Takashi Onishi, Masao Utiyama, and Eiichiro Sumita. Zettlemoyer. 2018. Adversarial example generation 2010. Paraphrase lattice for statistical machine with syntactically controlled paraphrase networks. translation. In Proceedings of the ACL 2010 Con- In Proceedings of the 2018 Conference of the North ference Short Papers, pages 1–5, Uppsala, Sweden. American Chapter of the Association for Computa- Association for Computational Linguistics. tional Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1875–1885, New Jeffrey Pennington, Richard Socher, and Christopher Orleans, Louisiana. Association for Computational Manning. 2014. GloVe: Global vectors for word rep- Linguistics. resentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- Robin Jia and Percy Liang. 2017. Adversarial exam- ing (EMNLP), pages 1532–1543. Association for ples for evaluating reading comprehension systems. Computational Linguistics. In Empirical Methods in Natural Language Process- ing (EMNLP). Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Zettlemoyer. 2018. Deep contextualized word rep- Mykel J Kochenderfer. 2017. Reluplex: An efficient resentations. In Proceedings of the 2018 Confer- SMT solver for verifying deep neural networks. In ence of the North American Chapter of the Associ- International Conference on Computer Aided Verifi- ation for Computational Linguistics: Human Lan- cation, pages 97–117. Springer. guage Technologies, Volume 1 (Long Papers), pages Diederik P. Kingma and Jimmy Ba. 2015. Adam: A 2227–2237, New Orleans, Louisiana. Association method for stochastic optimization. International for Computational Linguistics. Conference on Learning Representations. Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2018. Certified defenses against adversarial exam- 2016. Adversarial examples in the physical world. ples. In International Conference on Learning Rep- arXiv preprint arXiv:1607.02533. resentations.
Marco Tulio Ribeiro, Sameer Singh, and Carlos certified robustness for ReLU networks. In Proceed- Guestrin. 2018. Semantically equivalent adversar- ings of the 35th International Conference on Ma- ial rules for debugging NLP models. In Proceedings chine Learning, volume 80 of Proceedings of Ma- of the 56th Annual Meeting of the Association for chine Learning Research, pages 5276–5285. PMLR. Computational Linguistics (Volume 1: Long Papers), pages 856–865. Association for Computational Lin- Eric Wong and Zico Kolter. 2018. Provable defenses guistics. against adversarial examples via the convex outer ad- versarial polytope. In International Conference on Dinghan Shen, Guoyin Wang, Wenlin Wang, Mar- Machine Learning, pages 5283–5292. tin Renqiang Min, Qinliang Su, Yizhe Zhang, Chun- yuan Li, Ricardo Henao, and Lawrence Carin. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. 2018. Baseline needs more love: On simple Character-level convolutional networks for text clas- word-embedding-based models and associated pool- sification. In Advances in Neural Information Pro- ing mechanisms. In Proceedings of the 56th An- cessing Systems, pages 649–657. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 440– 450, Melbourne, Australia. Association for Compu- tational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 Conference on Empirical Methods in computer vision, pages 1631–1642. Association for Computational Linguistics. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199. Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. 2019. Ro- bustness may be at odds with F1-score. In Interna- tional Conference on Learning Representations. Jonathan Uesato, Brendan O’Donoghue, Pushmeet Kohli, and Aron van den Oord. 2018. Adversarial risk and the dangers of evaluating against weak at- tacks. In ICML, pages 5032–5041. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard- ner, and Sameer Singh. 2019. Universal trigger se- quences for attacking and analyzing NLP. In Em- pirical Methods in computer vision (EMNLP). Chenglong Wang, Rudy Bunel, Krishnamurthy Dvi- jotham, Po-Sen Huang, Edward Grefenstette, and Pushmeet Kohli. 2019. Knowing when to stop: Eval- uation and verification of conformity to output-size specifications. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR). Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018. Formal security analysis of neural networks using symbolic inter- vals. In 27th USENIX Security Symposium (USENIX Security 18), pages 1599–1614, Baltimore, MD. USENIX Association. Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and In- derjit Dhillon. 2018. Towards fast computation of
A Experimental Setup this oracle is extremely computationally expensive (especially in character-level perturbations). On the A.1 Dataset Statistics other hand, verification via IBP provides a lower The SST dataset consists of 67,349 training, 872 bound on the worst-case results, but this is gener- validation, and 1,821 test samples with binary senti- ally loose for arbitrary networks. IBP-verifiable ment annotations. The AG News contains 120,000 training succeeds in tightening these bounds and training and 7,600 test samples with 4 classes. results in much improved rates of IBP-verification at test time, compared to all other training methods. A.2 Detailed Setup We furthermore can observe that models trained We select model architectures to achieve a reason- to become verifiable (with IBP training objective) able tradeoff (Tsipras et al., 2019) between nom- achieve better adversarial F1-score and exhaus- inal F1-score and robust F1-score using the vali- tively verified F1-score, with a small (or no) dete- dation set. In the SST word-level experiments, we rioration in nominal F1-score compared to normal use a 1-layer convolutional network with 100 ker- training. nels of width 5, followed by a ReLU, an average B.2 SST Word Embeddings Comparison pool, and a linear layer. We use pre-trained 300- dimensional GloVe embeddings (Pennington et al., In Figures 5 and 6, we show the experimental re- 2014), and use counter-fitted embeddings (Mrksˇic´ sults of different models and metrics using GloVe et al., 2016) in Section 4.6. The pre-trained word and counter-fitted embeddings, respectively. embeddings are fixed during training. In the SST B.3 AG News character-level experiments, we use a 1-layer con- volutional network with 100 kernels of width 5, In Figure 7, we compare normal training, adver- followed by a ReLU, an average pool, followed sarial training, data augmentation, and verifiable by a linear layer. We set the character embedding training models with four metrics under various dimension to 150, randomly initialise them, and perturbation budgets on the AG News dataset at the fine-tune the embeddings during training. In the character level. In Figure 7d, our verifiable trained AG News character-level experiments, we follow model achieves not only the strongest adversarial the setup in Zhang et al. (2015) using lower-case and oracle F1-score, but achieves very tight bounds letters only and truncate the character sequences to with respect to the oracle results. Note IBP ver- have at most 300 characters during training. We ification only requires 2 forward passes to verify use a 1-layer convolutional network with 100 ker- any examples, whereas oracle evaluation (exhaus- nels of width 10, followed by a ReLU, an average tive search) uses up to 260,282 forward passes for pool, and two fully-connected layers with 100 hid- examining a single example at δ = 2. den units, followed by a linear layer. We set the character embedding dimension to 150, randomly initialise them, and fine-tune the embeddings dur- ing training. Note since the proposed technique is efficient, we can scale up to deeper networks for better nominal F1-score at the cost of verified F1-score, as the bounds become looser. We use Adam (Kingma and Ba, 2015) as our optimisation method, perform early stopping, and tune our hyperparameters (learning rate, loss ratio κ) on the validation set. B Additional Experimental Results and Discussion B.1 Ease of Verification (Computation of True Robustness) For every training method, we can compute the true robustness using exhaustive verification. However,
80 60 40 20 0 1 2 3 4 5 6 Perturbation Budget )%( ycaruccA 84.8 84.8 84.8 84.8 84.8 84.8 7 76 6. .9 1 72.4 71.3 70.5 70.7 70.5 80 71.6 69.8 69.3 69.0 60 40 13.1 20 7.1 6.0 5.8 5.6 5.5 0 1 2 3 4 5 6 Perturbation Budget (a) Normal Training )%( ycaruccA 85.0 85.0 85.0 85.0 85.0 85.0 80.0 78.1 76.8 76.7 76.4 76.7 78.9 76.4 74.6 73.9 73.9 Acc. Adv. Acc Oracle 9.9 6.4 5.8 5.6 5.6 5.6 IBP-verified (b) Adversarial Training 80 60 40 20 0 1 2 3 4 5 6 Perturbation Budget )%( ycaruccA 85.4 85.4 85.4 85.4 85.4 85.4 7 77 7. .7 2 7 74 3. .5 1 7 72 1. .7 6 7 72 1. .4 2 7 72 1. .2 0 72.4 80 60 40 20 11.9 6.8 6.2 5.9 5.9 5.9 0 1 2 3 4 5 6 Perturbation Budget (c) Data Aug. Training )%( ycaruccA 81.7 81.7 81.7 81.7 81.7 81.7 78.4 77.2 77.2 76.8 76.9 76.8 7 78 2. .2 5 76.8 76.5 76.3 76.3 Acc. 60.4 Adv. Acc 44.5 Oracle 31.2 22.5 IBP-verified 17.2 (d) Verifiable Training (IBP) Figure 5: SST word-level models with different training objectives (trained at δ=3) using GloVe embeddings against different perturbation budgets in nominal F1-score, adversarial F1-score, exhaustively verified F1-score (Oracle), and IBP verified F1-score. Note that exhaustive verification is not scalable to perturbation budget 6 and beyond. 80 60 40 20 0 1 2 3 4 5 6 Perturbation Budget )%( ycaruccA 82.6 82.6 82.6 82.6 82.6 82.6 79.2 78.3 78.1 78.1 78.0 78.0 80 79.2 78.2 78.0 77.9 77.9 60 46.3 40 26.4 20.0 17.4 14.5 12.6 20 0 1 2 3 4 5 6 Perturbation Budget (a) Normal Training )%( ycaruccA 84.0 84.0 84.0 84.0 84.0 84.0 81.8 81.1 80.7 80.7 80.7 80.7 81.8 80.9 80.6 80.5 80.5 Acc. 44.6 Adv. Acc 27.0 Oracle 20.1 16.6 14.5 12.5 IBP-verified (b) Adversarial Training 80 60 40 20 0 1 2 3 4 5 6 Perturbation Budget )%( ycaruccA 84.7 84.7 84.7 84.7 84.7 84.7 80.2 79.1 78.6 78.6 78.5 78.6 80 80.1 79.0 78.5 78.4 78.4 60 44.4 40 26.7 19.4 16.5 13.9 12.1 20 0 1 2 3 4 5 6 Perturbation Budget (c) Data Aug. Training )%( ycaruccA 81.4 81.4 81.4 81.4 81.4 81.4 7 79 9. .7 7 7 79 9. .0 0 7 79 9. .1 0 7 79 8. .0 9 7 79 8. .0 9 79.0 76.7 72.3 Acc. 65.6 58.8 Adv. Acc 50.2 43.2 Oracle IBP-verified (d) Verifiable Training (IBP) Figure 6: SST word-level models (trained at δ=3) using counter-fitted embeddings against different perturba- tion budgets in nominal F1-score, adversarial F1-score, exhaustively verified F1-score (Oracle), and IBP verified F1-score. Note that exhaustive verification is not scalable to perturbation budget 6 and beyond.
80 60 40 20 0 1 2 3 4 5 6 Perturbation Budget )%( ycaruccA 89.5 89.5 89.5 89.5 89.5 89.5 82.6 75.4 80 78.9 67.4 65.1 59.0 51.7 60 45.1 40 20 0.0 0.0 0.0 0.0 0.0 0.0 0 1 2 3 4 5 6 Perturbation Budget (a) Normal Training )%( ycaruccA 90.5 90.5 90.5 90.5 90.5 90.5 8 88 6. .0 3 8 85 1. .5 6 82.7 80.1 77.6 75.1 Acc. Adv. Acc Oracle IBP-verified 0.0 0.0 0.0 0.0 0.0 0.0 (b) Adversarial Training 80 60 40 20 0 1 2 3 4 5 6 Perturbation Budget )%( ycaruccA 88.4 88.4 88.4 88.4 88.4 88.4 95 83.4 77.5 90 80.9 72.0 71.1 64.5 85 57.2 80 51.2 75 70 65 60 0.0 0.0 0.0 0.0 0.0 0.0 55 1 2 3 4 5 6 Perturbation Budget (c) Data Augmentation Training )%( ycaruccA 87.55 87.55 87.55 87.55 87.5587.55 87.16 87.13 87.08 87.08 87.0587.04 Acc. 8 87 6. .1 94 6 8 87 6. .0 59 8 85.87 83.71 Adv. Acc 76.74 Oracle IBP-verified 59.13 (d) Verifiable Training (IBP) Figure 7: AG News character-level models with different training objectives (trained at δ=3) against different perturbation budgets in nominal F1-score, adversarial F1-score, exhaustively verified F1-score (Oracle), and IBP verified F1-score. Note that exhaustive verification is not scalable to perturbation budget 3 and beyond.
