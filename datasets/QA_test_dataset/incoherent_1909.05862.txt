Learning Symbolic Physics with Graph Networks Miles D. Cranmer Rui Xu Peter Battaglia Princeton University Princeton University DeepMind Princeton, NJ, USA Princeton, NJ, USA London, UK mcranmer@princeton.edu ruix@princeton.edu peterbattaglia@google.com Shirley Ho Flatiron Institute New York City, NY, USA shirleyho@flatironinstitute.org Abstract We introduce an approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot gen- eralization. Our experiments show that our graph network models, which imple- ment this inductive bias, can learn message representations equivalent to the true force vector when trained on n-body gravitational and spring-like simulations. We use symbolic regression to fit explicit algebraic equations to our trained model’s message function and recover the symbolic form of Newton’s law of gravitation without prior knowledge. We also show that our model generalizes better at infer- ence time to systems with more bodies than had been experienced during training. Our approach is extensible, in principle, to any unknown interaction law learned by a graph network, and offers a valuable technique for interpreting and inferring explicit causal theories about the world from implicit knowledge captured by deep learning. 1 Introduction Discovering laws through observation of natural phenomenon is the central challenge of the sci- ences. Modern deep learning also involves discovering knowledge about the world but focuses mostly on implicit knowledge representations, rather than explicit and interpretable ones. One rea- son is that the goal of deep learning is often optimizing test F1-score and learning efficiency in narrowly specified domains, while science seeks causal explanations and general-purpose knowl- edge across a wide range of phenomena. Here we explore an approach for imposing physically motivated inductive biases on neural networks, training them to predict the dynamics of physical systems and interpreting their learned representations and computations to discover the symbolic physical laws which govern the systems. Moreover, our results also show that this approach im- proves the generalization performance of the learned models. The first ingredient in our approach is the “graph network” (GN) [Battaglia et al., 2018], a type of graph neural network [Scarselli et al., 2009, Bronstein et al., 2017, Gilmer et al., 2017], which is effective at learning the dynamics of complex physical systems [Battaglia et al., 2016, Chang et al., 2016, Sanchez-Gonzalez et al., 2018, Mrowca et al., 2018, Li et al., 2018, Kipf et al., 2018]. We impose inductive biases on the architecture and train models with supervised learning to predict the dynamics of 2D and 3D n-body gravitational systems and a hanging string. If the trained models can accurately predict the physical dynamics of held-out test data, we can assume they have dis- covered some level of general-purpose physical knowledge, which is implicitly encoded in their Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019), Vancouver, Canada. 9102 voN 1 ]GL.sc[ 2v26850.9091:viXra
weights. Crucially, we recognize that the forms of the graph network’s message and pooling func- tions have correspondences to the forms of force and superposition in classical mechanics, respec- tively. The message pooling is what we call a “linearized latent space:” a vector space where latent representations of the interactions between bodies (forces or messages) are linear (summable). By imposing our inductive bias, we encourage the GN’s linearized latent space to match the true one. Some other interesting approaches for learning low-dimensional general dynamical models include Packard et al. [1980], Daniels and Nemenman [2015], and Jaques et al. [2019]. The second ingredient is using symbolic regression — we use eureqa from Schmidt and Lipson [2009] — to fit compact algebraic expressions to a set of inputs and messages produced by our trained model. eureqa works by randomly combining mathematical building blocks such as math- ematical operators, analytic functions, constants, and state variables, and iteratively searches the space of mathematical expressions to find the model that best fits a given dataset. The resulting symbolic expressions are interpretable and readily comparable with physical laws. The contributions of this paper are: 1. A modified GN with inductive biases that promote learning general-purpose physical laws. 2. Using symbolic regression to extract analytical physical laws from trained neural networks. 3. Improved zero-shot generalization to larger systems than those in training. 2 Model Graph networks are a type of deep neural network which operates on graph-structured data. The format of the graphs on which GNs operate is defined as 3-tuples1, G = (u, V, E), where: u ∈ RLu is a global attribute vector of length Lu, V = {v } is a set of node attribute vectors, v ∈ RLv of length Lv, and i i=1:Nv i E = {(e , r , s )} is a set of edge attribute vectors, e ∈ RLe of length Le, and indices k k k k=1:Ne k r , s ∈ {1:N v} of the “receiver” and “sender” nodes connected by the k-th edge. k k Our GN implementation is depicted in fig. 1. Note: it does not include global and edge attributes. This GN processes a graph by first computing pairwise interactions, or “messages”, e(cid:48) , between k nodes connected by edges, with a “message function”, φe : RLv ×RLv ×RLe → RLe(cid:48) . Next, the set of messages incident on each i-th receiver node are pooled into ¯e(cid:48) = ρe→v({e(cid:48) } ), where i k rk=i,k=1:Ne ¯e(cid:48) ∈ RLe(cid:48) , and ρe→v is a permutation-invariant operation which can take variable numbers of input i vectors, such as elementwise summation. Finally, the pooled messages are used to compute node updates, v(cid:48), with a “node update function”, φv : RLv × RLe(cid:48) → RLv(cid:48) . Our specific architectural i implementation is very similar to the “interaction network” (IN) variant [Battaglia et al., 2016]. The forms of φe, ρe→v, φv, and the associated input and output attribute vectors have correspon- dences to Newton’s formulation of classical mechanics, which motivated the original development of INs. The key observation is that e(cid:48) could learn to correspond to the force vector imposed on k the r -th body due to its interaction with the s -th body. In our examples, the force vector is equal k k to the derivative of the Lagrangian: δL , and this could be generally imposed if one knows d ( δL ) δq dt δq˙ and manually integrates the ODE with the output of the graph net. In a general n-body gravitational system in n dimensions, note that the forces are minimally represented in an Rn vector space. Thus, if Le(cid:48) = n, we exploit the GN’s “linearized latent space” for physical interpretability: we encourage e(cid:48) to be the force. k We sketch a non-rigorous proof-like demonstration of our hypothesis. Newtonian mechanics pre- scribes that force vectors, f ∈ F, can be summed to produce a net force, (cid:80) f = ¯f ∈ F, k k k which can then be used to update the dynamics of a body. Our model uses the i-th body’s pooled messages, ¯e(cid:48) , to update the body’s velocity via Euler integration, v(cid:48) = v + φv(v , ¯e(cid:48) ). If we as- i i i i i sume our GN is trained to predict velocity updates perfectly for any number of bodies, this means ¯f = (cid:80) f = φv((cid:80) e(cid:48) ) = φv(¯e(cid:48) ), where φv(·) = φv(v , ·). We have the result for ai single r ink= tei rak ction: ¯i f =rk f=i k = φi v(ei (cid:48) ) = i φv(¯e(cid:48) ). Thusi , we can substitute into the multi-interaction case:i (cid:80) k,r φk= v(i e(cid:48) ) =i φvk (, ¯erk (cid:48)= ) i = φv((cid:80)i i e(cid:48) ), and so φv has to be a linear rk=i i k i i i rk=i k i 1We adhere closely to the notation used in Battaglia et al. [2018] to formalize our model. 2
Single timestep Nodes Node Pairs First MLP Convert to Symbolic Equation Minimize ( ) via Symbolic Regression Messages ( ) Dimension Summed Message Second MLP ( ) Node Update meta-optimization synthesis to learn the Updated simulator Nodes Next timestep Figure 1: A schematic depicting how we extract physical knowledge from a GN. transformation. Therefore, for cases where φv is invertible (mapping between the same dimensional i space), e(cid:48) = (φv)−1(f ), and so the message vectors are linear transformations of the true forces k i k when Le(cid:48) = D. We demonstrate this hypothesis on trained GNs in section 3. 3 Experiments We set up 100,000 simulations with random masses and initial conditions for both a 1/r and 1/r2 force law in 2D, a 1/r2 law in 3D, and a string with an r2 force law between nodes in 2D with a global gravity, for 1000 time steps each. These laws are chosen arbitrarily as examples of different symbolic forms. The three n-body problems have six bodies in their training set, and the string has ten nodes, of which the two end nodes are fixed. We train a GN on each of these problems where we choose Le(cid:48) = D, i.e., the length of the message vectors in the GN matches the dimensionality of the force vectors: 2 for the 2D simulations and 3 for the 3D simulations. Our GN, a pure TensorFlow [Abadi et al., 2015] model, has both φe and φv as three-hidden-layer multilayer perceptrons (MLPs) with 128 hidden nodes per layer with ReLU activations. We optimize the L1 loss between the predicted velocity update and the true velocity update of each node. Once we have a trained model, we record the messages, e(cid:48) , for all bodies over 1000 new simulations k for each environment. We fit a linear combination of the vector components of the true force, f , to k each component of e(cid:48) , as can be seen in fig. 2 for 1/r. The results for each system show that the e(cid:48) k k vectors have learned to be a linear combination of the components when Le(cid:48) = D. We see similar linear relations for all other simulations. We are also able to find the force law when it is unknown by using symbolic regression to fit an algebraic function that approximates φe. We demonstrate this on the trained GN for the 1/r problem using eureqa from Schmidt and Lipson [2009] to fit algebraic equations that fit the message. We allow it to use algebraic operators +, −, ×, /, as well as input variables (∆x and ∆y for component separation, r for distance, and m for sending body mass) and real constants. Complexity is scored 2 by counting the number of occurences of each operator, constant, and input variable. This returns a list of the models with the lowest mean square error at each complexity. We parametrize Occam’s razor to find the “best” algebraic model by first sorting the best models by complexity, and then taking the model that maximizes the fractional drop in mean square error (MSE) over the next simplest model: arg max (−∆ log(MSE )/∆c), where c is the complexity. The best model found c c by the symbolic regression for the first output element of φe is (0.46m ∆y − 1.55m ∆x)/r2, 2 2 3
Figure 2: These plots demonstrate that the graph network’s messages have learned to be linear transformations of the two vector components of the true force: f and f , for the 1/r law in 2D. x y which is a linear combination of the components of the true force, m rˆ/r. We can see this is 2 approximately the same linear transformation as the components in the left plot of fig. 2, but this algebraic expression was learned from scratch. We now test whether the GN will generalize to more nodes better than a GN with a larger Le(cid:48). This is because it is possible for a GN to “cheat” with a high dimension message-passing space, trained on a fixed number of bodies. One example of cheating would be for φe to concatenate each sending node’s properties along the message, and φv to calculate forces from these and add them. When a new body is added, this calculation might break. While it is still possible for φe to develop an elaborate encoding scheme with Le(cid:48) = D to cheat at this problem, it seems more natural for φe to learn the true force when Le(cid:48) = D and therefore show improved generalization to a greater number of nodes. Figure 3: These plots demonstrate the improvement in generalization from minimizing the message passing space. The loss of GNs with different message-passing space dimension (Le(cid:48)), trained on a 6-body and 4-body system, in the left and right plots, respectively (indicated by the vertical line), are tested on a variable number of bodies in a 1/r2 simulation in 3D. We test the hypothesis of better generalization with Le(cid:48) = D in fig. 3, by training GNs with different Le(cid:48) on the 3D 1/r2 simulations. The observed trend is that systems with Le(cid:48) > D see their loss blow up with a larger number of bodies — presumably because they have “cheated” slightly and not learned the force law in φe but in a combination of φe and φv, whereas the Le(cid:48) ∈ {2, 3} systems’ φe has learned a projection of the true forces and is able to generalize better for greater number of bodies. A conclusion of this may be that one can optimize GNs by minimizing Le(cid:48) to the known minimum dimension required to transmit information (e.g., 3 for 3D forces), or, if this dimension is unknown, until the loss drops off. 4
4 Conclusion We have demonstrated an approach for imposing physically motivated inductive biases on graph net- works to learn interpretable representations and improved zero-shot generalization. We have shown through experiment that our graph network models which implement this inductive bias can learn message representations equivalent to the true force vector for n-body gravitational and spring-like simulations in 2D and 3D. We also have demonstrated a generic technique for finding an unknown force law: symbolic regression models to fit explicit algebraic equations to our trained model’s mes- sage function. Because GNs have more explicit sub-structure than their more homogeneous deep learning relatives (e.g., plain MLPs, convolutional networks), we can draw more fine-grained inter- pretations of their learned representations and computations. Finally, we have demonstrated that our model generalizes better at inference time to systems with more bodies than had been experienced during training. Acknowledgments: Miles Cranmer and Rui Xu thank Professor S.Y. Kung for insightful suggestions on early work, as well as Zejiang Hou for his comments on an early presentation. Miles Cranmer would like to thank David Spergel for advice on this project, and Thomas Kipf, Alvaro Sanchez, and members of the DeepMind team for helpful comments on a draft of this paper. We thank the referees for insightful comments that both improved this paper and inspired future work. References M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefow- icz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane´, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Va- sudevan, F. Vie´gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https: //www.tensorflow.org/. Software available from tensorflow.org. P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in neural information processing systems, pages 4502–4510, 2016. P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tac- chetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017. M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum. A compositional object-based approach to learning physical dynamics. arXiv preprint arXiv:1612.00341, 2016. B. C. Daniels and I. Nemenman. Automated adaptive inference of phenomenological dynamical models. Nature Communications, 6(1):1–8, Aug. 2015. ISSN 2041-1723. J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pages 1263–1272. JMLR. org, 2017. M. Jaques, M. Burke, and T. Hospedales. Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video. arXiv:1905.11169 [cs], May 2019. T. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel. Neural relational inference for interacting systems. arXiv preprint arXiv:1802.04687, 2018. Y. Li, J. Wu, R. Tedrake, J. B. Tenenbaum, and A. Torralba. Learning particle dynamics for manip- ulating rigid bodies, deformable objects, and fluids. arXiv preprint arXiv:1810.01566, 2018. D. Mrowca, C. Zhuang, E. Wang, N. Haber, L. F. Fei-Fei, J. Tenenbaum, and D. L. Yamins. Flex- ible neural representation for physics prediction. In Advances in Neural Information Processing Systems, pages 8799–8810, 2018. 5
N. H. Packard, J. P. Crutchfield, J. D. Farmer, and R. S. Shaw. Geometry from a time series. Physical review letters, 45(9):712, 1980. A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia. Graph networks as learnable physics engines for inference and control. arXiv preprint arXiv:1806.01242, 2018. F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009. M. Schmidt and H. Lipson. Distilling free-form natural laws from experimental data. Science, 324 (5923):81–85, 2009. 6
