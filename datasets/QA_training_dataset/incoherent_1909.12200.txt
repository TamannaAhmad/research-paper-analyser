Scaling data-driven robotics with reward sketching and batch supervised learning Serkan Cabi, Sergio Gómez Colmenarejo, Alexander Novikov, Ksenia Konyushkova, Scott Reed, Rae Jeong, Konrad Z˙ ołna, Yusuf Aytar, David Budden, Mel Vecerik, Oleg Sushkov, David Barker, Jonathan Scholz, Misha Denil, Nando de Freitas, Ziyu Wang Deepmind Abstract—By harnessing a growing dataset of robot experience, 1 we learn control policies for a diverse and increasing set of related manipulation tasks. To make this possible, we introduce reward sketching: an effective way of eliciting human preferences Demonstrations 5 to learn the reward function for a new task. This reward 2 function is then used to retrospectively annotate all historical data, collected for different tasks, with predicted rewards for the new task. The resulting massive annotated dataset can then Evaluation Reward Sketching be used to learn manipulation policies with batch supervised learning (RL) from visual input in a completely off-line way, i.e., without interactions with the real robot. This approach makes it 4 3 possible to scale up RL in robotics, as we no longer need to run NeverEnding the robot for each step of learning. We show that the trained storage batch RL agents, when deployed in real robots, can perform a variety of challenging tasks involving multiple interactions among Batch RL Learning Reward rigid or deformable objects. Moreover, they display a significant degree of robustness and generalization. In some cases, they even Fig. 1: Our cyclical approach for never-ending collection of outperform human teleoperators. data and continual learning of new tasks consists of five I. INTRODUCTION stages: (1) generation of observation-action pairs by either Deep learning has successfully advanced many areas of teleoperation, scripted policies or trained agents, (2) a novel artificial intelligence, including vision [39, 26], speech recog- interactive approach for eliciting preferences for a specific new nition [24, 46, 4], computer vision [17], and rein- task, (3) learning the reward function for the new task and forcement learning (RL) [49, 63]. The success of deep learning applying this function to automatically label all the historical in each of these fields was made possible by the availability of data, (4) applying batch RL to learn policies purely from huge amounts of labeled training data. Researchers in vision the massive growing dataset, without online interaction, and and language can easily train and evaluate deep neural networks (5) evaluation of the learned policies. on standard datasets with crowdsourced annotations such as MNIST [58], COCO [45] and CLEVR [33]. In simulated of all historical data with any of the learned reward functions, environments like video games, where experience and rewards and (iii) harnessing the large annotated datasets to learn policies are easy to obtain, deep RL is tremendously successful in purely from stored data via batch RL. outperforming top skilled humans by ingesting huge amounts Existing RL approaches for real-world robotics mainly of data [63, 69, 9]. The OpenAI Five DOTA bot [9] processes focus on tasks where hand-crafted reward mechanisms can 180 years of simulated experience every day to play at a be developed. Simple behaviours such as learning to grasp professional level. Even playing simple Atari games typically objects [35] or learning to fly [23] by avoiding crashing can requires 40 days of game play [49]. In contrast, in robotics be acquired by reward engineering. However, as the task we lack abundant data since data collection implies execution complexity increases, this approach does not scale well. We on a real robot, which cannot be accelerated beyond real time. propose a novel way to specify rewards that allows to generate Furthermore, task rewards do not naturally exist in the real- reward labels for a large number of diverse tasks. Our approach world robotics as it is the case in simulated environments. relies on human judgments about progress towards the goal to The lack of large datasets with reward signals has limited the train task-specific reward functions. Annotations are elicited effectiveness of deep RL in robotics. from humans in the form of per-timestep reward annotations This paper presents a data-driven approach to apply deep using a process we call reward sketching, see Fig. 2. The RL effectively to learn to perform manipulation tasks on real sketching procedure is intuitive for humans, and allows them robots from vision. Our solution is illustrated in Fig. 1. At its to label many timesteps rapidly and accurately. We use the heart are three important ideas: (i) efficient elicitation of user human annotations to train a ranking reward model, which is preferences to learn reward functions, (ii) automatic annotation then used to annotate all other episodes. 0202 nuJ 4 ]OR.sc[ 3v00221.9091:viXra
Fig. 2: Reward sketching procedure. Sketch of a reward function for stack_green_on_red task. A video sequence (top) with a reward sketch (bottom), shown in blue. Reward is the perceived progress towards achieving the target task. The annotators are instructed to indicate successful timesteps with reward high enough to reach green area. NeverEnding storage episode 1 reward sketch 1 episode 2 reward sketch 2 Learning Reward episode 3 reward sketch 3 predicted rewards Fig. 3: Retrospective reward assignment. The reward function Fig. 4: Each row is an example episode of a successful is learned from a limited set of episodes with reward sketches. task illustrating: (1) the ability to recover from a mistake The learned reward function is applied to a massive dataset of in stack_green_on_red task, (2) robustness to adversarial episodes from NeverEnding Storage. All historical episodes perturbations in the same task, (3) generalization to unseen are now labelled with a newly learned reward function. initial conditions in the same task, 4) generalizing to previously unseen objects in a lift_green task, (5) the ability to lift To generate enough data to train data-demanding deep deformable objects, (6) inserting a USB key, (7) inserting a neural network agents, we record experience continuously and USB key despite moving the target computer. persistently, regardless of the purpose or quality of the behavior. We collected over 400 hours of multiple-camera videos (Fig. 6), to tightly close the loop of human input, reward learning proprioception, and actions from behavior generated by human and policy learning poses substantial engineering challenges. teleoperators, as well as random, scripted and trained policies. Nevertheless, this work is essential to advance the data-driven By using deep reward networks obtained as a result of reward robotics. For example, we store all robot experience including sketching, it becomes possible to retrospectively assign rewards demonstrations, behaviors generated by trained policies or to any past or future experience for any given task. Thus, the scripted random policies. To be useful in learning, this data learned reward function allows us to repurpose a large amount needs to be appropriately annotated and queried. This is of past experience using a fixed amount of annotation effort per achieved thanks to a design of our storage system dubbed task, see Fig. 3. This large dataset with task-specific rewards NeverEnding Storage (NES). can now be used to harness the power of deep batch RL. This multi-component system (Fig. 1) allows us to solve For any given new task, our data is necessarily off-policy, and a variety of challenging tasks (Fig. 4) that require skillful is typically off-task (i.e., collected for other tasks). In this case, manipulation, involve multi-object interaction, and consist of batch RL [41] is a good method to learn visuomotor policies. many time steps. An example of such task is stacking arbitrarily Batch RL effectively enables us to learn new controllers without shaped objects. In this task, small perturbations at the beginning execution on the robot. Running RL off-line gives researchers can easily cause failure later: The robot not only has to achieve several advantages. Firstly, there is no need to worry about wear a successful grasp, but it must also grasp the first object in a and tear, limits of real-time processing, and many of the other way that allows for safe placement on top of the second object. challenges associated with operating real robots. Moreover, Moreover, the second object may have a small surface area researchers are empowered to train policies using their batch which varies how demanding the task is. Learning policies RL algorithm of choice, similar to how vision researchers are directly from pixels makes the task more challenging, but empowered to try new methods on MNIST. To this end, we eliminates the need for feature engineering and allows for release datasets [16] with this paper. additional generalization capacity. While some of our tasks can The integration of all the elements into a scalable system be solved effectively with scripted policies, learning policies
that generalize to arbitrary shapes, sizes, textures and materials G Task Specific Evaluate remains a formidable challenge, and hence the focus of this Task Agnostic paper is on making progress towards meeting this challenge. F A As shown in Fig. 4, the policies learned with our approach Execute Teleoperate Robot solve a variety of tasks including lifting and stacking of rigid/deformable objects, as well as USB insertion. Importantly, B Cloud E NeverEnding thanks to learning from pixels, the behaviour generalizes to Learn Q/pi Storage new object shapes and to new initial conditions, recovers from mistakes and is robust to some real-time adversarial interference. D Labeled C Fig. 9 shows that the learned policies can also solve tasks more Learn R Sketch Experience effectively than human teleoperators. To better view our results and general approach, we highly recommend watching the No Human Operator Human Operator accompanying video on the project website. The remainder of this paper is organized as follows. Sec. II Fig. 5: Structure of the data-driven workflow. Each step is introduces the methods, focusing on reward sketching, reward described in Sec. II and the figure highlights which steps are learning and batch RL, but also provides the bigger context performed on the robot or not, involving human operator or highlighting the engineering contributions. Sec. III is devoted not and if they are task-specific or task-agnostic. to describing our experimental setup, network architectures, providing examples of successful behavior with high rewards, benchmark results, and an interactive insertion task of industrial which are also easy to interpret and judge for humans. In RL, relevance. Sec. IV explores some of the related work. we circumvent the problem of exploration: Instead of requiring II. METHODS that the agent explores the state space autonomously, we use expert knowledge about the intended outcome of the task to The general workflow is illustrated in Fig. 1 and a more guide the agent. In addition to full episodes of demonstrations, detailed procedure is presented in Fig. 5. NES accumulates when an agent controls the robot, interactive interventions can a large dataset of task-agnostic experience. A task-specific be also performed: A human operator can take over from, or reward model allows us to retrospectively annotate data in return control to, an agent at any time. This data is useful for NES with reward signals for a new task. With rewards, we can fixing particular corner cases that the agents might encounter. then train batch RL agents with all the data in NES. The robot is controlled with a 6-DoF mouse with an The procedure for training an agent to complete a new task additional gripper button (see the video) or hand-held virtual has the following steps which are described in turn in the reality controller. A demonstrated sequence contains pairs of remainder of the section: observations and corresponding actions for each time step t: A. A human teleoperates the robot to provide first-person ((x , a ), . . . , (x , a ), . . . , (x , a )). Observations x contain 0 0 t t T T t demonstrations of the target task. all available sensor data including raw pixels from multiple B. All robot experience, including demonstrations, is accu- cameras as well as proprioceptive inputs (Fig. 6). mulated into NES. B. NeverEnding Storage C. Humans annotate a subset of episodes from NES (includ- ing task-specific demos) with reward sketches. NES captures all of the robot experience generated across D. A reward model for the target task is trained using the all tasks in a central repository. This allows us to make use fixed amount of labelled experience. of historical data each time when learning a new target task, E. An agent for the target task is trained using all experience instead of generating a new dataset from scratch. NES includes in NES, using the predicted reward values. teleoperated trajectories for various tasks, human play data, F. The resulting policy is deployed on a real robot, while and experience from the execution of either scripted or learned recording more data into NES. can further be annotated. policies. For every trajectory we store recordings from several G. Occasionally we select an agent for careful evaluation, to cameras and sensors in the robot cage (Fig. 6). The main track overall progress on the task. innovation in NES is the introduction of a rich metadata system into the RL training pipeline. It is implemented as a relational A. Teleoperation database that can be accessed using SQL-type queries. We To specify a new target task, a human operator first attach environment and policy metadata to every trajectory remotely controls the robot to provide several successful (e.g., date and time of operation), as well as arbitrary human- (and occasionally unsuccessful1) examples of completing readable labels and reward sketches. This information allows the task. By employing the demonstration trajectories, we us to dynamically retrieve and slice the data relevant for a facilitate both reward learning and supervised learning particular stage of our training pipeline. tasks. Demonstrations help to bootstrap the reward learning by C. Reward Sketching The second step in task specification is reward sketching. 1We notice that in our dataset around 15% of human demonstrations fail to accomplish the task at the end of the episode. We ask human experts to provide per-timestep annotations
of reward using a custom user interface. As illustrated in by the following two hinge losses: Fig. 2, the user draws a curve indicating the progress towards L (ψ) = max {0, r (x ) − r (x ) + µ } 1 accomplishing the target task as a function of time, while the rank ψ t ψ q r s(xq)−s(xt)>µs interface shows the frame corresponding to the current cursor L success(ψ) = max {0, τ r1 − r ψ(x)} 1 s(x)>τs + position. This intuitive interface allows a single annotator to max {0, r (x) − τ } 1 ψ r2 s(x)<τs produce hundreds of frames of reward annotations per minute. The total loss is obtained by adding these terms: L + To sketch an episode, a user interactively selects a frame rank λL . In our experiments, we set µ = 0.2, µ = 0.1, x and provides an associated reward value s(x ) ∈ [0, 1]. success s r t t τ = 0.85, τ = 0.9, τ = 0.7, and λ = 10. The sketching interface allows the annotator to draw reward s r1 r2 curves while “scrubbing” through a video episode, rather than E. Batch RL annotating frame by frame. This efficient procedure provides We train policies using batch RL [41]. In batch RL, the a rich source of information about the reward across the entire new policy is learned using a single batch of data generated episode. The sketches for an episode {s(x t)}|T t=1 are stored by different previous policies, and without further execution in NES as described in Sec. II-B. on the robot. Our agent is trained using only distributional The reward sketches allow comparison of perceived value RL [7], without any feature pretraining, behaviour cloning (BC) of any two frames. In addition, the green region in Fig. 2 initialization, any special batch correction terms, or auxiliary is reserved for frames where the goal is achieved. For each losses. We do, however, find it important to use the historical task the episodes to be annotated are drawn from NES. They data from other tasks. include both the demonstrations of the target task, as well as Our choice of distributional RL is partly motivated by the experience generated for prior tasks. Annotating data from success of this method for batch RL in Atari [1]. We compare prior tasks ensures better coverage of the state space. the distributional and non-distributional RL alternatives in our Sketching is particularly suited for tasks where humans experiments. We note that other batch RL methods (see Sec. IV) are able to compare two timesteps reliably. Typical object might also lead to good results. Because of this, we release our manipulation tasks fall in this category, but not all robot tasks datasets [16] and canonical agents [28] to encourage further are like this. For instance, it would be hard to sketch tasks investigation and advances to batch RL algorithms in robotics. where variable speed is important, or with cycles as in walking. We use an algorithm similar to D4PG [7, 28] as our training While we are aware of these limitations, the proposed approach algorithm. It maintains a value network Q(x , hQ, a | θ) and a t t does however cover many manipulation tasks of interest as policy network π(x, hπ | φ). Given the effectiveness of recurrent t shown here. We believe future work should advance interfaces value functions [36], both Q and π are recurrent with hQ and t to address a wider variety of tasks. hπ representing the corresponding recurrent hidden states. The t target networks have the same structure as the value and policy D. Reward Learning networks, but are parameterized by different parameters θ(cid:48) and φ(cid:48), which are periodically updated to the current parameters The reward annotations produced by sketching are used to of the original networks. train a reward model. This model is then used to predict reward Given the Q function, we update the policy using DPG [62]. values for all experience in NES (Fig. 3). As a result, we can As in D4PG, we adopt a distributional value function [8] and leverage all historical data in training a policy for a new task, minimize the associated loss to learn the critic. During learning, without manual human annotation of the entire repository. we sample a batch of sequences of observations and actions Episodes annotated with reward sketches are used to train a {xi, ai, · · · , xi } and use a zero start state to initialize all t t t+n i reward function in the form of neural network with parameters recurrent states at the start of sampled sequences. We then ψ in a supervised manner. We find that although there is update φ and θ using BPTT [70]. high agreement between annotators on the relative quality of Since NES contains data from many different tasks, a timesteps within an episode, annotators are often not consistent randomly sampled batch from NES may contain data mostly in the overall scale of the sketched rewards. We therefore adopt irrelevant to the task at hand. To increase the representation an intra-episode ranking approach to learn reward functions, of data from the current task, we construct fixed ratio batches, rather than trying to regress the sketched values directly. with 75% of the batch drawn from the entirety of NES and Specifically, given two frames x t and x q in the same episode, 25% from the data specific to the target task. This is similar we train the reward model to satisfy two conditions. First, to the solution proposed in previous work [54], where fixed if frame x t is (un)successful according to the sketch s(x t), ratio batches are formed with agent and demonstration data. it should be (un)successful according the estimated reward F. Execution function r (x ). The successful and unsuccessful frames in ψ t reward sketches are defined by exceeding or not a threshold Once an agent is trained, we can run it on the real robot. By τ , the (un)successful frames in the predicted reward exceed running the agent, we collect more experience, which can be s (or not) a threshold τ (τ ). Second, if s(x ) is higher than used for reward sketching or RL in future iterations. Running r1 r2 t s(x ) by a threshold µ , then r (x ) should be higher than the agent also allows us to observe its performance and make q s ψ t r (x ) by another threshold µ . These conditions are captured judgments about the steps needed to improve it. ψ q r
Type No. Episodes No. steps Hours Teleoperation 6.2 K 1.1 M 31.9 lift_green 8.5 K 1.5 M 41.3 Wrist depth stack_green_on_red 10.3 K 2.0 M 56.1 Basket back Wrist wide angle 1 random_watcher 13.1 K 2.6 M 70.9 Total 37.9 K 7.0 M 193.3 (a) RGB dataset. Wrist wide angle 2 Basket front Type No. Episodes No. steps Hours Teleoperation 2.8 K 568 K 15.8 lift_cloth 13.3 K 2.4 M 66.0 Training object set Basket front random_watcher 6.0 K 1.2 M 32.1 Total 36.5 K 6.9 M 191.2 Fig. 6: The robot senses and records all data acquired with its 3 cage cameras, 3 wrist cameras (wide angle and depth) and (b) Deformable dataset. proprioception. It also records its actions continuously. The TABLE I: Dataset statistics. Total includes off-task data not robot is trained with a wide variety of object shapes, textures listed in individual rows, teleoperation and tasks lift_green, and sizes to achieve generalization at deployment time. stack_green_on_red, lift_cloth partly overlap. In early workflow iterations, before the reward functions To generate initial datasets for training we use a scripted are trained with sufficient coverage of state space, the policies policy called the random_watcher. This policy moves the end often exploit “delusions” where high rewards are assigned effector to randomly chosen locations and opens and closes the to undesired behaviors. To fix a reward delusion, a human gripper at random times. When following this policy, the robot annotator sketches some of the episodes where the delusion occasionally picks up or pushes the objects, but is typically is observed. New annotations are used to improve the reward just moving in free space. This data not only serves to seed the model, which is used in training a new policy. For each initial iteration of learning, but removing it from the training target task, this cycle is typically repeated 2–3 times until datasets degrades performance of the final agents. the predictions of a reward function are satisfactory. The datasets contain a significant number of teleoperated episodes. The majority are recorded via interactive teleoperation III. EXPERIMENTS (Sec. II-A), and thus require limited human intervention. A. Experimental Setup Only about 600 full teleoperated episodes correspond to the Robotic setup: Our setup consists of a Sawyer robot with a lift_green or stack_green_on_red tasks. Robotiq 2F-85 gripper and a wrist force-torque sensor facing There are 894, 1201, and 585 sketched episodes for the a 35 × 35 cm basket. The action space has six continuous lift_green, stack_green_on_red and lift_cloth tasks, degrees of freedom, corresponding to Cartesian translational respectively. Approximately 90% of the episodes are used for and rotational velocity targets of the gripper pinch point and training and 10% for validation. The sketches are not obtained one binary control of gripper fingers. The agent control loop all at once, but accumulated over several iterations of the is executed at 10Hz. For safety, the pinch point movement is process illustrated in Fig. 1. At the first iteration, the humans restricted to be in a 35 × 35 × 15 cm workspace with maximum annotate randomly sampled demonstrations. In next iterations, rotations of 30◦, 90◦, and 180◦ around each axis. the annotations are usually done on agent data, and occasionally Observations are provided by three cameras around the cage, on demonstrations or random watcher data. Note that only a as well as two wide angle cameras and one depth camera small portion of data from NES is annotated. mounted at the wrist, and proprioceptive sensors in the arm Agent network architecture: The agent network is illustrated (Fig. 6). NES captures all of the observations, and we indicate in Fig. 7. Each camera is encoded using a residual network what subset is used for each learned component. followed by a spatial softmax keypoint encoder with 64 Tasks and datasets: We focus on 2 subsets of NES, with channels [42]. The spatial softmax layer produces a list of data recorded during manipulation of 3 variable-shape rigid 64 (x, y) coordinates. We use one such list for each camera objects coloured red, green and blue (rgb dataset, Fig. 6), and 3 and concatenate the results. deformable objects: a soft ball, a rope and a cloth (deformable Before applying the spatial softmax, we add noise from the dataset, Fig. 4, row 5). The rgb dataset is used to learn policies distribution U[−0.1, 0.1] to the logits so that the network learns for two tasks: lift_green and stack_green_on_red, and to concentrate its predictions, as illustrated with the circles the deformable dataset is used for the lift_cloth task. in Fig. 7. Proprioceptive features are concatenated, embedded Statistics for both datasets are presented in Tab. I which with a linear layer, layer-normalized [5], and finally mapped describes how much data is teleoperated, how much comes through a tanh activation. They are then appended to the from the target tasks and how much is obtained by random camera encodings to form the joint input features. scripted policies. Each episode lasts for 200 steps (20 seconds) The actor network π(x) consumes these joint input features unless it is terminated earlier for safety reasons. directly. The critic network Q(x, a) additionally passes them
U cS aB m w err aist 16 filters 32 filters 32 filters 2D expectation A Pc rti oo pn rs io + 64 filters coordinates list 256 256 M3x a3 x c po on ov l R Re eL LU U, , 3 3x x3 3 c co on nv v, 48 64 R Re eL LU U, , 3 3x x3 3 c co on nv v, 24 32 R Re eL LU U, , 3 3x x3 3 c co on nv v, 12 16 sS op fta mti aa xl x x 1 2 ., , y y1 2 96 16 filters 32 filters 32 filters 32 filters . . LSTMs PV oa liclu ye h a en ad d s 3x3 conv 3x3 conv ReLU x , y Max pool Max pool 1x1 conv 64 64 48 24 12 12 256 256 128 64 32 16 16 A cd ad mit eio rn aa sl Proprio Fig. 7: Agent network architecture. Only the wrist camera encoder is shown here, but in practice we encode each camera independently and concatenate the results. through a linear layer, concatenates the result with actions passed through a linear layer, and maps the result through a linear layer with ReLU activations. The actor and critic iteration 1 networks each use two layer-normalized LSTMs with 256 hidden units. Action outputs are further processed through a tanh layer placing them in the range [−1, 1], and then re-scaled iteration 2 to their native ranges before being sent to the robot. The agent for lift_green and stack_green_on_red tasks observes two cameras, a basket front left camera (80 × 128) and one of wrist-mounted wide angle cameras iteration 3 (96 × 128) (Fig. 6). The agent for lift_cloth uses an Fig. 8: Iterative improvement of the agent on task additional back left camera (80 × 128). stack_green_on_red. Each iteration corresponds to a cycle Reward network architecture: The reward network is a through the steps as shown in Fig. 1. With more training data, non-recurrent residual network with a spatial softmax layer the performance of agent improves. [42] as in the agent network architecture. We also use the proprioceptive features as in the agent learning. As the sketched 2) hard: more diverse objects (less well represented in the values are in the range of [0, 1], the reward network ends with training data), smaller red objects with diverse locations; a sigmoid non-linearity. 3) unseen: green objects that were never seen during training, Training: We train multiple RL agents in parallel and large red objects. briefly evaluate the most promising ones on the robot. Each Each condition specifies 10 different initial positions of the agent is trained for 400k update steps. To further improve objects (set by a human operator) as well as the initial pose of performance, we save all episodes from RL agents, and sketch the robot (set automatically). The hard and unseen conditions more reward curves if necessary, and use them when training are especially challenging, since they require the agent to cope the next generation of agents. We iterated this procedure 2–3 with novel objects and novel object configurations. times and at each iteration the agent becomes more successful We use the same 3 evaluation sets for both the and more robust. Three typical episodes from three steps of lift_green and stack_green_on_red tasks. To evaluate improvement in stack_green_on_red task are depicted in the lift_cloth task, we randomize the initial conditions Fig. 8. They correspond to agents trained using approximately at every trial. As a quality metric, we measure the rate of 82%, 94% and 100% of the collected data. In the first iteration, successfully completed episodes, where success is indicated the agent could pick a green block, but drops it. In the second by a human operator. iteration, the agent attempts stacking a green block on red, and only in the third iteration it succeeds in it. Next, we report the B. Results performance of the final agents. Evaluation: While the reward and policy are learned from Results on the rgb dataset are summarized in Tab. II. Our data, we cannot assess their ultimate quality without running agent achieves a success rate of 80% for lifting and 60% for the agent on the real robot. That is, we need to evaluate whether stacking. Even with rarely seen objects positioned in adversarial our agents learned using the stored datasets transfer to the real ways, the agent is quite robust with success rates being 80% robot. As the agent is learned off-line, good performance on and 40%, respectively. Remarkably, when dealing with objects the real robot is a powerful indicator of generalization. that were never seen before, it can lift or stack them in 50% To this end, we conducted controlled evaluations on the and 40% of cases (see Fig. 4 for examples of such behavior). physical robot with fixed initial conditions across different The success rate of our agent for the lift_cloth task in 50 policies. For the lift_green and stack_green_on_red episodes with randomized initial conditions is 74%. datasets, we devise three different evaluation conditions with Our results compare favorably with those of Zhu et al. [73], varying levels of difficulty: where block lifting and stacking success rates are 64% and 1) normal: basic rectangular green blocks (well represented 35%. Note that these results are not perfectly comparable due in the training data), large red objects close to the center; to different physical setups, but we believe they provide some
Agent Normal Hard Unseen Our approach 80% 80% 50% No random watcher data 80% 70% 20% agent Only lift data 0% 0% 0% Non-distributional RL 30% 20% 10% (a) lift_green human Agent Normal Hard Unseen 0s. 3s. 6s. 9s. 12s. Our approach 60% 40% 40% No random watcher data 50% 30% 30% Fig. 9: Agent vs human in stack_green_on_red task. We Only stacking data 0% 10% 0% show frames of an episode performed by an agent (top) and a Non-distributional RL 20% 0% 0% human (bottom) after every 3 seconds. The agent accomplishes (b) stack_green_on_red. the task faster than a human operator. TABLE II: The success rate of our agent and ablations for a given task in different difficulty settings. Recall that out agent is trained off-line. guidance. Wulfmeier et al. [72] also attempted reward learning with the block stacking task. Instead of learning directly from pixels, they rely on QR-code state estimation for a fixed set of cubes, whereas our policies can handle objects of various Fig. 10: USB-insertion task success rate during the process of shapes, sizes and material properties. Jeong et al. [31] achieve on-line training. It illustrates the rapid progress of training a 62% F1-score on block stacking (but with a fixed set of large robot to solve an industrially relevant task. blocks) using a sim2real approach with continuous 4-DoF control. In contrast, we can achieve similar performance with a C. Interactive Insertion variety of objects and more complex continuous 6-DoF control. An alternative way to obtain a policy is to perform data To understand the benefits of relabelling the past experience collection, reward learning and policy learning in a tight loop. with learned reward functions, we conduct the ablations with Here, the human operator interactively refines the learned fixed reward functions and varying training subsets for RL reward function on-line at the same time when a policy is agents. Firstly, we train the lifting (stacking) policy using only learned. In this experiment, the policy is learned from scratch the lifting (stacking) episodes. Using only task-specific data without relying on historical data and batch RL, which is is interesting because the similarity between training data and possible in less data-demanding applications. In this section, target behavior is higher (i.e., the training data is more on- we present an example of this approach applied to industrially policy). Secondly, we train an agent with access to data from relevant task: insert a USB key into a computer port. all tasks, but no access to the random_watcher data. As this We consider 6-DoF velocity control. The velocity actions data is unlikely to contain relevant to the task episodes, we are fed to a stateful safety controller, which uses a previously want to know how much it contributes to the final performance. learned model to limit excess forces and a Mujoco inverse kinematics model to infer target joint velocities. Episodes are Tab. II show the results of these two ablations. Remarkably, set to last 15 seconds with 10 Hz control, for a total of 150 using only a task-specific dataset dramatically degrades the steps. Both the policy and reward model use wrist camera policy (its performance is 0% in almost all scenarios). Random images of size 84 × 84 pixels. watcher data proves to be valuable as it contributes up to an At the start of each episode, the robot position is set within additional 30% improvement, showing the biggest advantage a 6 × 6 × 6 cm region with 8.6◦ rotation in each direction, and in the hardest case with unseen objects. the allowed workspace is 8 × 8 × 15 cm with 17.2◦ rotation. We also evaluate the effect of distributional value functions. This is known to be significant amount of variation for such Confirming previous findings in Atari [1], the results in the task. Episodes are terminated with a discount of zero when last rows of Tab. II show that distributional value functions are the robot reached the boundary of the workspace. For faster essential for good performance in batch RL. convergence, a smaller network architecture is chosen with 3 For qualitative results, we refer the reader to the accom- convolutional layers and 2 fully connected layers. At the start panying video and Fig. 4 that demonstrate the robustness of the experiment, 100 human demonstrations are collected of our agents. The robot successfully deals with adversarial and annotated with sketches. perturbations by a human operator, stacking several unseen and This experiment is repeated 3 times. The average success non-standard objects and lifting toys, such as a robot and a rate of the agent as a function of time is shown in Fig. 10. The pony. Our agents move faster and are more efficient compared agent reaches over 80% success rate within 8 hours. During this to a human operator in some cases as illustrated in Fig. 9. time, the human annotator provides 65 ± 10 additional reward
sketches. This experiment demonstrates that it is possible to However, BC requires high-quality consistent demonstrations of solve an industrial robot task from vision using human feedback the target task and as such, it cannot benefit from heterogeneous within a single working day. data. Moreover, BC policies generally cannot outperform the Two successful episodes of USB insertion are shown in Fig. 4 human demonstrator. Demonstrations could be also used in in two last rows. In the first example the robot successfully RL [51, 57] to address the exploration problem. As in prior inserts a key using only pixel inputs. As only vision input is works [67, 54, 68], we use demonstrations as part of the agent used during training and actions are defined with respect to the experience and train with temporal difference learning in a wrist frame, the resulting policy is robust to unseen positional model-free setting. changes. In the second example, the agent (which is trained on Several recent large-scale robotic datasets were released the unperturbed state) can perform insertion despite moving recently to advance the data-driven robotics. Roboturk [47] the input socket significantly. collects crowd-sourced demonstrations for three tasks with the mobile platform. The dataset is used in the experiments with IV. RELATED WORK online RL. MIME [61] dataset contains both human and robot RL has a long history in robotics [37, 53, 34, 25, 42, 43, demonstrations for 20 diverse tasks and its potential is tested in 35]. However, applying RL in this domain inherits all the the experiments with BC and task recognition. RoboNet [15] general difficulties of applying RL in the real world [18]. Most database focuses on transferring the experience across objects published works either rely on state estimation for a specific and robotic platforms. The large-scale collection of the data is task, or work in a very limited regime to learn from raw possible thanks to scripted policies. The strength of this dataset observations. These methods typically entail highly engineered is evaluated in action-conditioned video prediction and in action reward functions. In our work, we go beyond the usual scale prediction. Our dataset [16] is collected with demonstrations, of application of RL to robotics, learn from raw observations scripted policies as well as learned policies. This paper is the and without predefined rewards. first to show how to efficiently label such datasets with rewards Batch RL trains policies from a fixed dataset and, thus, it is and how to apply batch RL to such challenging domains. particularly useful in real-world applications like robotics. It V. CONCLUSIONS is currently an active area of research (see the work of Lange et al. [41] for an overview), with a number of recent works We have proposed a new data-driven approach to robotics. aimed at improving the stability [22, 30, 1, 40]. Its key components include a method for reward learning, In the RL-robotics literature, QT-Opt [35] is the closest retrospective reward labelling and batch RL with distributional approach to ours. The authors collect a dataset of over 580,000 value functions. A significant amount of engineering and grasps for several weeks with 7 robots. They train a distributed innovation was required to implement this at the present scale. Q-learning agent that shows remarkable generalization to To further advance data-driven robotics, reward learning and different objects. Yet, the whole system focuses on a single batch RL, we release the large datasets [16] from NeverEnding task: grasping. This task is well-suited for reward engineering Storage and canonical agents [28]. and scripted data collection policies. However, these techniques We found that reward sketching is an effective way to elicit are not easy to design for many tasks and, thus, relying on reward functions, since humans are good at judging progress them limits the applicability of the method. In contrast, we toward a goal. In addition, the paper also showed that storing collect the diverse data and we learn the reward functions. robot experience over a long period of time and across different Learning reward functions using inverse RL [52] achieved tasks allows to efficiently learn policies in a completely off-line tremendous success [20, 27, 44, 21, 48, 73, 6]. This class of manner. Interestingly, diversity of training data seems to be methods works best when applied to states or well-engineered an essential factor in the success of standard state-of-the-art features. Making it work for high-dimensional input spaces, RL algorithms, which were previously reported to fail when particularly raw pixels, remains a great challenge. trained only on expert data or the history of a single agent [22]. Learning from preferences has a long history [66, 50, 19, Our results across a wide set of tasks illustrate the versatility 64, 14, 32]. Interactive learning and optimization with human of our data-driven approach. In particular, the learned agents preferences dates back to works at the interface of machine showed a significant degree of generalization and robustness. learning and graphics [10, 11]. Preference elicitation is also This approach has its limitations. For example, it involves used for reward learning in RL [65, 71]. It can be done a human-in-the-loop during training which implies additional by whole episode comparisons [2, 3, 12, 59] or shorter clip cost. The reward sketching procedure is not universal and other comparisons [13, 29]. A core challenge is to engineer methods strategies might be needed for different tasks. Besides, the that acquire many preferences with as little user input as learned agents remain sensitive to significant perturbations in possible [38]. To deal with this challenge, our reward sketching the setup. These open questions are directions for future work. interface allows perceptual reward learning [60] from any, even ACKNOWLEDGMENTS unsuccessful trajectories. We would like to thank all the colleagues at DeepMind who Many works in robotics choose to learn from demonstrations teleoperated the robot for data collection. to avoid hard exploration problems of RL. For example, super- vised learning to mimic demontrations is done in BC [55, 56].
REFERENCES Challenges of real-world supervised learning. arXiv preprint arXiv:1904.12901, 2019. [1] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. [19] Stephen E Feinberg and Knley Larntz. Log-linear representation Striving for simplicity in off-policy deep supervised learning. for paired and multiple comparison models. Biometrika, 63(2): arXiv preprint arXiv:1907.04543, 2019. 245–254, 1976. [2] Riad Akrour, Marc Schoenauer, and Michèle Sebag. APRIL: [20] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost Active preference learning-based supervised learning. In learning: Deep inverse optimal control via policy optimization. ECMLPKDD, pages 116–131, 2012. In International Conference on Machine Learning, pages 49–58, [3] Riad Akrour, Marc Schoenauer, Michele Sebag, and Jean- 2016. Christophe Souplet. Programming by feedback. In International [21] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards Conference on Machine Learning, pages 1503–1511, 2014. with adversarial inverse supervised learning. In International [4] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Conference on Learning Representations, 2018. Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan [22] Scott Fujimoto, David Meger, and Doina Precup. Off-policy Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech deep supervised learning without exploration. arXiv e-prints, 2: End-to-end speech recognition in English and Mandarin. In art. arXiv:1812.02900, 2018. International Conference on Machine Learning, pages 173–182, [23] Dhiraj Gandhi, Lerrel Pinto, and Abhinav Gupta. Learning to fly 2016. by crashing. In International Conference on Intelligent Robots [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer and Systems, pages 3948–3955, 2017. normalization. arXiv preprint arXiv:1607.06450, 2016. [24] Alex Graves and Navdeep Jaitly. Towards end-to-end speech [6] Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to- recognition with recurrent neural networks. In International end differentiable adversarial imitation learning. In International Conference on Machine Learning, pages 1764–1772, 2014. Conference on Machine Learning, pages 390–399, 2017. [25] Roland Hafner and Martin Riedmiller. supervised learning [7] Gabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will in feedback control. Machine learning, 84(1-2):137–169, 2011. Dabney, Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, [26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep and Timothy Lillicrap. Distributed distributional deterministic residual learning for image recognition. In IEEE Computer Vision policy gradients. In International Conference on Learning and Pattern Recognition, pages 770–778, 2016. Representations, 2018. [27] Jonathan Ho and Stefano Ermon. Generative adversarial imitation [8] Marc G Bellemare, Will Dabney, and Rémi Munos. A distribu- learning. In Advances on Neural Information Processing Systems, tional perspective on supervised learning. In International pages 4565–4573, 2016. Conference on Machine Learning, pages 449–458, 2017. [28] Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth- [9] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Maron, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Cheung, Przemysław De˛biak, Christy Dennison, David Farhi, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 Novikov, Sergio GÃs¸mez Colmenarejo, Serkan Cabi, Caglar Gul- with large scale deep supervised learning. arXiv preprint cehre, Tom Le Paine, Andrew Cowie, Ziyu Wang, Bilal Piot, and arXiv:1912.06680, 2019. Nando de Freitas. Acme: A research framework for distributed [10] Eric Brochu, Nando de Freitas, and Abhijeet Ghosh. Active supervised learning. arXiv preprint arXiv:2006.00979, 2020. preference learning with discrete choice data. In Advances on [29] Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Neural Information Processing Systems, pages 409–416, 2007. Legg, and Dario Amodei. Reward learning from human [11] Eric Brochu, Tyson Brochu, and Nando de Freitas. A Bayesian preferences and demonstrations in Atari. In Advances on Neural interactive optimization approach to procedural animation design. Information Processing Systems, pages 8011–8023, 2018. In SIGGRAPH Symposium on Computer Animation, pages 103– [30] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, 112, 2010. Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, [12] Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott and Rosalind Picard. Way off-policy batch deep reinforcement Niekum. Extrapolating beyond suboptimal demonstrations learning of implicit human preferences in dialog. arXiv preprint via inverse supervised learning from observations. In arXiv:1907.00456, 2019. International Conference on Machine Learning, pages 783–792, [31] Rae Jeong, Yusuf Aytar, David Khosid, Yuxiang Zhou, Jackie 2019. Kay, Thomas Lampe, Konstantinos Bousmalis, and Francesco [13] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Nori. Self-supervised sim-to-real adaptation for visual robotic Shane Legg, and Dario Amodei. Deep supervised learning manipulation. arXiv preprint arXiv:1910.09470, 2019. from human preferences. In Advances on Neural Information [32] Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, Processing Systems, pages 4299–4307, 2017. Filip Radlinski, and Geri Gay. Evaluating the F1-score of implicit [14] Wei Chu and Zoubin Ghahramani. Preference learning with feedback from clicks and query reformulations in web search. Gaussian processes. In International Conference on Machine ACM Transactions on Information Systems, 25(2), 2007. Learning, pages 137–144, 2005. [33] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, [15] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. CLEVR: A Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey diagnostic dataset for compositional language and elementary Levine, and Chelsea Finn. RoboNet: Large-scale multi-robot visual reasoning. In IEEE Computer Vision and Pattern learning. In Conference on Robot Learning, 2019. Recognition, 2017. [16] DeepMind. Sketchy data, 2020. URL https://github.com/ [34] Mrinal Kalakrishnan, Ludovic Righetti, Peter Pastor, and Stefan deepmind/deepmind-research/tree/master/sketchy. Schaal. Learning force control policies for compliant manipu- [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina lation. In International Conference on Intelligent Robots and Toutanova. BERT: Pre-training of deep bidirectional transformers Systems, pages 4639–4644, 2011. for language understanding. In Proceedings of the 2019 [35] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Conference of the North American Chapter of the Association Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, for Computational Linguistics: Human Language Technologies, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Volume 1, pages 4171–4186, 2019. Scalable deep supervised learning for vision-based robotic [18] Gabriel Dulac-Arnold, Daniel J. Mankowitz, and Todd Hester. manipulation. In Conference on Robot Learning, pages 651–673,
2018. supervised learning. In International Conference on Machine [36] Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, Learning, pages 663–670, 2010. and Will Dabney. Recurrent experience replay in distributed [53] Jan Peters and Stefan Schaal. supervised learning of motor supervised learning. In International Conference on Learning skills with policy gradients. Neural networks, 21(4):682–697, Representations, 2018. 2008. [37] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement [54] Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi learning in robotics: A survey. The International Journal of Azar, Dan Horgan, David Budden, Gabriel Barth-Maron, Hado Robotics Research, 32(11):1238–1274, 2013. Van Hasselt, John Quan, Mel Vecˇerík, et al. Observe and [38] Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. look further: Achieving consistent performance on atari. arXiv Sequential line search for efficient visual design optimization preprint arXiv:1805.11593, 2018. by crowds. ACM Transactions on Graphics, 36(4):1–11, 2017. [55] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a [39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im- neural network. In Advances on Neural Information Processing agenet classification with deep convolutional neural networks. Systems, pages 305–313, 1989. In Advances on Neural Information Processing Systems, pages [56] Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau Bölöni, 1097–1105, 2012. and Sergey Levine. Vision-based multi-task manipulation for [40] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and inexpensive robots using end-to-end learning from demonstration. Sergey Levine. Stabilizing off-policy Q-learning via boot- In IEEE International Conference on Robotics & Automation, strapping error reduction. In Advances on Neural Information pages 3758–3765, 2018. Processing Systems, pages 11761–11771, 2019. [57] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giu- [41] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch lia Vezzani, John Schulman, Emanuel Todorov, and Sergey supervised learning. In supervised learning, pages 45–73. Levine. Learning complex dexterous manipulation with deep Springer, 2012. supervised learning and demonstrations. Robotics, Science [42] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. and Systems, 2018. End-to-end training of deep visuomotor policies. The Journal [58] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev of Machine Learning Research, 17(1):1334–1373, 2016. Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya [43] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Deirdre Quillen. Learning hand-eye coordination for robotic MNIST large scale visual recognition challenge. International grasping with deep learning and large-scale data collection. The Journal of Computer Vision, 115(3):211–252, 2015. International Journal of Robotics Research, 37(4-5):421–436, [59] Dorsa Sadigh, Anca D. Dragan, Shankar Sastry, and Sanjit A. 2018. Seshia. Active preference-based learning of reward functions. [44] Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: In Robotics, Science and Systems, 2017. Interpretable imitation learning from visual demonstrations. In [60] Pierre Sermanet, Kelvin Xu, and Sergey Levine. Unsupervised Advances on Neural Information Processing Systems, pages perceptual rewards for imitation learning. Robotics, Science and 3812–3822, 2017. Systems, 2017. [45] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, [61] Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Abhinav Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Gupta. Multiple interactions made easy (MIME): Large scale Zitnick. Microsoft COCO: Common objects in context. In demonstrations data for imitation. In Conference on Robot European Conference on Computer Vision, pages 740–755, 2014. Learning, pages 906–915, 2018. [46] Andrew Maas, Ziang Xie, Dan Jurafsky, and Andrew Ng. [62] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Lexicon-free conversational speech recognition with neural Wierstra, and Martin Riedmiller. Deterministic policy gradient networks. In Proceedings of the 2015 Conference of the algorithms. In International Conference on Machine Learning, North American Chapter of the Association for Computational pages 387–395, 2014. Linguistics: Human Language Technologies, pages 345–354, [63] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, 2015. Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, [47] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Max Spero, Albert Tung, Julian Gao, John Emmons, Anchit Mastering the game of go with deep neural networks and tree Gupta, Emre Orbay, Silvio Savarese, and Li Fei-Fei. RoboTurk: search. Nature, 529(7587):484, 2016. A crowdsourcing platform for robotic skill learning through [64] Hal Stern. A continuum of paired comparison models. imitation. In Conference on Robot Learning, pages 879–893, Biometrika, 77:265–273, 1990. 2018. [65] Malcolm J. A. Strens and Andrew W. Moore. Policy search using [48] Josh Merel, Yuval Tassa, Sriram Srinivasan, Jay Lemmon, Ziyu paired comparisons. Journal of Machine Learning Research, 3: Wang, Greg Wayne, and Nicolas Heess. Learning human 921–950, 2003. behaviors from motion capture by adversarial imitation. arXiv [66] LL Thurstone. A law of comparative judgement. Psychological preprint arXiv:1707.02201, 2017. Review, 34:273–286, 1927. [49] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. [67] Matej Vecˇerík, Todd Hester, Jonathan Scholz, Fumin Wang, Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothörl, Riedmiller, Andreas K. Fidjeland, and Georg Ostrovski et Thomas Lampe, and Martin Riedmiller. Leveraging demonstra- al. Human-level control through deep supervised learning. tions for deep supervised learning on robotics problems with Nature, 518(7540):529–533, 2015. sparse rewards. arXiv preprint arXiv:1707.08817, 2017. [50] F Mosteller. Remarks on the method of paired comparisons: I. [68] Mel Vecerik, Oleg Sushkov, David Barker, Thomas Rothörl, the least squares solution assuming equal standard deviations Todd Hester, and Jon Scholz. A practical approach to insertion and equal correlations. Psychometrika, 16:3–9, 1951. with variable socket position using deep supervised learning. [51] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech In IEEE International Conference on Robotics & Automation, Zaremba, and Pieter Abbeel. Overcoming exploration in rein- pages 754–760, 2019. forcement learning with demonstrations. In IEEE International [69] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Conference on Robotics & Automation, pages 6292–6299, 2018. Mathieu, Max Jaderberg, Wojciech M Czarnecki, Andrew [52] Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et al.
Alphastar: Mastering the real-time strategy game StarCraft II. DeepMind Blog, 2019. [70] Paul J Werbos et al. meta-optimization synthesis through time: What it does and how to do it. Proceedings of the IEEE, 78(10):1550–1560, 1990. [71] Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes Fürnkranz. A survey of preference-based supervised learning methods. Journal of Machine Learning Research, 18(136):1–46, 2017. [72] Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost To- bias Springenberg, Michael Neunert, Tim Hertweck, Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin Riedmiller. Regularized hierarchical policies for compositional transfer in robotics. arXiv preprint arXiv:1906.11228, 2019. [73] Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu, Tom Erez, Serkan Cabi, Saran Tunyasuvunakool, János Kramár, Raia Hadsell, Nando de Freitas, et al. Reinforcement and imitation learning for diverse visuomotor skills. Robotics, Science and Systems, 2018.
