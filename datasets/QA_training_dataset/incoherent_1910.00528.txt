Augmenting learning using symmetry in a biologically-inspired domain Shruti Mishra1,2, Abbas Abdolmaleki1, Arthur Guez1, Piotr Trochim1, Doina Precup1,3 1 DeepMind 2 Harvard University 3 McGill University Abstract Invariances to translation, rotation and other spatial transformations are a hallmark of the laws of motion, and have widespread use in the natural sciences to reduce the dimensionality of systems of equations e.g. [13, 3]. In supervised learning, such as in image classification tasks, rotation, translation and scale invariances are used to augment training datasets, such as in [7]. In this work, we use data augmentation in a similar way, exploiting symmetry in the quadruped domain of the DeepMind control suite [12] to add to the trajectories experienced by the actor in the actor-critic algorithm of Abdolmaleki et al. [1]. In a data-limited regime, the agent using a set of experiences augmented through symmetry is able to learn faster. Our approach can be used to inject knowledge of invariances in the domain and task to augment learning in robots, and more generally, to speed up learning in realistic robotics applications. 1 Introduction supervised learning (RL) agents are able to perform locomotion tasks using continuous actions and observations in animal-like domains such as the planar walker, half-cheetah, and humanoid [8, 6]. While the agents learn policies that successfully maximise a cumulative return, the behaviours of agents in such domains often appear idiosyncratic to humans in at least two ways. Most notably, the locomotion behaviour in animals is associated with a natural rhythm and stereotypy, such as in the case of a walking dog, galloping horse, or the tripod gait of an ant. In the absence of this spatial order and temporal rhythm in simulated agents, the behaviours of the learned policies can look unnatural. Idiosyncratic behaviour can arise in simulated agents because the physical simulation is incorrect, the task does not capture the tasks executed by embodied agents, or the constraints on a simulated agent are different from those on an embodied agent. The physical simulation itself is an inaccurate and incomplete model of the physical world – it allows perfect floors, instantaneous, noise-free actuation, and a windless environment. Embodied agents necessarily have behaviour policies that are robust to deviations from any precise environment crafted for simulation. Additionally, the mechanical and biological constraints that restrict the behaviour of an embodied agent, a robot or an animal, are different from those experienced by simulated agents. Embodied agents are also necessarily affected by notions of mechanical stress and fatigue. For example, an embodied walker would be less likely to drag a leg, due to the notion of wear, or flail around its arms, due to considerations of energy and efficiency. In this work, we leverage order in the spatial structure of the domain. Specifically, the vast majority of animals have at least one plane of external symmetry, which is reflected in their stereotyped gaits when they are walking or running. Similarly, for simulated bodies with nearly identical limbs, if the agent knows how to move one limb relative to the body in a particular way, it can do the same for the other similar limbs. We use symmetry as a natural inductive bias in order to reduce the space of shrutimishra@g.harvard.edu 9102 tcO 1 ]GL.sc[ 1v82500.0191:viXra
behaviour policies that can be achieved by agents, as well as to explore if this approach can boost the learning process. 2 Related work We try to use invariances in the physical domain/task to augment learning. Invariances to scale, orientation, translation and relative motion are common to the natural world and a hallmark of the dynamics of tangible objects and of continuous fields. The use of such invariances to reduce the dimensionality of the equations governing a physical process is thus ubiquitous in the natural sciences [3, 13]. In machine learning, supervised classification tasks use rotation, symmetry and scale invariances to augment training data to get improved performance [2, 11, 7]. In RL, symmetry and rotation invariances have been used to augment data in AlphaGo [10]. In robotics and continuous control applications, some ways to improve the real-world suitability and sample efficiency of RL agents include curriculum learning [4], learning from motion-capture data [9], and using constraint- based objective functions [5]. Our work follows the same theme of making the policies learned by RL agents more realistic, and aiming to make the learning process efficient. 3 Domain and tasks To illustrate the use of symmetry, we can use any animal-like domain with a saggital plane of symmetry, which is a plane of symmetry com- mon to several animals. We use the quadruped domain from Tassa et al. (2018) [12]. The body of the quadruped comprises a torso with four legs. Each leg is identical, with three hinge z y y joints, as shown in Figure 1 (left). The joints x are controlled by torque actuators. We consider x the move tasks in Tassa et al. (2018) [12], in Figure 1: Left: Quadruped domain, showing the which positive reward is given for an upright three joints on one of the legs (light grey), and orientation relative to the horizontal plane and a the sagittal plane in green. Right: Top view of forward velocity of the torso, relative to a speci- a quadruped, showing a fictitious adopted pose fied desired velocity, in the frame of reference of in terms of leg orientations (blue), and its corre- the torso. Within the move tasks, the walk task sponding pose (pink), mirrored about the axis of and run task are specified by different values of symmetry. The co-ordinate axes correspond to the desired velocity. Details of the reward function local frame of reference of the torso. are given in Tassa et al. (2018) [12]. The ob- servations comprise joint angles and velocities, forces and torques on the joints, the state of the actuators, and the torso velocity. This is illustrated in Figure 1 (right), by means of a pose with the legs shown in blue, and its corresponding mirrored pose, with the legs shown in pink. In the move tasks, for every observation- action pair that the agent encounters, there is a corresponding mirrored observation-action pair that can be obtained by reflection about the plane of symmetry. The mirroring operation for observations and actions are essentially matrix multiplications, and are obtained cheaply from the observations. The mirrored corresponding observation-action pair will result in the same forward displacement, and therefore the same reward. Therefore, for every trajectory executed by the agent, there is a corresponding mirrored trajectory that results in the same sequence of rewards. 4 Symmetric policy A symmetric gait such as a human walk is characterised by a phase difference between the legs, and an antisymmetric gait such as that of a galloping horse is characterised by an in-phase synchrony between the left and right halves of the body. When we use the term symmetric policy, we mean that the probability π (a , s ) = π(a, s), where s and a are obtained by mirrored mirrored mirrored mirrored reflecting the state s and action a, respectively, about a plane of symmetry. By this definition, the biological symmetric and antisymmetric gaits are both executed by a symmetric policy. 2
Algorithm 1 MPO with Mirrored Data 1: given batch-size (K), number of actions (N), Q-function Qˆ, old-policy π and replay-buffer k 2: initialize π from the parameters of π(k) θ 3: repeat 4: Sample states with batch of size N from replay buffer 5: Step 1: sample based policy (weights) 6: q(a |s ) = q , computed as: i j ij 7: for j = 1,...,K do 8: for i = 1,...,N do 9: a ∼ π (a|s ) i k j 10: Q = Qˆ(s , a ) ij j i 11: q ∝ exp(Q /temperature parameter)) ij ij 12: end for 13: end for 14: Calculate mirrored states and mirrored actions 15: Step 2: update parametric policy 16: Given the data-set {s , (a , q ) } and {smirrored, (amirrored, q ) } j i ij i=1...N j=1...K j i ij i=1...N j=1...K 17: Update the Policy by taking gradient of following weighted maximum likelihood objective 18: max ((cid:80)K (cid:80)N q log π (a |s ) + (cid:80)K (cid:80)N q log π (amirrored|smirrored)) θ j i ij θ i j j i ij θ i j 19: (subject to additional (KL) regularization) 20: until Fixed number of steps 21: return π (k+1) 5 Proposed algorithm We use MPO, an actor-critic algorithm described in Abdolmaleki et al. [1]. To encourage the agent to take symmetries into account, we augment the batch of experiences generated by the actor and stored in the replay buffer, by computing a set of corresponding mirrored trajectories for use by the learner of the MPO algorithm [1]. The MPO algorithm has two steps, policy evaluation and policy improvement. In this section, we summarise the algorithm and specify the modifications required to use data from mirrored trajectories, with a pseudocode of our approach given in algorithm 1. 5.1 Policy Evaluation We employ a 1-step temporal difference (TD) learning, fitting a parametric Q-function Qπ(s, a) with φ parameters φ by minimizing the squared (TD) error (cid:16) (cid:17)2 min r + γQπ(k−1) (s , a ) − Qπ(k) (s , a ) , φ t φ(cid:48) t+1 t+1 φ t t where a ∼ π(k−1)(a|s ) and r = r(s , a ), which we optimize via gradient descent. We let t+1 t+1 t t t φ(cid:48) be the parameters of a target network that is held constant for 250 steps (and then copied from the optimized parameters φ). Using the fact that for a symmetric policy, Q (s , a ) = mirrored mirrored Q(s, a), we also fit the parametric Q-function for Q (s , a ), mirrored mirrored min (cid:16) r + γQπ(k−1) (s , a ) − Qπ(k) (cid:0) smirrored, amirrored(cid:1)(cid:17)2 . φ t φ(cid:48) t+1 t+1 φ t t 5.2 Policy Improvement Step 1: We construct a non-parametric improved policy q. This is done by maximizing J¯(s, q) = E [Qˆ(s, a)] for states s drawn from a replay buffer R while ensuring that the solution stays close q(a|s) to the current policy π ; i.e. E [KL(q(a|s), π (a|s))] < (cid:15), as detailed in Abdolmaleki et al. [1]. k s∼R k Step 2: We fit a new parametric policy to samples from q(a|s) by solving the optimization problem π = argmin E [KL(q(a|s)(cid:107)π(a|s)], where π is the new and improved policy, which k+1 π µ(s) k+1 employs additional regularization [1]. To learn about/from mirrored data, we repeat steps 1 and 2 for mirrored states and mirrored actions calculated from original data. 3
ksat klaW ksat nuR Batch size = 256 normal training augmented training nruter evitalumuC nruter evitalumuC nruter evitalumuC nruter evitalumuC Batch size = 512 1000 1000 800 800 600 600 400 400 200 200 0 0 0 2e3 4e3 6e3 8e3 0 2e3 4e3 6e3 8e3 Episodes Episodes 1000 1000 800 800 600 600 400 400 200 200 0 0 0 2e3 4e3 6e3 8e3 0 2e3 4e3 6e3 8e3 Episodes Episodes Figure 2: Learning curves for 10 seeds of the walk (top row) and run (bottom row) tasks, for batch sizes 256 (left column) and 512 (right column). The plot shows the cumulative reward as a function of gradient steps. The normal training and augmented training are in blue and pink, respectively. The thick line shows the mean. 6 Experiments We refer to training with the MPO algorithm in [1] as “normal training” and training with the additional experience from the mirrored data, as described in Section 3, as “augmented training”. For the normal and augmented training conditions, we use the same hyper-parameters as in [1], with one main modification: we slowed down the actor to get a rate of trajectory generation of 1000 s of environment steps every 30 s of real time, to bring it closer to the data-generation rate of real robots. The learner thus operates in a data-limited regime. The resulting cumulative reward as a function of gradient steps is shown in Figure 2. For each of the training conditions, the augmented training condition achieves better performance, typically in fewer episodes. This preliminary result suggests that knowledge of symmetry, enforced through data augmentation in the policy optimisation step, is a useful bias for the agent to shape its policy. 7 Discussion We presented an approach for incorporating symmetry in the environment in an actor-critic architec- ture by augmenting the experiences generated by the actor. A different approach to inform an agent of symmetry in its domain could, for instance, compress the state representation, explicitly specifying the relationship between an observation-action pair and its mirrored version. This involves changing the underlying Markov decision process so that transitions are between “canonical" observations. In general, this means that transitions do not arise from a physical process and may require modifications to the algorithm to ensure that no undesirable bias has been introduced. A promising extension of this work would be to move towards a policy that explicitly encodes the symmetry using hierarchy, with similar legs sharing their lower-level policy with the corresponding actuators of other legs, and using experiences from the other legs to update their policies. Using symmetric policies leads to a change in the space of policies that can be achieved and therefore qualitative change in gait. We plan to investigate this further. In general, this work can also be useful for transfer learning to tasks 4
which are not symmetric; an agent may first learn to walk using a knowledge of symmetry to be data-efficient, and then learn to navigate an external non-uniform landscape. 8 Acknowledgements We would like to thank Ankit Anand, Eser Aygün, Tom Erez, Philippe Hamel, Yuval Tassa and Daniel Toyama. References [1] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018. [2] Henry S Baird. Document image defect models. In Structured Document Image Analysis, pages 546–556. Springer, 1992. [3] Grigory Isaakovich Barenblatt. Scaling, self-similarity, and intermediate asymptotics: di- mensional analysis and intermediate asymptotics, volume 14. Cambridge University Press, 1996. [4] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41–48. ACM, 2009. [5] Steven Bohez, Abbas Abdolmaleki, Michael Neunert, Jonas Buchli, Nicolas Heess, and Raia Hadsell. Value constrained model-free continuous control. arXiv preprint arXiv:1902.04623, 2019. [6] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. [7] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. MNIST classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. [8] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep supervised learning. International Conference on Learning Representations, 2016. [9] Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid control. arXiv preprint arXiv:1811.11711, 2018. [10] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess- che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016. [11] Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural networks applied to visual document analysis. In Icdar, volume 3, 2003. [12] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller. DeepMind control suite. https://arxiv.org/abs/1801.00690, January 2018. [13] Kenneth G Wilson. Renormalization group and critical phenomena. i. renormalization group and the kadanoff scaling picture. Physical review B, 4(9):3174, 1971. 5
