Actor Critic with Differentially Private Critic Jonathan Lebensold∗, William Hamilton†, Borja Balle‡, Doina Precup§ *†§ McGill University, Mila - Québec Artificial Intelligence Institute §Google DeepMind Abstract supervised learning algorithms are known to be sample inefficient, and often performance on one task can be substantially improved by leveraging information (e.g., via pre-training) on other related tasks. In this work, we propose a technique to achieve such knowledge transfer in cases where agent trajectories contain sensitive or private information, such as in the healthcare domain. Our approach leverages a differentially private policy evaluation algorithm to initialize an actor-critic model and improve the effectiveness of learning in downstream tasks. We empirically show this technique increases sample efficiency in resource-constrained control problems while preserving the privacy of trajectories collected in an upstream task. 1 Introduction While supervised learning (RL) is an attractive framework for modeling decision making under uncertainty, the sample inefficiency challenges are well known [29], and in some case can be surmounted by, for example, using a simulator in silico [10] or relying on some form of transfer learning [24, 20]. These solutions, however, rarely account for real-world constraints arising in tasks where data privacy must be addressed. Examples include hospitals sharing patient data to improve clinical decision-making, or navigational information collected from agents in the real-world [11, 31]. Thus, as more real-world problems are modelled as use cases for RL algorithms, privacy-preserving knowledge transfer between agent environments will become a deployment requirement. Differential privacy (DP) is a robust privacy-preserving technique for data analysis algorithms [3, 2]. Previous works on DP for sequential decision-making tasks have focused on the (contextual) bandits setting [22, 27, 26, 12], or developed RL methods which treat the rewards as sensitive [28]. We argue that such approaches do not address situations where a trusted aggregator wishes to use historical, potentially sensitive data to bootstrap an RL algorithm to learn on an untrusted environment. For example, aggregating sensitive patient information to train an agent which could then be shared and improved (e.g. through personalization) on a smaller, local dataset. In this paper we model such a scenario by assuming a number of untrusted agents (consumers) whose goal to solve an RL task in a sample-efficient manner by leveraging information obtained from a trusted aggregator (the producer). The setup we consider is described in Figure 1. Privacy risks are well studied in the supervised learning setting and DP provides safeguards against an attacker attempting to learn whether an individual record is included in the training set [7, 1]. In the context of RL, previous work has demonstrated how an attacker can infer information about training trajectories from a policy [19]. The clinical example we use to motivate our work is inspired by recent successes of RL in the context of sepsis treatments [13]. ∗jonathan.maloney-lebensold@mail.mcgill.ca †wlh@cs.mcgill.ca ‡borja.balle@gmail.com §dprecup@cs.mcgill.ca Privacy in Machine Learning Workshop, NeurIPS 2019 9102 tcO 41 ]GL.sc[ 1v67850.0191:viXra
Figure 1: DP Producer/Consumer preserves privacy with a single transfer from the producer. How best to transfer knowledge from one RL agent to another, even without privacy considerations, remains an area of active research. Informally, any technique where an algorithm is parametrized based on previous training can be considered a form of transfer learning [5]. The goal of most transfer learning is to use previously learned knowledge to speed up learning in a related task. Multi-task RL [25] and multi-agent RL [14], model distillation [21] and meta-learning [9] are strategies considered when the task may vary between environments or where a prior is assumed to increase sample efficiency. Transfer learning is also frequently used in supervised learning for vision and speech tasks [18, 30, 17]. In this work, we explore transfer learning in RL under DP constraints. Specifically, we investigate how actor-critic methods perform when initialized using a privatized First-Visit Monte Carlo estimate [23] of the value function. A trusted data aggregator, called the producer in our algorithm, uses DP-LSL [4], a differentially private policy evaluation algorithm, to learn a value function vˆ(s, w). Actor-critic, a commonly used policy gradient method in RL, enables our agent, the consumer, to iteratively improve an action-value function Q (s, a, θ) and a state-value function vˆ(s, w). Our π method uses the output of DP-LSL to initialize the actor-critic algorithm, effectively transfering knowledge between the producer and the consumers while preserving the privacy of the trajectories collected by the producer. Such an approach is desirable when the environment of the consumer may be considered data-limited. 2 Differentially Private Policy Evaluation with DP-LSL Differential privacy is achieved by introducing carefully calibrated noise into an algorithm. The goal of a differentially private algorithm is to bound the effect that any individual user’s contribution might have on the output while maintaining proximity to the original output. By limiting individual contributions, the potential risk of an adversary learning sensitive information about any one user is limited. A user must be defined in order to properly calibrate noise. We assume each user contributes a single trajectory x to a database of records X collected by the producer. Each x = i i ((s , a , r ), ...(s , a , r )) represents a trajectory of states, actions and rewards in [R , R ]. 1 1 1 T T T i min max For example, patient treatment decisions can be expressed as actions selected from a set A and health readings as observations from a state-space S, while rewards model the treatment outcome. The number of trajectories in X is denoted by m. The set of a states visited by a single trajectory is denoted by S . The First-Visit Monte Carlo estimate F of the value of a state s is defined as the xi X,s empirical average over X of γ-discounted sums of rewards F observed from the first visit to s on xi,s each trajectory x .5 i DP-LSL [4] is one of the few DP algorithms to support policy evaluation and provide a theoretical privacy and utility guarantee. By treating the estimation of the value function as a regularized least-squares regression problem based on Monte-Carlo estimates, F , we can guarantee a limit on X,s the influence of each trajectory to the value function. DP-LSL achieves differential privacy by adding Gaussian noise to the output of this regression problem; the noise is calibrated in a data-dependent manner to achieve ((cid:15), δ)-DP by using the smooth sensitivity framework [16]. 5Trajectories where s ∈/ S are ignored when computing F . xi X,s 2
More formally, to find the parameter vector θ ∈ Rm representing the value function, DP-LSL minimizes the objective function Jλ (θ) below, which includes a ridge penalty with λ > 0: X m Jλ (θ) = 1 (cid:88) (cid:88) ρ (cid:0) F − φ(cid:62)θ(cid:1)2 + λ (cid:107)θ(cid:107)2 (1) X m s xi,s s 2m 2 i=1 s∈Sxi . The regression weights 0 ≤ ρ ≤ 1 represent any initial prior/importance that we may ascribe to each s state, eg. depending on how frequently they are visited. This least-squares problem can be solved in closed-form to find a parameter vector θλ ∈ Rd yielding the value function Vˆ π = Φθλ , where Φ is X X a given feature matrix containing the features φ for each state. Gaussian noise is then applied to s the result. The utility analysis in [4] suggests that the regularization parameter λ must be carefully chosen as a function, which depends only on the number of trajectories m. 3 Actor Critic with Differentially Private Critic Our proposed algorithm comprises of two phases: a producer that uses historical data — considered confidential — to evaluate a policy, i.e. obtain the associated value function; and a consumer that uses an actor-critic algorithm initialized with the value function provided by the producer. Intuitively, such a prior should help the actor make initial estimates of actions taken. While we restrict ourselves to actor-critic, any algorithm that incorporates a value function could be used for the consumer phase. Algorithm 1: Actor-critic with Differentially-Private Critic Input: X, Φ, γ, R , ρ, λ, (cid:15), δ max Compute First-Visit Monte-Carlo Estimate F x,s Compute DP-LSL: θˆλ X Let w ← θˆλ X Run Actor-Critic with Critic vˆ(s, w) The producer is therefore responsible for policy evaluation i.e., attempting to learn the state-value function V π for a for a given policy π. Empirical results come from collecting sample trajectories x i using a learning algorithm (SARSA) [23]. While any learning algorithm could be used in practice, SARSA is appealing due to its relative simplicity and convergence properties at the limit. 4 Empirical Setup Our empirical results come from 2 domains: an MDP domain (100 states with two actions) and the OpenAI Gym [6] environment Taxi-V2 [8]. These experiments compare the benefit of value function transfer in consumer phase. The producer phase outputs a least-squares approximation of a differentially-private value function. By letting w ← θˆλ , we can then initialize vˆ(s, w). A X ridge-regularization parameter λ is parameterized based on a regularization term r where λ = rmp, p = 0.5 and r = 200. We fix privacy parameter δ = 1/m and vary (cid:15). Patient Treatment Progression The Markov-Decision Process (MDP) experiments can be re- garded as clinical in nature – patient’s data is encoded into a state vector representation similar to [13] – but it could easily be applied to other domains, such as autonomous navigation. We study two approaches to generating samples: taking only optimal actions (the agent selects an action based on argmax Q∗ (S, a) where the Q∗ is the optimal action-value function) and with an on-policy π π temporal-difference method, SARSA [23]. Our MDP consists of a chain of N states, where N = 100. In each state the agent has some probability p of staying and probability (1 − p) of advancing (as in supp. material). The environment has two actions: A and A where their probability of transitioning to the right is 0.1 and 0.9 respectively. A 0 1 reward of 1 is given when the agent reaches the final, absorbing state, and -1 for all other states. States are one-hot encoded in a vector of size 100. This setup illustrates a case of policy evaluation in the medical domain, where patients tend to progress through stages of recovery at different rates, and past states are not usually revisited (because in the medical domain, states contain historic information about past treatments). 3
Figure 2: MDP-100 and Taxi-V2 Task Transfer Results under DP-LSL Figure 3: Least Squares Transfer Performance (10k and 50k producer episodes) Taxi-V2 Taxi-V2 [8] is a discrete grid-world environment where the agent must pick up and drop off passengers at the right location. While this is still a relatively simple environment, it provides a classic scenario where an agent is able to draw on prior experience without leaking information trajectories performed before initialization. Results We find in Figure 2 that even with a layer of DP noise, our critic benefits from an initial- ization of the value function estimate. In an environment where the number of episodes available is finite and regulatory frameworks inhibit sharing data, such a transfer learning approach could provide tangible benefits while minimizing the user harm through parameter sharing. Further experiments in Figure 3 illustrate that having more sample episodes m will likely improve the quality of the transfer. We also find that some transfer is better than no transfer. For example in Taxi-V2, we achieve convergence after 10,000 episodes, whereas without transfer it took the agent 15,000 episodes. We also note that changing our privacy budget (cid:15) by two orders of magnitude does not vary the resulting transfer, meaning that we can benefit from initialization with limited risks to individual privacy. 5 Conclusion We presented a motivated set of use cases for applying a differentially-private critic in the RL setting. The definition of the producer and consumer trust-model is common in real-world deployments and fits with existing transfer learning approaches where data centralization is difficult. Our preliminary results suggest a measurable improvement in sample efficiency through task transfer. We look forward to exploring how this framework could be extended so that the consumer’s critic could then be shared with the producer by leveraging ideas coming from the Federated Learning literature [15]. Acknowledgments We thank Joey Bose, Mike Rabbat and Maxime Wabartha for discussion and comments. This work was supported in part by Google DeepMind and Mila. 4
References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep Learning with Differential Privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security - CCS’16, pages 308–318, New York, New York, USA, 2016. ACM Press. [2] Mohammad Al-Rubaie and J Morris Chang. Privacy-preserving machine learning: Threats and solutions. IEEE Security & Privacy, 17(2):49–58, 2019. [3] Apple. Learning with Privacy at Scale. Ml, 1:1–25, 2017. [4] Borja Balle, Maziar Gomrokchi, and Doina Precup. Differentially Private Policy Evaluation. International Conference on Machine Learning (ICML), pages 2130–2138, jun 2016. [5] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in neural information processing systems, pages 153–160, 2007. [6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. [7] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. Technical report, Google, 2018. [8] Thomas G. Dietterich. Hierarchical supervised learning with the MAXQ Value Function Decomposition. Journal of Artificial Intelligence Research, 13:227–303, 2000. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. International Conference on Machine Learning (ICML), mar 2017. [10] Ian Fox and Jenna Wiens. supervised learning for blood glucose control: Challenges and opportunities. ICML 2019 Workshop RL4RealLife, 2019. [11] Lex Fridman, Benedikt Jenik, and Jack Terwilliger. Deeptraffic: Driving fast through dense traffic with deep supervised learning. arXiv preprint arXiv:1801.02805, 2018. [12] Pratik Gajane, Tanguy Urvoy, and Emilie Kaufmann. Corrupt bandits for preserving local privacy. arXiv preprint arXiv:1708.05033, 2017. [13] Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care. Nature Medicine, 24(11):1716–1720, 2018. [14] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6379–6390, 2017. [15] H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. In International Conference on Learning Representations, 2018. [16] Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Smooth sensitivity and sampling in private data analysis. In Proceedings of the 39th Annual ACM Symposium on Theory of Computing, San Diego, California, USA, June 11-13, 2007, pages 75–84, 2007. [17] Charles C. Onu, Jonathan Lebensold, William L. Hamilton, and Doina Precup. Neural Transfer Learning for Cry-based Diagnosis of Perinatal Asphyxia. Interspeech, jun 2019. [18] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid- level image representations using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1717–1724, 2014. 5
[19] Xinlei Pan, Weiyao Wang, Xiaoshuai Zhang, Bo Li, Jinfeng Yi, and Dawn Song. How you act tells a lot: Privacy-leakage attack on deep supervised learning. CoRR, abs/1904.11082, 2019. [20] Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-Mimic: Deep Multitask and Transfer supervised learning. ICLR, 2016. [21] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy Distillation. ICLR 2016, page 13, 2015. [22] Roshan Shariff and Or Sheffet. Differentially private contextual linear bandits. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada., pages 4301–4311, 2018. [23] Richard S. Sutton and Andrew G. Barto. supervised learning: An Introduction. The MIT Press, second edition, 2018. [24] Matthew E Taylor and Peter Stone. Transfer learning for supervised learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633–1685, 2009. [25] Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask supervised learning. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 4496–4506, 2017. [26] Aristide Charles Yedia Tossou and Christos Dimitrakakis. Achieving privacy in the adversarial multi-armed bandit. In Thirty-First AAAI Conference on Artificial Intelligence, 2017. [27] Aristide CY Tossou and Christos Dimitrakakis. Algorithms for differentially private multi-armed bandits. In Thirtieth AAAI Conference on Artificial Intelligence, 2016. [28] Baoxiang Wang and Nidhi Hegde. Private q-learning with functional noise in continuous spaces. CoRR, abs/1901.10634, 2019. [29] Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. International Conference on Learning Representations, 2017. [30] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320–3328, 2014. [31] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep supervised learning. In 2017 IEEE international conference on robotics and automation (ICRA), pages 3357–3364. IEEE, 2017. 6
