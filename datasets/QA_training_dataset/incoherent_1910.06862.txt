Approximate Inference in Discrete Distributions with Monte Carlo Tree Search and Value Functions Lars Buesing, Nicolas Heess, Theophane Weber DeepMind {lbuesing,heess,theophane}@google.com Abstract A plethora of problems in AI, engineering and the sciences are naturally formalized as inference in discrete probabilistic models. Exact inference is often prohibitively expensive, as it may require evaluating the (unnormalized) target density on its entire domain. Here we consider the setting where only a limited budget of calls to the unnormalized density oracle is available, raising the challenge of where in the domain to allocate these function calls in order to construct a good approximate solution. We formulate this problem as an instance of sequential decision-making under uncertainty and leverage methods from supervised learning for probabilistic inference with budget constraints. In particular, we propose the TREESAMPLE algorithm, an adaptation of Monte Carlo Tree Search to approximate inference. This algorithm caches all previous queries to the density oracle in an explicit search tree, and dynamically allocates new queries based on a "best-first" heuristic for exploration, using existing upper confidence bound methods. Our non-parametric inference method can be effectively combined with neural networks that compile approximate conditionals of the target, which are then used to guide the inference search and enable generalization across multiple target distributions. We show empirically that TREESAMPLE outperforms standard approximate inference methods on synthetic factor graphs. 1 Introduction Probabilistic (Bayesian) inference formalizes reasoning under uncertainty based on first principles [1, 2], with a wide range of applications in cryptography [3], error-correcting codes [4], bio-statistics [5], particle physics [6], generative modelling [7], causal reasoning [8] and countless others. Inference problems are often easy to formulate, e.g. by multiplying non-negative functions that each reflect independent pieces of information, yielding an unnormalized target density (UTD). However, extracting, i.e. inferring, knowledge from this UTD representation, such as marginal distributions of variables, is notoriously difficult and essentially amounts to solving the SUMPROD problem [9]: M (cid:88) (cid:88) (cid:89) · · · exp ψ (x , . . . , x ), m 1 N x1 xN m=1 (cid:81) where the UTD here is given by exp ψ . For discrete distributions, inference is #P-complete [10], and m m thus at least as hard as (and suspected to be much harder than) NP-complete problems [11]. The hardness of exact inference, which often prevents its application in practice, has led to the development of numerous approximate methods, such as Markov Chain Monte Carlo (MCMC) [12], Sequential Monte Carlo (SMC) methods [13] and Variational Inference (VI) [14]. Whereas exact inference methods essentially need to evaluate and sum the UTD over its entire domain in the worst case, approximate methods attempt to reduce computation by concentrating evaluations of the UTD on regions of the domain that contribute most to the probability mass. The exact locations of high-probability regions are, however, often unknown a-priori, and different approaches use a variety of means to identify them efficiently. In continuous domains, Hamiltonian 1 9102 tcO 51 ]GL.sc[ 1v26860.0191:viXra
Monte Carlo and Langevin sampling, for instance, guide a set of particles towards high density regions by using gradients of the target density [15, 16]. In addition to a-priori knowledge about the target density (such as a gradient oracle), adaptive approximation methods use the outcome of previous evaluations of the UTD to dynamically allocate subsequent evaluations on promising parts of the domain [17, 18]. This can be formalized as an instance of decision-making under uncertainty, where acting corresponds to evaluating the UTD and the goal is to discover probability mass in the domain [19]. Form this viewpoint, approximate inference methods attempt to explore the target domain based on a-priori information about the target density as well as on partial feedback from previous evaluations of the UTD. In this work, we propose a new approximate inference method for discrete distributions, termed TREESAM- PLE, that is motivated by the correspondence between probabilistic inference and decision-making highlighted previously in the literature, e.g. [20, 21, 22, 23, 24, 25]. TREESAMPLE approximates a joint distribution over multiple discrete variables by the following sequential decision-making approach: Variables are inferred / sampled one variable at a time based on all previous ones in an arbitrary, pre-specified ordering. An explicit tree-structured cache of all previous UTD evaluations is maintained, and a heuristic inspired by Upper Con- fidence Bounds on Trees (UTC) [26] for trading off exploration around configurations that were previously found to yield high values of UTD and configurations in regions that have not yet been explored, is applied. Algorithmically, TREESAMPLE amounts to a variant of Monte Carlo Tree Search (MCTS) [27], modified so that it performs integration rather than optimization. In contrast to other approximate methods, it leverages systematic, backtracking tree search with a "best-first" exploration heuristic. Inspired by prior work on combining MCTS with function approximation [28], we proceed to augment TREESAMPLE with neural networks that parametrically cache previously computed approximate solutions of inference sub-problems. These networks represent approximate conditional densities and correspond to state-action value function in decision-making and supervised learning. This caching mechanism (under suitable assumptions) allows to generalize search knowledge across branches of the search tree for a given target density as well as across inference problems for different target densities. In particular, we experimentally show that suitably structured neural networks such as Graph Neural Networks [29] can efficiently guide the search even on new problem instances, therefore reducing the effective search space massively. The paper is structured as follows. In sec. 2 we introduce notation and set up the basic inference problem. In sec. 3, this inference problem is cast into the language of sequential decision-making and the TREESAMPLE algorithm is proposed. We show in sec. 4 empirically, that TREESAMPLE outperforms closely related standard approximate inference algorithms. We conclude with a discussion of related work in sec. 5. 2 Discrete Inference with Computational Budget Constraints 2.1 Notation Let X = (X , . . . , X ) ∼ P ∗ be a discrete random vector taking values x = (x , . . . , x ) in X := 1 N X 1 N {1, . . . , K}N , and let x := (x , . . . , x ) ∈ X be its n-prefix and define x ∈ X analogously. We ≤n 1 n ≤n <n <n assume the distribution P ∗ is given by a factor graph. Denote with γ∗ its density (probability mass function) X and with γˆ the corresponding unnormalized density: M M (cid:88) (cid:88) (cid:88) log γ∗(x) = ψ (x) − log exp ψ (x) = log γˆ(x) − log Z, (1) m m m=1 x∈X m=1 where Z is the normalization constant. We assume that all factors ψ , defined in the log-domain, take values m in R ∪ {−∞}. Furthermore, scope(ψ ) for all m = 1, . . . , M are assumed known, where scope(ψ ) ⊆ m m {1, . . . , N } is the index set of the variables that ψ takes as input. We denote the densities of the conditionals m P ∗ as γ∗(x |x ). Xn|x<n n n <n 2
2.2 Problem Setting and Motivation Consider the problem of constructing a tractable approximation P to P ∗ . In this context, we define tractable X X as being able to sample from P (say in polynomial time in N ). Such a P then allows Monte Carlo estimates X X o Pf ∗E aP gX∗ a[ if n] . ≈ ThE isPX se[ tf u] pfo isr aa nny exfu an mc pti lo en of f mof oi dn et ler ce os mt i pn ild ao tiw onns [t 3re 0a ]m . Wta esk as ssw ui mth eo tu ht ah ta tv hi eng cot mo t po uu tc ah tioth ne alo cr oig si tn oa fl X inference in P ∗ is dominated by evaluating any of the factors ψ . Therefore, we are interested in compiling a X m good approximation P using a fixed computational budget: X Input: Factor oracles ψ , . . . , ψ with known scope(ψ ); budget B ∈ N of pointwise evaluations 1 M m of any ψ m Output: Approximation P ≈ P ∗ that allows tractable sampling X X A brute force approach would exhaustively compute all conditionals P ∗ , P ∗ up to P ∗ and resort X1|∅ X2|x1 XN |x<N to ancestral sampling. This entails explicitly evaluating the factors ψ everywhere, likely including "wasteful" m evaluations in regions of X with low density γ∗, i.e. parts of X that do not significantly contribute to P ∗ . X Instead, it may be more efficient to construct an approximation P that concentrates computational budget on X those parts of the domain X where the density γ∗, or equivalently γˆ, is suspected to be high. For small budgets B, determining the points where to probe γˆ should ideally be done sequentially: Having evaluated γˆ on values x1, . . . , xb with b < B, the choice of xb+1 should be informed by the previous results γˆ(x1), . . . , γˆ(xb). If e.g. the target density is assumed to be "smooth", a point x "close" to points xi with large γˆ(xi) might also have a high value γˆ(x) under the target, making it a good candidate for future exploration (under appropriate definitions of "smooth" and "close"). In this view, inference presents itself as a structured exploration problem of the form studied in the literature on sequential decision-making under uncertainty and supervised learning, in which we decide where to evaluate γˆ next in order to reduce uncertainty about its exact values. As presented in detail in the following, borrowing from the RL literature, we will use a form of tree search that preferentially explores points xj that share a common prefix with previously found points xi with high γˆ. 3 Approximate Inference with Monte Carlo Tree Search In the following, we cast sampling from P ∗ as a sequential decision-making problem in a suitable maximum- X entropy Markov Decision Process (MDP). We show that the target distribution P ∗ is equal to the solution, X i.e. the optimal policy, of this MDP. This representation of P ∗ as optimal policy allows us to leverage standard X methods from RL for approximating P ∗ . Our definition of the MDP will capture the following intuitive X procedure: At each step n = 1, . . . , N we decide how to sample X based on the realization x of X n <n <n that has already been sampled. The reward function of the MDP will be defined such that the return (sum of rewards) of an episode will equal the unnormalized target density log γˆ, therefore "rewarding" samples that have high probability under the target. 3.1 Sequential Decision-Making Representation We first fix an arbitrary ordering over the variables X , . . . , X ; for now any ordering will do, but see the 1 N discussion in sec. 5. We then construct an episodic, maximum-entropy MDP M = ((X , . . . , X ), A, ◦, Rπ) 1 ≤N consisting of episodes of length N . The state space at time step n is X and the action space is A = ≤n {1, . . . , K} for all n. State transitions from x to x are deterministic: Executing action a ∈ A in state <n ≤n x ∈ X at step n results in setting x to a, or equivalently the action a is appended to the current state, <n <n n i.e. x = x ◦ a = (x , . . . , x , a). A stochastic policy π in this MDP is defined by probability densities ≤n <n 1 n−1 π (a|x ) over actions conditioned on x for n = 1, . . . , N . It induces a joint distribution P π over X with n <n <n X the density π(x) = (cid:81)N π (x |x ). Therefore, the space of stochastic policies is equivalent to the space of n=1 n n <n distributions over X. We define the maximum-entropy reward function Rπ of M based on the scopes of the factors ψ as m follows: 3
Definition 1 (Reward). For n = 1 . . . , N , we define the reward function R : X → R ∪ {−∞}, as the sum n ≤n over factors ψ that can be computed from x , but not already from x , i.e. : m ≤n <n (cid:88) R (x ) := ψ(x ), (2) n ≤n ≤n ψ∈Mn where M := { ψ | max(scope(ψ )) = n }. We further define the maximum-entropy reward: n m m Rπ(x ) = R (x ) − log π (x |x ). (3) n ≤n n ≤n n n <n To illustrate this definition, assume ψ is only a function of x ; then it will contribute to R . If, however, it 1 n n is has full support scope(ψ ) = {1, . . . , N }, then it will contribute to R . Evaluating R at any input incurs a 1 N n cost of |M | towards the budget B. This completes the definition of M. From the reward definition follows n that we can write the logarithm of the unnormalized target density as the return, i.e. sum of rewards (without entropy terms): N (cid:88) log γˆ(x) = R (x ). (4) n ≤n n=1 We now establish that the MDP M is equivalent to the initial inference problem by using the standard definition of the value V π(x ) of a policy as expected return conditioned on x , i.e. V π(x ) := E [(cid:80)N Rπ ] n <n <n n <n π n(cid:48)=n n(cid:48) where the expectation E is taken over P π The following straight-forward observation holds: π X≥n|x<n Observation 1 (Equivalence of inference and max-ent MDP). The value V π := V π(∅) of the initial state x := ∅ under π in the maximum-entropy MDP M is given by the negative KL-divergence between P π and 0 X the target P ∗ up to the normalization constant Z: X V π = −D [P π (cid:107)P ∗ ] + log Z. (5) KL X X The optimal policy π∗ = arg max V π is equal to the target conditionals γ∗(x |x ): π n n <n π∗(x |x ) = γ∗(x |x ) n n <n n n <n V ∗ = log Z. Therefore, solving the maximum-entropy MDP M is equal to finding all target conditionals P ∗ , and running the optimal policy π∗ yields samples from P ∗ . In order to convert the above MDP into a reX prn e|x se< nn tation X that facilitates finding a solution, we use the standard definition of the state-action values as Qπ(x |x ) := n n <n R (x ) + V π (x ). This definition together with observation 1 directly results in (see appendix for proof): n ≤n n+1 ≤n Observation 2 (Target conditionals as optimal state-action values). The target conditional is proportional to the optimal state-action value function, i.e. γ∗(x |x ) = exp(Q∗ (x |x ) − V ∗(x )) where the normalizer is n n <n n n <n n <n given by the value V ∗(x ) = log (cid:80) exp Q∗ (x |x ). Furthermore, the optimal state-action values obey n <n xn n n <n the soft Bellman equation: K (cid:88) Q∗ (x |x ) = R (x ) + log exp Q∗ (x |x ). (6) n n <n n ≤n n+1 n+1 ≤n xn+1=1 3.2 TREESAMPLE Algorithm In principle, the soft-Bellman equation 6 can be solved by backwards dynamic programming in the following way. We can represent the problem as a K-ary tree T ∗ over nodes corresponding to all partial configurations (cid:83)N X , root ∅ and each node x being the parent of K children x ◦ 1 to x ◦ K. One can compute n=0 ≤n <n <n <n all Q-values by starting from all KN leafs x ∈ X for which we can compute the state-action values ≤N N 4
Algorithm 1 TREESAMPLE sampling procedure 1: procedure SAMPLE(tree T , default state-action values Qφ) 2: x ← ∅ 3: for n = 1, . . . , N do 4: if x ∈ T then 5: a ∼ softmax Q n(·|x) 6: else 7: a ∼ softmax Qφ(·|x) n 8: end if 9: x ← x ◦ a 10: end for 11: return x 12: end procedure Q∗ (x |x ) = R (x) and solve eqn. 6 in reverse order. Furthermore, a simple softmax operation on each N N <N N Q∗ yields the target conditional γ∗(x |x ). Unfortunately, this requires exhaustive evaluation of all factors. n n n <n As an alternative to exhaustive evaluation, we propose the TREESAMPLE algorithm for approximate inference. The main idea is to construct an approximation P consisting of a partial tree T ⊆ T ∗ and X approximate state-actions values Q with support on T . A node in T at depth n corresponds to a prefix x , <n with the attached vector of state-action values Q (·|x ) = (Q (x = 1|x ), . . . , Q (x = K|x )) ≈ n <n n n <n n n <n Q∗(·|x ) for its K children x ◦ 1 to x ◦ K (which might not be in tree themselves). Sampling from P <n <n <n X is defined in algorithm 1: The tree is traversed from the root ∅ and at each node, a child is sampled from the softmax distribution defined by Q. If at any point, a node x is reached that is not in T , the algorithm falls ≤n back to a distribution defined by a user-specified, default state-action value function Qφ; we will also refer to Qφ as prior state-action value function as it assigns a state-action value before / without any evaluation of the reward. Later, we will discuss using learned, parametric functions for Qφ. In the following we describe how the partial tree T is constructed using a given, limited budget of B of evaluations of the factors ψ . m 3.2.1 Tree Construction with Soft-Bellman MCTS TREESAMPLE leverages the correspondence of approximate inference and decision-making that we have discussed above. It consists of an MCTS-like algorithm to iteratively construct the tree T underlying the approximation P . Given a partially-built tree T , the tree is expanded (if budget is still available) using a X heuristic inspired by Upper Confidence Bound (UCB) methods [31]. It aims to expand the tree at branches expected to have large contributions to the probability mass by taking into account how important a branch currently seems, given by its current Q-value estimates, as well as a measure of uncertainty of this estimate. The latter is approximated by a computationally cheap heuristic based on the visit counts of the branch, i.e. how many reward evaluations have been made in this branch. The procedure prefers to explore branches with high Q-values and high uncertainty (low visit counts); it is given in full in algorithm 2 in the appendix, but is briefly summarized here. Each node x in T , in addition to Q (·|x ), also keeps track of its visit count η(x ) ∈ N and the <n n <n <n cached reward evaluation R (x ). For a single tree expansion, T is traversed from the root by choosing at n−1 <n each intermediate node x the next action a ∈ {1 . . . , K} in the following way: <n η(x )1/2 arg max Q (a|x ) + c · max(Qφ(a|x ), (cid:15)) <n . (7) n <n n <n 1 + η(x ◦ a) a=1,...,K <n Here, the hyperparameters c > 0 and (cid:15) > 0 determine the influence of the second term, which can be seen as a form of exploration bonus and which is computed from the inverse visit count of the action a relative to the visit counts of the patent. This rule is inspired by the PUCT variant employed in [28], but using the default value Qφ for the exploration bonus. When a new node xnew (cid:54)∈ T at depth n is reached, the reward function 5
R (xnew) is evaluated, decreasing our budget B. The result is cached and the node is added T ← T ∪ xnew n using Qφ to initialize Q (·|xnew). Then the Q-values are updated: On the path of the tree-traversal that led n+1 to xnew, the values are back-upped in reverse order using the soft-Bellman equation. This constitutes the main difference to standard MCTS methods, which employ max- or averaging backups. This reflects the difference of sampling / integration to the usual application of MCTS to maximization / optimization problems. Once the entire budget is spent, T with its tree-structured Q is returned. 3.2.2 Consistency As argued above, the exact conditionals γ∗ (·|x ) can be computed by exhaustive search in exponential time. n+1 ≤n Therefore, a reasonable desideratum for any inference algorithm is that given a large enough budget B ≥ M KN the exact distribution is inferred. In the following we show that TREESAMPLE passes this basic sanity check. The first important property of TREESAMPLE is that a tree T has the exact conditional γ n∗ +1(·|x ≤n) if the unnormalized target density has been evaluated on all states with prefix x during tree construction. To make ≤n this statement precise, we define T ⊆ T as the sub-tree of T consisting of node x and all its descendants x≤n ≤n in T . We call a sub-tree T fully expanded, or complete, if all partial states with prefix x are in T . x≤n ≤n x≤n With this definition, we have the following lemma (proof in the appendix): Lemma 1. Let T be a fully expanded sub-tree of T . Then, for all nodes x(cid:48) in T , i.e. m ≥ n and x≤n ≤m x≤n x(cid:48) = x , the state-action values are exact and in particular the node x has the correct value: ≤n ≤n ≤n Q (·|x(cid:48) ) = Q∗ (·|x(cid:48) ) m+1 ≤m m+1 ≤m V (x ) = V ∗ (x ) n+1 ≤n n+1 ≤n Furthermore, constructing the full tree T ∗ with TREESAMPLE incurs a cost of at most M KN evaluations of any of the factors ψ , as there are KN leaf node in T ∗ and constructing the path from the root ∅ to each m leaf requires at most M oracle evaluations. Therefore, TREESAMPLE with B ≥ M KN expands the entire tree and the following result holds: Corollary 1 (Exhaustive budget consistency). TREESAMPLE outputs the correct target distribution P ∗ for X budgets B ≥ M KN . 3.3 Augmenting TREESAMPLE with Learned Parametric Priors TREESAMPLE explicitly allows for a "prior" Qφ over state-action values with parameters φ. It functions as a parametric approximation to Q∗ ∝ log γ∗. In principle, an appropriate Qφ can guide the search towards n n regions in X where probability mass is likely to be found a-priori by the following two mechanisms. It scales the exploration bonus in the PUCT-like decision rule eqn. 7, and it is used to initialize the state-action values Q for a newly expanded node in the search tree. In the following we discuss scenarios and potential benefits of learning the parameters φ. In principle, if Qφ comes from an appropriate function class, it can transfer knowledge within the inference problem at hand. Assume we spent some of the available search budget on TREESAMPLE to build an approximation T . Due to the tree-structure, search budget spent in one branch of the tree does not benefit any other sibling branch. For many problems, there is however structure that would allow for generalizing knowledge across branches. This can be achieved via Qφ, e.g. one could train Qφ to approximate the Q-values of the current T , and (under the right inductive bias) knowledge would transfer to newly expanded branches. A similar argument can be made for parametric generalization across problem instances. Assume a given a family of distributions {P i } for some index-set I. If the different distributions P i share structure, it is possible to X i∈I X leverage search computations performed on P i for inference in P j to some degree. A natural example for this X X is posterior inference in the same underlying model conditioned on different evidence / observations, similar e.g. to amortized inference in variational auto-encoders [7]. Besides transfer, there is a purely computational reason for learning a parametric Qφ. The memory footprint of TREESAMPLE grows linearly with the search 6
Figure 1: Comparison of TREESAMPLE to SMC on inference in 1000 randomly generated Markov chains. Left: Approximation error D [P (cid:107)P ∗ ] as a function of the budget B in log-scale, showing that SCM needs KL X X more than 30 times the budget of TREESAMPLE to generate comparable approximations. Right: Energy and entropy contributions to the D KL[P X (cid:107)P X∗ ] for all 1000 experiments for B = 104, showing that TREESAMPLE finds approximations with both higher entropy and lower energy. budget B. For large problems with large budgets B (cid:29) 0, storing the entire search tree in memory might not be feasible. In this case, compiling the current tree periodically into Qφ and rebuilding it from scratch under prior Qφ and subsequent refinement using TREESAMPLE may be preferable. Concretely, we propose to train Qφ by regression on state-action values Q generated by TREESAMPLE. For generalization across branches, Q approximates directly the distribution of interest, for transfer across distributions, Q approximates the source distribution, and we apply the trained Qφ for inference search in a different target distribution. We match Qφ to Q by minimizing the expected difference of the values: (cid:34) N (cid:35) (cid:88) φ∗ = arg min E (cid:107)Qφ(·|X ) − Q (·|X )(cid:107)2 . φ PX n <n n <n 2 n=1 In practice we optimize this loss by stochastic gradient descent in a distributed learner-worker architecture detailed in the experimental section. 4 Experiments In the following, we empirically compare TREESAMPLE to other baseline inference methods on different families of distributions. We quantify approximation error by the Kullback-Leibler divergence: (cid:34) M (cid:35) (cid:88) D [P (cid:107)P ∗ ] = log Z −E ψ (X) − H [P ] (8) KL X X PX m X m=1 (cid:124) (cid:123)(cid:122) (cid:125) := = log Z + ∆D [P (cid:107)P ∗ ], KL X X where we refer to the second term in eqn. 8 as negative expected energy, and the last term is the entropy of the approximation. We can get unbiased estimates of these using samples from P . For intractable target X distributions, we compare different inference methods using ∆D := D − log Z, which is tractable to KL KL approximate and preserves ranking of different approximation methods. As baselines we consider the following: Sequential Importance Sampling (SIS), Sequential Monte Carlo (SMC) and for a subset of the environments also Gibbs sampling (GIBBS) and sampling with loopy belief 7
D or ∆D CHAIN PERMUTED CHAIN FACTOR GRAPHS 1 FACTOR GRAPHS 2 KL KL SIS 11.61 ± 1.74 9.23 ± 0.34 -21.97 ± 2.47 -31.70 ± 2.32 SMC 1.94 ± 0.48 7.08 ± 0.36 -24.09 ± 2.85 -35.90 ± 2.47 GIBBS – – -18.67 ± 1.80 -25.12 ± 1.48 BP exact exact -21.50 ± 0.18 -31.48 ± 0.48 TREESAMPLE 0.53 ± 0.17 3.41 ± 0.41 -28.89 ± 1.94 -38.70 ± 2.29 Table 1: Approximation error (lower is better) for different inference methods on four distribution classes. Results are averages and standard deviations over 1000 randomly generated distributions for each class. Budget was set to B = 104. propagation (BP); details are given in the appendix. We use the baseline methods in the following way: We generate a set of particles {xi} of size I such that we exhaust the budget B, and then return the (potentially i≤I weighted) sum of atoms (cid:80) piδ(x, xi) as the approximation density; here δ is the Kronecker delta, and i≤I pi are either set to 1/I for GIBBS, BP and to the self-normalized importance weights for SIS and SMC. Hyperparameters for all methods where tuned individually for different families of distributions on an initial set of experiments and then kept constant across all reported experiments. For further details, see the appendix. For SIS and SMC, the proposal distribution plays a comparable role to the state-action prior in TREESAMPLE. Therefore, for all experiments we used the same parametric family for Qφ for TREESAMPLE, SIS and SMC. For the sake of simplicity, in the experiments we measured and constrained the inference budget B in terms of reward evaluations, i.e. each pointwise evaluate of a R incurs a cost of one, instead of factor evaluations. n 4.1 TREESAMPLE without Parametric Value Function We first investigated inference without learned parametric Qφ. Instead, we used the simple heuristic of setting ∀a∀n Qφ(a|x ) := (N − n) log K, which corresponds to the state-action values when all factors ψ ≡ 0 n <n m vanish everywhere. 4.1.1 Chain Distributions We initially tested the algorithms on inference in chain-structured factor graphs (CHAINS). These allow for exact inference in linear time, and therefore we can get unbiased estimates of the true Kullback-Leibler divergences. We report results averaged over 103 different chains of length N = 10 with randomly generated unary and binary potential functions; for details, see appendix. The number of states per variable was set to K = 5, yielding KN ≈ 107 states in total. The results, shown in fig. 1 as a function of the inference budget B, show that TREESAMPLE outperforms the SMC baseline (see also tab. 1). In particular, TREESAMPLE generates approximations of similar quality compared to SMC with a roughly 30 times smaller budget. We further investigated the energy and entropy contributions to D KL separately. We define ∆energy= E P ∗ [(cid:80) ψ] − E PX [(cid:80) ψ] (lower is better), and ∆entropy= H[P X ]−H[P X∗ ] (higher is better). Fig. 1 shows that TREX ESAMPLE finds approximations that have lower energy as well as higher entropy compared to SMC. A known limitation of tree search methods is that they tend to under-perform for shallow (here small N ) decision-making problems with large action spaces (here large K). We performed experiments on chain distributions with varying K and N while keeping the state-space size approximately constant, i.e. N log K ≈ const. We confirmed that for very shallow, bushy problems with log K (cid:29) N , SMC outperforms TREESAMPLE, whereas TREESAMPLE dominates SMC in all other problem configurations, see fig. 3 in the appendix. Next, we considered chain-structured distributions where the indices of the variables X do not correspond n to the ordering in the chain; we call these PERMUTEDCHAINS. These are in general more difficult to solve as they exhibit "delayed" rewards, i.e. binary chain potentials ψ (X , X ) depend on non-consecutive m σ(n) σ(n+1) variables. This can create "dead-end" like situations, that SMC, not having the ability to backtrack, can get easily stuck in. Indeed, we find that SCM performs only somewhat better on this class of distributions than SIS, 8
value func. ∅ MLP GNN single graph N/A Yes Yes No No No No N N/A 20 20 20 12 16 24 train Qφ trained by SMC: ∆Dφ – +1.63 -0.19 -0.97 -1.00 -1.17 -0.64 KL – [-1.60,+2.87] [-2.68,+1.49] [-2.41,+0.40] [-1.52, -0.58] [-1.42,-0.46] [-0.84,-0.32] Qφ + SMC: ∆D +2.72 +2.93 +1.64 +2.56 +2.00 +1.64 +2.10 KL [1.02, 4.54] [-1.54,+4.32] [-1.61,+3.46] [+0.58,+4.42] [+1.72, +2.19] [+1.38,+2.05] [+1.50,+2.68] Qφ trained by TREESAMPLE: ∆Dφ – -3.61 -3.86 -2.05 -2.12 -2.52 -1.83 KL – [-5.73,-0.60] [-6.03, -0.85] [-3.58, -0.55] [-2.23,-1.99] [-2.63,-2.40] [-2.13,-1.76] Qφ + TREESAMPLE: ∆D KL 0.00 -3.63 -3.87 -2.23 -2.22 -2.64 -2.35 [-1.47, 1.68] [-5.72,-0.64] [-6.05, -0.88] [-3.75, -0.73] [-2.30,-2.05] [-2.79,-2.55] [-2.46,-2.10] Table 2: Approximation error for inference in factor graphs with TREESAMPLE and SMC, for different types of value functions and training regimes. Results are relative to TREESAMPLE w/o value function, lower is better. See main text for details. whereas TREESAMPLE achieves better results by a wide margin. Results on both families of distributions are shown in tab. 1. 4.1.2 Factor Graphs We also tested the inference algorithms on two classes of non-chain factor graphs, denoted as FACTORGRAPHS1 and FACTORGRAPHS2. Distributions in FACTORGRAPHS1 are over N = 10 variables with K = 5 states each. Factors were randomly generated with maximum degree d of 4 and their dK values where iid drawn from N (0, 1). Distributions in FACTORGRAPHS2 are over N = 20 binary variables, i.e. K = 2. These distributions are generated by two types of of factors: NOT (degree 2) and MAJORITY (max degree 4), both taking values in {0, 1}. Results are shown in tab. 1. For both families of distributions, TREESAMPLE outperforms all considered baselines by a wide margin. We found that GIBBS generally failed to find configurations with high energy due to slow mixing. BP-based sampling was observed to generate samples with high energy but small entropy, yielding results comparable to SIS. 4.2 TREESAMPLE with Parametric Value Functions Next, we investigated the performance of TREESAMPLE, as well as SMC, with additional parametric state-action value functions Qφ (used as proposal for SMC). We focused on inference problems from FACTORGRAPHS2. We implemented the inference algorithm as a distributed architecture consisting of a worker and a learner process, both running simultaneously. The worker requests an inference problem instance, and performs inference either with TREESAMPLE or SMC with a small budget of B = 2500 using the current parametric Qφ. After building the approximation P , 128 independent samples xi ∼ P are drawn from it and the inferred X X Q-values Q (·|xi ) for i = 1, . . . , 128 and n = 1, . . . , N are written into a replay buffer as training data; n ≤n following this, the inference episode is terminated, the tree is flushed and a new episode starts. The learner process samples training data from the replay for updating the parametric Qφ with an SGD step on a minibatch of size 128; then the updated model parameters φ are sent to the worker. We tracked the error of the inference performed on the worker using the unnormalized ∆D as a function of the number of completed inference KL episodes. We expect ∆D to decrease, as Qφ adapts to the inference problem, and therefore becomes better KL at guiding the search. Separately, we also track the inference performance of only using the value function Qφ without additional search around it, denoted as ∆Dφ . This is a purely parametric approximation to the KL inference problem, trained by samples from TREESAMPLE and SMC respectively. We observed that ∆D KL as well as ∆Dφ stabilized after roughly 1500 inference episodes for all experiments. Results were then averaged KL 9
over episodes 2000-4000 and are shown in tab. 2. To facilitate comparison, all results in tab. 2 are reported relative to ∆D KL for TREESAMPLE without value functions. In general, experimental results with learned value functions exhibited higher degrees of variability with some outliers. Results in tab. 2 therefore report median results over 20 runs as well as 25% and 75% percentiles. We first performed a simple set of "sanity-check" experiments on TREESAMPLE with parametric value functions in a non-transfer setting, where the worker repeatedly solves the same inference problem arising from a single factor graph. As value function, we used a simple MLP with 4-layers and 256 hidden units each. As shown in the second column of tab. 2, approximation error ∆D decreases significantly compared to plain KL TREESAMPLE without value functions. This corroborates that the value function can indeed cache part of the previous search trees and facilitate inference if training and testing factor graphs coincide. Furthermore, we observed that once Qφ is fully trained, the inference error ∆Dφ obtaind using only Qφ is only marginally KL worse than ∆D KL using Qφ plus TREESAMPLE-search on top of it; see row four and five in tab. 2 respectively. This indicates that the value function was powerful enough in this experiment to almost cache the entire search computation of TREESAMPLE. Next, we investigated graph neural networks (GNNs) [29] as value functions Qφ. This function class can make explicit use of the structure of the factor graph instances. Details about the architecture can be found in [29] and the appendix, but are briefly described in the following. GNNs consist of two types of networks, node blocks and edge blocks (we did not use global networks), that are connected according to the factor graph at hand, and executed multiple times mimicking a message-passing like procedure. We used three node block networks, one for each type of graph node, i.e. variable node (corresponding to a variable X ), n NOT-factors and MAJORITY-factors. We used four edge block networks, namely one for each combination of {incoming,outgoing}×{NOT, MAJORITY}. Empirically, we found that GNNs slightly outperform MLPs in the non-transfer setting, see third column of tab. 2. The real advantage of GNNs comes into play in a transfer setting, when the worker performs inference in a new factor graph for each episode. We keep the number of variables fixed (N = N = 20) but vary the train test number and configuration of factors across problems. GNNs successfully generalize across graphs, see fourth column of tab. 2. This is due to their ability to make use of the graph topology of a new factor graph instance, by connecting its constituent node and edge networks accordingly. Furthermore, the node and edge networks evidently learned generic message passing computations for variable nodes as well as NOT/MAJORITY factor nodes. The results show that a suitable Qφ generalizes knowledge across inference problems, leading to less approximation error on new distributions. Furthermore, we investigated a transfer setting where the worker solves inference problems on factor graphs of sizes N = 12, 16 or 24, but performance is tested on graphs train of size N = 20; see columns five to seven in tab. 2. Strikingly, we find that the value functions generalize as test well across problems of different sizes as they generalize across problems of the same size. This demonstrates that prior knowledge can successfully guide the search and greatly facilitate inference. Finally, we investigated the performance of SMC with trained value functions Qφ; see rows one and two in tab. 2. Overall, we found that performance was worse compared to TREESAMPLE: Value functions Qφ trained by SMC were found to give worse results ∆Dφ compared to those trained by TREESAMPLE, and overall KL inference error was worse compared to TREESAMPLE. Interestingly, we found that once Qφ is fully trained, performing additional SMC on top of it made results worse. Although initially counter-intuitive, these results are sensible in our problem setup. The entropy of SMC approximations ≈ log I is essentially given by the number of particles I that SMC produces; this number is limited by the budget B that can be used to compute importance weights. Once a parametric Qφ is trained, it does not need to make any further calls to the ψ m factors, and can therefore exhibit much higher entropy, therefore making ∆Dφ smaller than ∆D . KL KL 5 Related Work TREESAMPLE is based on the connection between probabilistic inference and maximum-entropy decision- making problems established by previous work. This connection has mostly been used to solve RL problems with inference methods e.g. [20, 32, 33, 21]. Closely related to our approach, this relationship has also been 10
used in the reverse direction, i.e. to solve inference problems using tools from RL [34, 22, 23, 24, 25], however without utilizing tree search and emphasizing the importance of exploration for inference. The latter has been recognized in [19], and applied to hierarchical partitioning for inference in continuous spaces, see also [35]. In contrast to this, we focus on discrete domains with sequential decision-making utilizing MCTS and value functions. Soft-Bellman backups, as used here (also referred to as soft Q-learning) and their connection to entropy-regularized RL have been explored in e.g. [36, 37]. For approximating general probabilistic inference problems, the class of Markov Chain Monte Carlo (MCMC) methods has proven very successful in practice. There, a transition operator is defined such that the target distribution is stationary under this operator. Concretely, MCMCs methods operate on a fully specified, approximate sample which is then perturbed iteratively. Transition operators are usually designed specifically for families of distributions in order to leverage problem structure for achieving fast mixing. However, mixing times are difficult to analyze theoretically and hard to monitor in practice [38]. TREESAMPLE circumvents the mixing problem by generating a new sample "from scratch" when returning to the root node and then iteratively stepping through the dimensions of the random vector. Furthermore, TREESAMPLE can make use of powerful neural networks for approximating conditionals of the target, thus caching computations for related inference problems. Although, adaptive MCMC methods exist, they usually only consider small sets of adaptive parameters [18]. Recently, MCMC methods have been extended to transition operators generated by neural networks, which are trained either by adversarial training, meta learning or mixing time criteria [39, 40, 41, 42]. However, these were formulated for continuous domains and rely on differentiability and thus do not carry over straight-forward to discrete domains. Our proposed algorithm is closely related to Sequential Monte Carlo (SMC) methods [13], another class of broadly applicable inference algorithms. Often, these methods are applied to generate approximate samples by sequentially sampling the dimensions of a random vector, e.g. in particle filtering for temporal inference problems [43]. Usually, these methods do not allow for backtracking, i.e. re-visiting previously discarded partial configurations, although few variants with some back-tracking heuristics do exist [44, 45]. This is contrast to the TREESAMPLE algorithm, which decides at every iteration where to expand the current tree based on a full tree-traversal from the root and therefore allows for backtracking an arbitrary number of steps. Furthermore, we propose to train value functions which approximately marginalize over the "future" (i.e. variables following the one in question in the ordering), thus taking into account relevant downstream effects. [46, 47] introduce adaptive NN proposals, i.e. value functions in our formulation, but these are trained to match the "filtering" distribution, thus they do not marginalize over the future. In the decision-making formulation, this corresponds to learning proposals based on immediate rewards instead of total returns. However, recent work in continuous domains has begun to address this [48, 49, 50, 51], however, they do not make use of guided systematic search. Recently, distilling inference computations into parametric functions as been extended to discrete distribu- tions based on the framework of variational inference. [34, 52] highlight connections to the REINFORCE gradi- ent estimator [53] and propose various value function-like control variates for reducing its variance. Multiple studies propose to utilize continuous relaxation of discrete variables to make use of so-called reparametrization gradients for learning inference computations, e.g. [54]. In addition to the approximate inference methods discussed above, there are numerous algorithms for exact inference in discrete models. One class of methods called Weighted Model Counting (WMC) algorithms, is based on representing the target probability distribution as Boolean formulas with associated weights, and convert inference into the problem of summing weights over satisfying assignments of the associated SAT problem [55]. In particular, it has been shown that DPLL-style SAT solvers [56] can be extended to exactly solve general discrete inference problems [57, 58], often outperforming other standard methods such as the junction tree algorithm [59]. Similar to TREESAMPLE, this DPLL-based approach performs inference by search, i.e. it recursively instantiates variables of the SAT problem. Efficiency is gained by chaching solved sub-problems [60] and heuristics for adaptively choosing the search order of variables [57]. We expect that similar techniques could be integrated into the TREESAMPLE algorithm, potentially greatly improving its efficiency. In contrast to WMC methods, TREESAMPLE dynamically chooses the most promising sub-problems to spend compute on via the UCT-like selection rule which is informed by all previous search tree expansions. 11
6 Discussion Structured distributions P ∗ , such as factor graphs, Bayesian networks etc, allow for a very compact repre- X sentation of an infinitely large set of beliefs, e.g. P ∗ implies beliefs over f (X) for every test function f , X including marginals, moments etc. This immediately raises the question: "What does it mean to ’know’" a distribution? (paraphrased from [61]). Obviously, we need to perform probabilistic inference to "convert the implicit knowledge" of P ∗ (given by e.g. factors) into "explicit knowledge" in terms of the beliefs of interest X (quoted from [62]). If the dimension of X is anything but very small, this inference process cannot be assumed to be "automatic", but ranks among the most complex computational problems known, and large amounts of computational resources have to be used to just approximate the solution. In other challenging computational problems such as optimization, integration or solving ordinary differential equations, it has been argued that the results of computations that have not yet been executed are to be treated as unobserved variables, and knowledge about them to be expressed as beliefs [63, 61]. This would imply for the inference setting considered in this paper, that we should introduce second-order, or meta-beliefs over yet-to-be-computed first-order beliefs implied by P ∗ . Approximate inference could then proceed analogously to Bayesian optimization: Evaluate X the factors of P ∗ at points that result in the largest reduction of second-order uncertainty over the beliefs of X interest. However, it is unclear how such meta-beliefs can be treated in a tractable way. Instead of such a full Bayesian numerical treatment involving second-order beliefs, we adopted cheaper Upper Confidence Bound heuristics for quantifying uncertainty. For sake of simplicity, we assumed in this paper that the computational cost of inference is dominated by evaluations of the factor oracles. This assumption is well justified e.g. in applications, where some factors represent large scale scientific simulators [6], or in modern deep latent variable models, where a subset of factors is given by deep neural networks that take potentially high-dimensional observations as inputs. If this assumption is violated, i.e. all factors can be evaluated cheaply, the comparison of TREESAMPLE to SMC and other inference methods will become less favourable for the former. TREESAMPLE incurs an overhead for traversing a search tree before expanding it, attempting to use the information of all previous oracle evaluations. If these are cheap, a less sequential and more parallel approach, such as SMC, might become more competitive. We expect that TREESAMPLE can be improved and extended in many ways. Currently, the topology of the factor graph is only partially used for the reward definition and potentially for graph net value functions. One obvious way to better leverage it would be to check if after conditioning on a prefix x , corresponding to a <n search depth n, the factor graph decomposes into independent components that can be solved independently. Furthermore, TREESAMPLE uses a fixed ordering of the variables. However, a good variable ordering can potentially make the inference problem much easier. Leveraging existing or developing new heuristics for a problem-dependent and dynamic variable ordering could potentially increase the inference efficiency of TREESAMPLE. 12
References [1] Richard T Cox. Probability, frequency and reasonable expectation. American journal of physics, 14(1):1– 13, 1946. [2] Edwin T Jaynes. Probability theory: The logic of science. Cambridge university press, 2003. [3] AM Turing. The applications of probability to cryptography. Report, GCHQ, Cheltenham, UK, 1941. [4] Robert J McEliece, David JC MacKay, and Jung-Fu Cheng. Turbo decoding as an instance of Pearl’s “belief propagation” algorithm. IEEE Journal on selected areas in communications, 16(2):140–152, 1998. [5] Mark D Robinson, Davis J McCarthy, and Gordon K Smyth. edgeR: a Bioconductor package for differential expression analysis of digital gene expression data. Bioinformatics, 26(1):139–140, 2010. [6] Atılım Günes¸ Baydin, Lei Shao, Wahid Bhimji, Lukas Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi, Bradley Gram-Hansen, Gilles Louppe, et al. Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale. arXiv preprint arXiv:1907.03382, 2019. [7] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013. [8] Judea Pearl. Causality: models, reasoning and inference, volume 29. Springer, 2000. [9] Rina Dechter. Bucket elimination: A unifying framework for reasoning. Artificial Intelligence, 113(1- 2):41–85, 1999. [10] Dan Roth. On the hardness of approximate reasoning. Artificial Intelligence, 82(1-2):273–302, 1996. [11] Larry Stockmeyer. On approximation algorithms for # P. SIAM Journal on Computing, 14(4):849–861, 1985. [12] W Keith Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 1970. [13] Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential Monte Carlo samplers. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006. [14] Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183–233, 1999. [15] Radford M Neal et al. MCMC using Hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11):2, 2011. [16] Gareth O Roberts and Osnat Stramer. Langevin diffusions and Metropolis-Hastings algorithms. Methodol- ogy and computing in applied probability, 4(4):337–357, 2002. [17] Vikash Mansinghka, Daniel Roy, Eric Jonas, and Joshua Tenenbaum. Exact and approximate sampling by systematic stochastic search. In Artificial Intelligence and Statistics, pages 400–407, 2009. [18] Christophe Andrieu and Johannes Thoms. A tutorial on adaptive MCMC. Statistics and computing, 18(4):343–373, 2008. [19] Xiaoyu Lu, Tom Rainforth, Yuan Zhou, Jan-Willem van de Meent, and Yee Whye Teh. On Exploration, Exploitation and Learning in Adaptive Importance Sampling. arXiv preprint arXiv:1810.13296, 2018. [20] Peter Dayan and Geoffrey E Hinton. Using expectation-maximization for supervised learning. Neural Computation, 9(2):271–278, 1997. 13
[21] Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforce- ment learning by approximate inference. In Twenty-Third International Joint Conference on Artificial Intelligence, 2013. [22] Theophane Weber, Nicolas Heess, S. M. Ali Eslami, John Schulman, David Wingate, and David Silver. Reinforced variational inference. In In Advances in Neural Information Processing Systems (NIPS) Workshop on Advances in Approximate Bayesian Inference. 2015, 2015. [23] David Wingate and Theophane Weber. Automated variational inference in probabilistic programming. arXiv preprint arXiv:1301.1299, 2013. [24] John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient Estimation Using Stochastic Computation Graphs. CoRR, abs/1506.05254, 2015. [25] Théophane Weber, Nicolas Heess, Lars Buesing, and David Silver. Credit Assignment Techniques in Stochastic Computation Graphs. CoRR, abs/1901.01761, 2019. [26] Levente Kocsis and Csaba Szepesvári. Bandit based Monte-Carlo planning. In European conference on machine learning, pages 282–293. Springer, 2006. [27] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of Monte Carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1–43, 2012. [28] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. nature, 529(7587):484, 2016. [29] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [30] Adnan Darwiche. A logical approach to factoring belief networks. KR, 2:409–420, 2002. [31] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235–256, 2002. [32] Hagai Attias. Planning by probabilistic inference. In AISTATS, 2003. [33] Matt Hoffman, Arnaud Doucet, Nando de Freitas, and Ajay Jasra. Trans-dimensional MCMC for Bayesian policy learning. In Proceedings of the 20th International Conference on Neural Information Processing Systems, pages 665–672. Curran Associates Inc., 2007. [34] Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv preprint arXiv:1402.0030, 2014. [35] Tom Rainforth, Yuan Zhou, Xiaoyu Lu, Yee Whye Teh, Frank Wood, Hongseok Yang, and Jan-Willem van de Meent. Inference trees: Adaptive inference with exploration. arXiv preprint arXiv:1806.09550, 2018. [36] John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft Q-learning. arXiv preprint arXiv:1704.06440, 2017. [37] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep supervised learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018. 14
[38] Mary Kathryn Cowles and Bradley P Carlin. Markov chain Monte Carlo convergence diagnostics: a comparative review. Journal of the American Statistical Association, 91(434):883–904, 1996. [39] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-MC: Adversarial training for MCMC. In Advances in Neural Information Processing Systems, pages 5140–5150, 2017. [40] Daniel Levy, Matthew D Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with neural networks. arXiv preprint arXiv:1711.09268, 2017. [41] Kirill Neklyudov, Pavel Shvechikov, and Dmitry Vetrov. Metropolis-Hastings view on variational inference and adversarial training. arXiv preprint arXiv:1810.07151, 2018. [42] Tongzhou Wang, Yi Wu, Dave Moore, and Stuart J Russell. Meta-learning MCMC proposals. In Advances in Neural Information Processing Systems, pages 4146–4156, 2018. [43] Arnaud Doucet and Adam M Johansen. A tutorial on particle filtering and smoothing: Fifteen years later. Handbook of nonlinear filtering, 12(656-704):3, 2009. [44] Martin Klepal, Stephane Beauregard, et al. A backtracking particle filter for fusing building plans with PDR displacement estimates. In 2008 5th Workshop on Positioning, Navigation and Communication, pages 207–212. IEEE, 2008. [45] Peter Grassberger. Sequential Monte Carlo methods for protein folding. arXiv preprint cond-mat/0408571, 2004. [46] Shixiang Shane Gu, Zoubin Ghahramani, and Richard E Turner. Neural adaptive sequential Monte Carlo. In Advances in Neural Information Processing Systems, pages 2629–2637, 2015. [47] Kira Kempinska and John Shawe-Taylor. Adversarial Sequential Monte Carlo. In Bayesian Deep Learning (NIPS Workshop), 2017. [48] Pieralberto Guarniero, Adam M Johansen, and Anthony Lee. The iterated auxiliary particle filter. Journal of the American Statistical Association, 112(520):1636–1647, 2017. [49] Jeremy Heng, Adrian N Bishop, George Deligiannidis, and Arnaud Doucet. Controlled sequential Monte Carlo. arXiv preprint arXiv:1708.08396, 2017. [50] Dieterich Lawson, George Tucker, Christian A Naesseth, Chris J Maddison, Ryan P Adams, and Yee Whye Teh. Twisted variational sequential Monte Carlo. In Bayesian Deep Learning Workshop, NIPS, 2018. [51] Alexandre Piché, Valentin Thomas, Cyril Ibrahim, Yoshua Bengio, and Chris Pal. Probabilistic planning with sequential Monte Carlo methods. ICLR 2019, 2018. [52] Andriy Mnih and Danilo J Rezende. Variational inference for Monte Carlo objectives. arXiv preprint arXiv:1602.06725, 2016. [53] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist supervised learning. Machine learning, 8(3-4):229–256, 1992. [54] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. [55] Mark Chavira and Adnan Darwiche. On probabilistic inference by weighted model counting. Artificial Intelligence, 172(6-7):772–799, 2008. [56] Martin Davis, George Logemann, and Donald Loveland. A machine program for theorem-proving. Communications of the ACM, 5(7):394–397, 1962. 15
[57] Tian Sang, Paul Beame, and Henry Kautz. Solving Bayesian networks by weighted model counting. In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI-05), volume 1, pages 475–482. AAAI Press, 2005. [58] Fahiem Bacchus, Shannon Dalmao, and Toniann Pitassi. Solving # SAT and Bayesian inference with backtracking search. Journal of Artificial Intelligence Research, 34:391–442, 2009. [59] Steffen L Lauritzen and David J Spiegelhalter. Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society: Series B (Methodological), 50(2):157–194, 1988. [60] Fahiem Bacchus, Shannon Dalmao, and Toniann Pitassi. Algorithms and complexity results for # SAT and Bayesian inference. In 44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings., pages 340–351. IEEE, 2003. [61] Persi Diaconis. Bayesian numerical analysis. Statistical decision theory and related topics IV, 1:163–175, 1988. [62] Samuel Gershman. personal communication. [63] Jonas Mocˇkus. On Bayesian methods for seeking the extremum. In Optimization Techniques IFIP Technical Conference, pages 400–404. Springer, 1975. [64] Marc Mezard, Marc Mezard, and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009. [65] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 16
A Details for TREESAMPLE algorithm We define a search tree T in the following way. Nodes in T at depth n ∈ {0, 1, . . . , N } are indexed by the (partial) state x ∈ {1, . . . , K}n, and the root is denoted by ∅. Each node x at depth n = len(x) keeps track of the corresponding reward evaluation R (x) and the following quantities for all its children: n 1. visit counts η (·|x) = (η (1|x), . . . , η (K|x)) ∈ NK over the children, n+1 n+1 n+1 2. state-action values Q (·|x) ∈ RK, n+1 3. prior state-action values Qφ (·|x) ∈ RK, and n+1 4. a boolean vector C (·|x) ∈ {0, 1}K if its children are complete (i.e. fully expanded, see below). n+1 Standard MCTS with (P)UCT-style tree traversals applied to the inference problem can in general visit any state-action pair multiple times; this is desirable behavior in general MDPs with stochastic rewards, where reliable reward estimates require multiple samples. However, the reward R in our MDP is deterministic as defined in eqn. 2, and therefore there is no benefit in re-visiting fully-expanded sub-trees. To prevent the TREESAMPLE algorithm from doing so, we explicitly keep track at each node if the sub-tree rooted in it is fully-expanded; such a node is called complete. Initially no internal node is complete, only leaf nodes at depth N are tagged as complete. In the backup stage of the tree-traversal, we tag a visited node as complete if it is a node of depth N (corresponding to a completed sample) or if all its children are complete. We modify the action selection eqn. 7 such that the arg max is only taken over actions not leading to complete sub-trees. The TREESAMPLE algorithm is given in full in algorithm 2. B Proofs B.1 Observation 2 Proof. This observation has been proven previously in the literature, but we will give a short proof here for completeness. We show the statement by determining the optimal policy and value function by backwards dynamic programming (DP). We anchor the DP induction by defining the optimal value function at step N + 1 as zero, i.e. V ∗ (x ) = 0. Using the law of iterated expectations, we can decompose the optimal value N+1 ≤N function in the following way for any n = N, . . . , 1: (cid:34) N (cid:35) (cid:88) V ∗(x ) = max E R − log π n <n P π n(cid:48) n(cid:48) πn,...,πN X≥n|x<n n(cid:48)=n (cid:34) N (cid:35) (cid:88) = max E max E R − log π P π P π n(cid:48) n(cid:48) πn Xn|x<n πn+1,...,πN X>n|x<n◦Xn n(cid:48)=n (cid:34) (cid:34) N (cid:35)(cid:35) (cid:88) = max E R − log π + max E R − log π P π n n P π n(cid:48) n(cid:48) πn Xn|x<n πn+1,...,πN X>n|x<n◦Xn n(cid:48)=n+1 = max E (cid:2) R − log π + V ∗ (cid:3) . P π n n n+1 πn Xn|x<n Therefore, assuming by induction that V ∗ has been computed, we can find the optimal policy π∗ and value n+1 n V ∗ at step n by solving: n K arg max (cid:88) f (a) · (cid:0) R (x ◦ a) − log f (a) + V ∗ (x ◦ a)(cid:1) (9a) n <n n+1 <n f:{1,...,K}→[0,1] a=1 K (cid:88) subject to f (a) = 1. (9b) a=1 17
The solution to this optimization problem can be found by the calculus of variations (omitted here) and is given by: log π∗(x |x ) ∝ R (x ) + V ∗ (x ) = Q∗ (x |x ), n <n n ≤n n+1 ≤n n n <n where we used the definition of the optimal state-action value function. Furthermore, at the optimum, the objective eqn. 9a assumes the value: K (cid:88) V ∗(x ) = log exp Q∗ (x |x ). n <n n n <n xn=1 This expression, together with the definition of Q∗ establishes the soft-Bellman equation. The optimal value V ∗ is also exactly the log-normalizer for π∗. Therefore, we can write: n+1 n log π∗(x |x ) = Q∗ (x |x ) − V ∗ (x ). n n <n n n <n n+1 ≤n B.2 Proof of Lemma 1 Proof. We will show this statement by induction on the depth d := N − n of the sub-tree T with root x≤n x . For d = 0, i.e. n = N , the state-action values Q (·|x ) := − log K are defined such that ≤n N+1 ≤N V (x ) = 0, which is the correct value. Consider now the general case 1 ≤ d ≤ N . Let T (cid:48) be the N+1 ≤N x≤n sub-tree before the last tree traversal that expanded the last missing node x , ie T = T (cid:48) ∪ x ; for ≤N x≤n x≤n ≤N an illustration see fig. 2. The soft-Bellman backups of the last completing tree-traversal on the path leading to x are by construction all of the following form: For any node x on the path, all children except for ≤N ≤m one correspond to already completed sub-trees (before the last traversal). The sub-tree of the one remaining child is completed by the last traversal. All complete sub-trees on the backup path are of depth smaller than d and therefore by induction their roots have the correct values V ∗ (x ). Hence evaluating the soft-Bellman m+1 ≤m backup eqn. 6 (with the true noiseless reward R) yields the correct value for x . ≤n C Details for Experiments C.1 Baseline Inference Methods C.1.1 SIS and SMC For each experiment we determined the number of SIS and SMC particles I such that the entire budget B was used. We implemented SMC with an resampling threshold t ∈ [0, 1], i.e. a resampling step was executed when the effective sample size (ESS) was smaller than tI. The threshold t was treated as a hyperparameter; SMC with t = 0 was used as SIS results. C.1.2 BP We used the algorithm outline on p. 301 from [64]. For generating a single approximate sample from the target, the following procedure was executed. Messages from variable to factor nodes were initialized as uniform; then N message-passing steps, each consisting of updating factor-variable and variable-factor messages were message performed. X was then sampled form the resulting approximate marginal, and the messages from X to its 1 1 neighboring factors were set to the corresponding atom. This was repeated until all variables X were sampled, n generating one approximate sample from the joint P ∗ . X In total, we generated multiple samples with the above algorithm such that the budget B was exhausted. The number N of message-passing steps before sampling each variable X |X was treated as a message n <n hyperparameter. 18
C.1.3 GIBBS We implemented standard Gibbs sampling. All variables were initially drawn uniformly from {1, . . . , K} , and N iterations, each consisting of updating all variables in the fixed order X to X , were executed. This Gibbs 1 N generated a single approximate sample. We repeated this procedure to generate multiple samples such that the budget B was exhausted. We treated N as a hyperparameter. Gibbs C.2 Hyperparameter optimization For each inference method (except for SIS) we optimized one hyperparameter on a initial set of experiments. For TREESAMPLE, we fixed (cid:15) = 0.1 and optimized c from eqn. 7. Different hyperparameter values were used for different families of distributions. Hyperparameters were chosen such as to yield lowest ∆D . KL C.3 Details for Synthetic Distributions C.3.1 Chains The unary potentials ψ (x ) for n = 1, . . . , N for the chain factor graphs where randomly generated in the n n following way. The values of ψ (x = k) for n = 1, . . . , N and k = 1, . . . , K where jointly drawn from a n n GP over the two dimensional domain {1, . . . , N } × {1, . . . , K} with an RBF kernel with bandwidth 1 and scale 0.5. Binary potentials ψ (x , x ) were set to 2.5 · d(x , x ), where d(x , x ) is the distance n,n+1 n n+1 n n+1 n n+1 between x and x on the 1-d torus generated by constraining 1 and K to be neighbors. n n+1 C.3.2 PermutedChains We first uniformly drew random permutations σ : {1, . . . , N } → {1, . . . , N }. We then randomly generated conditional probability tables for P ∗ by draws from a symmetric Dirichlet with concentration Xσ(n)|xσ(n−1) parameter α = 1. These were then used as binary factors. C.3.3 FactorGraphs1 We generated factor graphs for this family in the following way. First, we constructed Erdo˝s-Rényi random graphs with N nodes with edge probability p = 2 log(N )/N ; graphs with more than one connected component were rejected. For each clique in this graph we inserted a random factor and connected it to all nodes in the clique; graphs with cliques of size > 4 where rejected. For applying the sequential inference algorithms TREESAMPLE, SIS and SMC, variables in the graph were ordered by a simple heuristic. While iterating over factors in order of descending degree, all variables in the current factor were were added to the ordering until all were accounted for. C.3.4 FactorGraphs2 We generated factor graphs for this family over binary random variables K = 2 in the following way. Variables X and X for n = 0, . . . , N/2 − 1 were connected with a NOT factor, which carries out the 2n+1 2(n+1) computation XOR(X , X ). We then constructed Erdo˝s-Rényi random graphs of size N/2 over 2n+1 2(n+1) all pairs of nodes (X , X ) with edge probability p = 3 log(N/2)/N ; graphs with more than one 2n+1 2(n+1) connected component were rejected. For each clique in this intermediate graph we inserted a MAJORITY factor and connected it to either to X or X ; graphs with cliques of size > 4 where rejected. MAJORITY 2n+1 2(n+1) factors return a value of 1.0 if half or more nodes in its neighborhood are 2 and return 0 otherwise. The output values of all factors were also scaled by 2.0; otherwise the resulting distributions were found to be very close to uniform. 19
x ≤n (cid:46). ↓.(cid:38). (cid:46). ↓.(cid:38). x ≤m (cid:46). ↓.(cid:38). (cid:46). ↓.(cid:38). leaf x ≤N Figure 2: Completing a sub-tree yields the exact Q-value. Assume the tree traversal shown in blue completes the sub-tree T rooted in x . Then, by construction, the soft-Bellmann backups along this path, at every x≤n ≤n intermediate node x for m > n, take as input the values of all children. By construction, all but one children ≤m correspond to complete sub-trees of a smaller depth; these have the correct values by induction. The other remaining child corresponds to a sub-tree that was completed by the last traversal and therefore has also the correct value. C.4 Details For Experiments w/ Value Functions All neural networks were trained with the ADAM optimizer [65] with a learning rate of 3·10−4 and mini-batches of size 128. The replay buffer size was set to 104. The MLP value function used for the experiment consisted of 4 hidden layers with 256 units each with RELU activation functions. Increasing the number of units or layers did not improve results. The GNN value function Qφ(·|x ) was designed as follows. Each variable node in the factor graph was ≤n given a (16 + K)-dimensional feature vector, and each edge node a 16-dimensional feature vector. The input prefix x was encoded in a one-hot manner in the first K components of the variable feature vectors for ≤n variables up to n; for variables > n the first K components were set to 0. All edge features were initialized to 0. All node and edge block networks where chosen to be MLPs with 3 hidden layers and 16 units each with RELU activations. Results did not improve with deeper or wider networks. The resulting GNN was iterated 4 times; interestingly more iterations actually reduced final performance somewhat. The output feature vector of the GNN at variable node x was then passed to a linear layer with K outputs yielding the vector n (Qφ(x = 1|x ), . . . , Qφ(x = K|x )). n+1 ≤n n+1 ≤n 20
Figure 3: Approximation error for inference in chain graphs as a function of varying chain length N ; the number K of states per variable was abjusted such that the total domain size N log K stayed roughly constant. TREESAMPLE (red) performed worse than SMC (turquoise) for short and wide chains, but performs better everywhere else. 21
Algorithm 2 TREESAMPLE procedures 1: globals reward function R, prior state-action value function Qφ 2: procedure TREESAMPLE(budget B) 3: initialize empty tree T ← ∅ 4: available budget b ← B 5: while b > M do 6: T , ∆b ← TREETRAVSERSAL (T ) 7: b ← b − ∆b 8: end while 9: return tree T 10: end procedure 11: procedure TREETRAVERSAL(tree T ) // traversal 12: x ← ∅ 13: while x ∈ T do 14: n ← len(x) 15: a ← Q-UCT(η n+1(·|x), Q n+1(·|x), Qφ n+1(·|x), C n+1(·|x)) 16: x ← x ◦ a 17: end while // expansion 18: if x (cid:54)∈ T then 19: T ← T ∪ EXPAND(x) 20: used budget ∆b ← |M n| // see def. 1 21: end if // backup 22: for n = len(x), . . . , 1, 0 do 23: V n+1(x ≤n) = log (cid:80)K a(cid:48)=1 exp Q n+1(a(cid:48)|x ≤n) 24: Q n(x n|x <n) ← R n(x n|x <n) + V n+1(x ≤n) 25: C n(x n|x <n) ← min a(cid:48) C n+1(a(cid:48)|x ≤n) 26: η n(x n|x <n) ← η n(x n|x <n) + 1 27: end for 28: return T , ∆b 29: end procedure 30: procedure Q-UCT(η n+1(·|x), Q n+1(·|x), Qφ n+1(·|x), C n+1(·|x)) 31: return arg max of eqn. 7 over in-complete children {a|C n+1(a|x) = 0} 32: end procedure 33: procedure EXPAND(state x) 34: n ← len(x) 35: evaluate reward function R n(x n|x <n) 36: initialize η n+1(·|x) ← (0, . . . , 0) 37: if n = N then // x is leaf 38: initialize Q n+1(a · |x) ← − log K for all a ∈ {1, . . . , K} 39: initialize C n+1(·|x) ← (1, . . . , 1) 40: else 41: evaluate prior Qφ (·|x) n+1 42: initialize Q n+1(·|x) ← Qφ n+1(·|x) 43: initialize C n+1(·|x) ← (0, . . . , 0) 44: end if 45: return node x with η n+1(·|x), Q n+1(·|x), Qφ n+1(·|x), C n+1(·|x), R n(x n|x <n) 46: end procedure 22
