Adaptive Exploration in Linear Contextual Bandit Botao Hao Tor Lattimore Csaba Szepesv´ari Princeton University Deepmind Deepmind and University of Alberta Abstract online recommendation systems [2, 18]. In this paper we propose a new algorithm for this problem that Contextual bandits serve as a fundamental is asymptotically optimal, computationally efficient model for many sequential decision mak- and empirically well-behaved in finite-time regimes. ing tasks. The most popular theoretically As a consequence of asymptotic optimality, the al- justified approaches are based on the opti- gorithm adapts to easy instances where it achieves mism principle. While these algorithms can sub-logarithmic regret. be practical, they are known to be subop- Popular approaches for regret minimisation in contex- timal asymptotically. On the other hand, tual bandits include ε-greedy [15], explicit optimism- existing asymptotically optimal algorithms based algorithms [9, 20, 7, 1], and implicit ones, such for this problem do not exploit the linear as Thompson sampling [3]. Although these algo- structure in an optimal way and suffer from rithms enjoy near-optimal worst-case guarantees and lower-order terms that dominate the regret can be quite practical, they are known to be arbi- in all practically interesting regimes. We trarily suboptimal in the asymptotic regime, even in start to bridge the gap by designing an the non-contextual linear bandit [16]. algorithm that is asymptotically optimal We propose an optimisation-based algorithm that and has good finite-time empirical perfor- estimates and tracks the optimal allocation for mance. At the same time, we make con- each context/action pair. This technique is most nections to the recent literature on when well known for its effectiveness in pure exploration exploration-free methods are effective. In- [6, 12, 10, and others]. The approach has been used deed, if the distribution of contexts is well in regret minimisation in linear bandits with fixed behaved, then our algorithm acts mostly action sets [16] and structured bandits [8]. The last greedily and enjoys sub-logarithmic regret. two articles provide algorithms for the non-contextual Furthermore, our approach is adaptive in case and hence cannot be applied directly to our set- the sense that it automatically detects the ting. More importantly, however, the algorithms are nice case. Numerical results demonstrate not practical. The first algorithm uses a complicated significant regret reductions by our method three-phase construction that barely updates its es- relative to several baselines. timates. The second algorithm is not designed to handle large action spaces and has a ‘lower-order’ 1 INTRODUCTION term in the regret that depends linearly on the num- ber of actions and dominates the regret in all prac- Stochastic contextual linear bandits, the problem tical regimes. This lower-order term is not merely we consider, is interesting due to its rich structure a product of the analysis, but also reflected in the and also because of its potential applications, e.g., in experiments (see Section 5.4 for details). The most closely related work is by Ok et al. [19] who Proceedings of the 23rdInternational Conference on Artificial study a supervised learning setting. A stochastic Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. contextual bandit can be viewed as a Markov decision PMLR: Volume 108. Copyright 2020 by the author(s). 0202 raM 51 ]GL.sc[ 2v69960.0191:viXra
process where the state represents the context and (Section 3). Section 4 introduces our new algorithm, the transition is independent of the action. The which is claimed to match the lower bound. A proof structured nature of the mentioned paper means our sketch of this claim is presented in the same section. setting is covered by their algorithm. Again, however, Section 5 presents experiments to illuminate the be- the algorithm is too general to exploit the specific haviour of the new algorithm in comparison to its structure of the contextual bandit problem. Their strongest competitors. Section 6 discusses remaining algorithm is asymptotically optimal, but suffers from notable open questions. lower-order terms that are linear in the number of Notation Let [n] = {1, 2, . . . , n}. For a vector x actions and dominate the regret in all practically and positive semidefinite matrix A we let (cid:107)x(cid:107) = interesting regimes. In contrast, our algorithm is √ A x(cid:62)Ax. The cardinality of a set A is denoted by asymptotically optimal, but also practical in finite- |A|. horizon regimes, as will be demonstrated by our experiments. 2 PROBLEM SETTING The contextual linear bandit also serves as an inter- esting example where the asymptotics of the problem We consider the stochastic K-armed contextual lin- are not indicative of what should be expected in ear bandit with a horizon of n rounds and M possible finite-time (see the second scenario in Section 5.2). contexts. The assumption that the contexts are dis- This is in contrast to many other bandit models crete cannot be dropped but as we shall at least M where the asymptotic regret is also roughly optimal will not play an important role in the regret bounds. in finite time [17]. There is an important lesson here. This assumption would hold for example in a rec- Designing algorithms that optimize for the asymp- ommender system if users are clustered into finitely totic regret may make huge sacrifices in finite-time. many groups. For each context m ∈ [M ] there is a Another interesting phenomenon is related to the known feature/action set Am ⊂ Rd with |Am| = K. idea of ‘natural exploration’ that occurs in contextual The interaction protocol is as follows. First the envi- bandits [5, 13]. A number of authors have started ronment samples a sequence of independent contexts to investigate the striking performance of greedy al- (c )n from an unknown distribution p over [M ] t t=1 gorithms in contextual bandits. In most bandit set- and each context is assumed to appear with positive tings the greedy policy does not explore sufficiently probability. At the start of round t the context c t and suffers linear regret. In some contextual bandit is revealed to the learner, who may use their obser- problems, however, the changing features ensure the vations to choose an action X t ∈ A t = Act. The algorithm cannot help but explore. Our algorithm reward is and analysis highlights this effect (see Section 3.1 Y = (cid:104)X , θ(cid:105) + η , t t t for details). If the context distribution is sufficiently rich, then the algorithm is eventually almost com- where (η )n is a sequence of independent standard t t=1 pletely greedy and enjoys sub-logarithmic regret. As Gaussian random variables and θ ∈ Rd is an unknown opposed to the cited previous works, our algorithm parameter. The Gaussian assumption can be relaxed achieves this under the cited favourable conditions to conditional sub-Gaussian assumption for the regret while at the same time it satisfies the standard op- upper bound, but is necessary for the regret lower timality guarantees when the favourable conditions bound. Throughout, we consider a frequentist setting do not hold. As another contribution, we prove that in the sense that θ is fixed. For simplicity, we assume algorithms based on optimism, similarly to the new each Am spans Rd and (cid:107)x(cid:107) ≤ 1 for all x ∈ ∪ Am. 2 m algorithm, also enjoy sub-logarithmic regret in the The performance metric is the cumulative expected rich-context distribution setting (Theorem 3.9), and regret, which measures the difference between the hence differences appear in lower order terms only expected cumulative reward collected by the omni- between these algorithms. scient policy that knows θ and the learner’s expected The rest of the paper is organized as follows. We cumulative reward. The optimal arm associated with first introduce the problem setting (Section 2), which context m is x∗ = argmax (cid:104)x, θ(cid:105). Then the ex- m x∈Am we follow by presenting our asymptotic lower bound pected cumulative regret of a policy π when facing 2
the bandit determined by θ is Theorem 3.3 (Asymptotic Lower Bound). Under the same conditions as Lemma 3.2, (cid:34) n n (cid:35) (cid:88) (cid:88) Rπ(n) = E (cid:104)x∗ , θ(cid:105) − Y . Rπ(n) θ ct t lim inf θ ≥ C(θ, A1, . . . , AM ) , (3.4) t=1 t=1 n→∞ log(n) Note that this cumulative regret also depends on where C(θ, A1, . . . , AM ) is defined as the optimal the context distribution p and action sets. They value of the following optimisation problem: are omitted from the notation to reduce clutter and M because there will never be ambiguity. (cid:88) (cid:88) inf α ∆m (3.5) x,m x αx,m∈[0,∞] m=1 x∈Am 3 ASYMPTOTIC LOWER BOUND subject to the constraint that for any context m and suboptimal arm x ∈ Am, We investigate the fundamental limit of linear con- textual bandit by deriving its instance-dependent (cid:32) (cid:88)M (cid:88) (cid:33)−1 (∆m)2 x(cid:62) α xx(cid:62) x ≤ x . (3.6) asymptotic lower bound. First, we define the class x,m 2 m=1 x∈Am of policies that are taken into consideration. Definition 3.1 (Consistent Policy). A policy π is Given the result in Lemma 3.2, the proof of Theorem called consistent if the regret is subpolynomial for 3.3 follows exactly the same idea of the proof of any bandit in that class and all context distributions: Corollary 2 in [16] and thus is omitted here. Later on we will prove a matching upper bound in Theorem R θπ(n) = o(nε), for all ε > 0 and all θ ∈ Rd. (3.1) 4.3 and argue that our asymtotical lower bound is sharp. The next lemma is the key ingredient in prov- Remark 3.4. In the above we adopt the convention ing the asymptotic lower bound. Given a con- that ∞ × 0 = 0 so that α ∆m = 0 whenever x,m x text m and x ∈ Am let ∆m x = (cid:104)x∗ m − x, θ(cid:105) be ∆m x = 0. The inverse of a matrix with infinite entries the suboptimality gap. Furthermore, let ∆ min = is defined by passing to the limit in the obvious way, min m∈[M] min x∈Am,∆m x >0 ∆m x . and is not technically an inverse. Lemma 3.2. Assume that p(m) > 0 for all m ∈ [M ] Remark 3.5. Let us denote {α∗ } as x,m x∈Am,m∈[M] and that x∗ is uniquely defined for each context m an optimal solution to the above optimisation prob- m and let π be consistent. Then for sufficiently large n lem. It serves as the optimal allocation rule for each the expected covariance matrix arm such that the cumulative regret is minimized subject to the width of the confidence interval of each (cid:34) n (cid:35) G¯ = E (cid:88) X X(cid:62) , (3.2) sub-optimal arm is small. Specifically, α x∗ ,m log(n) n t t can be interpreted as the approximate optimal num- t=1 ber of times arm x should be played having observed is invertible. Furthermore, for any context m and context m. any arm x ∈ Am, Remark 3.6. Our lower bound may also be derived li nm →s ∞up log(n)(cid:13) (cid:13)x − x∗ m(cid:13) (cid:13)2 G¯− n 1 ≤ (∆ 2m x )2 . (3.3) f sr to om chaa stm icor ce ong te en xe tr ua al l b bo au nn dd ito cf aO n k beet via el w. e[1 d9 a], ss ain kc ie nda of Markov decision process. We use an alternative proof technique and the two lower bound statements The proof is deferred to Appendix A.1 in the sup- have different forms. The proof is included for com- plementary material. Intuitively, the lemma shows pleteness. that any consistent policy must collect sufficient sta- tistical evidence at confidence level 1 − 1/n that Example 3.7. When M = 1 and A1 = {e , . . . , e } 1 d suboptimal arms really are suboptimal. This corre- is the standard basis vectors, the problem reduces sponds to ensuring that the width of an appropriate to classical multi-armed bandit and C(θ, A1) = confidence interval (cid:112) 2 log(n)(cid:107)x − x∗ (cid:107) is approx- (cid:80) 2/∆ , which matches the well-known m G¯− n 1 x∈A1,∆x>0 x imately smaller than the sub-optimality gap ∆m. asymptotic lower bound by [14]. x 3
The constant C(θ, A1, . . . , AM ) depends on both C(θ, A1, . . . , AM ) = 0. Therefore our upper bound the unknown parameter θ and the action sets will show that when the set of optimal actions A1, . . . , AM , but not the context distribution p. In {x∗, . . . , x∗ } spans Rd our new algorithm satisfies 1 M this sense there is a certain discontinuity in the hard- Rπ(n) ness measure C as a function of the context dis- lim inf θ = 0 . n→∞ log(n) tribution. More precisely, problems where p(m) is arbitrarily close to zero may have different regret Remark 3.8. The choice of α x,m above shows that asymptotically than the problem obtained by remov- when {x∗, . . . , x∗ } span Rd, then an asymptotically 1 M ing context m entirely. Clearly as p(m) tends to zero optimal algorithm only needs to play suboptimal the mth context is observed with vanishingly small arms sub-logarithmically often, which means the al- probability in finite time and hence the asymptoti- gorithm is eventually very close to the greedy al- cally optimal regret may not be representative of the gorithm. Bastani et al. [5], Kannan et al. [13] also finite-time hardness. investigate the striking performance of greedy algo- rithms in contextual bandits. However, [5] assume 3.1 Sub-logarithmic regret the covariate diversity on the context distribution while [13] assume the context is artificially perturbed Our matching upper and lower bounds reveal the with noise – these assumptions make these works interesting phenomenon that if the action sets satisfy brittle. In addition, [5] only provide a rate-optimal certain conditions, then sub-logarithmic regret is algorithm while our algorithm is optimal in constants possible. Consider the scenario that the set of optimal (see Theorem 4.3 for details). arms {x∗, . . . , x∗ } spans Rd 1. Let Λ ∈ R be a large 1 M constant to be defined subsequently and for each As claimed in the introduction, we also prove that context m and arm x ∈ Am, let α be 0 if x (cid:54)= x∗ , algorithms based on optimism can enjoy bounded x,m m and be Λ if else. Then, regret when the set of optimal actions spans the space of all actions. The proof of the following theorem is M M (cid:88) (cid:88) (cid:88) given in Appendix B.7. α xx(cid:62) = Λ x∗ x∗(cid:62) . (3.7) x,m m m Theorem 3.9. Consider the policy π that plays m=1 x∈Am m=1 optimistically by Since the set of optimal arms spans Rd it holds that for any context m and arm x ∈ Am, X t = argmax(cid:104)θ(cid:98)t−1, x(cid:105) + (cid:107)x(cid:107) G−1β t1/2 . x∈Act t (cid:32) (cid:88)M (cid:33)−1 Suppose that θ is such that {x∗, . . . , x∗ } spans Rd. x(cid:62) x∗ x∗(cid:62) x < ∞ . (3.8) 1 M m m Then, for suitable (β )n with β = O(d log(t)), it m=1 t t=1 t holds that lim sup Rπ(n) < ∞. n→∞ θ Combining Eq. (3.7) and Eq. (3.8), Note, the choice of (β ) for which the above theorem (cid:32) M (cid:33)−1 t √ (cid:88) (cid:88) holds also guarantees the standard O(cid:101)(d n) minimax x(cid:62) α xx(cid:62) x x,m bound for this algorithm, showing that LinUCB can m=1 x∈Am adapt online to this nice case. (cid:32) M (cid:33)−1 (cid:88) = Λ−1x(cid:62) x∗ x∗(cid:62) x . m m 4 OPTIMAL ALLOCATION m=1 MATCHING Hence, the constraint in Eq. (3.6) is satisfied for suf- ficiently large Λ. Since with this choice of (α ) x,m we have (cid:80)M (cid:80) α ∆m = 0, it follows that The instance-dependent asymptotic lower bound pro- m=1 x∈Am x,m x vides an optimal allocation rule. However, the opti- 1This condition is both sufficient and necessary. More mal allocation {α∗ } depends on the unknown x,m x,m precisely, sub-logarithmic regret is possible if and only if sub-optimality gap. In this section, we present a the optimal arms span the space that is spanned by all novel matching algorithm that simultaneously esti- the available actions. Since in this paper we assume the action set spans Rd for simplicity, the two conditions are mates the unknown parameter θ using least squares equivalent. and updates the allocation rule. 4
4.1 Algorithm holds for any x ∈ Act: L puet llsN ox f ( at r) m= x (cid:80) aftt s e= r1 rI o( uX ns d = t anx d) Gbe t =th (cid:80)e t sn =u 1m Xb se Xr s(cid:62)of . (cid:107)x(cid:107)2 G− t−1 1 ≤ max (cid:110) (∆(cid:98) min( ft n− 1))2 , (∆(cid:98) c xt( ft n− 1))2 (cid:111) . The least squares estimator is θ(cid:98)t = G− t 1 (cid:80)t x=1 X sY s. (4.3) For each context m the estimated sub-optimality The algorithm exploits if Eq. (4.3) holds and explores gap of arm x ∈ Am is ∆(cid:98) m x (t) = max y∈Am(cid:104)y − otherwise, as explained below. x, θ(cid:98)t(cid:105) and the estimated optimal arm is x (cid:98)∗ m(t) = argmax x∈Am(cid:104)x, θ(cid:98)t(cid:105). The minimum nonzero esti- Exploitation. The algorithm exploits by taking mated gap is the greedy action: ∆(cid:98) min(t) = min min ∆(cid:98) m x (t) . X t = argmax x(cid:62)θ(cid:98)t−1. (4.4) m∈[M] x∈Am,∆(cid:98) m x (t)>0 x∈Act Next, we define a similar optimisation problem as in Exploration. The algorithm explores when (3.5) but with a different normalisation. Eq. (4.3) does not hold. This means that some Definition 4.1. Let f be the constant given by actions have not been explored sufficiently. There n,δ are two cases to consider. First, when there exists f n,δ = 2(1 + 1/ log(n)) log(1/δ) + cd log(d log(n)) , an arm x(cid:48) ∈ Act such that (4.1) where c is an absolute constant. We write f n = N x(cid:48)(t − 1) < min(T xc (cid:48)t(∆(cid:98) (t − 1)), f n/∆(cid:98) 2 min(t − 1)), f n,1/n. For any ∆(cid:101) ∈ [0, ∞)|∪mAm| define T (∆(cid:101) ) as a the algorithm then computes two actions solution of the following optimisation problem: (cid:88)M (cid:88) b = argmin N x(t − 1) (T xm)xm ,min ∈[0,∞] m=1 x∈Am T xm∆(cid:101) m x , (4.2) 1 x∈Act min(T xct(∆(cid:98) (t − 1)), f n/∆(cid:98) 2 min(t − 1)) b = argmin N (t − 1). (4.5) 2 x subject to x∈Act ∆2 Let s(t) be the number of exploration rounds defined (cid:107)x(cid:107)2 ≤ x , ∀x ∈ Am, m ∈ [M ]. H T−1 f n in Algorithm 1. If N b2(t − 1) ≤ ε ts(t) the algorithm plays arm X = b – a form of forced exploration. t 2 and that H T = (cid:80)M m=1 (cid:80) x∈Am T xmxx(cid:62) is invertible. Otherwise the algorithm plays arm X t = b 1. Finally, rounds where an x(cid:48) ∈ Act with the required property If ∆(cid:101) is an estimate of ∆, we call the solution T (∆(cid:101) ) does not exist are called wasted. In these rounds the an approximated allocation rule in contrast to the algorithm acts optimistically as LinUCB [1]: optimal allocation rule defined in Remark 3.5. Our (cid:113) algorithm alternates between exploration and ex- X t = argmax x(cid:62)θ(cid:98)t−1 + f n,1/(s(t))2(cid:107)x(cid:107) G−1 , (4.6) ploitation, depending on whether or not all the arms x∈Act t−1 have satisfied the approximated allocation rule. We where f is defined in Eq. (4.1). The com- are now ready to describe the algorithm, which starts n,1/(s(t))2 plete algorithm is presented in Algorithm 1. with a brief initialisation phase. Remark 4.2. The naive forced exploration can be Initialisation In the first d rounds the algorithm improved by calculating a barycentric spanner [4] chooses any action X in the action set such that X for each action set and then playing the least played t t is not in the span of {X , . . . , X }. This is always action in the spanner. In normal practical setups this 1 t−1 possible by the assumption that Am spans Rd for all makes very little difference, where the forced explo- contexts m. At the end of the initialisation phase G ration plays a limited role. For finite-time worst-case t is guaranteed to be invertible. analysis, however, it may be crucial, since otherwise the regret may depend linearly on the number of ac- Main phase In each round after the initialisation tions, while using the spanner guarantees the forced phase the algorithm checks if the following criterion exploration is sample efficient. 5
4.2 Asymptotic Upper Bound Algorithm 1 Optimal Allocation Matching Input: exploration parameter ε , exploration t Our main theorem states that Algorithm 1 is asymp- counter s(d) = 0. totically optimal under mild assumptions. # initialisation Theorem 4.3. Suppose that T m(∆) is uniquely de- for t = 1 to d do fined and T m(·) is continuous atx ∆ for all contexts m Observe an action set Act, pull arm X t such that x X is not in the span of {X , . . . , X }. and actions x ∈ Am. Then the policy π proposed t 1 t−1 oam end in Algorithm 1 with ε = 1/ log(log(t)) satisfies t for t = d + 1 to n do Rπoam(n) Observe an action set Act and solve the optimi- lim sup θ ≤ C(θ, A1, . . . , AM ). (4.7) sation problem (4.2) based on the estimated gap log(n) n→∞ ∆(cid:98) (t − 1). if (cid:107)x(cid:107)2 ≤ max{ ∆(cid:98) 2 min(t−1) , (∆(cid:98) c xt(t−1))2 }, ∀x ∈ Together with the asymptotic lower bound in Theo- G− t−1 1 fn fn rem 3.3, we can argue that optimal-allocation match- Act, then # exploitation ing algorithm is asymptotical optimal and the lower bound in Eq. (3.4) is sharp. Pull arm X t = argmax x∈Act x(cid:62)θ(cid:98)t−1. else Remark 4.4. The assumption that T xm(·) is con- # exploration tinuous at ∆ is used to ensure the stability of our s(t) = s(t − 1) + 1 algorithm. We prove that the uniqueness assumption if N x(t−1) ≥ min(T x(∆(cid:98) (t−1)), f n/(∆(cid:98) min(t− actually implies continuity (Lemma C.5 in the supple- 1)))2, ∀x ∈ Act, then mentary material) and thus the continuity assump- Pull arm according to LinUCB in (4.6). tion could be omitted. There are, however, certain else corner cases where uniqueness does not hold. For ex- Calculate b 1, b 2 as in Eq. (4.5). ample when θ = (1, 0)(cid:62), A = {(1, 0), (0, 1), (0, −1)}. if N b2(t − 1) ≤ ε ts(t − 1) then Pull arm X = b . t 2 else 4.3 Proof Sketch Pull arm X t = b 1. end The complete proof is deferred to Appendix A.2 in end the supplementary material. At a high level the anal- end ysis of the optimisation-based approach consists of Update θ(cid:98)t, ∆(cid:98) c xt(t), ∆(cid:98) min(t). three parts. (1) Showing that the algorithm’s esti- end mate of the true parameter is close to the truth in finite time. (2) Showing that the algorithm subse- quently samples arms approximately according to the Regret while exploiting The criterion in unknown optimal allocation and (3) Showing that Eq. (4.3) guarantees that the greedy action is optimal the greedy action when arms have been sampled suffi- with high probability in exploitation rounds. To see ciently according to the optimal allocation is optimal this, note that if t is an exploitation round, then the with high probability. Existing optimisation-based sub-optimality gap of greedy action X satisfies the t algorithms suffer from dominant ‘lower-order’ terms following with high probability: because they use simple empirical means for Part (cid:115) (1), while here we use the data-efficient least-squares log(log(n)) ∆ct (cid:46) < ∆ . estimator. Xt 1 ∨ N Xt(t − 1) min We let Explore = F-Explore ∪ UW-Explore ∪ Since the instantaneous regret either vanishes or is W-Explore be the set of exploration rounds, de- larger than ∆ , we have composed into disjoint sets of forced exploration min (X = b ), unwasted exploration (X = b ) and   t 1 t 2 wasted exploration (LinUCB), and let Exploit be the E  (cid:88) ∆c tt = o(log(n)). set of exploitation rounds. 6 t∈Exploit
Regret while exploring Based on the design of experiments are deferred to Appendix D in the sup- our algorithm, the regret while exploring is decom- plementary material. posed into three terms, To save computation, we follow the lazy-update ap-     proach, similar to that proposed in Section 5.1 of (cid:88) (cid:88) E  ∆ct  = E  ∆ct  [1]: The idea is to resolve the optimisation problem Xt Xt t∈Explore t∈F-Explore (4.2) whenever det(G t) increases by a constant factor     (1 + ζ) and in all scenarios we choose (the arbitrary (cid:88) (cid:88) + E  ∆ct  + E  ∆ct  . value) ζ = 0.1. All codes were written in Python. To Xt Xt t∈W-Explore t∈UW-Explore solve the convex optimisation problem (4.2), we use the CVXPY library [11]. Shortly we argue that the regret incurred in W-Explore ∪ UW-Explore is at most logarithmic and 5.1 Fixed Action Set hence the regret in rounds associated with forced exploration is sub-logarithmic: Finite-armed linear bandits with fixed action set are   a special case of linear contextual bandits. Let d = 2 (cid:88) E  ∆c Xt t = O(ε n|Explore|) = o(log(n)) . and let the true parameter be θ = (1, 0)(cid:62). The ac- t∈F-Explore tion set A = {x 1, x 2, x 3} is fixed and x 1 = (1, 0)(cid:62), x = (0, 1)(cid:62), x = (1 − u, 5u)(cid:62). We consider The regret in W-Explore is also sub-logarithmic. 2 3 u = {0.1, 0.2}. By construction, x is the optimal To see this, we first argue that |W-Explore| = 1 arm. From Figure 1, we observe that LinUCB suffers O(|UW-Explore|) since each context has posi- significantly more regret than our algorithm. The tive probability. Combining with the fact that reason is that if u is very small, then x and x point |UW-Explore| is logarithmic in n and the regret of 1 3 in almost the same direction and so choosing only LinUCB is square root in time horizon, these arms does not provide sufficient information to   quickly learn which of x or x is optimal. On the 1 3 (cid:88) E  ∆c tt = o(log(n)) . other hand, x 2 and x 1 point in very different direc- t∈W-Explore tions and so choosing x allows a learning agent to 2 quickly identify that x is in fact optimal. LinUCB The regret in UW-Explore is logarithmic in n with the 1 stops pulling x once it is optimistic and thus fails asymptotically optimal constant using the definition 2 to find the right balance between information and of the optimal allocation: reward. Our algorithm, however, takes this into con- (cid:104) (cid:105) E (cid:80) ∆ct sideration by tracking the optimal allocation ratios. t∈UW-Explore t lim sup = C(θ, A1, . . . , AM ) . log(n) n→∞ Of course many details have been hidden here, which are covered in detail in the supplementary material. 5 EXPERIMENTS Figure 1: Fixed action set. The results are averaged In this section, we first compare our proposed al- over 100 realisations. Here and also later, the shaded gorithm and LinUCB [1] on some specific problem areas show the standard errors. instances to showcase their strengths and weaknesses. We examine OSSB [8] on instances with large action sets to illustrate its weakness due to not using the 5.2 Changing Action Set linear structure everywhere. Since Combes et al. [8] We consider a simple but representative case when demonstrated that OSSB dominates the algorithm there are only two action sets A1 and A2 available. of Lattimore and Szepesva´ri [16], we omit this algo- rithm from our experiments. In the end, we include Scenario One. In each round, A1 is drawn with the comparison with LinTS [3]. Some additional probability 0.3 while A2 is drawn with probability 7
Figure 2: Changing action sets. The left panel is for Figure 3: The left panel is for bounded regret and scenario one and the right panel is for scenario two. right panel is for large action space. The results are The results are averaged over 100 realisations. averaged over 100 realisations. 0.7. Set A1 contains x1 = (1, 0, 0)(cid:62), x1 = (0, 1, 0)(cid:62), tains three actions: x2 1 = (0, 1)(cid:62), x2 2 = (−1, 0)(cid:62), 1 2 and x1 = (0.9, 0.5, 0)(cid:62), while set A2 contains x2 = x2 3 = (−1, 0). As discussed before, x1 1 and x2 1 are 3 1 (0, 1, 0)(cid:62), x2 = (0, 0, 1)(cid:62), and x2 = (0, 0.5, 0.9)(cid:62). the optimal arms for each action set and they span 2 3 The true parameter θ is (1, 0, 1)(cid:62). From the left R2. The results are shown in the left subpanel of Figure 3. The regret of our algorithm appears to panel of Figure 2, we observe that LinUCB, while have stopped growing after a short period of increase. starts better, eventually again suffers more regret In line with Theorem 3.9, LinUCB is seen to achieve than our algorithm. bounded regret in this problem. Scenario Two. In each round, A1 is drawn with 5.4 Large, Fixed Action Set probability 0.99, while A2 is drawn with probability 0.01. Set A1 contains three actions: x1 = (1, 0)(cid:62), 1 We let d = 2 and θ = (1, 0)(cid:62). We generate 100 uni- x1 = (0, 1)(cid:62), x1 = (0.9, 0.5)(cid:62), while set A2 contains 2 3 formly distributed on the d-dimensional unit sphere three actions: x2 = (0, 1)(cid:62), x2 = (−1, 0)(cid:62), x2 = 1 2 3 (fixed action set). The results are shown in the right (−1, 0). Apparently, x1 and x2 are the optimal arms 1 1 subfigure of Figure 3. When the action space is for each action set and they span R2. Based on large, OSSB suffers significantly large regret and be- the allocation rule in Section 3.1, the algorithm is comes unstable due to not using the linear structure advised to pull actions x1 and x2 very often based 1 1 everywhere. The regret of (the theoretically justi- on asymptotics. However, since the probability that fied version of) LinTS is also very large due to the A2 is drawn is extremely small, we are very likely to unnecessary variance factor required by its theory. fall back to wasted exploration and use LinUCB to explore. Thus, in the short term, our algorithm will 6 DISCUSSION suffer from the drawback that optimistic algorithms also suffer from and what is described in Section 5.1. We presented a new optimisation-based algorithm Although, the asymptotics will eventually “kick in”, for linear contextual bandits that is asymptotically it may take extremely long time to see the benefits optimal and adapts to both the action sets and un- of this and the algorithm’s finite-time performance known parameter. The new algorithm enjoys sub- will be poor. Indeed, this is seen on the right panel logarithmic regret when the collection of optimal of Figure 2, which shows that in this case LinUCB actions spans Rd, a property that we also prove for and our algorithm are nearly indistinguishable. optimism-based approaches. There are many open 5.3 Sublinear/Bounded Regret questions. A natural starting point is to prove near- minimax optimality of the new algorithm, possibly Earlier we have argued that when the optimal arms with minor modifications. Our work also highlights of all action sets span Rd, our algorithm achieves sub- the dangers of focusing too intensely on asymptotics, logarithmic regret. Here, we experimentally study which for contextual bandits hide completely the de- this interesting case. We consider M = 2. In each pendence on the context distribution. This motivates round, A1 is drawn with probability 0.8 while A2 is the intriguing challenge to understand the finite-time drawn with probability 0.2 and the true parameter θ instance-dependent regret. Another open direction is is (1, 0)(cid:62).Set A1 contains three actions: x1 = (1, 0)(cid:62), to consider the asymptotics when the context space 1 x1 = (0, 1)(cid:62), x1 = (0.9, 0.5)(cid:62), while set A2 con- is continuous, which has not seen any attention. 2 3 8
Acknowledgements Tong Zhang, editors, 21st Annual Conference on Learning Theory - COLT, pages 355–366, Csaba Szepesv´ari gratefully acknowledges funding 2008. from the Canada CIFAR AI Chairs Program, Amii [10] R´emy Degenne, Wouter M Koolen, and Pierre and NSERC. M´enard. Non-asymptotic pure exploration by solving games. arXiv preprint arXiv:1906.10431, References 2019. [1] Yasin Abbasi-Yadkori, D´avid P´al, and Csaba [11] Steven Diamond and Stephen Boyd. CVXPY: Szepesv´ari. Improved algorithms for linear A Python-embedded modeling language for con- stochastic bandits. In Advances in Neural In- vex optimization. Journal of Machine Learning formation Processing Systems, pages 2312–2320, Research, 17(83):1–5, 2016. 2011. [12] A. Garivier and E. Kaufmann. Optimal best arm [2] Deepak Agarwal, Bee-Chung Chen, Pradheep identification with fixed confidence. In V. Feld- Elango, Nitin Motgi, Seung-Taek Park, Raghu man, A. Rakhlin, and O. Shamir, editors, 29th Ramakrishnan, Scott Roy, and Joe Zachariah. Annual Conference on Learning Theory, vol- Online models for content optimization. In Ad- ume 49 of Proceedings of Machine Learning Re- vances in Neural Information Processing Sys- search, pages 998–1027, Columbia University, tems, pages 17–24, 2009. New York, New York, USA, 23–26 Jun 2016. [3] Shipra Agrawal and Navin Goyal. Thompson PMLR. sampling for contextual bandits with linear pay- [13] Sampath Kannan, Jamie H Morgenstern, Aaron offs. In International Conference on Machine Roth, Bo Waggoner, and Zhiwei Steven Wu. Learning, pages 127–135, 2013. A smoothed analysis of the greedy algorithm [4] Baruch Awerbuch and Robert Kleinberg. On- for the linear contextual bandit problem. In line linear optimization and adaptive routing. Advances in Neural Information Processing Sys- Journal of Computer and System Sciences, 74 tems, pages 2227–2236, 2018. (1):97–114, 2008. [14] Tze Leung Lai and Herbert Robbins. Asymptoti- [5] Hamsa Bastani, Mohsen Bayati, and Khasha- cally efficient adaptive allocation rules. Advances yar Khosravi. Mostly exploration-free algo- in applied mathematics, 6(1):4–22, 1985. rithms for contextual bandits. arXiv preprint [15] John Langford and Tong Zhang. The epoch- arXiv:1704.09011, 2017. greedy algorithm for contextual multi-armed [6] H. P. Chan and T. L. Lai. Sequential generalized bandits. In Proceedings of the 20th International likelihood ratios and adaptive treatment alloca- Conference on Neural Information Processing tion for optimal sequential selection. Sequential Systems, pages 817–824, 2007. Analysis, 25:179–201, 2006. [16] Tor Lattimore and Csaba Szepesv´ari. The end [7] Wei Chu, Lihong Li, Lev Reyzin, and Robert of optimism? an asymptotic analysis of finite- Schapire. Contextual bandits with linear payoff armed linear bandits. In Artificial Intelligence functions. In Proceedings of the Fourteenth In- and Statistics, pages 728–737, 2017. ternational Conference on Artificial Intelligence [17] Tor Lattimore and Csaba Szepesv´ari. Bandit and Statistics, pages 208–214, 2011. algorithms. preprint, 2019. [8] Richard Combes, Stefan Magureanu, and [18] Lihong Li, Wei Chu, John Langford, and Alexandre Proutiere. Minimal exploration in Robert E. Schapire. A contextual-bandit ap- structured stochastic bandits. In Advances in proach to personalized news article recommen- Neural Information Processing Systems, pages dation. In Proceedings of the 19th International 1763–1771, 2017. Conference on World Wide Web, WWW ’10, pages 661–670, 2010. [9] Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under [19] Jungseul Ok, Alexandre Proutiere, and Dami- bandit feedback. In Rocco A. Servedio and anos Tranos. Exploration in structured rein- 9
forcement learning. In Advances in Neural In- formation Processing Systems, pages 8874–8882, 2018. [20] Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395–411, 2010. [21] Alexandre B. Tsybakov. Introduction to Non- parametric Estimation. Springer, 1st edition, 2008. ISBN 0387790519, 9780387790510. [22] Roman Vershynin. Introduction to the non- asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010. 10
Supplement to “Adaptive Exploration in Linear Contextual Bandit” In Section A, we provide main proofs for asymptotic lower bound and upper bound. In Section B, we prove several main lemmas. In Section C, some supporting lemmas are presented for the sake of completeness. A Proofs of Asymptotic Lower and Upper Bounds First of all, we define the sub-optimal action set as Am = Am \ {x : ∆m = 0} and denote A = ∪M Am and − x m=1 A = ∪M Am. − m=1 − A.1 Proof of Lemma 3.2 The proof idea follows if G¯ is not sufficiently large in every direction, then some alternative parameters are n not sufficiently identifiable. Step One. We fix a consistent policy π and fix a context m ∈ [M ] as well as a sub-optimal arm x ∈ Am. − Consider another parameter θ(cid:101) ∈ Rd such that it is close to θ but x∗ is not the optimal arm in bandit θ(cid:101) for m action set Am. Specifically, we construct H(x − x∗ ) θ(cid:101) = θ + m (∆m + ε), (cid:107)x − x∗ (cid:107)2 x m H where H ∈ Rd×d is some positive semi-definite matrix and ε > 0 is some absolute constant that will be specified later. Since the sub-optimality gap ∆(cid:101) m x∗ satisfies m (cid:104)x − x∗ , θ(cid:101)(cid:105) = (cid:104)x − x∗ , θ(cid:105) + ∆m + ε = ε > 0, (A.1) m m x it ensures that x∗ is ε-suboptimal in bandit θ(cid:101). m We define T x(n) = (cid:80)n t=1 I(X t = x) and let P and P (cid:101) be the measures on the sequence of outcomes (X 1, Y 1, . . . , X n, Y n) induced by the interaction between the policy and the bandit θ and θ(cid:101) respectively. By the definition of G¯ in (3.2), we have n 1 1 2 (cid:107)θ − θ(cid:101)(cid:107)2 G¯ n = 2 (θ − θ(cid:101))(cid:62)G¯ n(θ − θ(cid:101)) 1 (cid:104) (cid:88) (cid:105) = 2 (θ − θ(cid:101))(cid:62)E T x(n)xx(cid:62) (θ − θ(cid:101)) x∈A 1 (cid:88) (cid:104) (cid:105) = 2 E T x(n) (cid:104)x, θ − θ(cid:101)(cid:105)2. x∈A Applying the Bretagnolle-Huber inequality inequality in Lemma C.1 and divergence decomposition lemma in Lemma C.2, it holds that for any event D, 1 (cid:16) 1 (cid:17) 2 (cid:107)θ − θ(cid:101)(cid:107)2 G¯ n = KL(P, P (cid:101)) ≥ log 2(P(D) + P (cid:101)(Dc)) . (A.2) Step Two. In the following, we start to derive a lower bound of Rπ(n), θ n M (cid:104) (cid:88) (cid:105) (cid:104) (cid:88) (cid:88) (cid:105) Rπ(n) = E (cid:104)x∗ − X , θ(cid:105) = E (cid:104)x∗ − X , θ(cid:105) θ ct t m t t=1 m=1 t:ct=m (cid:104) (cid:88) (cid:105) (cid:104) (cid:88) (cid:105) ≥ E (cid:104)x∗ − X , θ(cid:105) = E ∆m m t Xt t:ct=m t:ct=m n n (cid:104) (cid:88) (cid:105) (cid:104) (cid:88) (cid:88) (cid:105) ≥ ∆ E I(X (cid:54)= x∗ ) = ∆ E I(c = m) − I(c = m)I(X = x∗ ) , min t m min t t t m t:ct=m t=1 t=1 1
where the first inequality comes from the fact that (cid:104)x∗ − X , θ(cid:105) ≥ 0 for all m ∈ [M ]. Define the event D as m t follows, n n (cid:110) (cid:88) 1 (cid:88) (cid:111) D = I(c = m)I(X = x∗ ) ≤ I(c = m) . (A.3) t t m 2 t t=1 t=1 When event D holds, we will only pull at most half of total rounds for the optimal action of action set m. Then it holds that n n (cid:104)(cid:16) (cid:88) (cid:88) (cid:17) (cid:105) Rπ(n) ≥ ∆ E I(c = m) − I(c = m)I(X = x∗ ) I(D) θ min t t t m t=1 t=1 n (cid:104) 1 (cid:88) (cid:105) ≥ ∆ E I(c = m)I(D) . min 2 t t=1 Define another event B as follows, n B = (cid:110) 1 (cid:88) I(c = m) ≥ np m − δ/2(cid:111) , (A.4) 2 t 2 t=1 where δ > 0 will be chosen later and p is the probability that the environment picks context m. From the m definition of c , we have E[(cid:80)n I(c = m)] = np . By the standard Hoeffding’s inequality [22], it holds that t t=1 t m P(cid:16) 1 (cid:88)n I(c = m) − np m ≥ − δ (cid:17) ≥ 1 − exp(− 2δ2 ), 2 t 2 2 n t=1 which implies P(Bc) ≤ exp(−2δ2/n). By the definition of events D, B in (A.3),(A.4), we have n (cid:104) 1 (cid:88) (cid:105) Rπ(n) ≥ ∆ E I(c = m)I(D)I(B) θ min 2 t t=1 (cid:104) 1 δ (cid:105) ≥ ∆ E ( np − )I(D)I(B) min 2 m 2 1 δ = ∆ ( np − )P(D ∩ B) min 2 m 2 1 δ ≥ ∆ ( np − )(P(D) − P(Bc)). min 2 m 2 Letting δ = np /2, we have m np (cid:16) np2 (cid:17) Rπ(n) ≥ ∆ m P(D) − exp(− m ) . (A.5) θ min 4 2 On the other hand, we let E (cid:101) is taken with respect to probability measures P (cid:101). Then Rπ(n) can be lower θ(cid:101) bounded as follows, M n (cid:104) (cid:88) (cid:88) (cid:105) R θπ (cid:101)(n) = E (cid:101) I(c t = m)∆(cid:101) m Xt m=1 t=1 n (cid:104) (cid:88) (cid:105) ≥ E (cid:101) I(c t = m)I(X t = x∗ m) ∆(cid:101) m x∗ , m t=1 2
where we throw out all the sub-optimality gap terms except ∆(cid:101) m x∗ . Using the fact that ∆(cid:101) m x∗ is ε-suboptimal, m m it holds that n (cid:104) (cid:88) (cid:105) R θπ (cid:101)(n) ≥ εE (cid:101) ( I(c t = m)I(X t = x∗ m))I(Dc) t=1 n (cid:104) 1 (cid:88) (cid:105) > εE (cid:101) 2 I(c t = m)I(Dc) t=1 n (cid:104) 1 (cid:88) (cid:105) ≥ εE (cid:101) 2 I(c t = m)I(Dc)I(B) t=1 np δ ≥ ε( m − )P (cid:101)(Dc ∩ B) 2 2 np δ ≥ ε( m − )(P (cid:101)(Dc) − P (cid:101)(Bc)) 2 2 np δ 2δ2 ≥ ε( m − )(P (cid:101)(Dc) − exp(− )) 2 2 n np np np2 = ε m P (cid:101)(Dc) − ε m exp(− m ). (A.6) 4 4 2 Now we have derived the lower bounds (A.5)(A.6) for Rπ(n), Rπ(n) respectively. θ θ(cid:101) Step Three. Combining the lower bounds of Rπ(n) and Rπ(n) together, it holds that θ θ(cid:101) np (cid:16) (cid:17) np np2 R θπ(n) + R θπ (cid:101)(n) ≥ 4m P(D)∆ min + P (cid:101)(Dc)ε − 4m exp(− 2m )(ε + ∆ min). Letting ε ≤ ∆ , we have min np (cid:16) (cid:17) np np2 R θπ(n) + R θπ (cid:101)(n) ≥ ε 4m P(D) + P (cid:101)(Dc) − 4m exp(− 2m )2∆ min. This implies Rπ(n) + Rπ(n) 1 np2 θ εnp /4θ(cid:101) + ε exp(− 2m )2∆ min ≥ P(D) + P (cid:101)(Dc). (A.7) m Plugging (A.7) into (A.2), we have 1 (cid:16) 1 (cid:17) 2 (cid:107)θ − θ(cid:101)(cid:107)2 G¯ n ≥ log 2(P(D) + P (cid:101)(Dc)) (cid:16) 1 (cid:17) ≥ log R θπ( εn n) p+ mR /θπ (cid:101) 8(n) + 1 ε exp(− np 22 m )4∆ min (cid:16) n (cid:17) = log R θπ( εn p) m+R /8θπ (cid:101)(n) + n ε exp(− np 22 m )4∆ min (cid:16) Rπ(n) + Rπ(n) 4n np2 (cid:17) = log(n) − log θ θ(cid:101) + exp(− m )∆ . εp /8 ε 2 min m Dividing by log(n) for both sides, we reach (cid:107)θ − θ(cid:101)(cid:107)2 G¯ n ≥ 1 − log (cid:16) R θπ( εn p) m+R /8θπ (cid:101)(n) + 4 εn exp(− np 22 m )∆ min(cid:17) . 2 log(n) log(n) From the definition of consistent policies (3.1), it holds that log(Rπ(n) + Rπ(n)) lim sup θ θ(cid:101) ≤ 0. log(n) n→∞ 3
In addition, by using the fact that lim n exp(−n) = 0, it follows that n→∞ lim inf (cid:107)θ − θ(cid:101)(cid:107)2 G¯ n ≥ 1. (A.8) n→∞ 2 log(n) Step Four. Let’s denote (cid:107)x − x∗ (cid:107)2 (cid:107)x − x∗ (cid:107)2 ρ (H) = m G¯− n 1 m HG¯ nH . n (cid:107)x − x∗ (cid:107)4 m H Then we can rewrite 1 (∆m + ε)2 2 (cid:107)θ − θ(cid:101)(cid:107)2 G¯ n = 2(cid:107)x −x x∗ (cid:107)2 ρ n(H). m G¯− n 1 Plugging this into (A.8) and letting ε to zero, we see that ρ (H) 2 lim inf n ≥ . (A.9) n→∞ (cid:107)x − x∗ (cid:107)2 log(n) (∆m)2 m G¯− n 1 x Now, we consider the following lemma, extracted from the proof of Theorem 25.1 of the book by [17]. The detailed proof is deferred to Section B.6. Lemma A.1. Let {G } be a sequence of d × d positive definite matrices, s ∈ Rd. For H positive n n≥0 (cid:107)s(cid:107)2 (cid:107)s(cid:107)2 semi-definite d × d matrix such that (cid:107)s(cid:107) H > 0 and n ≥ 0, let ρ n(H) = G− n (cid:107)1 s(cid:107)4 HGnH . Assume that H lim inf λmin(Gn) > 0 and that for some c > 0, n→∞ log(n) ρ (H) lim inf n ≥ c . (A.10) n→∞ (cid:107)s(cid:107)2 log(n) G− n 1 Then, lim sup log(n)(cid:107)s(cid:107)2 ≤ 1/c. n→∞ G− n 1 The proof of lim inf λmin(Gn) > 0 could refer Appendix C in [16]. Clearly, this lemma with G = G¯ , n→∞ log(n) n n c = 2/(∆m)2, H = lim G¯−1/(cid:107)G¯−1(cid:107) and s = x − x∗ gives the desired statement. x n→∞ n n m (cid:4) A.2 Proof of Theorem 4.3: Asymptotic Upper Bound We write ∆ = max ∆m and abbreviate R(n) = Rπ(n). From the design of the initialisation, G is max x,m x θ t guaranteed to be invertible since each Am is assumed to span Rd. The regret during the initialisation is at most d∆ ≈ o(log(n)) and thus we ignore the regret during initialisation in the following. max First, we introduce a refined concentration inequality for the least square estimator constructed by adaptive data. The proof could refer to the proof of Theorem 8 in [16]. Lemma A.2. Suppose for t ≥ d, G is invertible. For any δ ∈ (0, 1), we have t P(cid:16) ∃t ≥ d, ∃x ∈ A, such that (cid:12) (cid:12)(cid:104)x, θ(cid:98)t(cid:105) − (cid:104)x, θ(cid:105)(cid:12) (cid:12) ≥ (cid:107)x(cid:107) G−1f n1 ,/ δ2(cid:17) ≤ δ, t and (cid:16) 1 (cid:17) f = 2 1 + log(1/δ) + cd log(d log(n)), (A.11) n,δ log(n) where c > 0 is some universal constant. We write f = f for short. n n,1/n 4
Let us define the event B as follows t (cid:110) (cid:111) B t = ∃t ≥ d, ∃x ∈ A, such that |x(cid:62)θ(cid:98)t − x(cid:62)θ| ≥ (cid:107)x(cid:107) G−1f n1/2 . (A.12) t From Lemma A.2, we have P(B ) ≤ 1/n by choosing δ = 1/n. We decompose the cumulative regret with t respect to event B as follows, t n (cid:104) (cid:88) (cid:88) (cid:105) R(n) = E ∆ctI(X = x) x t t=1 x∈Act − n n (cid:104) (cid:88) (cid:88) (cid:105) (cid:104) (cid:88) (cid:88) (cid:105) = E ∆ctI(X = x, B ) + E ∆ctI(X = x, Bc) . (A.13) x t t x t t t=1 x∈Act t=1 x∈Act − − To bound the first term in (A.13), we observe that (cid:104) (cid:105) E (cid:80)n t=1 (cid:80) x∈Act ∆c xtI(X t = x, B t) lim sup − log(n) n→∞ (cid:104) (cid:105) = lim sup E (cid:80)n t=1 ∆c Xt tI(B t) ≤ lim sup ∆ max (cid:80)n t=1 P(B t) = lim sup ∆ max (cid:80)n t=1 n1 log(n) log(n) log(n) n→∞ n→∞ n→∞ ∆ = lim sup max = 0. (A.14) log(n) n→∞ To bound the second term in (A.13), we define the event D as follows, t,ct (cid:40) (cid:41) D = ∀x ∈ Act, (cid:107)x(cid:107)2 ≤ max (cid:110) (∆(cid:98) min(t))2 , (∆c xt(t))2 (cid:111) . (A.15) t,ct G−1 f f t n n When D occurs, the algorithm exploits at round t. Otherwise, the algorithm explores at round t. We t,ct decompose the second term in (A.13) as the exploitation regret and exploration regret: n (cid:104) (cid:88) (cid:88) (cid:105) E ∆ctI(X = x, Bc) x t t t=1 x∈Act − n n (cid:104) (cid:88) (cid:88) (cid:105) (cid:104) (cid:88) (cid:88) (cid:105) = E ∆ctI(X = x, Bc, D ) + E ∆ctI(X = x, Bc, Dc ) . (A.16) x t t t,ct x t t t,ct t=1 x∈Act t=1 x∈Act − − We bound those two terms in Lemmas A.3-A.4 respectively. Lemma A.3. The exploitation regret satisfies (cid:104) (cid:105) E (cid:80)n (cid:80) ∆ I(X = x, Bc, D ) lim sup t=1 x∈Ac −t x t t t,ct = 0 (A.17) log(n) n→∞ Lemma A.4. The exploration regret satisfies (cid:104) (cid:105) E (cid:80)n (cid:80) ∆ I(X = x, Bc, Dc ) lim sup t=1 x∈Ac −t x t t t,ct ≤ C(θ, A1, . . . , AM ), (A.18) log(n) n→∞ where C(θ, A1, . . . , AM ) is defined in Theorem 3.3. Combining Lemmas A.3-A.4 together, we reach our conclusion. (cid:4) 5
B Proofs of Several lemmas B.1 Proof of Lemma A.3: Exploitation Regret When Bc defined in (A.12) occurs, we have t max (cid:12) (cid:12)(cid:104)θ(cid:98)t − θ, x(cid:105)(cid:12) (cid:12) ≤ (cid:107)x(cid:107) G−1f n1/2. (B.1) x∈A t When D defined in (A.15) occurs, we have t,m (cid:110) ∆(cid:98) 2 (t) (∆(cid:98) m(t))2 (cid:111) (∆(cid:98) m(t))2 (cid:107)x(cid:107)2 ≤ max min , x = x , (B.2) G−1 f f f t n n n holds for any action x ∈ Am and ∆(cid:98) m x (t) > 0. If x∗ m = x (cid:98)∗ m(t), there is no regret occurred. Otherwise, putting (B.1) and (B.2) together with the optimal action x∗ , it holds that m |(cid:104)θ(cid:98)t − θ, x∗ m(cid:105)| ≤ (cid:107)x∗ m(cid:107) G− t 1f n1/2 ≤ ∆(cid:98) m x∗ m(t). (B.3) We decompose the sub-optimality gap of x∗ (t) as follows, (cid:98)m (cid:104)x∗ , θ(cid:105) − (cid:104)x∗ (t), θ(cid:105) m (cid:98)m = (cid:104)x∗ m, θ − θ(cid:98)t(cid:105) + (cid:104)x∗ m, θ(cid:98)t(cid:105) − (cid:104)x (cid:98)∗ m(t), θ − θ(cid:98)t(cid:105) − (cid:104)x (cid:98)∗ m(t), θ(cid:98)t(cid:105) = (cid:104)x∗ m, θ − θ(cid:98)t(cid:105) − ∆(cid:98) m x∗ (t) + (cid:104)x (cid:98)∗ m(t), θ(cid:98)t − θ(cid:105) m ≤ (cid:104)x (cid:98)∗ m(t), θ(cid:98)t − θ(cid:105). (B.4) For each x ∈ A, we define (cid:110) ∆ (cid:111) τ x = min N : ∀t ≥ d, D t,ct occurs, N x(t) ≥ N, implies |(cid:104)θ(cid:98)t − θ, x(cid:105)| ≤ m 2 in . (B.5) When N (t) ≥ τ , it holds that x(cid:98)∗ m(t) x(cid:98)∗ m(t) ∆ |(cid:104)θ(cid:98)t − θ, x (cid:98)∗ m(t)(cid:105)| ≤ m 2 in . Together with (B.4), we have ∆ (cid:104)x∗ , θ(cid:105) − (cid:104)x∗ (t), θ(cid:105) ≤ min . m (cid:98)m 2 Combining this with the fact that the instantaneous regret either vanishes or is larger than ∆ , it indicates min x∗ = x∗ (t). Therefore, we can decompose the exploitation regret with respect to event {N (t) ≥ τ } m (cid:98)m x(cid:98)∗ m(t) x(cid:98)∗ m(t) as follows, n (cid:104) (cid:88) (cid:88) (cid:105) E ∆ctI(X = x, Bc, D ) x t t t,ct t=1 x∈Act − M n (cid:104) (cid:88) (cid:88) (cid:88) (cid:16) (cid:17)(cid:105) ≤ E ∆mI X = x, Bc, D , N (t) ≥ τ x t t t,m x(cid:98)∗ m(t) x(cid:98)∗ m(t) m=1 t=1 x∈Am − M n (cid:104) (cid:88) (cid:88) (cid:88) (cid:16) (cid:17)(cid:105) + E ∆mI X = x, Bc, D , N (t) < τ . (B.6) x t t t,m x(cid:98)∗ m(t) x(cid:98)∗ m(t) m=1 t=1 x∈Am − 6
During exploiting the algorithm always executes the greedy action. When x∗ = x∗ (t) the first term in (B.6) m (cid:98)m results in no regret. For the second term in (B.6), we have M n (cid:104) (cid:88) (cid:88) (cid:88) (cid:16) (cid:17)(cid:105) E ∆mI X = x, Bc, D , N < τ x t t t,m x(cid:98)∗ m(t) x(cid:98)∗ m(t) m=1 t=1 x∈Am − M n (cid:104) (cid:88) (cid:88) (cid:16) (cid:17)(cid:105) ≤ E I Bc, D , N (t) < τ ∆ t t,m x(cid:98)∗ m(t) x(cid:98)∗ m(t) max m=1 t=1 M (cid:88) (cid:88) (cid:88) ≤ E(τ )∆ ≤ E[τ ]∆ . (B.7) x max x max m=1 x∈A x∈A It remains to bound E[τ ] for any x ∈ A. Let x (cid:110) (cid:111) Λ = min λ ≥ 1 : ∀t ≥ d, |(cid:104)θ(cid:98)t − θ, x(cid:105)| ≤ (cid:107)x(cid:107) G−1f n1 ,/ 12 /λ . t From the definition of τ in (B.5), we have x (cid:110) ∆ (cid:111) τ ≤ max N : (f /N )1/2 ≥ min , x n,1/λ 2 which implies τ ≤ 4f /∆2 . From Lemma A.2, we know that P(Λ ≥ λ) ≤ 1/λ, which implies x n,1/Λ min E[log Λ] ≤ 1. Overall, 4E[f ] 8(1 + 1/ log(n)) + 4cd log(d log(n)) E[τ ] ≤ Λ ≤ . (B.8) x ∆2 ∆2 min min Combining (B.6)-(B.8) together, we reach (cid:104) (cid:105) E (cid:80)n (cid:80) ∆ I(x = x, Bc, D ) lim sup t=1 x∈Ac −t x t t t,ct log(n) n→∞ (B.9) (cid:0) (cid:1) |A|∆ 8(1 + 1/ log(n)) + 4cd log(d log(n)) max ≤ lim sup = 0. ∆2 log(n) n→∞ min This ends the proof. (cid:4) B.2 Proof of Lemma A.4: Exploration Regret If all the actions x ∈ A satisfy (cid:110) (cid:111) N x(t) ≥ min f n/∆(cid:98) 2 min(t), T x(∆(cid:98) (t)) , (B.10) the following holds using Lemma C.4, (cid:110) ∆(cid:98) 2 (t) (∆(cid:98) ct(t))2 (cid:111) (cid:107)x(cid:107)2 ≤ max min , x , for any x ∈ A. G−1 f f t n n In other words, this implies if there exists an action x such that (B.10) does not hold, e.g. Dc occurs, there t,ct must exist an action x(cid:48) ∈ A (x and x(cid:48) may not be the identical) satisfying (cid:110) (cid:111) N x(cid:48)(t) ≤ min f t/∆(cid:98) 2 min(t), T x(cid:48)(∆(cid:98) (t)) . Based on the criterion in Algorithm 1, we should explore. However, if x(cid:48) does not belong to Act and all the actions within Act have been explored sufficiently according to the approximation optimal allocation, this 7
exploration is interpreted as “wasted”. To alleviate the regret of the wasted exploration, the algorithm acts optimistically as LinUCB. Let’s define a set that records the index of action sets that has not been fully explored until round t, (cid:110) (cid:111) M t = m : ∃x ∈ Am, N x(t) ≤ min{f n/∆(cid:98) 2 min(t), T x(∆(cid:98) (t))} . (B.11) When Dc occurs, it means that M (cid:54)= ∅. If Dc occurs but c does not belong to M , the algorithm suffers t,ct t t,ct t t a wasted exploration. We decompose the exploration regret according to the fact if c belongs to M , t t n (cid:104) (cid:88) (cid:88) (cid:105) E ∆ I(X = x, Bc, Dc x t t t,ct t=1 x∈Act − n (cid:104) (cid:88) (cid:88) (cid:105) = E ∆ I(X = x, Bc, Dc , c ∈ M ) x t t t,ct t t t=1 x∈Act − (cid:124) (cid:123)(cid:122) (cid:125) Rue:unwasted exploration n (cid:104) (cid:88) (cid:88) (cid:105) + E ∆ I(X = x, Bc, Dc , c ∈/ M ) . (B.12) x t t t,ct t t t=1 x∈Act − (cid:124) (cid:123)(cid:122) (cid:125) Rwe:wasted exploration We will bound the unwasted exploration regret and wasted exploration regret in the following two lemmas respectively. Lemma B.1. The regret during the unwasted explorations satistifies R lim sup ue ≤ C(θ, A , . . . , A ). (B.13) log(n) 1 M n→∞ The detailed proof is deferred to Section B.3. Lemma B.2. The regret during the wasted explorations satisfies R lim sup we = 0. (B.14) log(n) n→∞ The detailed proof is deferred to Section B.5. Putting (B.12)-(B.14) together, we reach (cid:104) (cid:105) lim sup E (cid:80)n t=1 (cid:80) x∈Ac −t ∆c xtI(X t = x, B tc, D tc ,ct) ≤ C(θ, A1, . . . , AM ), log(n) n→∞ which ends the proof. (cid:4) B.3 Proof of Lemma B.1: Unwasted Exploration First, we derive a lower bound for each N (t) during the unwasted exploration. Denote s(t) as the number x of rounds for unwasted explorations until round t. Indeed, forced exploration can guarantee a lower bound for N (t): min N (t) ≥ ε s(t)/2. We prove this by the contradiction argument. Assume this is not x x∈A x t true. There may exist s(t)/2 rounds {t , . . . , t } ⊂ {1, . . . , t} such that min N (t) ≤ ε s(t). After |A| 1 s(t)/2 x∈A x t 8
such rounds, we have min N (t) is incremented by at least 1 which implies min N (t) ≥ s(t)/(2|A|). If x x x x ε ≤ 1/|A|, it leads to the contradiction. This is satisfied when t is large since ε = 1/ log(log t). t t Second, we set β = 1/ log(log(n)) and define n (cid:110) (cid:111) ζ = min s : ∀t ≥ s, ∀x ∈ A, such that |(cid:104)x, θ(cid:98)t(cid:105) − (cid:104)x, θ(cid:105)| ≤ β n . (B.15) Then we decompose the regret during unwasted explorations with respect to event {s(t) ≥ ζ} as follows, n (cid:104) (cid:88) (cid:88) (cid:105) R = E ∆ I(X = x, Bc, Dc , c ∈ M ) ue x t t t,ct t t t=1 x∈Act − n (cid:104) (cid:88) (cid:88) (cid:105) = E ∆ I(X = x, Bc, Dc , s(t) ≥ ζ, c ∈ M ) x t t t,ct t t t=1 x∈Act − (cid:124) (cid:123)(cid:122) (cid:125) I1 n (cid:104) (cid:88) (cid:88) (cid:105) + E ∆ I(X = x, Bc, Dc , s(t) < ζ, c ∈ M ) . (B.16) x t t t,ct t t t=1 x∈Act − (cid:124) (cid:123)(cid:122) (cid:125) I2 To bound I , we have 2 n n (cid:104) (cid:88) (cid:105) (cid:104) (cid:88) (cid:105) I = E ∆ I(Bc, Dc , c ∈ M , s(t) < ζ) ≤ ∆ E I(s(t) < ζ, c ∈ M , Dc ) ≤ ∆ E[ζ]. 2 Xt t t,ct t t max t t t,ct max t=1 t=1 It remains to bound E[ζ]. Let’s define (cid:110) (cid:16) 2 (cid:17)1/2(cid:111) Λ = min λ : ∀t : D tc ,ct, ∀x ∈ A, s(t) ≥ s, such that |(cid:104)x, θ(cid:98)t(cid:105) − (cid:104)x, θ(cid:105)| ≤ ε s(t) f n,1/λ . t From the definition of ζ in (B.15), we have (cid:110) (cid:16) f (cid:17)1/2 (cid:111) n,1/λ ζ ≤ max s : ≥ β , ε s n t which implies 2f n,1/Λ ζ ≤ . (B.17) ε β2 t n In addition, we define (cid:110) (cid:111) Λ(cid:48) = min λ : ∀t ≥ d, ∀x ∈ A, such that |(cid:104)x, θ(cid:98)t(cid:105) − (cid:104)x, θ(cid:105)| ≤ (cid:107)x(cid:107) G−1f n1 ,/ 12 /λ . t Using the lower bound of N (t), it holds that x 1 2 (cid:107)x(cid:107)2 ≤ ≤ . G−1 N (t) ε s(t) t x t By Lemma A.2, we have (cid:16) 1 (cid:17) (cid:16) 1 (cid:17) P Λ ≥ ≤ P Λ(cid:48) ≥ ≤ δ, δ δ which implies that E[log Λ] ≤ 1. From (B.17), 2(1 + 1/ log(n)) + cd log(log(d log(n))) E[ζ] ≤ . (B.18) ε β2 n n 9
From (B.18), we have I ∆ E[ζ] lim sup 2 ≤ lim sup max = 0, (B.19) log(n) log(n) n→∞ n→∞ since β and ε are both sub-logarithmic. It remains to bound I . When s(t) ≥ ζ, from the definition of ζ in n n 1 (B.15) we have (cid:104)x, θ(cid:98)t(cid:105) − (cid:104)x, θ(cid:105) ≤ β n, holds for any x ∈ A. For each m ∈ [M ], we have ∆(cid:98) x∗ m(t) = (cid:104)θ(cid:98)t, x (cid:98)∗ m(t)(cid:105) − (cid:104)θ(cid:98)t, x∗ m(cid:105) = (cid:104)θ(cid:98)t, x (cid:98)∗ m(t)(cid:105) − (cid:104)θ, x (cid:98)∗ m(t)(cid:105) − (cid:104)θ(cid:98)t, x∗ m(cid:105) + (cid:104)θ, x∗ m(cid:105) − (cid:104)θ, x∗ m(cid:105) + (cid:104)θ, x (cid:98)∗ m(t)(cid:105) ≤ 2β − ∆ . t min When n is sufficiently large, it holds that β n ≤ ∆ min/2. This implies ∆(cid:98) x∗ m(t) = 0 such that x∗ m = x (cid:98)∗ m(t) for all t : s(t) > ζ. For notation simplicity, we denote E = Bc ∩ Dc ∩ {s(t) ≥ ζ} ∩ {c ∈ M }. When E occurs, t t t,ct t t t the algorithm is in the unwasted exploration stage and x∗ = x∗ (n). m (cid:98)m When D tc ,ct occurs and c t ∈ M t, there exists x(cid:48) ∈ Act such that N x(cid:48)(t) ≤ min(f n/∆(cid:98) 2 min(t), T x(cid:48)(∆(cid:98) (t))). From the design of Algorithm 1, it holds that • If x = b 1, then N x(t) ≤ min(f n/∆(cid:98) 2 min(t), T x(∆(cid:98) (t))). • If x = b 2, then N x(t) = min x∈Act N x(t) ≤ min(f n/∆(cid:98) 2 min(t), T x(cid:48)(∆(cid:98) (t))). Since the algorithm either pulls b or b in the unwasted exploration, it implies an upper bound for s(t): 1 2 (cid:88) s(t) ≤ N x(t) ≤ |A| max min(f n/∆(cid:98) 2 min(t), T x(∆(cid:98) (t))). (B.20) x x∈Act Let Λ be the random variable given by (cid:26) (cid:27) Λ = min λ : max |(cid:104)x, θ(cid:98)t − θ(cid:105)| ≤ (cid:107)x(cid:107) G−1f n1 ,/ 12 /λ for all t ∈ [n] , x∈A t where f is defined in Eq. (A.11). By the concentration inequality Lemma A.2, for any λ ≥ 1, n,1/λ P (Λ ≥ λ) ≤ 1/λ . (B.21) Hence the event F = {Λ ≥ n} satisfies P (F ) ≤ 1/n. Denote αm(∆) = T m(∆)/f where T m(∆) is the x x n x solution of optimisation problem in Definition 4.1 with true ∆. Given υ > 0 let (cid:110) (cid:111) υ(δ) = sup (cid:107)α(∆) − α(∆(cid:101) )(cid:107) ∞ : (cid:107)∆(cid:101) − ∆(cid:107) ∞ ≤ δ , where α(∆) = {αm(∆)} . By continuity assumption of α at ∆ we have lim υ(δ) = 0. Moreover, x x∈Am,m∈[M] δ→0 let’s define (cid:26) (cid:27) τ δ = min t : max |(cid:104)x, θ(cid:98)s − θ(cid:105)| ≤ δ/2 for all x ∈ A and s ≥ t . x∈A Since N (t) ≥ ε s(t)/2, x n (cid:115) 2f m x∈a Ax |(cid:104)x, θ(cid:98)t − θ(cid:105)| ≤ ε nsn (, tΛ ) . 10
Therefore the number of exploration steps at time τ is bounded by s(τ ) ≤ 8f ε−1δ−2. δ δ n,1/Λ n Let (δ )∞ be a sequence with lim δ = 0 and log(log(n))/δ2 = o(log(n)). I decomposed as n n=1 n→∞ n n 11 n (cid:104) (cid:88) (cid:88) (cid:105) I = E ∆ I(X = x, E ) 11 x t t t=1 x∈Act − n (cid:104) (cid:88) (cid:88) (cid:105) ≤ E [s(τ )] + E ∆ I(X = x, E ) . (B.22) δn x t t t=τδn x∈Ac −t The first term in (B.22) is bounded by 8 E [s(τ )] ≤ E[f ] = o(log(n)) , δn ε δ2 n,1/Λ n n where we used the assumption on (δ ) and the fact that E[f ] = O(log log(n)). By the continuity n n,1/Λ assumption, the following statement holds n (cid:88) (cid:16) (cid:17) I(X t = x, E t) ≤ ε ns(n) + f n min 1/∆(cid:98) 2 min(n), α xct(∆(cid:98) (n))/2 t=τδn+1 (cid:16) 1 (cid:17) ≤ ε s(n) + f min , (αct(∆) + υ(δ ))/2 . (B.23) n n ∆(cid:98) 2 (n) x n min The second term in (B.22) is bounded by n (cid:104) (cid:88) (cid:88) (cid:105) E ∆ I(X = x, E ) x t t t=τδn x∈Ac −t M n (cid:104) (cid:88) (cid:88) (cid:88) (cid:105) ≤ E ∆ I(X = x, E ) x t t m=1 x∈Am t=1 − M M (cid:104) (cid:88) (cid:88) (cid:105) (cid:104) (cid:88) (cid:88) (cid:105) ≤ E ∆ ε s(n)I(E ) + E ∆ f (αm(∆) + υ(δ ))/2I(E ) . x n n x n x n n m=1 x∈Am m=1 x∈Am − − To bound the second term, we take the limit as n tends to infinity and the fact that lim υ(δ ) = 0 and n→∞ n f ∼ 2 log(n) shows that n M 1 (cid:104) (cid:88) (cid:88) (cid:105) lim sup E ∆ f (αm(∆) + υ(δ ))/2I(E ) ≤ C(θ, A1, . . . , AM ). (B.24) log(n) x n x n n n→∞ m=1 x∈Am − We bound the first term in the following lemma. The detailed proofs are deferred to Section B.4. Lemma B.3. The regret contributed by the forced exploration satisfies (cid:104) (cid:105) E (cid:80) ∆ ε s(n)I(E ) x∈Act x n n lim sup − = 0. log(n) n→∞ This ends the proof. (cid:4) 11
B.4 Proof of Lemma B.3: Forced Exploration Regret By the upper bound of unwasted exploration counter s(n) in (B.20), it holds that M M (cid:88) (cid:88) (cid:88) (cid:88) ∆m x ε ns(n)I(E n) ≤ ∆m x ε n|A| max min(f n/∆(cid:98) 2 min(n), T x(∆(cid:98) (n)))I(E n) x m=1 x∈Am m=1 x∈Am − − M (cid:88) (cid:88) ≤ ε n|A| ∆m x f n/∆(cid:98) min(n)I(E n). m=1 x∈Am − When event E occurs, n (∆m)2 (∆m)2 max x ≤ max x x(cid:54)=x(cid:98)∗ m(n) (∆(cid:98) x(n))2 x(cid:54)=x(cid:98)∗ m(n) (∆m x − 2β n)2 (cid:16) 4(∆m − β )β (cid:17) 16β = max 1 + x n n ≤ 1 + n , x(cid:54)=x(cid:98)∗ m(n) (∆m x − 2β n)2 ∆ min For any x ∈ Am, 1 ∆(cid:98) min(n) ≥ 1 + 16β /∆ ∆ min. (B.25) n min Since ε = 1/(log log(n)), we have n (cid:80) ∆ ε s(n)I(E) lim sup x∈A− x n = 0. (B.26) log(n) n→∞ This ends the proof. (cid:4) B.5 Proof of Lemma B.2: Wasted Exploration First, we define (cid:110) (cid:111) F s = ∃t ≥ d, ∃x : (cid:104)x, θ(cid:98)t(cid:105) − (cid:104)x, θ(cid:105) ≥ (cid:107)x(cid:107) G−1f n1 ,/ 12 /s2 , (B.27) t where f is defined in Lemma A.2. From Lemma A.2, we also have P(F ) ≤ 1/s2. Let s(cid:48)(t), s(t) be the n,1/s2 s number of rounds for wasted explorations, unwasted explorations until round t accordingly, and x∗ is the t optimal arm at round t. We decompose the regret as follows (cid:104) (cid:88) (cid:105) (cid:104) (cid:88) (cid:105) R ≤ E I(F )∆ + E I(Fc )(cid:104)x∗ − X , θ(cid:105) . (B.28) we s(cid:48)(t) max s(cid:48)(t) t t t∈wasted t∈unwasted (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) I1 I2 To bound I , we have 1 n n (cid:88) (cid:88) 1 1 I ≤ P(F )∆ ≤ ∆ = (2 − )∆ . (B.29) 1 s max s2 max n max s=1 s=1 To bound I 2, let’s denote θ(cid:101)t as the optimistic estimator. Following the standard one step regret decomposition (See the proof of Theorem 19.2 in [17] for details), it holds that (cid:104)x∗ − X , θ(cid:105) = (cid:104)x∗, θ(cid:105) − (cid:104)X , θ(cid:105) t t t t ≤ (cid:104)X t, θ(cid:101)t(cid:105) − (cid:104)X t, θ(cid:105) = (cid:104)X t, θ(cid:98)t − θ(cid:105) + (cid:104)X t, θ(cid:101)t − θ(cid:98)t(cid:105). 12
When Fc occurs, we have s(cid:48)(t) (cid:104)X t, θ(cid:98)t − θ(cid:105) ≤ (cid:107)X t(cid:107) G−1f n,1/(s(cid:48)(t)2), (cid:104)X t, θ(cid:101)t − θ(cid:98)t(cid:105) ≤ (cid:107)X t(cid:107) G−1f n,1/(s(cid:48)(t)2). t t Putting the above results together, we have (cid:104)x∗ − X , θ(cid:105) ≤ 2(cid:107)X (cid:107) f 1/2 . t t t G−1 n,1/(s(cid:48)(t)2) t Applying Lemma C.3, we can bound I as follows 2 I ≤ E(cid:104) 2f 1/2 (cid:88) (cid:107)X (cid:107) (cid:105) 2 n,1/(s(cid:48)(t)2) t G−1 t t∈wasted (cid:114) (cid:104) (cid:16) s(cid:48)(n) + d (cid:17)(cid:105) ≤ E 2f 1/2 2s(cid:48)(n)d log . (B.30) n,1/(s(cid:48)(t)2) d Recall that p = min p be the minimum probability that each action set arrives. It is easy to see min m m P(c ∈ M |Dc ) = P(c ∈ M |M (cid:54)= ∅) = (cid:80) p ≥ p . We bound s(cid:48)(n) by s(n) as follows t t t,ct t t t m∈Mt m min n n (cid:104) (cid:88) (cid:16) (cid:17)(cid:105) (cid:88) E[s(cid:48)(n)] = E I Dc , c ∈/ M = P(Dc )P(c ∈/ M |Dc ) t,ct t t t,ct t t t,ct t=1 t=1 n 1 (cid:88) ≤ P(Dc )P(c ∈ M |Dc ) p t,ct t t t,ct min t=1 n 1 (cid:104) (cid:88) (cid:16) (cid:17)(cid:105) 1 = E I Dc , c ∈ M = E[s(n)]. (B.31) p t,ct t t p min min t=1 Putting (B.29)-(B.31) together, The regret in the wasted exploration can be upper bounded by (cid:114) 1 2 (cid:16) s(n)/p + d (cid:17) R ≤ (2 − )∆ + 2d log min f s(n)/p , (B.32) we n max p d n,(pmin/s(n))2 min min where f is defined in (A.2). n,(pmin/s(n))2 Next, we recall the upper bound (B.20) for the number of pulls in unwasted exploration, (cid:110) (cid:111) s(n) ≤ |A| max min f n/∆(cid:98) min(n), T x(∆(cid:98) (n)) x ≤ |A|f n/∆(cid:98) min(n). From (B.25), we have 1 ∆2 ∆(cid:98) min(n) ≥ 1 + δ ∆ min ≥ ∆ +min 16β , n min n where β = 1/ log(log(n)). Overall, we see s(n) ≤ O(log(n)). Plugging this into (B.32), we reach n R lim sup we = 0. log(n) n→∞ This ends the proof. (cid:4) 13
B.6 Proof of Lemma A.1 First, we start by the following claim: Claim B.4. Assume H is a sequence of d × d positive definite matrices such that H → H and H is positive n n semidefinite. Then, HH−1H → H as n → ∞. n Proof. Without loss of generality, we can assume that H is given in the block matrix form (cid:18) (cid:19) A 0 H = 0 0 where A is a nonsingular m × m matrix with m > 0. (If m = 0, H is the all zero matrix and the claim trivially holds.) Consider the same block partitioning of H : n (cid:18) (cid:19) A B H = n n , n B(cid:62) D n n where A is thus also an m × m matrix. Clearly, A = lim A and A is nonsingular (or H would be n n→∞ n n n singular), while B → B and D → D where all entries in B and D are zero. Then, as is well known, n n (cid:18) A−1 + A−1B S−1B(cid:62)A−1 −A−1B S−1(cid:19) H−1 = n n n n n n n n n . n −S−1B(cid:62)A−1 S−1 n n n n where S = D − B(cid:62)A−1B is the Schur-complement of block D of matrix H . Note that n n n n n n n (cid:18) A(A−1 + A−1B S−1B(cid:62)A−1)A 0(cid:19) HH−1H = n n n n n n . n 0 0 Since the matrix inverse is continuous if the limit is nonsingular, A−1 → A−1. Clearly, it suffices to show n that A−1 + A−1B S−1B(cid:62)A−1 → A−1. Hence, it remains to check that A−1B S−1B(cid:62)A−1 → 0. This follows n n n n n n n n n n n because B → B and D → D and S → D − B(cid:62)A−1B = 0 where D = 0 and B = 0. n n n Proof of Lemma A.1. Let L = lim sup log(n)(cid:107)s(cid:107)2 . We need to prove that L ≤ 1/c. Without loss n→∞ G− n 1 of generality, assume that L > 0 (otherwise there is nothing to be proven) and that for some H positive semidefinite matrix, ζ ∈ R and κ ∈ R ∪ {∞}, (i) log(n)(cid:107)s(cid:107)2 → L; (ii) H = G−1/||G−1|| → H; (iii) G− n 1 n n n λ (G )/ log(n) → ζ > 0 and (iv) ρn(H) → κ ≥ c. We claim that (cid:107)s(cid:107) > 0, hence ρ (H) is min n log(n)(cid:107)s(cid:107)2 H n G− n 1 well-defined and in particular ρ (H) → 1 as n → ∞. If this was true, then the proof was ready since n L = lim log(n)(cid:107)s(cid:107)2 G− n 1 = 1 = 1/κ ≤ 1/c . n→∞ ρ n(H) lim n→∞ log(ρ nn )(cid:107)(H s(cid:107)) 2 G− n 1 Hence, it remains to show the said claim. We start by showing that (cid:107)s(cid:107) > 0. For this note that H ||G−1|| = 1/λ (G ) and hence n min n λ (G ) (cid:107)s(cid:107)2 = min n (cid:107)s(cid:107)2 log(n) . G− n 1 log(n) G− n 1 ||G− n 1|| Taking the limit of both sides, we get (cid:107)s(cid:107)2 → ζL > 0. Now, H ρ (H) = (cid:107)s(cid:107)2 G− n 1(cid:107)s(cid:107)2 HGnH = (cid:107)s(cid:107)2 Hn(cid:107)s(cid:107)2 HHn−1H n→ →∞ (cid:107)s(cid:107)2 H (cid:107)s(cid:107)2 H = 1 , n (cid:107)s(cid:107)4 (cid:107)s(cid:107)4 (cid:107)s(cid:107)4 H H H where we used Claim B.4. 14
B.7 Proof of Theorem 3.9 Suppose that {x∗ : m ∈ [M ]} spans Rd. Recall that LinUCB chooses m X t = argmax(cid:104)x, θ(cid:98)t−1(cid:105) + ||x|| G−1 β t1/2 , x∈Act t−1 where β = O(d log(t)) is chosen so that t (cid:16) (cid:17) P ||θ(cid:98)t − θ|| Gt ≥ β t ≤ 1/t3 , which is known to be possible [17, §20]. Define F t to be the event that ||θ(cid:98)t −θ|| Gt ≥ β t. Then the instantaneous pseudo-regret of LinUCB is bounded by (cid:113) ∆ ≤ 1 + (cid:104)x∗ − X , θ(cid:105) ≤ 1 + 2β1/2||X || ≤ 1 + 2 β ||G−1|| , t Ft t t Ft t t G− t 1 Ft t t where the matrix norm is the operator name (in this case, maximum eigenvalue). Let τ = 1+max{t : F holds}, t which satisfies E[τ ] = O(1). The cumulative regret after τ is bounded almost surely by (cid:88)n (cid:104)x∗ − X , θ(cid:105) = O (cid:0)√ n log(n)(cid:1) , t t t=τ where the Big-Oh hides constants that only depend on the dimension. Hence all optimal arms are played linearly often after τ , which by the assumption that {x∗ : m ∈ [M ]} spans Rd implies that ||G−1|| = O(1/t). m t Hence the instantaneous regret for times t ≥ τ satisfies (cid:32)(cid:114) (cid:33) β ∆ = O t . t t Since ∆ ∈ {0} ∪ [∆ , 1], it follows that the regret vanishes once ∆ < ∆ . But by the previous argument t min t min and the assumption on β we have for t ≥ τ that t (cid:32)(cid:114) (cid:33) (cid:113) log(t) ∆ ≤ 2 β ||G−1|| = O . t t t t Hence for sufficiently large t (independent of n) the regret vanishes, which completes the proof. C Supporting Lemmas Lemma C.1 (Bretagnolle-Huber Inequality). Let P and P (cid:101) be two probability measures on the same measurable space (Ω, F). Then for any event D ∈ F, 1 (cid:16) (cid:17) P(D) + P (cid:101)(Dc) ≥ exp −KL(P, P (cid:101)) , (C.1) 2 where Dc is the complement event of D (Dc = Ω \ D) and KL(P, P (cid:101)) is the KL-divergence between P and P (cid:101), which is defined as +∞, if P is not absolutely continuous with respect to P (cid:101), and is (cid:82) dP(ω) log dP (ω) Ω d(cid:101)P otherwise. The proof can be found in the book of Tsybakov [21]. When KL(P, P (cid:101)) is small, we may expect the probability measure P is close to the probability measure P (cid:101). Note that P(D) + P(Dc) = 1. If P (cid:101) is close to P, we may expect P(D) + P (cid:101)(Dc) to be large. 15
Lemma C.2 (Divergence Decomposition). Let P and P (cid:101) be two probability measures on the sequence (A , Y , . . . , A , Y ) for a fixed bandit policy π interacting with a linear contextual bandit with standard 1 1 n n Gaussian noise and parameters θ and θ(cid:101) respectively. Then the KL divergence of P and P (cid:101) can be computed exactly and is given by 1 (cid:88) KL(P, P (cid:101)) = 2 E[T x(n)] (cid:104)x, θ − θ(cid:101)(cid:105)2 , (C.2) x∈A where E is the expectation operator induced by P. This lemma appeared as Lemma 15.1 in the book of Lattimore and Szepesva´ri [17], where the reader can also find the proof. Lemma C.3. Let {X }∞ be a sequence in Rd satisfying (cid:107)X (cid:107) ≤ 1 and G = (cid:80)t X X(cid:62). Suppose that t t=1 t 2 t s=1 t t λ (G ) ≥ c for some strictly positive c. For all n > 0, it holds that min d n (cid:114) (cid:88) d + n (cid:107)X (cid:107) ≤ 2nd log( ). t G− t 1 d t=d+1 Lemma C.4. Let ε > 0 and denote T (∆(cid:98) (n)) ∈ R|A| as the solution of the optimisation problem defined in Definition 4.1. Then we define (cid:110) (cid:111) S ε(∆(cid:98) (n)) = min εf n, T (∆(cid:98) (n)) . Then for all x ∈ A, (cid:110) ε2 ∆(cid:98) 2 (n) (cid:111) (cid:107)x(cid:107)2 ≤ max , x . H−1 f f Sε(∆(cid:98) (n)) n n This is Lemma 17 in the book of Lattimore and Szepesv´ari [16], where the reader can also find the proof. Lemma C.5. Suppose that T m(·) is uniquely defined at ∆. Then it is continuous at ∆. x Proof. Suppose it is not continuous. Then there exists a sequence (∆ )∞ with lim ||∆ − ∆|| = 0 and n n=1 n→∞ n for which lim T m(∆ ) (cid:54)= T m(∆) for some m and x ∈ Am. Since ∆ → ∆ it follows that for sufficiently n→∞ x n x n large n the optimal actions with respect to ∆ are the same as ∆. Hence, for sufficiently large n, by the n definition of the optimisation problem, T m (∆ ) = ∞ = T m (∆) . x∗ n x∗ m m Therefore there exists a context m and suboptimal action x (cid:54)= x∗ such that lim T m(∆ ) (cid:54)= T m(∆). It is m n→∞ x n x easy to check that the value of the optimisation problem is continuous. Specifically, that M M (cid:88) (cid:88) (cid:88) (cid:88) lim T m(∆ ) = T m(∆) . x n x n→∞ m=1 x∈Am m=1 x∈Am Hence lim sup T m(∆ ) < ∞ for x =(cid:54) x∗ . Therefore a compactness argument shows there exists a cluster n→∞ x n m point S of the allocation (T (∆ ))∞ with Sx (cid:54)= T x (∆) for some m and x (cid:54)= x∗ . And yet by the previous n n=1 m m m display M M (cid:88) (cid:88) (cid:88) (cid:88) Sm = T m(∆) . x x m=1 x∈Am m=1 x∈Am Since the constraints of the optimisation problem are continuous it follows that S also satisfies the constraints in the optimisation problem and so S (cid:54)= T (∆) is another optimal allocation, contradicting uniqueness. Therefore T m(·) is continuous at ∆. x 16
Figure 4: The left panel is for small size action set and the right panel is for randomly generated θ. The results are averaged over 100 realisations. D Additional Experiments In this section, we consider two more experiment settings in Figure 4. 1. Small size action set. We conduct the experiments with the number of action set equal to 5. Comparing with large size action set (Section 5.4), we found that OAM still outperforms OSSB but the improvement is smaller, as one might expect. 2. Randomly generated θ. For each replication, θ is randomly generated from multivariate normal with variance 10 and we normalise θ such that its (cid:96) norm is 1. OAM still outperforms OSSB for randomly 2 generated θ. In addition, we compare with the heuristic LinTS (remove all the variance blowup factors and use a Gaussian prior). We find that the heuristic LinTS enjoys the best performance by a modest margin. Analysing heuristic LinTS, however, remains a fascinating open problem. As far as we are aware, it is not known whether or not it even achieves sublinear regret in the worst case. 17
