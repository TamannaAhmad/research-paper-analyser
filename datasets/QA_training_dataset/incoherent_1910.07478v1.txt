Adaptive Trade-Offs in Off-Policy Learning Mark Rowland* Will Dabney* Rémi Munos DeepMind DeepMind DeepMind Abstract turns, which do not account for the fact that data is generated off-policy. Further research has corroborated A great variety of off-policy learning algo- this observation [Hernandez-Garcia and Sutton, 2019]. rithms exist in the literature, and new break- This raises two central research questions: (i) How throughs in this area continue to be made, im- can we understand the strong performance of uncor- proving theoretical understanding and yield- rected returns? (ii) Can we distil these advantages, ing state-of-the-art supervised learning al- and combine them with existing off-policy algorithms gorithms. In this paper, we take a unifying to improve their performance? view of this space of algorithms, and consider their trade-offs of three fundamental quan- One of the principal contributions of this paper is to tities: update variance, fixed-point bias, and show that the performance of all off-policy evaluation contraction rate. This leads to new perspec- algorithms can be decomposed into just three funda- tives of existing methods, and also naturally mental quantities: contraction rate, fixed-point bias, yields novel algorithms for off-policy evalua- and variance; see Figure 1 for a preliminary illustra- tion and control. We develop one such algo- tion. Intuitively, fixed-point bias describes the error of rithm, C-trace, demonstrating that it is able an algorithm in the limit of infinite data, contraction to more efficiently make these trade-offs than rate describes the speed at which an algorithm ap- existing methods in use, and that it can be proaches its infinite-data limit, and variance describes scaled to yield state-of-the-art performance to what extent randomly observed data can perturb in large-scale environments. the algorithm. This decomposition yields an interpretation of the em- 1 Introduction pirical success of uncorrected returns, and an answer to question (i) above; namely, that they are efficiently Off-policy learning is crucial to modern reinforcement making a trade-off between fixed-point bias and the learning, allowing agents to learn from memorised data, other fundamental quantities. Further, this suggests demonstrations, and exploratory behaviour [Szepesvári, an answer to question (ii) — that we may be able to 2010, Sutton and Barto, 2018]. As such, it is a long- improve existing off-policy algorithms by incorporating studied problem, with a variety of well-understood a means of making such a trade-off. This leads us to associated algorithms; see [Precup et al., 2000, Kakade the development of C-trace, a new off-policy algorithm and Langford, 2002, Dudík et al., 2014, Thomas and that achieves strong empirical performance in several Brunskill, 2016, Munos et al., 2016, Mahmood et al., large-scale environments. 2017, Farajtabar et al., 2018] for a representative selec- tion of publications. We develop the trade-off framework mentioned above in Section 2, proving the existence of the three funda- However, this paper is motivated by the observation mental quantities described above, and showing that that in spite of this theoretical progress, several state-of- all off-policy algorithms necessarily make an implicit the-art value-based supervised learning agents (no- trade-off between these quantities. We then use this tably Rainbow [Hessel et al., 2018] and R2D2 [Kaptur- framework to develop a new off-policy learning algo- owski et al., 2019]) eschew these off-policy algorithms, rithm, C-trace, in Section 3, and study its contraction attaining better performance by using uncorrected re- and convergence properties. We then demonstrate its empirical effectiveness in tabular domains and when ap- plied to two deep supervised learning agents, DQN [Mnih et al., 2015] and R2D2 [Kapturowski et al., 2019], Preprint. *Equal contribution. in Section 4. 9102 tcO 61 ]GL.sc[ 1v87470.0191:viXra
Adaptive Trade-Offs in Off-Policy Learning 1.0 0.8 0.6 0.4 0.2 0.0 0 1 Fixed-point bias etar noitcartnoc citylanA 10-2 10-3 10-4 10-5 0 1 Fixed-point bias ecnairav laciripmE 10-2 10-3 10-4 10-5 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE Figure 1: Trade-offs made by n-step uncorrected returns (dark blue [n = 1] through to light blue [n = 20]), n-step importance corrected returns (dark green [n = 1] through to light green [n = 3]), Retrace (red open circle). Also pictured is the new method α-Retrace (dark red [α = 1] through to light red [α = 0]), introduced in Section 3. All quantities are calculated for a fixed evaluation problem in a small, randomly generated MDP; see Appendix Section C.1 for further environment details. In each plot, the magnitude of the points illustrates the relative scale of the third trade-off variable. 1.1 Notation and preliminary definitions as input a value function estimate Q and a trajectory (x , a , r ) given by following µ, and outputs an up- t t t t≥0 Throughout, we consider a Markov decision process date for Q(x , a ). There is an associated evaluation 0 0 (MDP) with finite state space X , finite action space operator T : RX ×A → RX ×A, given by A, discount factor γ ∈ [0, 1), transition kernel P : (cid:104) (cid:12) (cid:105) X × A → P(X ), reward distributions R : X × A → (T Q)(x, a)=E Tˆ(Q, (X , A , R )∞ )(cid:12)X = x, A = a , µ t t t t=0 (cid:12) 0 0 P(R), and some initial state distribution ν ∈ P(X ). 0 Given a Markov policy π : X → P(A), we write for all Q ∈ RX ×a and (x, a) ∈ X × A. (X , A , R ) for the process describing the sequence t t t t≥0 Definition 1.2. The contraction rate of an operator of states visited, actions taken, and rewards received T : RX ×A → RX ×A is given by by an agent acting in the MDP according to π, so that R t|X t, A t ∼ R(X t, A t) for all t ≥ 0. Additionally, Γ = sup (cid:107)T Q − T Q(cid:48)(cid:107) /(cid:107)Q − Q(cid:48)(cid:107) , ∞ ∞ we write r(x, a) for the expected immediate reward Q,Q(cid:48)∈RX×A received after taking action a in state x. Given a Q(cid:54)=Q(cid:48) policy π, the task of evaluation is to learn the function Qπ(x, a) = E [(cid:80)∞ γtR |X = x, A = a], where E An operator is said to be contractive if Γ < 1. We can π t=0 t 0 0 π also consider state-action specific contraction rates via denotes expectation with respect to the distribution the quantities sup |(T Q)(x, a)−(T Q(cid:48))(x, a)|/(cid:107)Q− over trajectories induced by π. The task of control is to Q(cid:54)=Q(cid:48) Q(cid:48)(cid:107) . identify the Markov policy π∗ maximising the quantity ∞ E [Qπ(X , A )], where A ∼ π(·|X ), and X ∼ ν . Definition 1.3. For a contractive operator T target- 0 0 0 0 0 0 We also define the one-step evaluation operator T µ : ing a policy π, the fixed-point bias of T is given by RX ×A → RX ×A associated with a Markov policy µ : (cid:107)Qπ − Qˆπ(cid:107) 2, where Qˆπ is the unique fixed point of T X → P(A) by (guaranteed to exist by the contractivity of T ). Definition 1.4. The variance of an update (T µQ)(x, a) = (1) rule Tˆ stochastically approximating an opera- (cid:88) r(x, a) + P (x(cid:48)|x, a)µ(a(cid:48)|x(cid:48))Q(x(cid:48), a(cid:48)) , tor T at approximate value function Q and an initial state-action distribution ν ∈ P(X × A) is x(cid:48)∈X ,a(cid:48)∈A (cid:104) (cid:104) (cid:12) (cid:105)(cid:105) E E (cid:107)Tˆ(Q, (X , A , R )∞ )−T Q(cid:107)2(cid:12)X , A . for all Q ∈ RX ×A, and (x, a) ∈ X × A. (X0,A0)∼ν µ t t t t=0 2(cid:12) 0 0 We now briefly give formal definitions of the key con- 2 Contraction, bias, and variance in cepts we seek to analyse in this paper. off-policy evaluation Definition 1.1. An evaluation update rule for evaluating a policy π under a behaviour policy µ is a We begin with two motivating examples from recent function Tˆ : RX ×A × (X × A × R)∗ → R that takes research in off-policy evaluation methods, illustrating
Mark Rowland*, Will Dabney*, Rémi Munos examples of the types of trade-offs we seek to describe where we write ρ¯ = min(1, ρ ), and ρ¯ = (cid:81)t ρ¯ t t s:t u=s u in this paper. for each 1 ≤ s ≤ t, and define the temporal difference (TD) error ∆ by n-step uncorrected returns. Recently proposed s agents such as Rainbow [Hessel et al., 2018] and R2D2 (cid:104) (cid:105) ∆ d=ef r + γE Qˆ(x , A) − Qˆ(x , a ) . [Kapturowski et al., 2019] have made use of the uncor- s s A∼π(·|xs+1) s+1 s s rected n-step return in constructing off-policy learning algorithms. Consistent with these results, Hernandez- By clipping the importance weights associated with Garcia and Sutton [2019] observed that these uncor- each TD error, the variance associated with the up- rected updates frequently outperformed off-policy cor- date rule is reduced relative to importance-weighted rections. Given an estimate Qˆ of the action-value func- returns, whilst no bias is introduced; the fixed point tion Qπ, the n-step uncorrected target for Qˆ(x , a ), of the associated Retrace operator remains the true 0 0 given a trajectory (x , a , r , x , a , r , . . . , x ) of expe- action-value function Qπ. However, the clipping of 0 0 0 1 1 1 n rience generated according to behaviour policy µ, is the importance weights effectively cuts the traces in given by the update, resulting in the update placing less weight on later TD errors, and thus worsening the contrac- n−1 (cid:88) γsr + γnE (cid:104) Qˆ(x , A)(cid:105) . (2) tion rate of the corresponding operator. Thus, Retrace s A∼π(·|xn) n can be interpreted as trading off a reduction in up- s=0 date variance for a larger contraction rate, relative to The adjective uncorrected contrasts this update target importance-weighted n-step returns. against the n-step importance-weighted return target, which takes the following form: We discuss more examples of off-policy learning algo- n−1 rithms in Section 5. We also note that λ-variants of the (cid:88) ρ 1:sγsr s + ρ 1:n−1γnE A∼π(·|xn)(cid:104) Qˆ(x n, A)(cid:105) , (3) algorithms described above also exist; for clarity and s=0 conciseness, we limit our exposition to the case λ = 1 where we write ρ = π(a |x )/µ(a |x ), and ρ = in the main paper, noting that the results straightfor- t t t t t s:t (cid:81)t ρ for each 1 ≤ s ≤ t. Empirically, the former wardly extend to λ ∈ (0, 1). u=s u has been observed to work very well in these recent We now briefly return to Figure 1, which quantitatively works, whilst the latter is often too unstable to be used; illustrates the trade-offs discussed above. We highlight this fact is often attributed to the high variance of the several interesting observations. Whilst all importance- importance-weighted update, with the uncorrected up- weighted updates have no fixed-point bias, their vari- date having relatively low variance by comparison. We ance grows exceptionally quickly with n. Retrace man- also observe that the uncorrected update is a stochas- ages to achieve a similar contraction rate to the 3-step tic approximation to the operator (T µ)n−1T π, whilst importance-weighted update, but without incurring the importance-weighted update is a stochastic approx- high variance. Our new algorithm, α-Retrace, appears imation to (T π)n. From this, it follows that under to be Pareto efficient relative to the n-step uncorrected usual stochastic approximation conditions, a sequence methods in the left-most plot; for any contraction rate of importance-weighted updates will converge to the that an n-step uncorrected method achieves, there is a true action-function Qπ associated with π, whilst the value of α such that α-Retrace achieves this contrac- uncorrected updates will converge to the value function tion rate whilst incurring less fixed-point bias; this is of the time-inhomogeneous policy that follows π for corroborated by further empirical results in Appendix a single step, followed by n − 1 steps of µ, and then Section B. repeats; see Proposition B.1 in Appendix Section B for further explanation. 2.1 Downstream tasks and bounds The above discussion shows that we may view the use of uncorrected returns as trading off update variance Whilst the trade-offs at the level of individual updates for F1-score of the operator fixed point; an example described above are straightforward to describe, in of the classical bias-variance trade-off in statistics and supervised learning we are ultimately interested machine learning, albeit in the context of fixed-point in one of two problems, either evaluation or control, iteration. defined formally below. Retrace. Munos et al. [2016] proposed an off-policy The evaluation problem. Given a target policy evaluation update target, Retrace, given in its forward- π, a budget of experience generated from a behaviour view version by policy µ, and a computational budget, compute an ac- Qˆ(x 0, a 0)+(cid:88) ρ¯ 1:sγs∆ s , (4) curate approximation Qˆ to Qπ, in the sense of incurring s≥0 low error (cid:107)Qˆ − Qπ(cid:107), for some norm (cid:107) · (cid:107).
Adaptive Trade-Offs in Off-Policy Learning The control problem. Given a budget of experience where d is the discounted state-action visitation (x,a),π and computation, find a policy π such that expected distribution for trajectories initialised with (x, a), fol- return under π is maximised. lowing π thereafter. Denoting the variance, contraction rate, and fixed-point bias of Tˆ for a particular MDP It is intuitively clear that for each of these problems, M ∈ M by V(M ), Γ(M ) and B(M ) respectively, we an evaluation scheme with low contraction rate, low have update variance, and low fixed-point bias is advanta- geous, but no update is known to possess all three of sup (cid:20) (cid:112) V(M ) + 2R max Γ(M ) + B(M )(cid:21) ≥ these attributes simultaneously. What is less clear is 1 − γ M∈M how these three properties should be traded off against sup (1 − δ)D(Z, π, δ)R /(1 − γ) . one another in designing an efficient off-policy learning max δ∈(0,1) algorithm. For example, how much fixed-point accu- racy should one be willing trade off in exchange for a In addition to the above results, which we believe to be halved update variance? Questions such as these in novel, there is extensive literature exploring particular general have complicated dependence on the precise aspects of these trade-offs, which we discuss further structure of the update rule, the policies in question, in Section 5. Having made this space of trade-offs be- and the environment too, and so it seems unlikely that tween contraction, bias, and variance explicit, a natural much progress can be made here in great generality. questions is how other update rules might be modified However, it is possible to make some progress. to exploit different parts of the space. In particular, Proposition 2.1. Consider the task of evaluation of a incurring some amount of fixed-point bias for reduced policy π under behaviour µ, and consider an update rule variance made by n-step uncorrected returns in Rain- Tˆ which stochastically approximates the application bow and R2D2 is particularly effective in practice — of an operator T(cid:101), with contraction rate Γ and fixed is there a way to introduce a similar trade-off in an point Q(cid:101), to an initial estimate Q. Then we have the algorithm with adaptive trace lengths, such as Retrace? following decomposition: We explore this question in the next section. (cid:104) (cid:105) E (cid:107)TˆQ − Qπ(cid:107) ≤ ∞ 3 New off-policy updates: α-Retrace (cid:104) (cid:105)1/2 E (cid:107)TˆQ − T(cid:101)Q(cid:107)2 2 + Γ(cid:107)Q − Q(cid:101)(cid:107) ∞ + (cid:107)Q(cid:101) − Qπ(cid:107) 2 . and C-trace (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Contraction Fixed-point bias (Root) variance The Retrace update in Equation (4) has been observed, in certain scenarios, to cut traces prematurely [Mah- This result gives some sense of how these trade-offs feed mood et al., 2017]; that is, using n-step uncorrected into evaluation quality; related decompositions are also returns for suitable n leads to a superior contraction possible, which we describe in Appendix Section B. We rate relative to Retrace, outweighing the correspond- next show that there really is a trade-off to be made, ing incurred bias. A natural question is how Retrace in the sense that it is not possible for an update based can be modified to overcome this phenomenon. In on limited data to simultaneously have low variance, the language of Section 2, is there a way that Retrace contraction rate, and fixed-point bias across a range of can be adapted so as to trade off contraction rate for MDPs. fixed-point bias? The reduced contraction rate comes Theorem 2.2. Consider an update rule Tˆ with cor- from cases where the truncated importance weights responding operator T , and consider the collection min(1, π(a t|x t)/µ(a t|x t)) appearing in (4) are small, so M = M(X , A, P, γ, R ) of MDPs with common a natural way to improve the contraction rate is to max state space, action space, transition kernel, and dis- move the target policy closer towards the behaviour. count factor (but varying rewards, with maximum im- mediate reward bounded in absolute value by R ). Algorithm 1 α-Retrace for policy iteration max Fix a target policy π, and a random variable Z, the Initialise target policy π and behaviour µ. set of transitions used by the operator Tˆ; these could for each policy improve(cid:101) ment round: do be transitions encountered in a trajectory following Select α ∈ [0, 1], and set new target policy π = the behaviour µ, or i.i.d. samples from the discounted απ + (1 − α)µ. (cid:101) state-action visitation distribution under µ. We denote Learn Qˆπ : X × A → R via Retrace under be- the mismatch between π and Z at level δ ∈ (0, 1) by haviour policy µ. Set π = Greedy(Qˆπ). (cid:101) D(Z, π, δ) d=ef max{d (x,a),π(Ω) | Ω ⊆ X × A s.t. Set new behaviour policy µ. P(Z ∩ Ω (cid:54)= ∅) ≤ δ , (x, a) ∈ X ×A} , end for
Mark Rowland*, Will Dabney*, Rémi Munos To this end, we propose α-Retrace, a family of algo- rithms that applies Retrace to a target policy given by a mixture of the original target and the behaviour, thus achieving the aforementioned trade-off. In Algorithm 1, we describe how α-Retrace can be used within a (mod- ified) policy iteration scheme for control. Note that 1-Retrace is simply the standard Retrace algorithm. We refer back to Figure 1, the left-most plot of which shows that this mixture coefficient precisely yields a trade-off between fixed-point bias and contraction rate that we sought at the end of Section 2. The means by which α should be set is left open at this stage; adjusting it allows a trade-off of contraction Figure 2: Interpolating between target policy π and be- rate and fixed-point bias. In Section 3.2, we describe haviour policy µ with α ∈ {0.0, 0.2, 0.8, 1.0} produces a stochastic approximation procedure for updating α different expected trajectories shown by each coloured online to obtain a desired contraction rate. line. As the mixture policy more closely resembles the behaviour policy, α-Retrace allows more off-policy data Specificity to Retrace. Whilst the mixture target to be used (dashed line, numbers indicate expected of α-Retrace is a natural choice, we highlight that this trace-length), cuts traces (coloured points) later, yield- choice is in fact specific to the structure of Retrace. ing lower contraction rates equivalent to n-step meth- In Appendix Section D.1, we visualise trade-offs made ods with larger n. C-trace adapts α online to achieve by analogous adjustment to the TreeBackup update a stable trace length throughout training. [Precup et al., 2000], showing that mixing the behaviour policy into the target simply leads to an accumulation of fixed-point bias, with limited benefits in terms of strictly monotonic iff π and µ are (x, a)-distinguishable contraction rate or variance. under µ. The exact contraction rate of α-Retrace is thus 3.1 Analysis sup C(α|x, a), which inherits the continu- (x,a)∈X ×A ity and monotonicity properties of the state-action- We now provide several results describing the contrac- dependent rates. Our next result motivates the use of tion rate of α-Retrace in more detail, and how the α-Retrace within control algorithms. fixed-point bias introduced by α < 1 may be useful in the case of control tasks. We begin with a preliminary Proposition 3.3. Consider a target policy π, let µ definition. be the behavioural policy, and assume that there is a unique greedy action a∗(x) ∈ A with respect to Qπ at Definition 3.1. For a state-action pair (x, a) ∈ each state x for each x ∈ X . Then there exists a value X × A, Two policies π , π are said to be (x, a)- 1 2 of α ∈ (0, 1) such that the greedy policy with respect to distinguishable under a third policy µ if there exists x(cid:48) ∈ X in the support of the discounted state visitation the fixed point of α-Retrace coincides with the greedy policy with respect to Qπ, and the contraction rate for distribution under µ starting from state-action pair (x, a), such that π (·|x(cid:48)) (cid:54)= π (·|x(cid:48)), and are said to be this α-Retrace is no greater than that for 1-Retrace. 1 2 Further, if π and µ are (x, a)-distinguishable under µ (x, a)-indistinguishable under µ otherwise. for all (x, a) ∈ X × A, then the contraction rate of Proposition 3.2. The operator associated with the α-Retrace is strictly lower than that of 1-Retrace. α-Retrace evaluation update for evaluating π given behaviour µ has a state-action-dependent contraction 3.2 C-trace: adapting α online rate of An empirical shortcoming of Retrace noted earlier is C(α|x, a) d=ef 1 − (1 − γ)× (5) its tendency to pessimistically cut traces. Adapting (cid:34) (cid:88)∞ (cid:89)t (cid:12) (cid:12) (cid:35) the mixture parameter α within Retrace(α) yields a E µ γt ((1 − α) + αρ¯ s)(cid:12) (cid:12)(X 0, A 0) = (x, a) , natural way to ensure that a desired trace length (or (cid:12) t=0 s=1 contraction rate) is attained. In this section, we propose C-trace, which uses α-Retrace updates whilst dynami- for each (x, a) ∈ X × A. Viewed as a function of α ∈ cally adjusting α to attain a target contraction rate Γ; [0, 1], this contraction rate is continuous, monotonically a schematic illustration is given in Figure 2. increasing, with minimal value 0, and maximal value no greater than γ. Further, the contraction rate is The contraction rate sup C(α|x, a) is difficult (x,a)∈X ×A
Adaptive Trade-Offs in Off-Policy Learning to estimate online, so we work instead with the averaged Q obtained from applying Retrace updates target- k+1 contraction rate C (α) = E [C(α|X, A)], where ing α(φ )π + (1 − α(φ ))µ to Q with trajectory k + 1, ν (X,A)∼ν k k k ν is the training distribution over state-action pairs; using stepsize ε . Then we have α(φ ) → α(φ∗) =: α∗ k k where clear, we will drop ν from notation. It follows and Q → Qα∗π+(1−α∗)µ almost surely. k straighforwardly from Proposition 3.2 that C (α) is ν Truncated trajectory corrections. The method monotonic in α. This suggests that a standard Robbins- described above for adaptation of α is impractical in Monro stochastic approximation update rule for α may scenarios where episodes are particularly long, when be applied to guide C (α) towards Γ — we describe ν the MDP is non-episodic, and when only partial seg- such a scheme below. To avoid optimisation issues ments of trajectories can be processed at once. Since with the constraint α ∈ [0, 1], we parameterise α as such cases often arise in practice, this motivates modifi- σ(φ), where σ is the standard sigmoid function, and cations to the update of (6). Here, we describe one such φ ∈ R is an unconstrained real variable. For brevity, modification which will be crucial to the deployment we will simply write α(φ). Since σ is monotonic and of C-trace in large-scale agents in Section 4. Given a continuous, the contraction rate is still monotonic and truncated trajectory (x , a , r )N , Retrace necessarily continuous in φ. t t t t=0 must cut traces after at most N time steps, and so Recall from (5) that the contraction rate C(α|x, a) of can achieve a contraction rate of γN at the very lowest. the α-Retrace operator with target π and behaviour µ We thus adjust the target contraction rate accordingly, can be expressed as an expectation over trajectories and arrive at the following update: following µ, and thus can be unbiasedly approximated (cid:16) (cid:17) φ = φ − ε Cˆ(k)(α(φ )) − max(Γ, γN ) . (7) using such trajectories; given an i.i.d. sequence of k+1 k k k trajectories (x(k), a(k), r(k)) , we write Cˆ(k)(α(φ)) for t t t t≥0 4 Experiments the corresponding estimates of C(α(φ)). If the target contraction rate is Γ, we can adjust an initial parameter φ ∈ R using these estimates according to the Robbins- Having explored the types of trade-offs α-Retrace 0 Monro rule makes relative to existing off-policy algorithms, we now (cid:16) (cid:17) investigate the performance of these methods in the φ k+1 = φ k − ε k Cˆ(k)(α(φ k)) − Γ ∀k ≥ 0 , (6) downstream tasks of evaluation and control described in Section 2.1. for some sequence of stepsizes (ε )∞ . The following k k=0 result gives a theoretical guarantee for the correctness Evaluation. In the left sub-plot of Figure 3, we com- of this procedure. pare the performance of α-Retrace, n-step uncorrected updates, and n-step importance-weighted updates, for Proposition 3.4. Let (x(k), a(k), r(k))∞ be an i.i.d. t t t t=0 various values of the parameters concerned, at an off- sequence of trajectories following µ, with initial state- policy evaluation task. In this particular task, the action distribution given by ν. Let Γ be a target con- environment is given by a chain MDP (see Appendix traction rate such that C (1) ≥ Γ. Let the stepsizes ν Section C.1), the target policy is optimal, and the be- (ε )∞ satisfy the usual Robbins-Monro conditions k k=0 haviour is the uniform policy. We plot Q-function L2 (cid:80)∞ ε = ∞, (cid:80)∞ ε2 < ∞. Then for any initial k=0 k k=0 k error against number of environment steps; see full value φ following the updates in (6), we have φ → φ∗ 0 k details in Appendix Section C.2. Standard error is in probability, where φ∗ ∈ R is the unique value such indicated by the shaded regions. that C (α(φ∗)) = Γ. ν The best performing methods vary as a function of C-trace thus consists of interleaving α-Retrace evalua- the number of environment steps experienced. For low tion updates with α parameter updates as in (6). numbers of environments steps, the best performing Convergence analysis. It is possible to further de- methods n-step uncorrected updates for large n, and velop the theory in Proposition 3.4 to prove convergence α-Retrace for α close to 0. Intuitively, in this regime, of C-trace as a whole, using techniques going back to a good contraction rate outweighs fixed-point bias. As those of Bertsekas and Tsitsiklis [1996] for convergence the number of environment steps increases, the fixed- of TD(λ), and more recently used by Munos et al. [2016] point bias of the uncorrected methods kicks in, and the to prove convergence of a control version of Retrace, optimally-performing α gradually increases from close as the following result shows. to 0 to close to 1. Note that typically the extremely high variance of the importance-weighted updates proclude Theorem 3.5. Assume the same conditions as Propo- them from attaining any reasonable level of evaluation sition 3.4, and additionally that: (i) trajectory lengths error. have finite second moment; (ii) immediate rewards are bounded. Let (φ )∞ be defined as in Equa- Control. In the right sub-plot of Figure 3, we compare k k=0 tion (6) and (Q )∞ be a sequence of Q-functions, with the performance of a variety of modified policy iteration k k=0
Mark Rowland*, Will Dabney*, Rémi Munos 1.0-Retrace 0.1-Retrace 0.9-Retrace 0.0-Retrace 0.8-Retrace 1-step Uncorrected 0.7-Retrace 3-step Uncorrected 0.6-Retrace 7-step Uncorrected 0.5-Retrace 19-step Uncorrected 0.7-Retrace 1-step IW 0.6-Retrace 2-step IW 0.5-Retrace 3-step IW Figure 3: Left: Performance of a variety of off-policy evaluation methods on a small MDP; for further details, see text of Section 4. Right: Performance of a variety of modified policy iteration methods on a small MDP; for further details, see text of Section 4. methods, each using a different off-policy evaluation the earlier stated goal of bridging the gap between the method. We use the same MDP as in the evaluation ex- performance of uncorrected returns and more principled ample above, and plot the sub-optimality of the learned off-policy algorithms in deep supervised learning. policy (measured as difference between expected return under a uniformly random initial state for optimal 4.2 C-trace-DQN and learned policies) against the number of policy im- provement steps performed. In this experiment, the To illustrate the flexibility of C-trace as an off-policy behaviour policy is fixed as uniform throughout. As learning algorithm, we also demonstrate its perfor- with evaluation, we see that initial improvements in mance within a DQN architecture [Mnih et al., 2015]. policy are strongest with highly-contractive evaluation We use Double DQN [Van Hasselt et al., 2016] as a methods incorporating some fixed-point bias, with less- baseline, and modify the one-step Q-learning rule to biased approaches catching up (and ultimately surpass- use n-step uncorrected returns, Retrace, and C-trace. ing) when greater amounts of environment interaction As for the R2D2 experiment, we set the C-trace contrac- are allowed. tion target using n = 10, demonstrating the robustness of this C-trace hyperparameter across different archi- 4.1 C-trace-R2D2 tectures. Further, we found the behaviour of C-trace to be generally robust to the choice of n; see Appendix To test the performance of our methods at scale, we Section F.2. Full experimental specifications are given adapted R2D2 [Kapturowski et al., 2019] to replace the in Appendix Section E.2, with detailed results in Ap- original n-step uncorrected update with Retrace and pendix Section F.2; a high-level summary is displayed C-trace. For C-trace we targeted the contraction rate in Figure 4. All sequence-based methods significantly given by an n-step uncorrected update, using a discount outperform Double DQN, as we would expect. We rate of γ = 0.997 and n = 10. Based on the Pareto notice that the performance gap between n-step and efficiency of α-Retrace relative to n-step uncorrected Retrace is not as large here as for R2D2 and hypoth- returns exhibited empirically in small-scale MDPs, we esize this is due to DQN being much more off-policy conjectured that this should lead to improved perfor- than R2D2. As with the R2D2 experiments we see mance. The agent was trained on the Atari-57 suite that C-trace-DQN achieves similar learning speed as of environments [Bellemare et al., 2013] with the same the targeted n-step update, but with improved final experimental setup as in [Kapturowski et al., 2019], performance. One interpretation of these results is that a description of which we include in Appendix Sec- the improved contraction rate of C-trace allows it to tion E.1. High-level results are displayed in Figure 4, learn significantly faster than Retrace, while the better plotting mean human-normalised performance, median fixed-point error allows it to find a better long-term human-normalised performance, and mean human-gap solution than n-step uncorrected. (across the 57 games) against wall-clock training time; detailed results are given in Appendix Section F.1. 5 Related work C-trace-R2D2 attains comparable or superior perfor- mance relative to R2D2 and Retrace-R2D2 in all A central observation of this work is that the fixed-point three performance measures. Thus, not only does C- bias can be explicitly traded-off to improve contraction trace-R2D2 match state-of-the-art performance for dis- rates. To our understanding, this is the first work to tributed value-based agents on Atari, it also achieves directly study this possibility, and further to draw atten-
Adaptive Trade-Offs in Off-Policy Learning Distributed Training (R2D2) Single-Actor Training (DQN) Figure 4: High-level performance of variants of R2D2 (top row) and DQN (bottom row) on the Atari suite of environments. R2D2-based methods are averages of two seeds. DQN-based methods are averages of three seeds. (Left) Mean human-normalized score, (Center) median human-normalized score, and (Right) human gap. tion to three fundamental quantities to be traded-off the absence of function approximation. More recently, in off-policy learning. However, investigating trade- this approach also led to several advances in policy offs in off-policy RL, and in particular parametrising gradient methods [Schulman et al., 2015, 2017] based methods to allow a spectrum of algorithms is a long- on trust regions. Although not the focus of this work, standing research topic [Sutton and Barto, 2018]. The there has been also been much progress on correct- most closely related methods come from a line of work ing state-visitation distributions [Sutton et al., 2016, that consider the bias-variance trade-off due to boot- Thomas and Brunskill, 2016, Hallak and Mannor, 2017, strapping. In our framework, we understand this as Liu et al., 2018], another form of off-policy correction a trade-off between variance and contraction rate, but important in function approximation, as illustrated in without modifying the fixed-point. The recently intro- the classic counterexample of Baird [1995]. duced Q(σ) algorithm uses the σ hyperparameter to mix between importance-weighted n-step SARSA and 6 Conclusion TreeBackup [De Asis et al., 2018]. In another recent re- lated approach, Shi et al. [2019] uses σ to mix between TreeBackup(λ) and Q(λ), although neither of these We have highlighted the fundamental role of variance, approaches adaptively set σ based on observed data. fixed-point bias, and contraction rate in off-policy learn- We have developed an adaptive method for adjusting ing, and described how existing methods trade off these α to achieve a desired trace length, and believe an in- quantities. With this perspective, we developed novel teresting direction for future work would be to develop off-policy learning methods, α-Retrace and C-trace, the adaptive methods described in this paper for use and incorporated the latter into several deep RL agents, in other families of off-policy learning algorithms. leading to strong empirical performance. Interesting questions for future work include applying Conservatively updating policies within control algo- the adaptive ideas underlying C-trace to other families rithms is a well-established practice; Kakade and Lang- of off-policy algorithms, investigating whether there ford [2002] consider a trust-region method for policy exist new off-policy learning algorithms in unexplored improvement, motivated by inexact policy evaluation areas of the space of trade-offs, and developing a deeper due to function approximation. In contrast, in this understanding of the relationship between these funda- work we consider regularised policy improvement as a mental properties of off-policy learning algorithms and means of improving evaluation of future policies, even in downstream performance on large-scale control tasks.
Mark Rowland*, Will Dabney*, Rémi Munos Acknowledgements Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Thanks in particular to Hado van Hasselt for detailed Silver. Rainbow: Combining improvements in deep comments and suggestions on an earlier version of supervised learning. In AAAI Conference on this paper, and thanks to Bernardo Avila Pires, Diana Artificial Intelligence, 2018. Borsa, Steven Kapturowski, Bilal Piot, Tom Schaul, and Yunhao Tang for interesting conversations during Sham Kakade and John Langford. Approximately op- the course of this work. timal approximate supervised learning. In Inter- national Conference on Machine Learning, 2002. References Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Rémi Munos. Recurrent experience TW Archibald, KIM McKinnon, and LC Thomas. On replay in distributed supervised learning. In In- the generation of Markov decision processes. Journal ternational Conference on Learning Representations, of the Operational Research Society, 46(3):354–361, 2019. 1995. Diederik P Kingma and Jimmy Ba. Adam: A method Leemon Baird. Residual algorithms: Reinforcement for stochastic optimization. In International Confer- learning with function approximation. In Machine ence on Learning Representations, 2015. Learning Proceedings. Elsevier, 1995. Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. Zhou. Breaking the curse of horizon: Infinite-horizon The arcade learning environment: An evaluation off-policy estimation. In Neural Information Process- platform for general agents. Journal of Artificial ing Systems, 2018. Intelligence Research, 47:253–279, jun 2013. Ashique Rupam Mahmood, Huizhen Yu, and Richard S Dimitri P Bertsekas and John N Tsitsiklis. Neuro- Sutton. Multi-step off-policy learning without im- dynamic programming, volume 5. Athena Scientific portance sampling ratios. arXiv, 2017. Belmont, MA, 1996. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Shalabh Bhatnagar, Richard S Sutton, Mohammad Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Ghavamzadeh, and Mark Lee. Natural actor–critic Graves, Martin Riedmiller, Andreas K Fidjeland, algorithms. Automatica, 45(11):2471–2482, 2009. Georg Ostrovski, et al. Human-level control through deep supervised learning. Nature, 518(7540):529, Kristopher De Asis, J Fernando Hernandez-Garcia, 2015. G Zacharias Holland, and Richard S Sutton. Multi- step supervised learning: A unifying algorithm. Rémi Munos, Tom Stepleton, Anna Harutyunyan, and In AAAI Conference on Artificial Intelligence, 2018. Marc Bellemare. Safe and efficient off-policy rein- forcement learning. In Neural Information Processing Miroslav Dudík, Dumitru Erhan, John Langford, and Systems, 2016. Lihong Li. Doubly robust policy evaluation and op- timization. Statistical Science, 29(4):485–511, 2014. Bilal Piot, Matthieu Geist, and Olivier Pietquin. Differ- ence of convex functions programming for reinforce- Mehrdad Farajtabar, Yinlam Chow, and Mohammad ment learning. In Neural Information Processing Ghavamzadeh. More robust doubly robust off-policy Systems, 2014. evaluation. In International Conference on Machine Learning, 2018. Doina Precup, Rich Sutton, and Satinder Singh. El- igibility traces for off-policy policy evaluation. In Matthieu Geist and Bruno Scherrer. Off-policy learning International Conference on Machine Learning, 2000. with eligibility traces: A survey. The Journal of Machine Learning Research, 15(1):289–333, 2014. Herbert Robbins and Sutton Monro. A stochastic approximation method. Ann. Math. Statist., 22(3): Assaf Hallak and Shie Mannor. Consistent on-line off- 400–407, 09 1951. policy evaluation. In International Conference on Machine Learning, 2017. John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region J Fernando Hernandez-Garcia and Richard S Sutton. policy optimization. In International Conference on Understanding multi-step deep reinforcement learn- Machine Learning, 2015. ing: A systematic study of the DQN target. arXiv, 2019. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Adaptive Trade-Offs in Off-Policy Learning Radford, and Oleg Klimov. Proximal policy opti- mization algorithms. arXiv, 2017. Longxiang Shi, Shijian Li, Longbing Cao, Long Yang, and Gang Pan. TBQ(σ): Improving efficiency of trace utilization for off-policy supervised learning. In International Conference on Autonomous Agents and Multiagent Systems, 2019. Richard S Sutton and Andrew G Barto. supervised learning: An introduction. MIT press, 2018. Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of off-policy temporal-difference learning. Journal of Machine Learning Research, 17(1):2603–2631, 2016. Csaba Szepesvári. Algorithms for reinforcement learn- ing. Synthesis lectures on artificial intelligence and machine learning, 4(1):1–103, 2010. Philip Thomas and Emma Brunskill. Data-efficient off- policy policy evaluation for supervised learning. In International Conference on Machine Learning, 2016. Hado Van Hasselt, Arthur Guez, and David Silver. Deep supervised learning with double Q-learning. In AAAI Conference on Artificial Intelligence, 2016. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. Du- eling network architectures for deep supervised learning. In International Conference on Machine Learning, 2016.
Mark Rowland*, Will Dabney*, Rémi Munos Appendices: Adaptive Trade-Offs in Off-Policy Learning A Proofs Proposition 2.1. Consider the task of evaluation of a policy π under behaviour µ, and consider an update rule Tˆ which stochastically approximates the application of an operator T(cid:101), with contraction rate Γ and fixed point Q(cid:101), to an initial estimate Q. Then we have the following decomposition: (cid:104) (cid:105) E (cid:107)TˆQ − Qπ(cid:107) ≤ ∞ (cid:104) (cid:105)1/2 E (cid:107)TˆQ − T(cid:101)Q(cid:107)2 2 + Γ(cid:107)Q − Q(cid:101)(cid:107) ∞ + (cid:107)Q(cid:101) − Qπ(cid:107) 2 . (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Contraction Fixed-point bias (Root) variance Proof. Note that by the triangle inequality: (cid:104) (cid:105) (cid:104) (cid:105) E (cid:107)TˆQ − Qπ(cid:107) ∞ ≤ E (cid:107)TˆQ − T(cid:101)Q(cid:107) ∞ + (cid:107)T(cid:101)Q − Q(cid:101)(cid:107) ∞ + (cid:107)Q(cid:101) − Qπ(cid:107) ∞ . Now, observing (cid:107)T(cid:101)Q − Q(cid:101)(cid:107) ∞ = (cid:107)T(cid:101)Q − T(cid:101)Q(cid:101)(cid:107) ∞ ≤ Γ(cid:107)Q − Q(cid:101)(cid:107) ∞ yields the second term on the right-hand side of the stated bound. Using the inequality (cid:107) · (cid:107) ≤ (cid:107) · (cid:107) and Jensen’s inequality yields the remaining terms. ∞ 2 Theorem 2.2. Consider an update rule Tˆ with corresponding operator T , and consider the collection M = M(X , A, P, γ, R ) of MDPs with common state space, action space, transition kernel, and discount factor (but max varying rewards, with maximum immediate reward bounded in absolute value by R ). Fix a target policy π, max and a random variable Z, the set of transitions used by the operator Tˆ; these could be transitions encountered in a trajectory following the behaviour µ, or i.i.d. samples from the discounted state-action visitation distribution under µ. We denote the mismatch between π and Z at level δ ∈ (0, 1) by D(Z, π, δ) d=ef max{d (Ω) | Ω ⊆ X × A s.t. (x,a),π P(Z ∩ Ω (cid:54)= ∅) ≤ δ , (x, a) ∈ X ×A} , where d is the discounted state-action visitation distribution for trajectories initialised with (x, a), following (x,a),π π thereafter. Denoting the variance, contraction rate, and fixed-point bias of Tˆ for a particular MDP M ∈ M by V(M ), Γ(M ) and B(M ) respectively, we have (cid:20) (cid:21) sup (cid:112) V(M ) + 2R max Γ(M ) + B(M ) ≥ 1 − γ M∈M sup (1 − δ)D(Z, π, δ)R /(1 − γ) . max δ∈(0,1) Proof. The high-level approach to the proof is to exhibit two MDPs M , M ∈ M which with high probability 0 1 under the data used by Tˆ, cannot be distinguished. This yields a high probability lower bound on the evaluation error that the operator Tˆ achieves on the two MDPs. This in turn implies that the mean-squared error quantity of Proposition 2.1 cannot be uniformly low across M and M , and this yields a lower bound for the quantity on 0 1 the right-hand side of the bound appearing in Proposition 2.1, as required. Using the notation introduced in the statement of the theorem, for a given δ ∈ (0, 1), let (x∗, a∗) ∈ X × A, Ω∗ ⊆ X be quantities achieving the maximum in the definition of D(Z, π, δ). Thus, with probability at least 1 − δ, none of the state-action pairs used by the algorithm Tˆ are contained in Ω∗. Now define two MDPs M , M with common state space X , action space A, transition kernel P , and discount 0 1 factor γ, with reward functions r , r : X × A → R defined by 0 1 (cid:40) 0 if x (cid:54)∈ Ω∗ r (x, a) = i (−1)iR if x ∈ Ω∗ . max
Adaptive Trade-Offs in Off-Policy Learning Now, the Q-functions associated with these two MDPs can be calculated by (I − γP π)−1r and (I − γP π)−1r , 0 1 and so we can read off their difference in the (x∗, a∗) coordinate as (cid:88) 1 d (x, a)2R = 2D(Z, π, δ)R max . 1 − γ (x∗,a∗),π max 1 − γ (x,a)∈Ω∗ Thus, with probability 1 − δ, the algorithm must make an error of at least D(Z, π, δ)R /(1 − γ) on one of max the MDPs M and M , as measured the L∞ norm. This implies that the expected L∞ error appearing on the 0 1 left-hand side of the bound in Proposition 2.1 is at least (1 − δ)D(Z, π, δ)R /(1 − γ) for one of the MDPs M max 0 and M . Thus, the statement of the theorem follows by taking a supremum over δ ∈ (0, 1). 1 Proposition 3.2. The operator associated with the α-Retrace evaluation update for evaluating π given behaviour µ has a state-action-dependent contraction rate of C(α|x, a) d=ef 1 − (1 − γ)× (5) (cid:34) (cid:88)∞ (cid:89)t (cid:12) (cid:12) (cid:35) E γt ((1 − α) + αρ¯ )(cid:12)(X , A ) = (x, a) , µ s (cid:12) 0 0 (cid:12) t=0 s=1 for each (x, a) ∈ X × A. Viewed as a function of α ∈ [0, 1], this contraction rate is continuous, monotonically increasing, with minimal value 0, and maximal value no greater than γ. Further, the contraction rate is strictly monotonic iff π and µ are (x, a)-distinguishable under µ. Proof. The α-Retrace operator for evaluation of π given behaviour µ corresponds to the standard Retrace operator for evaluation of πα = απ + (1 − α)µ given behaviour µ. Thus, from the analysis of Munos et al. [2016], the contraction rate of the α-Retrace operator specific to a particular state-action pair (x, a) ∈ X × A may be immediately written down as  (cid:12)  t (cid:18) (cid:19)(cid:12) 1 − (1 − γ)E µ (cid:88) γt (cid:89) min 1, απ(A t|X t) µ+ (A(1 |X− α ) )µ(A t|X t) (cid:12) (cid:12) (cid:12)X 0 = x, A 0 = a t≥0 s=1 t t (cid:12)  (cid:12)  t (cid:18) (cid:18) (cid:19)(cid:19)(cid:12) =1 − (1 − γ)E µ (cid:88) γt (cid:89) (1 − α) + α min 1, π µ(( AA t || XX t )) (cid:12) (cid:12) (cid:12)X 0 = x, A 0 = a . t≥0 s=1 t t (cid:12) To see that this is a continuous function of α ∈ [0, 1], we note that the integrand of the expectation above is clearly a continuous function of α, and is uniformly dominated by the constant function equal to (1 − γ)−1. By the dominated convergence theorem, continuity of the above expression follows. Since the integrand is non-negative and bounded above by (1 − γ)−1, the contraction rate must lie in the interval [0, γ] for all α ∈ [0, 1]. For monotonicity, we show that each term E (cid:34) (cid:89)t (cid:18) (1 − α) + α min (cid:18) 1, π(A t|X t) (cid:19)(cid:19)(cid:12) (cid:12) (cid:12)X = x, A = a(cid:35) (8) µ µ(A |X ) (cid:12) 0 0 t t (cid:12) s=1 is monotonic decreasing in α, meaning that the contraction rate is monotonic increasing in α. To this end, observe that the integrand of the expectation above almost-surely takes the form (cid:81)t (1 − z α) for some coefficients s=1 s z ∈ [0, 1]. The derivative with respect to α of this expression is (cid:80)t −z (cid:81)t (1 − z α), which is non-positive s s=1 s s(cid:48)(cid:54)=s s(cid:48) for α ∈ [0, 1]. It is again straightforward to apply the dominated convergence theorem to this derivative to obtain that the derivative of Expression (8) is non-positive for all α ∈ [0, 1], and we thus obtain monotonicity as required. Finally, for strict monotonicity, note that if π and µ are not distinguishable under µ, then all truncated importance weights in the expressions above are equal to 1 almost-surely under the distribution over states visited when following µ. Hence, the contraction rate is in fact constant as a function of α, and we therefore do not have strict monotonicity. On the other hand, if π and µ are (x, a)-distinguishable under µ, then there exists a t ∈ N such that in the integrand of the expectation in Expression (8), in one of the terms constituting the product, the coefficient of α is less than 0 with positive probability. Thus, the integrand is strictly monotonic with positive probability, and hence Expression (8) itself is strictly monotonic, proving strict monotonicity of the contraction rate, as required.
Mark Rowland*, Will Dabney*, Rémi Munos Proposition 3.3. Consider a target policy π, let µ be the behavioural policy, and assume that there is a unique greedy action a∗(x) ∈ A with respect to Qπ at each state x for each x ∈ X . Then there exists a value of α ∈ (0, 1) such that the greedy policy with respect to the fixed point of α-Retrace coincides with the greedy policy with respect to Qπ, and the contraction rate for this α-Retrace is no greater than that for 1-Retrace. Further, if π and µ are (x, a)-distinguishable under µ for all (x, a) ∈ X × A, then the contraction rate of α-Retrace is strictly lower than that of 1-Retrace. Proof. That the greedy policies coincide follows as a consequence of the continuity of Qν with respect to the policy ν and the positivity of the minimum action gap ∆ = inf (Qπ(x, a∗(x)) − Qπ(x, a)); α may be x∈X ,a(cid:54)=a∗(x) selected so that e.g. (cid:107)Qπα − Qπ(cid:107) ≤ ∆/2. The contraction result follows from the monotonicity property derived ∞ in Proposition 3.2. Proposition 3.4. Let (x(k), a(k), r(k))∞ be an i.i.d. sequence of trajectories following µ, with initial state-action t t t t=0 distribution given by ν. Let Γ be a target contraction rate such that C (1) ≥ Γ. Let the stepsizes (ε )∞ satisfy ν k k=0 the usual Robbins-Monro conditions (cid:80)∞ ε = ∞, (cid:80)∞ ε2 < ∞. Then for any initial value φ following the k=0 k k=0 k 0 updates in (6), we have φ → φ∗ in probability, where φ∗ ∈ R is the unique value such that C (α(φ∗)) = Γ. k ν Proof. The proof follows from an application of standard stochastic approximation theory to the solution of the root-finding problem C (α(φ)) = Γ. Firstly, by Proposition 3.2, the function φ (cid:55)→ C (α(φ)) is continuous and ν ν monotonic on R. By the assumption that C (1) ≥ Γ, it follows that φ (cid:55)→ C (α(φ)) is strictly monotonic, and ν ν moreover by inspecting the proof of Proposition 3.2, has positive derivative everywhere. By the intermediate value theorem, there exists a unique value φ∗ ∈ R such that C (α(φ∗)) = Γ. ν Now note that for each φ ∈ R, the random variables Cˆ(k)(α(φ)) are i.i.d. unbiased, bounded estimators of C (α(φ)). Thus, the scheme (6) is a standard stochastic approximation scheme for the the root of a monotonic ν function, and the conditions of Theorem 2 of [Robbins and Monro, 1951] are satisfied, enabling us to conclude that φ → φ∗ in probability, as required. t Theorem 3.5. Assume the same conditions as Proposition 3.4, and additionally that: (i) trajectory lengths have finite second moment; (ii) immediate rewards are bounded. Let (φ )∞ be defined as in Equation (6) and (Q )∞ k k=0 k k=0 be a sequence of Q-functions, with Q obtained from applying Retrace updates targeting α(φ )π + (1 − α(φ ))µ k+1 k k to Q with trajectory k + 1, using stepsize ε . Then we have α(φ ) → α(φ∗) =: α∗ and Q → Qα∗π+(1−α∗)µ k k k k almost surely. Proof. Convergence in probability of α := α(φ ) to α∗ has been shown in Proposition 3.4; it is straightforward k k to upgrade this to almost-sure convergence using standard stochastic approximation theory. The intuition for the remainder of the proof is that when α is close to α∗, the C-trace updates are close to those of standard Retrace k targeting the policy α∗π + (1 − α∗)µ, which are known to converge under the conditions of the theorem. This is made rigorous by decomposing the update on the Q-function from the (k + 1)th trajectory as Desired update Martingale noise Perturbation (cid:122) (cid:125)(cid:124) (cid:123) (cid:122) (cid:125)(cid:124) (cid:123) (cid:122) (cid:125)(cid:124) (cid:123) Q k+1 = (1 − ε (cid:101)k) (cid:12) Q k + ε (cid:101)k (cid:12) Rα∗ Q k + (Q k+1 − (1 − ε (cid:101)k) (cid:12) Q k − ε (cid:101)k (cid:12) Rαk Q k) +ε (cid:101)k (cid:12) (Rαk Q k − Rα∗ Q k) , where Rα denotes the Retrace operator targeting απ+(1−α)µ, and with ε (x, a) = ε E[(cid:80) 1 |(x , a ) = (cid:101)k k t (xt,at)=(x,a) 0 0 (x, a)], and (cid:12) the Hadamard product and 1 the vector of 1’s. It is then possible to appeal to Proposition 4.5 of Bertsekas and Tsitsiklis [1996] that Q → Qα∗π+(1−α∗)µ almost surely, using the assumptions of theorem. k B Additional results B.1 Operators for time-inhomogeneous policies In this section, we provide a result which rigorously proves the connection between the n-step uncorrected target and the time-inhomogeneous policy mentioned in Section 2.
Adaptive Trade-Offs in Off-Policy Learning Proposition B.1. The n-step uncorrected update corresponding to the target n−1 (cid:88) γsr + γnE (cid:104) Qˆ(x , A)(cid:105) , s A∼π(·|xn) n s=0 with the trajectory generated under µ, is a stochastic approximation to the operator (T µ)n−1T π, with fixed point given by the Q-function for the time-inhomogeneous policy which follows π at timesteps t satisfying t ≡ n − 1 mod n, and µ otherwise. Proof. We begin by taking the expectation of the update target conditional on the initial state-action pair, and showing that it is equal to ((T µ)n−1T πQˆ)(x , a ). We proceed by induction. In the case n = 1, the expectation 0 0 of the update is given by (cid:104) (cid:104) (cid:105)(cid:12) (cid:105) E R(X , A ) + γE Qˆ(X , A) (cid:12)X = x , A = a µ 0 0 A∼π(·|X1) 1 (cid:12) 0 0 0 0 =r(x , a ) + (cid:88) P (x(cid:48)|x, a)γ (cid:88) π(a(cid:48)|x(cid:48))Qˆ(x(cid:48), a(cid:48)) 0 0 x(cid:48)∈X a(cid:48)∈A =(T πQˆ)(x , a ) , 0 0 as required. For the inductive step, we assume the result holds for some n ≥ 1. Now observe that by conditioning on (X , A ), we have 1 1 E (cid:34) (cid:88)n γsR(X , A ) + γn+1E (cid:104) Qˆ(X , A)(cid:105)(cid:12) (cid:12) (cid:12)X = x , A = a (cid:35) µ s s A∼π(·|Xn) n+1 (cid:12) 0 0 0 0 (cid:12) s=0 (cid:88) (cid:88) =r(x , a ) + γ P (x |x , a ) µ(a |x )× 0 0 1 0 0 1 1 x1∈X a1∈A E (cid:34) (cid:88)n γs−1R(X , A ) + γn+1E (cid:104) Qˆ(X , A)(cid:105)(cid:12) (cid:12) (cid:12)X = x , A = a (cid:35) s s A∼π(·|Xn) n+1 (cid:12) 1 1 1 1 (cid:12) s=1 ( =a) r(x , a ) + γ (cid:88) P (x |x , a ) (cid:88) µ(a |x )((T µ)n−1T πQˆ)(x , a ) 0 0 1 0 0 1 1 1 1 x1∈X a1∈A = (T µ(T µ)n−1T πQˆ)(x , a ) 0 0 = ((T µ)nT πQˆ)(x , a ) , 0 0 as required, with (a) following from the induction hypothesis. Finally, for the interpretation of the fixed point of (T µ)n−1T π, observe that the time-inhomogeneous policy described in the statement of the proposition, which we denote πµn−1 follows a stream of Markovian policies with period n, so it is possible to write down an n-step Bellman equation for its Q-function Qπµn−1. Doing so yields Qπµn−1 (x, a) = E (cid:34)n (cid:88)−1 γsR(X , A ) + γnQπµn−1 (X , A )(cid:12) (cid:12) (cid:12)X = x, A = a(cid:35) A1:n−1∼µ(·|X1:n−1) s s n n (cid:12) 0 0 An∼π(·|Xn) s=0 (cid:12) = E (cid:34)n (cid:88)−1 γsR(X , A ) + γnE (cid:104) Qπµn−1 (X , A )(cid:105)(cid:12) (cid:12) (cid:12)X = x, A = a(cid:35) . µ s s An∼π(·|Xn) n n (cid:12) 0 0 (cid:12) s=0 We recognise the right-hand side as the operator (T µ)n−1T π, and thus Qπµn−1 is the fixed point of this operator. B.2 Further decompositions of evaluation error In addition to the decomposition given in Proposition 2.1, there are decompositions of evaluation error based on other norms that may be of interest. We state one such decomposition below, and also note that there is also scope to use different norms to define the fundamental traded-off quantities, such as using the L∞ norm to define an alternative notion of fixed-point bias, that lead to further decompositions.
Mark Rowland*, Will Dabney*, Rémi Munos Proposition B.2. Consider the task of evaluation of a policy π under behaviour µ, and consider an update rule Tˆ which stochastically approximates the application of an operator T(cid:101), with contraction rate Γ and fixed point Q(cid:101), to an initial estimate Q. Then we have the following decomposition:   (cid:104) (cid:105) (cid:104) (cid:105) E (cid:107)TˆQ − Qπ(cid:107)2 ≤ 3 E (cid:107)TˆQ − T Q(cid:107)2 + Γ2|X ||A|(cid:107)Q − Q(cid:101)(cid:107)2 + (cid:107)Q(cid:101) − Qπ(cid:107)2  . 2  2 ∞ 2  (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (Squared) contraction (Squared) fixed-point bias Variance Proof. The inequality is obtained in a manner analogous to that of Proposition 2.1. First, a Cauchy-Schwarz-style argument yields (cid:104) (cid:105) (cid:104) (cid:104) (cid:105) (cid:105) E (cid:107)TˆQ − Qπ(cid:107)2 ≤ 3 E (cid:107)TˆQ − T(cid:101)Q(cid:107)2 + (cid:107)T(cid:101)Q − Q(cid:101)(cid:107)2 + (cid:107)Q(cid:101) − Qπ(cid:107)2 . 2 2 2 2 Then, the inequality (cid:107) · (cid:107) ≤ |X ||A|(cid:107) · (cid:107) is applied, together with the definition of T as a contraction mapping 2 ∞ under (cid:107) · (cid:107) with contraction modulus Γ, to yield the statement. ∞ C Experimental details C.1 Environments Dirichlet-Uniform random MDPs. These random MDPs are specified by two parameteters: the number of states, n , and the number of actions, n . Transition distributions P (·|x, a) are sampled i.i.d. from a s a Dirchlet(1, . . . , 1) distribution for each X × A. Each immediate reward distribution is given by a Dirac delta, with locations drawn i.i.d. from the Uniform([−1, 1]) distribution. Garnet MDPs. Garnet MDPs [Archibald et al., 1995, Piot et al., 2014, Bhatnagar et al., 2009, Geist and Scherrer, 2014] are drawn from a distribution specified by three numbers: the number of states, n , the number s of actions, n , and the branching factor, n . Each transition distribution P (·|x, a) is given by n−1 (cid:80)nb δ , a b b i=1 zi(x,a) where z (x, a) are drawn uniformly without replacement from the set of states of the MDP, independently for 1:nb each state-action pair (x, a) ∈ X × A. (cid:98)n /10(cid:99) states are selected uniformly without replacement, such that any s transition out of these states yields a reward of 1, whilst all other transitions in the MDP yield a reward of 0. Chain MDP. Our chain MDP is specified by a number of states n , identified with the set {1, . . . , n }. State n s s s is terminal. Two actions, left and right, are available at each state, which deterministically move the agent into the corresponding state (taking action left in state 1 causes the agent to remain in state 1). Every transition caused by the action right incurs a reward of −1, unless the transition is into state n , in which case a reward of s 50 is received. C.2 Additional details for plots appearing in the main paper Figure 1. We use a Dirichlet-Uniform random MDP (see Section C.1) with 5 states and 3 actions. The target π and behaviour µ policies were sampled independently, so that each distribution π(·|x) and µ(·|x) are independent draws from the Dirichlet(1, . . . , 1) distribution. We use a discount rate of 0.9, and a uniform initial state distribution. The variance variable is estimated by simulating 5000 trajectories of length 100, from an initial Q-function estimate set to 0. Figure 3. In both tasks, the environment is the chain described in Section C.1 with n = 20. In both tasks, s all learning algorithms use a learning rate of 0.1, and the discount factor is set to 0.9 throughout. In the control task, policy improvement is interleaved with 100 steps of environment experience, which are used by the relevant evaluation algorithm. All Retrace-derived methods use λ = 1. In both evaluation and control tasks, the experiments were repeated 200 times to estimate the standard error by bootstrapping, which is indicated in the plots by the shaded regions.
Adaptive Trade-Offs in Off-Policy Learning 1.0 0.8 0.6 0.4 0.2 0.0 0 50 100 Fixed-point bias etar noitcartnoc citylanA 103 102 101 100 10-1 0 50 100 Fixed-point bias ecnairav laciripmE 103 102 101 100 10-1 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE Figure 5: Trade-offs made by n-step uncorrected methods (n = 1 (light blue) through to n = 50 (dark blue), n-step importance-weighted methods (n = 1 (dark green) through to n = 4 (dark green), α-Retrace (α = 1 (dark red) through to α = 0 (light red)), and α-TreeBackup (α = 1 (dark purple) through to α = 0 (light purple)). Results are shown for the chain environment, and evaluation of a Dirichlet(1, . . . , 1) policy under behaviour generated by an independently sampled Dirichlet(1, . . . , 1) policy. D Further experimental results D.1 Further trade-off plots In this section, we give several further examples of trade-offs made by off-policy algorithms. We begin by examining the trade-offs made by TreeBackup, for which the update target (for a target policy π given a trajectory generated according to a behaviour policy µ) is stated below for completeness. s Qˆ(x , a )+(cid:88) γs (cid:89) π(a |x ) (cid:16) r + γE (cid:104) Qˆ(x , A)(cid:105) − Qˆ(x , a )(cid:17) . 0 0 u u s A∼π(·|xs+1) s+1 s s s≥0 u=1 We show that mixing in a proportion 1 − α of the behaviour policy into the target in TreeBackup (which we dub α-TreeBackup) leads to fundamentally different trade-off behaviour than in α-Retrace; see Figure 5. As can be seen in the plot, mixing in the behaviour policy leads to limited improvements in contraction rate relative to the trade-off achieved by α-Retrace, whilst incurring significant fixed-point bias. We next demonstrate the robustness of the behaviour exhibited in Figure 1 in a variety of environments, and with a variety of target/behaviour policy pairings. As in Figure 1, α-Retrace is illustrated in red, with dark red corresponding to α = 1 through to α = 0 in light red. n-step uncorrected methods are illustrated in blue, ranging from n = 1 (dark blue) through to n = 50 (light blue). n-step importance-weighted methods are illustrated in green, ranging from n = 1 (dark green) through to n = 4 (light green). Results are given for a Dirichlet-Uniform random MDP (Figure 6), a random garnet MDP (Figure 7), and the chain MDP described in Section C.1 (Figure 8). In all cases, we use a discount rate γ = 0.9, a learning rate for each algorithm of 0.1, and the variance variable is estimated from 5000 i.i.d. trajectories of length 100. All Retrace methods use λ = 1 (as presented in the main paper).
Mark Rowland*, Will Dabney*, Rémi Munos 1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 Fixed-point bias etar noitcartnoc citylanA 101 100 10-1 0 1 2 3 Fixed-point bias ecnairav laciripmE 101 100 10-1 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (a) Target policy: uniform. Behaviour policy: Dirichlet(1, . . . , 1) random. 1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 Fixed-point bias etar noitcartnoc citylanA 102 101 100 10-1 0 1 2 3 Fixed-point bias ecnairav laciripmE 102 101 100 10-1 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (b) Target policy: Dirichlet(1, . . . , 1) random. Behaviour policy: Independent Dirichlet(1, . . . , 1) random. 1.0 0.8 0.6 0.4 0.2 0.0 0 1 2 3 Fixed-point bias etar noitcartnoc citylanA 101 100 10-1 0 1 2 3 Fixed-point bias ecnairav laciripmE 101 100 10-1 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (c) Target policy: Dirichlet(1, . . . , 1) random. Behaviour policy: uniform. 1.0 0.8 0.6 0.4 0.2 0.0 0 20 40 Fixed-point bias etar noitcartnoc citylanA 102 101 100 10-1 0 20 40 Fixed-point bias ecnairav laciripmE 102 101 100 10-1 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (d) Target policy: optimal. Behaviour policy: uniform. 1.0 0.8 0.6 0.4 0.2 0.0 0 20 40 Fixed-point bias etar noitcartnoc citylanA 103 102 101 100 10-1 0 20 40 Fixed-point bias ecnairav laciripmE 103 102 101 100 10-1 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (e) Target policy: optimal. Behaviour policy: Dirichlet(1, . . . , 1) random. 1.0 0.8 0.6 0.4 0.2 0.0 0 2 4 Fixed-point bias etar noitcartnoc citylanA 101 100 10-1 0 2 4 Fixed-point bias ecnairav laciripmE 102 101 100 10-1 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (f) Target policy: optimal. Behaviour policy: optimal, with uniform exploration at probability 0.1. Figure 6: Trade-off plots for a Dirichlet-Uniform random MDP with 20 states and 3 actions, with a variety of target policy/behaviour policy pairings.
Adaptive Trade-Offs in Off-Policy Learning 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 Fixed-point bias etar noitcartnoc citylanA 100 10-1 10-2 10-3 0.00 0.02 0.04 Fixed-point bias ecnairav laciripmE 100 10-1 10-2 10-3 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (a) Target policy: uniform. Behaviour policy: Dirichlet(1, . . . , 1) random. 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.1 0.2 Fixed-point bias etar noitcartnoc citylanA 100 10-1 10-2 10-3 0.0 0.1 0.2 Fixed-point bias ecnairav laciripmE 100 10-1 10-2 10-3 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (b) Target policy: Dirichlet(1, . . . , 1) random. Behaviour policy: Independent Dirichlet(1, . . . , 1) random. 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 Fixed-point bias etar noitcartnoc citylanA 10-1 10-2 10-3 0.00 0.02 0.04 Fixed-point bias ecnairav laciripmE 10-1 10-2 10-3 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (c) Target policy: Dirichlet(1, . . . , 1) random. Behaviour policy: uniform. 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.5 1.0 Fixed-point bias etar noitcartnoc citylanA 100 10-1 10-2 10-3 0.0 0.5 1.0 Fixed-point bias ecnairav laciripmE 100 10-1 10-2 10-3 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (d) Target policy: optimal. Behaviour policy: uniform. 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.5 1.0 Fixed-point bias etar noitcartnoc citylanA 102 101 100 10-1 10-2 10-3 0.0 0.5 1.0 Fixed-point bias ecnairav laciripmE 102 101 100 10-1 10-2 10-3 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (e) Target policy: optimal. Behaviour policy: Dirichlet(1, . . . , 1) random. 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.05 0.10 Fixed-point bias etar noitcartnoc citylanA 10-1 10-2 10-3 0.0 0.1 Fixed-point bias ecnairav laciripmE 10-1 10-2 10-3 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (f) Target policy: optimal. Behaviour policy: optimal, with uniform exploration at probability 0.1. Figure 7: Trade-off plots for a garnet random MDP with 20 states and 3 actions, with a variety of target policy/behaviour policy pairings.
Mark Rowland*, Will Dabney*, Rémi Munos 1.0 0.8 0.6 0.4 0.2 0.0 0 20 40 60 Fixed-point bias etar noitcartnoc citylanA 103 102 101 0 20 40 Fixed-point bias ecnairav laciripmE 103 102 101 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (a) Target policy: uniform. Behaviour policy: Dirichlet(1, . . . , 1) random. 1.0 0.8 0.6 0.4 0.2 0.0 0 50 100 Fixed-point bias etar noitcartnoc citylanA 103 102 101 100 10-1 0 50 100 Fixed-point bias ecnairav laciripmE 103 102 101 100 10-1 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (b) Target policy: Dirichlet(1, . . . , 1) random. Behaviour policy: Independent Dirichlet(1, . . . , 1) random. 1.0 0.8 0.6 0.4 0.2 0.0 0 20 40 60 Fixed-point bias etar noitcartnoc citylanA 103 102 101 100 0 20 40 Fixed-point bias ecnairav laciripmE 103 102 101 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (c) Target policy: Dirichlet(1, . . . , 1) random. Behaviour policy: uniform. 1.0 0.8 0.6 0.4 0.2 0.0 0 50 100 Fixed-point bias etar noitcartnoc citylanA 103 102 101 100 0 50 100 Fixed-point bias ecnairav laciripmE 103 102 101 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (d) Target policy: optimal. Behaviour policy: uniform. 1.0 0.8 0.6 0.4 0.2 0.0 0 50 Fixed-point bias etar noitcartnoc citylanA 103 102 101 0 50 Fixed-point bias ecnairav laciripmE 103 102 101 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (e) Target policy: optimal. Behaviour policy: Dirichlet(1, . . . , 1) random. 1.0 0.8 0.6 0.4 0.2 0.0 0 5 10 Fixed-point bias etar noitcartnoc citylanA 102 101 0 5 10 Fixed-point bias ecnairav laciripmE 102 101 0.0 0.5 1.0 Analytic contraction rate ecnairav laciripmE (f) Target policy: optimal. Behaviour policy: optimal, with uniform exploration at probability 0.1. Figure 8: Trade-off plots for the chain MDP described in Section C.1, with 20 states, with a variety of target policy/behaviour policy pairings.
Adaptive Trade-Offs in Off-Policy Learning E Large-scale experiment details Episodes are limited to 30 minutes (108, 000 environment frames). When reporting numeric scores, as opposed to learning curves, we give final agent performance as undiscounted episodic returns. The computing architectures used to run the two agents correspond precisely to the descriptions given in Mnih et al. [2015], Kapturowski et al. [2019]. Mini-batches are drawn from an experience replay buffer as described in the baseline agent papers [Mnih et al., 2015, Kapturowski et al., 2019]. For Retrace and C-trace, the n-step loss function is modified to use the Retrace update for the, possibly modified, target policy. GPU training was performed on an NVIDIA Tesla V100. Only for R2D2 experiments, all agents (including Retrace-based algorithms) use the invertible value function rescaling of R2D2. Finally, for C-trace, the target policy is given by πˆ := (1 − α)π + αµ, where π is the greedy policy on the current action-values and µ is the (cid:15)-greedy policy followed by the actor generating the current trajectory. The value of α is adapted with each mini-batch using Robbins-Monro updates with truncated trajectory targets, as described in Section (3.2). The average observed contraction rate of Retrace over the mini-batch is calculated from the Retrace weights (see Equation (5)), N t (cid:18) (cid:18) (cid:19)(cid:19) Cˆ(α) = 1 − (1 − γ) (cid:88) γt (cid:89) (1 − α) + α min 1, π(a s|x s) . µ(a |x ) s s t=0 s=1 For simplicity we restate the Robbins-Monro update as a loss, scale up by 1000/(1 − γ) (to counter-act the small learning rate from Adam) and add it to the primary loss. We use a value of λ = 1.0 for all Retrace and C-trace large-scale experiments. We considered λ = 0.97, in keeping with published work, but found larger values to perform better overall. E.1 R2D2 experiments Network architecture. R2D2, and our Retrace variants, use the 3-layer convolutional network from DQN [Mnih et al., 2015], followed by an LSTM with 512 hidden units, which then feeds into a dueling architecture of size 512 [Wang et al., 2016]. Like the original R2D2, the LSTM receives the reward and one-hot action vector from the previous time step as inputs. Hyperparameters. The hyperparameters used for the R2D2 agents follow those of Kapturowski et al. [2019], and are reproduced in Table 1 for completeness. Number of actors 256 Actor parameter update interval 400 environment steps Sequence length m 80 (+ prefix of l = 40 for burn-in) Replay buffer size 4 × 106 observations (105 part-overlapping sequences) Priority exponent 0.9 Importance sampling exponent 0.6 Discount γ 0.997 Minibatch size 64 Optimiser Adam [Kingma and Ba, 2015] Optimiser settings learning rate = 10−4, ε = 10−3 Target network update interval 2500 updates (cid:112) Value function rescaling h(x) = sign(x)( |x| + 1 − 1) + (cid:15)x, (cid:15) = 10−3 Table 1: Hyperparameters values used in R2D2 experiments. E.2 DQN experiments Network architecture. The DQN, DoubleDQN, n-step and Retrace-based agents use the 3-layer convolutional network from DQN [Mnih et al., 2015], but unlike the R2D2 agents do not use an LSTM or dueling architecture.
Mark Rowland*, Will Dabney*, Rémi Munos Notice that the Retrace and C-trace agents are effectively using DoubleDQN-style updates due to the target probabilities not coming from the target network. Hyperparameters. For sequential DQN-agents (n-step and Retrace) we performed a preliminary hyperparameter sweep to determine appropriate learning rates for n-step and Retrace updates. We swept over learning rates (0.00025, 0.0001, 0.00005, 0.00001) for both algorithms, and for n-step we jointly swept over two values for n (3 and 5). These were run on four Atari 2600 games (Alien, Amidar, Assault, Asterix), with the best performing hyperparameters for each method used for the Atari-57 experiments. Interestingly, we found a small learning rate of 0.00001 worked best for both algorithms and that a larger n = 5 performed best for n-step. Both algorithms used a maximum sequence length of 16. Due to shortness of the sequence length we use truncated trajectory corrections as described in the main text. Note that the truncation max(Γ, γN ) is applied to each element of the sequence independently, therefore the value of N will begin at N = 16 for the first element and reduce to N = 1 for the final transition in the replay sequence.
Adaptive Trade-Offs in Off-Policy Learning F Further large-scale results F.1 Detailed R2D2 results We give further experimental results to complement the summary presented in the main paper; per-game training curves are given in Figure 9. Figure 9: Training curves for 57 Atari games for R2D2 with n-step uncorrected returns (light blue), Retrace-R2D2 (black), and C-trace-R2D2 (red).
Mark Rowland*, Will Dabney*, Rémi Munos F.2 Detailed DQN results We give further experimental results to complement the summary presented in the main paper. Results for varying the contraction hyperparameter are given in Figure 10, and per-game training curves for the main paper results are given in Figure 11. 600% 500% 400% 300% 200% 100% 0% 10 50 100 200 Millions of Training Samples erocS dezilamroN namuH Mean Human Normalized Score 150% 100% DoubleDQN Retrace 50% cTrace n-Step 0% 10 50 100 200 Millions of Training Samples erocS dezilamroN namuH Median Human Normalized Score 100% 80% 60% 40% 20% 0% 10 50 100 200 Millions of Training Samples erocS dezilamroN namuH Human Gap 600% 500% 400% 300% 200% 100% 0% 10 50 100 200 Millions of Training Samples erocS dezilamroN namuH Mean Human Normalized Score 150% 100% DoubleDQN Retrace 50% cTrace n-Step 0% 10 50 100 200 Millions of Training Samples erocS dezilamroN namuH Median Human Normalized Score 100% 80% 60% 40% 20% 0% 10 50 100 200 Millions of Training Samples erocS dezilamroN namuH Human Gap 600% 500% 400% 300% 200% 100% 0% 10 50 100 200 Millions of Training Samples erocS dezilamroN namuH Mean Human Normalized Score 150% 100% DoubleDQN Retrace 50% cTrace n-Step 0% 10 50 100 200 Millions of Training Samples erocS dezilamroN namuH Median Human Normalized Score 100% 80% 60% 40% 20% 0% 10 50 100 200 Millions of Training Samples erocS dezilamroN namuH Human Gap Figure 10: Atari-57 results for single-actor agent, as presented in the main text, but varying the C-trace contraction parameter: (top) γ5, (center) γ7, and (bottom) γ10. Notice that due to its adaptation of α, C-trace is highly robust to the choice of contraction target.
Adaptive Trade-Offs in Off-Policy Learning Figure 11: Training curves for 57 Atari games for Double DQN (grey), Double DQN with n-step uncorrected returns (light blue), Retrace-DQN (black), and C-trace-DQN (red).
