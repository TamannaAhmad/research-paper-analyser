Conditional Importance Sampling for Off-Policy Learning Mark Rowland Anna Harutyunyan Hado van Hasselt Diana Borsa Tom Schaul Rémi Munos Will Dabney DeepMind Abstract ance reduction techniques in off-policy supervised learning. These techniques include importance weight The principal contribution of this paper is a truncation [Munos et al., 2016, Espeholt et al., 2018] conceptual framework for off-policy reinforce- weighted importance sampling [Precup et al., 2000, ment learning, based on conditional expec- Mahmood et al., 2014], adaptive bootstrapping [Mah- tations of importance sampling ratios. This mood et al., 2017], variants of emphatic TD [Hallak framework yields new perspectives and un- et al., 2016], saddle-point formulations exploiting low- derstanding of existing off-policy algorithms, variance versions of SGD [Du et al., 2017, Johnson and reveals a broad space of unexplored algo- and Zhang, 2013, Defazio et al., 2014], empirical pro- rithms. We theoretically analyse this space, posal estimation [Hanna et al., 2019], doubly-robust and concretely investigate several algorithms approaches [Jiang and Li, 2016, Thomas and Brunskill, that arise from this framework. 2016], confidence bounds on returns [Thomas et al., 2015b,a, Metelli et al., 2018, Papini et al., 2019] and state distribution estimation [Xie et al., 2018, Liu et al., 1 Introduction 2018, Kallus and Uehara, 2019a,b, Uehara and Jiang, 2019, Hallak and Mannor, 2017, Gelada and Bellemare, Using off-policy data is crucial for many tasks in rein- 2019, Nachum et al., 2019]. forcement learning (RL), including for acquiring knowl- edge about diverse aspects of the environment [Sutton In this paper, we propose a new framework for vari- et al., 2011], learning from memorised data [Mnih et al., ance reduction in off-policy learning, conditional im- 2015, Schaul et al., 2016], exploration [Watkins and portance sampling (CIS), based on taking conditional Dayan, 1992], and learning to perform auxiliary tasks expectations of importance weights. This framework is [Schaul et al., 2015, Jaderberg et al., 2017, Bellemare motivated by the observation that when estimating a et al., 2019]. One of the fundamental techniques for return off-policy using standard importance sampling, correcting for the difference between the policy that gen- every action along a trajectory contributes to the im- erated the data and the policy that an algorithm aims portance weight, even if the action had no effect on the to learn about is importance sampling (IS) [Metropolis return observed. Intuitively, it would be preferable for and Ulam, 1949, Kahn and Harris, 1949], which was the importance weight to depend only on the return first introduced in off-policy RL by Precup et al. [2000]. itself; if two policies generate similar distributions of Importance sampling features as a core ingredient of returns, there should be no need to perform importance many off-policy algorithms [Maei, 2011, van Hasselt weighting at all. As just one application of the CIS et al., 2014, Munos et al., 2016, Jiang and Li, 2016, framework, we make this insight precise, and introduce Sutton et al., 2016], and is supported by strong theo- return-conditioned importance sampling (RCIS), a new retical understanding coming from the computational off-policy evaluation algorithm. Concretely, using nota- statistics literature [Robert and Casella, 2013, Särkkä, tion introduced formally in Section 2, given a random 2013]. return G, RCIS uses conditional importance weights of the form Importance sampling often suffers from high variance, Tes hp ie sc hia al sly mw oh te ivn atm eu dlt ti h-s ete stp ut dr yaj oe fct aor wie ids ear re anc go ens oi fde vr ae rd i-. E (cid:34)n (cid:89)−1 π(A t|X t) (cid:12) (cid:12) (cid:12) G(cid:35) , µ(A |X ) (cid:12) t t (cid:12) t=1 which integrates out noise in the trajectory that is Proceedings of the 23rdInternational Conference on Artificial Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. irrelevant in determining the return, leading to a lower- PMLR: Volume 108. Copyright 2020 by the author(s). variance importance weight. 0202 luJ 03 ]GL.sc[ 2v97470.0191:viXra
Conditional Importance Sampling for Off-Policy Learning However, return is just one possible variable to con- 2.1 Policy evaluation dition on. The central insight of the CIS framework is that there exists a large space of variables that the The evaluation problem with target policy π : X → importance weights can be conditioned on, with each P(A) is defined as estimation of the Q-function choice leading to a different off-policy algorithm. In the remainder of the paper, we give a mathematical (cid:34) (cid:88)∞ (cid:35) Qπ(x, a) := E γtR , (1) description of the general CIS framework, which then ηπ|(x,a) t allows us to make several further contributions: t=0 (i) We compare and analyse the statistical properties for all (x, a) ∈ X × A. The fundamental result of of CIS algorithms based on properties of the condi- value-based RL is that the Q-function in Expression (1) tioning variables. satisfies the Bellman equation T πQπ = Qπ [Bellman, (ii) We study several specific instantiations of algo- 1957], where the one-step Bellman evaluation operator rithms from this framework, including RCIS and T π : RX ×A → RX ×A is defined by state-conditioned importance sampling (SCIS, given by conditioning on the states visited by a trajectory (T πQ)(x, a) = E [R +γQ(X , A )] , ηπ|(x,a) 0 1 1 at each timestep). (iii) We develop practical versions of these algorithms, for all Q ∈ RX ×A and (x, a) ∈ X × A. As T π is a based on learning the conditional importance contraction in (RX ×A, (cid:107) · (cid:107) ), Qπ is its unique fixed ∞ weights in a supervised manner. point, and repeated application of T π to any initial We note that concurrently with this work, Liu et al. Q-function will converge to Qπ. An evaluation algo- [2020] also consider conditional importance sampling rithm may therefore seek to (approximately) perform in off-policy learning, establishing connections with the a recursion of the form Q k+1 ← T πQ k (k ≥ 1), with conditional Monte Carlo literature and undertaking the aim of converging to Qπ. More general classes of statistical analysis of these estimators. contractive operators with fixed point Qπ can also be considered, such as the Retrace operator [Munos et al., 2016], and the n-step Bellman operator, given by 2 Background (cid:34)n−1 (cid:35) (cid:88) Consider a Markov decision process (MDP) ((T π)nQ)(x, a) = E ηπ|(x,a) γtR t +γnQ(X n, A n) . (X , A, γ, P, R) with finite state space X , finite t=0 action space A, discount factor γ ∈ [0, 1), transition kernel P : X × A → P(X ), reward distribution 2.2 Off-policy policy evaluation probability mass function R : R × X × A → R (so that R(r, x, a) encodes the probability of observing reward Exact computation of the expectations defining the r after taking action a in state x), and initial state above operators is often intractable, and so Monte distribution ν ∈ P(X )1. Carlo2 estimators based on trajectories sampled from the environment are used [Bertsekas and Tsitsiklis, Given a Markov policy π : X → P(A), the 1996, Szepesvári, 2010, Sutton and Barto, 2018]. Fur- distribution of the process (X , A , R ) itself is t t t t≥0 ther, it is often desirable, or necessary, to use trajecto- defined by X ∼ ν, A |X , A , R ∼ 0 t 0:t 0:t−1 0:t−1 ries sampled from a different distribution ηµ, based on π(·|X t), R t|X 0:t, A 0:t, R 0:t−1 ∼ R(·|X t, A t), and a behaviour policy µ : X → P(A); in such cases, the X |X , A , R ∼ P (·|X , A ) for each t ≥ 0. We t+1 0:t 0:t 0:t t t problem is said to be off-policy. denote the full trajectory (X , A , R ) by τ , and t t t t≥0 use the notation τ to denote the partial trajectory A common estimator for the application of the n-step s:t (X , A , R , X , . . . , X ). We denote the distribution Bellman operator (T π)n to a Q-function Q at a specific s s s s+1 t of τ under the policy π by ηπ, and denote the distri- state-action pair (x, a) ∈ X × A is given by sampling bution of τ by ηπ for any 0 ≤ s ≤ t. We will also τ from ηµ | , and computing a bootstrapped re- s:t s:t 0:n 0:n (x,a) denote conditional versions of these distributions given turn, defined by (X , A ) = (x, a) in the manner ηπ| . 0 0 (x,a) n−1 1With some care, it is possible to show through the use G¯π 0:n := (cid:88) γtR t + γnV (X n; π) , (2) of measure theory that versions of many results in this t=0 paper hold in much greater generality, such as in classes of MDPs with continuous state and/or action spaces. For 2Throughout, we use the term “Monte Carlo” in its sta- the sake of accessibility and clarity of exposition, the main tistical sense, to mean sampled-based approximation of any paper focuses on the discrete case, but we discuss how these expectation, including those defining temporal difference results generalise in Appendix C.2 for the interested reader. algorithms.
Rowland, Harutyunyan, van Hasselt, Borsa, Schaul, Munos, Dabney where V (x; π) = E [Q(x, A)], and an impor- as the following calculation shows: A∼π(·|x) tance-weighting correction term, defined by ηπ | (τ ) 0:n (x,a) 0:n (6) ρπ,µ := (cid:89)t π(A i|X i) . (3) η 0µ :n| (x,a)(τ 0:n) s:t µ(A |X ) P (X |x , a )R(R |x , a ) i=s i i = 1 0 0 0 0 0 × P (X |x , a )R(R |x , a ) 1 0 0 0 0 0 for 1 ≤ s ≤ t, and finally forming the ordinary impor- (cid:81)n−1 π(A |X )R(R |X , A )P (X |X , A ) tance sampling (OIS) estimator t=1 t t t t t t+1 t t (cid:81)n−1 µ(A |X )R(R |X , A )P (X |X , A ) t=1 t t t t t t+1 t t G¯ 0O :I nS;π,µ := ρπ 1:, nµ −1G¯π 0:n . (4) = n (cid:89)−1 π(A t|X t) (7) µ(A |X ) t t t=1 Much research in off-policy learning is concerned with =ρπ,µ . constructing such estimators that have desirable statis- 1:n−1 tical properties, such as low variance and consistency. Noting also that the term G¯π in Equation (5) is simply Throughout, we will assume the support condition: 0:n a function of the random truncated trajectory τ , 0:n supp(π(·|x)) ⊆ supp(µ(·|x)) for all x ∈ X , (SC) we may now appeal to standard importance sampling theory, using the notation Ψ(τ ) = G¯π , to obtain 0:n 0:n a mild assumption that is sufficient for unbiased im- (cid:20) ηπ | (τ ) (cid:21) portance sampling, which is satisfied by exploratory E (cid:2) ρπ,µ G¯π (cid:3) =E 0:n (x,a) 0:n Ψ(τ ) behaviours such as ε-greedy. This is equivalent to ab- ηµ|(x,a) 1:n−1 0:n ηµ|(x,a) η 0µ :n| (x,a)(τ 0:n) 0:n solute continuity of π with respect to µ at each state; =E [Ψ(τ )] ηπ|(x,a) 0:n intuitively, this ensures that any trajectory that can =E (cid:2) G¯π (cid:3) , arise by following π is also realisable under µ. ηπ|(x,a) 0:n as required. 3 Preliminary analysis We highlight two points. Firstly, note that the argu- ment above did not depend on any special structure of As a warm-up and motivation for the conceptual frame- G¯π , other than that it was expressible as a function of work we present in the next section, we analyse some 0:n the truncated trajectory τ ; this analysis is therefore commonly-used off-policy Monte Carlo estimators. 0:n readily applicable to many other functions of the trajec- tory beyond n-step returns, as we will see in Section 4. 3.1 Ordinary importance sampling Secondly, note that within the proof we showed that the familiar product of ratios of action probabilities (7) We begin with a formal proof of the unbiasedness of the is precisely equal to the ratio of trajectory probabilities OIS estimator, a well-known result in the literature. In (6), a fact we will use in the remainder of the paper. this and many results that follow, we will be interested in distributions over trajectories conditioned on some 3.2 Per-decision importance sampling initial state-action pair (x, a) ∈ X × A; this will be present in the notation, but we avoid continuously Whilst the OIS target of Expression (4) is straightfor- mentioning it in the text for brevity. We examine wardly understood, it often has very high variance. A the proof of this result in some detail, since it will be popular variant that aims to address this shortcom- informative for the original results that follow. Proofs ing is given by the per-decision importance sampling of other results in the paper are given in Appendix A. (PDIS) [Precup et al., 2000] target: Proposition 3.1. Assume the support condition (SC) holds. For a trajectory drawn from ηµ| (x,a), the OIS G¯PDIS;π,µ = n (cid:88)−1 ρπ,µγtR + ρπ,µ γnV (X ; π) , (8) estimator in Expression (4) is unbiased for the output 0:n 1:t t 1:n−1 n of the n-step return operator (T π)n. That is, t=0 The intuition behind this estimator is that each indi- E (cid:2) ρπ,µ G¯π (cid:3) = E (cid:2) G¯π (cid:3) . (5) ηµ|(x,a) 1:n−1 0:n ηπ|(x,a) 0:n vidual reward is only weighted by importance ratios for actions that preceded the reward, it being unneces- Proof. We first observe that the ratio of policy proba- sary to account for the off-policyness of future actions. bilities that appears within the factor ρπ,µ can also This estimator is also unbiased, and is described in 1:n−1 be interpreted as the importance ratio for the condi- the literature as often having lower variance than the tional trajectory distributions ηµ | and ηπ | , OIS estimator. We show below that each constituent 0:n (x,a) 0:n (x,a)
Conditional Importance Sampling for Off-Policy Learning term of the PDIS estimator is lower variance than the 4 Conditional importance sampling: counterpart term in the OIS estimator. Theory Proposition 3.2. Assuming the support condition (SC), each term in the PDIS estimator has variance The proof of Proposition 3.2 highlights an important at most that of the corresponding term in the OIS observation; the PDIS estimator in Expression (8) can estimator. That is, for all 0 ≤ t ≤ n − 1, be interpreted as taking particular conditional expecta- tions of the OIS estimator in Expression (4) as a means Var (cid:0) ρπ,µγtR (cid:1) ≤ Var (cid:0) ρπ,µ γtR (cid:1) . of reducing variance. It will turn out that this process ηµ|(x,a) 1:t t ηµ|(x,a) 1:n−1 t of taking conditional expectations is a productive way of both discovering new off-policy importance sampling The proof technique provides the main insight giving methods, and also understanding their statistical prop- rise to the conditional importance sampling framework erties. For this reason, we take some time to spell out described in the next section, so we provide a sketch be- this logic more generally. low. The fundamental idea is to show that each term in Consider the problem of estimating E [Ψ(τ )], the estimator G¯PDIS;π,µ can be viewed as a conditional η 0π :n|(x,a) 0:n 0:n for some function Ψ of the truncated trajectory τ , expectation of a corresponding term in the estimator 0:n via importance sampling. A standard importance esti- G¯OIS;π,µ; we can then use the following well known 0:n mator, taking τ ∼ ηµ | , is given by variance decomposition for any two real-valued random 0:n 0:n (x,a) variables Z 1 and Z 2 with finite second moments: η 0π :n| (x,a)(τ 0:n) Ψ(τ ) . (10) ηµ | (τ ) 0:n Var(Z ) =Var(E [Z |Z ]) + E [Var(Z |Z )] 0:n (x,a) 0:n 1 1 2 1 2 ≥Var(E [Z |Z ]) , (9) If Ψ extracts an n-step return from the trajectory, this 1 2 yields the standard OIS estimator, and if Ψ extracts a with the inequality strict whenever Z 1 is not σ(Z 2)- single reward R t, this yields an individual term from measurable, or not a function of Z , using non-measure- the OIS estimator. In Section 3, we saw that in this 2 theoretic terminology. This idea is closely related to latter case, a way of reducing the variance of the re- the notion of Rao-Blackwellisation, a variance reduc- sulting estimator is to take the conditional expectation tion technique which is ubiquitous across statistics and given the random variables (X 0:t, A 0:t, R t), essentially signal processing [Casella and Berger, 2002, Särkkä, because Ψ(τ 0:n) = R t is expressible as a function of 2013, Robert and Casella, 2013]. (X 0:t, A 0:t, R t), and the trajectory importance weight is not expressible as a function of (X , A , R ), allow- 0:t 0:t t To apply this result to prove Proposition 3.2, consider ing some extraneous sources of noise to be integrated the term ρπ,µ γtR from the OIS estimator, and the out. We now formalise this in greater generality. 1:n−1 t term ρπ,µγtR from the PDIS estimator. A direct 1:t t Definition 4.1. Given a functional Ψ of a trajectory calculation yields τ , we say that Ψ factors through another functional 0:n E ηµ|(x,a)(cid:2) ρπ 1:, nµ −1γtR t(cid:12) (cid:12)X 0:t, A 0:t, R t(cid:3) Φ MDif Pth )e wre ite hxi Ψsts =a hth ◦i Φrd , f ou rn ec qti uo in vah le( nin tld ye ,p ifen Ψd (e τn 0t :no )f ct ah ne =ρπ 1:, tµγtR t E ηµ(cid:2) ρπ t+,µ 1:n−1(cid:12) (cid:12)X 0:t, A 0:t, R t(cid:3) be written as a function of Φ(τ 0:n) for all values of τ 0:n. =ρπ,µγtR . We say that Φ is a sufficient conditioning functional 1:t t (SCF) for Ψ. The final equality follows from the general fact that This notion of sufficient conditioning functionals sug- when the support condition (SC) is satisfied, the ex- gests the following general framework for constructing pectation of an importance weight with respect to the off-policy estimators, generalising the perspective of importance sampling distribution is 1. Thus, the PDIS PDIS given in the previous section. term really is a conditional expectation of the corre- sponding term in the OIS estimator. The bootstrap Conditional importance sampling. terms in the PDIS and OIS estimators are in fact equal, Given a target functional Ψ(τ 0:n), select an SCF and hence the result of Proposition 3.2 follows. Note Φ for Ψ and construct the estimator that Liu et al. [2020] also analyse the covariance terms, (cid:20) ηπ | (τ ) (cid:12) (cid:21) showing that it is possible for high covariances to out- E ηµ|(x,a) η0 µ:n |(x,a) (τ0:n ) (cid:12) (cid:12) (cid:12)Φ(τ 0:n) Ψ(τ 0:n) . (11) weigh the benefits of smaller per-term variance. 0:n (x,a) 0:n We are now ready to generalise the reasoning presented Through different choices of Ψ and Φ, this yields a in this section, and present the main conceptual frame- wide space of possible off-policy learning algorithms; work of the paper. we refer to this as the conditional importance sampling
Rowland, Harutyunyan, van Hasselt, Borsa, Schaul, Munos, Dabney (CIS) framework. We begin with some basic analysis of the properties of these estimators. Proposition 4.2. Assume the support condition (SC) holds. Given a trajectory functional Ψ and an associ- ated SCF Φ, the estimator in Expression (11) is unbi- ased for E [Ψ(τ )]. Further, its variance is no greater ηπ 0:n than that of the OIS estimator in Expression (10). Having established our framework and some basic prop- erties of the associated estimators, we now provide several examples to aid intuition. Figure 1: Schematic illustration of three traded-off quantities associated with CIS estimators. Examples: • By taking Ψ(τ ) = G¯π , Φ(τ ) = τ we recover 0:n 0:n 0:n 0:n the usual OIS estimator. The connection established in Proposition 4.3 will al- • By taking Ψ(τ ) = R , and Φ(τ ) = 0:n t 0:n low us to address the question of optimality: which (X , A , R ), we recover the terms of the PDIS 0:t 0:t t SCFs for Ψ yield the lowest variance estimator given estimator, as described in Section 3.2. in Expression (11)? • By taking Ψ(τ ) = R , and Φ(τ ) = (X , A , R ), 0:n t 0:n t t t Proposition 4.4. An SCF for Ψ for which the asso- we recover terms closely related to the marginalised ciated estimator in Expression (11) achieves minimal importance sampling estimator of Xie et al. [2018]. variance is Ψ itself. 4.1 Orderings and optimality This result gives guidance for choosing a conditioner Φ for a given target Ψ; we study several such algorithms Given the wide space of possible SCFs Φ for a given in more detail in Section 5. target Ψ encompassed by the CIS estimators in Expres- sion (11), we now turn our attention to understanding 4.2 Beyond sufficient conditioning the statistical properties of these estimators.3 functionals There is a natural preorder (cid:45) on SCFs for a given target Ψ, that specifies that for two such conditioners Φ and So far, we have enforced the condition that if Ψ is a 1 Φ , we have Φ (cid:45) Φ if there exists a function h such target functional, a conditioner Φ used to form the con- 2 1 2 that Φ = h ◦ Φ . The relation Φ (cid:45) Φ thus makes ditional importance-weighted term in Expression (11) 1 2 1 2 rigorous the notion “all information encoded about the should be such that Ψ(τ 0:n) is expressible in terms trajectory τ 0:n by Φ 1(τ 0:n) is also encoded by Φ 2(τ 0:n)”. of Φ(τ 0:n). This condition ensures that the resulting estimator is unbiased, as shown in Proposition 4.2. A second preorder that is particularly relevant to However, relaxing this condition gives an even greater studying the statistical properties of off-policy es- collection of off-policy estimators. Such estimators timators is that of having lower variance, denoted (cid:45) V. That is, Φ 1 (cid:45) V Φ 2 if Var(E (cid:2) ρπ 1:, nµ −1(cid:12) (cid:12)Φ 1(τ 0:n)(cid:3) ) ≤ wfo ir lm l ge ed nw eri at lh lyfu bn ect bio ian sa el ds ,Φ buw th ii nch ma ar ne yno cit rcS uC mF ss tafo nr ceΨ s Var(E (cid:2) ρπ 1:, nµ −1(cid:12) (cid:12)Φ 2(τ 0:n)(cid:3) ). Note that whilst the preorder may be particularly low-variance, allowing for a bias- (cid:45) is invariant to the MDP and policies π and µ in variance trade-off to be made. question, the variance preorder (cid:45) V is not. This poten- tially complicates our variance analysis; however, the Example: following proposition establishes a useful relationship • By taking Ψ(τ ) = (cid:80)n−1 γtR + γnV (X ; π), and 0:n t=0 t n between these two preorders. Φ(τ ) = 0 (i.e., a function independent of the 0:n trajectory), we recover n-step uncorrected returns, Proposition 4.3. For any given MDP, and pair of popularly used in deep supervised learning. policies π and µ satisfying (SC), and target functional Ψ, the variance preorder refines the inclusion preorder. That is, for any two SCFs Φ , Φ of Ψ, if Φ (cid:45) Φ , 4.3 Bias, variance, and estimation difficulty 1 2 1 2 then we have Φ 1 (cid:45) V Φ 2. We now discuss the various trade-offs inherent within 3It is possible to get a slightly more streamlined analysis the choice of Φ required by the CIS framework. Propo- by working with sigma-algebras, rather than functions of sition 4.2 shows that any Φ that is an SCF for Ψ yields the random trajectory. We restrict the exposition in the an unbiased off-policy estimator. As described in Sec- main paper to the functional perspective for accessibility and simplicity, but provide a measure-theoretic perspective tion 4.2, choosing Φ which is not an SCF for Ψ generally in Appendix C.1. results in the introduction of bias, but may also offer a
Conditional Importance Sampling for Off-Policy Learning further substantial reduction in variance. In addition, There are strong connections here to distributional re- there is the question of whether for a given Φ, the inforcement learning [Morimura et al., 2010, Bellemare importance weight above is available analytically (as in et al., 2017, Dabney et al., 2018], in which approxima- the case of per-decision importance sampling, for exam- tions to return distributions are learnt directly through ple), or whether the weight itself must be estimated, as interaction with the environment. is the case for several concrete CIS algorithms, RCIS and SCIS, which we describe in Section 5. Figure 1 5.2 Reward-conditioned and schematically illustrates the trade-offs between these state-conditioned importance sampling three quantities made by several algorithms in the CIS framework. The previous section establishes return-conditioned im- portance sampling as the optimal (with respect to esti- mator variance) unbiased means of importance weight- 5 Conditional importance sampling: ing an entire return. However, this leaves open the Algorithms question as to whether improvements can be made by importance weighting the individual terms of a return Having set out the CIS framework, we now investigate separately, as in per-decision importance sampling. If several novel algorithms which naturally arise from it. we interpret each reward R in the return G¯π as a t 0:n target in its own right, Proposition 4.4 shows that the 5.1 Return-conditioned importance sampling corresponding optimal unbiased importance weight is (cid:20) ηπ | (τ ) (cid:12) (cid:21) C tao rn gs ei td : e Ψr (t τa 0k :nin )g =th (cid:80)e n t=n − 0- 1s γte tp R tt ,ru ann dca ft oe ld lowre it nu grn tha es oo pu tir - E ηµ|(x,a) η0 0µ: :n n|( (x x, ,a a) )(τ0 0: :n n) (cid:12) (cid:12) (cid:12)R t . mality result of Proposition 4.4, taking the conditioner We refer to the use of these weights as reward- Φ = Ψ to be this return too. This yields a conditional conditioned importance sampling. Another estimator importance weight of the form of interest that we mention due to its connections with E (cid:34) ρπ,µ (cid:12) (cid:12) (cid:12)n (cid:88)−1 γtR (cid:35) . e bx ai ss et din rg eino fff o- rp co emlic ey nte lv ea al ru na inti gon is a gl ig veo nrit bh ym (ss ua bn od ptim mo ad lle yl )- ηµ|(x,a) 1:n−1(cid:12) (cid:12) t=0 t conditioning on the tuple (X t, A t, R t) instead of R t itself. In this case, we obtain the importance weight It is possible to express this conditional importance (cid:20) ηπ | (τ ) (cid:12) (cid:21) weight more directly, as the following result shows. E ηµ|(x,a) η0 µ:n |(x,a) (τ0:n ) (cid:12) (cid:12) (cid:12)X t, A t, R t , (13) Proposition 5.1. Assume the support condition (SC). 0:n (x,a) 0:n For a given policy µ let pµ| be the probability mass which can be shown (see Appendix A) to be equal to (x,a) function of (cid:80)n t=− 01 γtR t under ηµ| (x,a). Then we have pπ t | (x,a)(X t) × π(A t|X t) , (14) pµ| (X ) µ(A |X ) E ηµ|(x,a)(cid:34) ρπ 1:, nµ −1(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)n (cid:88) t=− 01 γtR t(cid:35) = p pπ µ|| (( xx ,, aa )) (( (cid:80)(cid:80) n tn t == −− 00 11 γγ tt RR tt )) . w athe tr ime p eπ t t| (x s, ta a) rtre it np gr(x e as,a e t) n st ts at t th ee -ad ci ts it or nit bu pt at i io rn (o xv ,e ar )th ane dsta fot le - (12) lowing π thereafter. Thus, learning this conditional importance weight is closely related to learning the That is, the optimal conditional importance weight difference between the two transition models pµ and t for the n-step bootstrapped return is the ratio of the pπ. For this reason, we refer to the use of the impor- t probabilities of the returns themselves under the target tance weight in Expression (13) as state-conditioned and behaviour distributions. This is appealing since importance sampling (SCIS). There are close ties with it shifts the focus from (potentially irrelevant) policy the state distribution estimation methods mentioned probabilities directly to probabilities of generating a earlier, such as marginalised importance sampling [Xie certain return value. Due to this property, we term the et al., 2018, 2019], which estimates a similar quantity, corresponding estimator the return-conditioned impor- but by focusing on learning these transitions distribu- tance sampling (RCIS) estimator, given by: tions separately, rather than their ratio directly, as well as the work of Liu et al. [2018], which learns a ratio of E ηµ|(x,a)(cid:2) ρπ 1:, nµ −1(cid:12) (cid:12)G(cid:3) G + ρπ 1:, nµ −1γnV (X n; π) , related distributions via a Bellman equation. where G = (cid:80)n t=− 01 γtR t. We note that several further 5.3 Importance weight regression variations of return-conditioned importance sampling are available, such as using an importance weight con- A crucial practical question about the conditional im- ditioned on the entire bootstrapped return. portance weights appearing in Equations (12) and (14)
Rowland, Harutyunyan, van Hasselt, Borsa, Schaul, Munos, Dabney (and indeed in the general CIS estimator in Equa- Results are shown for a chain environment in Figure 2. tion (11)), is how these should be estimated when We plot MSE for both OIS and PDIS, as well as con- they are not available analytically. A general approach ditional importance sampling versions of these algo- is given by solving the following regression problem: rithms, RCIS and SCIS, with the conditional impor- tance weights provided by a pre-computed oracle. The min E (cid:34)(cid:18) f (Φ(τ )) − η 0π :n| (x,a)(τ 0:n) (cid:19)2(cid:35) . (15) use of an oracle allows us to separate the variance reduc- θ ηµ|(x,a) θ 0:n η 0µ :n| (x,a)(τ 0:n) tion effects of conditional importance sampling from the potential errors introduced by the regression approach In words, we attempt to predict the trajectory impor- described in Section 5.3. In each of the four sub-plots tance weight via the function f parameterised by θ, of Figure 2, we vary one property of the estimation θ using solely the information contained in Φ(τ ). In problem, to illustrate how performance of the methods 0:n addition, a single regressor could be used across all under study changes. In all cases, we plot results for initial state-action pairs, taking these quantities as ad- three different settings of the parameter in question, ditional input (i.e., f (x, a, Φ(τ ))), and thus allowing with solid lines corresponding to low values of this pa- θ 0:n for generalisation across actions and states. In prac- rameter, and finely-dashed lines corresponding to high tice, global minimisation of this objective will likely values of the parameter; see Table 1. “Noise” refers to not be possible, and it may be desirable to modify the the transition noise added to the chain, β controls mis- objective to take into account the magnitude of the match between the target π and behaviour µ policies, by target term Ψ(τ ) (e.g. the n-step return) to reduce replacing the target with a mixture βπ + (1 − β)µ, and 0:n the variance of the resulting approximate solution, for “extra actions” describes how many extra (redundant) example. One such modified objective takes the form copies of each action are added to the environment. (cid:34)(cid:18)(cid:18) ηπ | (τ )(cid:19) (cid:19)2(cid:35) Table 1: Parameter values for Figure 2. min E f (Φ(τ ))− 0:n (x,a) 0:n Ψ(τ ) . θ ηµ|(x,a) θ 0:n η 0µ :n| (x,a)(τ 0:n) 0:n Line type Noise n β Extra actions (16) Solid 0% 2 0.1 0 Dashed 10% 4 0.5 1 The following result grounds these objectives. Finely-dashed 50% 7 1.0 3 Proposition 5.2. A global minimum for each of the objectives in Expressions (15) and (16) is given by In all cases, the CIS methods outperform their existing (cid:20) ηπ | (τ ) (cid:12) (cid:21) counterparts, with more pronounced improvements in f θ(Φ(τ 0:n)) = E ηµ|(x,a) η0 µ:n |(x,a) (τ0:n ) (cid:12) (cid:12) (cid:12)Φ(τ 0:n) . the presence of larger n, more transition noise, greater 0:n (x,a) 0:n off-policyness, and high level of action redundancy. 6 Experiments 6.2 Policy evaluation To complement the CIS framework and the theoretical We now consider the full task of off-policy policy evalu- analysis conducted in earlier sections, we provide sev- ation using n-step returns along trajectories generated eral simple illustrative experiments that demonstrate by a behaviour policy, with importance weights pro- (i) that CIS algorithms can deliver substantial vari- vided by existing and new CIS algorithms. We report ance reduction, and (ii) that the regression approach results for the same chain environment as for the oper- of Section 5.3 can be used to obtain practical imple- ator estimation experiments in Figure 3, with varying mentations of CIS algorithms. We exhibit results on a levels of transition noise and off-policyness as described classic chain environment, with both tabular and lin- in Table 1. We give results for online variants of CIS ear function approximation methods; full experiment algorithms by solving the empirical version of Expres- specifications are given in Appendix B. sion (15) (based on the observed trajectories) exactly for each different value of the functional observed; com- 6.1 Operator estimation plete results including the oracle versions of the CIS algorithms are given in Appendix B.4. We show re- We begin with the task of off-policy estimation of the sults for tabular evaluation, as well as versions using application of the n-step Bellman operator (T π)n to a tile-coding linear function approximation [Sutton and fixed Q-function via trajectories generated by following Barto, 2018] (full details in Appendix B.3). Generally, the behaviour policy µ. This serves as a precursor for we observe that the online versions of the CIS algo- off-policy evaluation, and allows us to disentangle the rithms generally give a noticeable improvement over variance reduction achieved by conditional importance their non-conditional versions. These results serve as a sampling from compounding bootstrapping effects. proof of concept that practical, online versions of the
Conditional Importance Sampling for Off-Policy Learning (a) Noise (b) -step (c) Off-policy (d) Redundancy Figure 2: Operator estimation MSE as a function of sample number for OIS, PDIS, RCIS, and SCIS, on a chain MDP with varying (a) levels of transition noise, (b) n-step updates, (c) separation of policies, and (d) redundancy in action sets, as outlined in Table 1. Shaded regions indicate bootstrapped 95% confidence intervals. Figure 3: Policy evaluation MSE as a function of number of trajectories for OIS, RCIS, PDIS, and SCIS, with both tabular and function approximation variants. Shaded regions indicate bootstrapped 95% confidence intervals. CIS algorithms introduced in Section 5 can improve There remain many interesting investigations to be over non-conditional baselines. We expect that with carried out towards theoretically and empirically un- further research into regression methods described in derstanding how the CIS framework interacts with com- Section 5.3, the gap between oracle and online CIS plementary approaches for variance reduction, such as algorithms can be narrowed. weighted importance sampling and importance weight truncation. We expect several further directions to prove fruitful for future work, including further explo- 7 Discussion ration of the space of CIS algorithms, scaling up CIS algorithms to work in combination with deep RL ar- We have unified several existing importance sampling chitectures, and further investigation into relationships algorithms via a new conceptual framework based on between particular CIS algorithms with other sub-fields conditional expectations of importance weights, allow- of RL (such as RCIS and distributional RL). ing for straightforward analysis and comparison, in addition to the development of new algorithms.
Rowland, Harutyunyan, van Hasselt, Borsa, Schaul, Munos, Dabney Acknowledgements shift. In AAAI Conference on Artificial Intelligence, 2019. We thank Adam White for detailed feedback on an ear- A. Hallak and S. Mannor. Consistent on-line off-policy lier version of this paper, and the anonymous reviewers evaluation. In International Conference on Machine for helpful comments during the review process. Learning (ICML), 2017. References A. Hallak, A. Tamar, R. Munos, and S. Mannor. Gen- eralized emphatic temporal difference learning: Bias- M. G. Bellemare, W. Dabney, and R. Munos. A dis- variance analysis. In AAAI Conference on Artificial tributional perspective on supervised learning. Intelligence, 2016. In International Conference on Machine Learning (ICML), 2017. J. P. Hanna, S. Niekum, and P. Stone. Importance sampling policy evaluation with an estimated behav- M. G. Bellemare, W. Dabney, R. Dadashi, A. A. Taiga, ior policy. In International Conference on Machine P. S. Castro, N. L. Roux, D. Schuurmans, T. Lat- Learning (ICML), 2019. timore, and C. Lyle. A geometric perspective on optimal representations for supervised learning. M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, Neural Information Processing Systems (NeurIPS), J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Rein- 2019. forcement learning with unsupervised auxiliary tasks. In International Conference on Learning Represen- R. Bellman. Dynamic programming. Princeton Univer- tations (ICLR), 2017. sity Press, 1st edition, 1957. N. Jiang and L. Li. Doubly robust off-policy value eval- D. P. Bertsekas and S. E. Shreve. Stochastic Optimal uation for supervised learning. In International Control: The Discrete-Time Case. Athena Scientific, Conference on Machine Learning (ICML), 2016. 2007. D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic R. Johnson and T. Zhang. Accelerating stochastic programming, volume 5. Athena Scientific, 1996. gradient descent using predictive variance reduction. In Neural Information Processing Systems (NIPS), P. Billingsley. Probability and measure. Wiley, 3rd 2013. edition, 1995. H. Kahn and T. E. Harris. Estimation of particle G. Casella and R. L. Berger. Statistical inference. transmission by random sampling. In Monte Carlo Duxbury, 2002. Method, volume 12 of Applied Mathematics Series, W. Dabney, M. Rowland, M. G. Bellemare, and pages 27–30. National Bureau of Standards, 1949. R. Munos. Distributional supervised learning N. Kallus and M. Uehara. Double reinforcement learn- with quantile regression. In AAAI Conference on ing for efficient off-policy evaluation in Markov deci- Artificial Intelligence, 2018. sion processes. arXiv, 2019a. A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A N. Kallus and M. Uehara. Efficiently breaking the fast incremental gradient method with support for curse of horizon: Double supervised learning in non-strongly convex composite objectives. In Neural infinite-horizon processes. arXiv, 2019b. Information Processing Systems (NIPS), 2014. T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, S. S. Du, J. Chen, L. Li, L. Xiao, and D. Zhou. Stochas- Y. Tassa, D. Silver, and D. Wierstra. Continuous tic variance reduction methods for policy evaluation. control with deep supervised learning. In In- In International Conference on Machine Learning ternational Conference on Learning Representations (ICML), 2017. (ICLR), 2016. L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, Q. Liu, L. Li, Z. Tang, and D. Zhou. Breaking the curse I. Dunning, S. Legg, and K. Kavukcuoglu. IM- of horizon: Infinite-horizon off-policy estimation. In PALA: Scalable distributed deep-RL with impor- Neural Information Processing Systems (NeurIPS), tance weighted actor-learner architectures. In Inter- 2018. national Conference on Machine Learning (ICML), Y. Liu, P.-L. Bacon, and E. Brunskill. Understand- 2018. ing the curse of horizon in off-policy evaluation via C. Gelada and M. G. Bellemare. Off-policy deep rein- conditional importance sampling. In International forcement learning by bootstrapping the covariate Conference on Machine Learning (ICML), 2020.
Conditional Importance Sampling for Off-Policy Learning H. R. Maei. Gradient temporal-difference learning al- T. Schaul, D. Horgan, K. Gregor, and D. Silver. Uni- gorithms. PhD thesis, University of Alberta, 2011. versal value function approximators. In International Conference on Machine Learning (ICML), 2015. A. R. Mahmood, H. van Hasselt, and R. S. Sutton. Weighted importance sampling for off-policy learn- T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prior- ing with linear function approximation. In Neural tized experience replay. In International Conference Information Processing Systems (NIPS), 2014. on Learning Representations (ICLR), 2016. D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, A. R. Mahmood, H. Yu, and R. S. Sutton. Multi- and M. Riedmiller. Deterministic policy gradient step off-policy learning without importance sampling algorithms. In International Conference on Machine ratios. arXiv, 2017. Learning (ICML), 2014. A. M. Metelli, M. Papini, F. Faccio, and M. Restelli. R. S. Sutton and A. G. Barto. supervised learning: Policy optimization via importance sampling. In An Introduction. The MIT Press, 2nd edition, 2018. Neural Information Processing Systems (NeurIPS), 2018. R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup. Horde: A scal- N. Metropolis and S. Ulam. The Monte Carlo method. able real-time architecture for learning knowledge J. Am. Stat. Assoc., 44:335, 1949. from unsupervised sensorimotor interaction. In Au- tonomous Agents and Multiagent Systems (AAMAS), V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve- 2011. ness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beat- R. S. Sutton, A. R. Mahmood, and M. White. An tie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, emphatic approach to the problem of off-policy D. Wierstra, S. Legg, and D. Hassabis. Human-level temporal-difference learning. The Journal of Ma- control through deep supervised learning. Nature, chine Learning Research, 17(1):2603–2631, 2016. 518(7540):529–533, Feb. 2015. C. Szepesvári. Algorithms for supervised learning. T. Morimura, M. Sugiyama, H. Kashima, H. Hachiya, Morgan & Claypool Publishers, 2010. and T. Tanaka. Nonparametric return distribution P. Thomas and E. Brunskill. Data-efficient off-policy approximation for supervised learning. In Inter- policy evaluation for supervised learning. In Inter- national Conference on Machine Learning (ICML), national Conference on Machine Learning (ICML), 2010. 2016. R. Munos, T. Stepleton, A. Harutyunyan, and M. Belle- P. Thomas, G. Theocharous, and M. Ghavamzadeh. mare. Safe and efficient off-policy reinforcement High confidence policy improvement. In Interna- learning. In Neural Information Processing Systems tional Conference on Machine Learning (ICML), (NIPS), 2016. 2015a. O. Nachum, Y. Chow, B. Dai, and L. Li. DualDICE: P. S. Thomas, G. Theocharous, and M. Ghavamzadeh. Behavior-agnostic estimation of discounted station- High-confidence off-policy evaluation. In AAAI Con- ary distribution corrections. In Neural Information ference on Artificial Intelligence, 2015b. Processing Systems (NeurIPS), 2019. M. Uehara and N. Jiang. Minimax weight and Q- M. Papini, A. M. Metelli, L. Lupo, and M. Restelli. Op- function learning for off-policy evaluation. arXiv, timistic policy optimization via multiple importance 2019. sampling. In International Conference on Machine H. van Hasselt, A. R. Mahmood, and R. S. Sutton. Learning (ICML), 2019. Off-policy TD(λ) with a true online equivalence. In D. Precup, R. S. Sutton, and S. P. Singh. Eligibil- Uncertainty in Artificial Intelligence (UAI), 2014. ity traces for off-policy policy evaluation. In Inter- C. Watkins and P. Dayan. Q-learning. Machine learn- national Conference on Machine Learning (ICML), ing, 8(3-4):279–292, 1992. 2000. T. Xie, Y.-X. Wang, and Y. Ma. Marginalized off-policy C. Robert and G. Casella. Monte Carlo statistical evaluation for supervised learning. In NeurIPS methods. Springer, 2013. Workshop on Causal Learning, 2018. S. Särkkä. Bayesian filtering and smoothing. Cambridge T. Xie, Y. Ma, and Y.-X. Wang. Towards optimal University Press, 2013. off-policy evaluation for supervised learning with
Rowland, Harutyunyan, van Hasselt, Borsa, Schaul, Munos, Dabney marginalized importance sampling. In Neural Infor- mation Processing Systems (NeurIPS), 2019.
Conditional Importance Sampling for Off-Policy Learning APPENDICES: Conditional Importance Sampling for Off-Policy Learning A Proofs Proposition 4.2. Assume the support condition (SC) holds. Given a trajectory functional Ψ and an associated SCF Φ, the estimator in Expression (11) is unbiased for E [Ψ(τ )]. Further, its variance is no greater than ηπ 0:n that of the OIS estimator in Expression (10). Proof. The proof of unbiasedness follows the logic of Proposition 3.1’s proof and the proof for the variance upper bound follows the logic of Proposition 3.2’s proof. Beginning with unbiasedness, we make the following calculation: (cid:20) (cid:20) ηπ | (τ ) (cid:12) (cid:21) (cid:21) (cid:20) (cid:20) ηπ | (τ ) (cid:12) (cid:21)(cid:21) E η 0µ :n|(x,a) E ηµ|(x,a) η0 0µ: :n n|( (x x, ,a a) )(τ0 0: :n n) (cid:12) (cid:12) (cid:12)Φ(τ 0:n) Ψ(τ 0:n) ( =a) E η 0µ :n|(x,a) E ηµ|(x,a) η0 0µ: :n n|( (x x, ,a a) )(τ0 0: :n n) Ψ(τ 0:n)(cid:12) (cid:12) (cid:12)Φ(τ 0:n) (cid:20) ηπ | (τ ) (cid:21) ( =b) E η 0µ :n|(x,a) η 00 µ :: nn | (( xx ,, aa )) (τ 00 :: nn ) Ψ(τ 0:n) ( =c) E [Ψ(τ )] , η 0π :n|(x,a) 0:n where (a) follows since Φ is an SCF for Ψ (and hence Ψ(τ ) is fully determined by Φ(τ )), (b) follows from the 0:n 0:n tower law of conditional expectations, and (c) follows from standard importance sampling theory. For the variance result, we observe that (cid:20) ηπ | (τ ) (cid:12) (cid:21) (cid:20) ηπ | (τ ) (cid:12) (cid:21) E η 0µ :n|(x,a) η0 0µ: :n n|( (x x, ,a a) )(τ0 0: :n n) (cid:12) (cid:12) (cid:12)Φ(τ 0:n) Ψ(τ 0:n) = E η 0µ :n|(x,a) η0 0µ: :n n|( (x x, ,a a) )(τ0 0: :n n) Ψ(τ 0:n)(cid:12) (cid:12) (cid:12)Φ(τ 0:n) , which follows since Φ is an SCF for Ψ. Therefore, this estimator is a conditional expectation of the OIS estimator ηπ | (τ ) 0:n (x,a) 0:n Ψ(τ ) , ηµ | (τ ) 0:n 0:n (x,a) 0:n and therefore the conclusion follows by direct application of Equation (9) which was used to establish Proposi- tion 3.2, taking Z = η 0π :n|(x,a)(τ0:n) Ψ(τ ) and Z = Φ(τ ). 1 η 0µ :n|(x,a)(τ0:n) 0:n 2 0:n Proposition 4.3. For any given MDP, and pair of policies π and µ satisfying (SC), and target functional Ψ, the variance preorder refines the inclusion preorder. That is, for any two SCFs Φ , Φ of Ψ, if Φ (cid:45) Φ , then we have 1 2 1 2 Φ 1 (cid:45) V Φ 2. Proof. Assume we have Φ (cid:45) Φ for two sufficient conditioning functionals Φ , Φ for Ψ. Since Φ (τ ) is a 1 2 1 2 1 0:n function of Φ (τ ), we have that E[ρπ,µ |Φ (τ )] = E[E[ρπ,µ |Φ (τ )]|Φ (τ )] by the tower property for 2 0:n 1:n−1 1 0:n 1:n−1 2 0:n 1 0:n conditional expectations. The statement now follows from the conditional variance formula (9). Proposition 4.4. An SCF for Ψ for which the associated estimator in Expression (11) achieves minimal variance is Ψ itself. Proof. This follows by first observing that Ψ(τ ) is a minimal sufficient conditioning functional for Ψ with respect 0:n to the ordering induced by (cid:45); this is immediate from the definition. Next, since (cid:45) V refines (cid:45) (by Proposition 4.3), we have that Ψ(τ 0:n) is also a minimal sufficient conditioning functional with respect to (cid:45) V, and the statement follows. Proposition 5.1. Assume the support condition (SC). For a given policy µ let pµ| be the probability mass (x,a) function of (cid:80)n−1 γtR under ηµ| . Then we have t=0 t (x,a) E (cid:34) ρπ,µ (cid:12) (cid:12) (cid:12)n (cid:88)−1 γtR (cid:35) = pπ| (x,a)((cid:80)n t=− 01 γtR t) . (12) ηµ|(x,a) 1:n−1(cid:12) (cid:12) t=0 t pµ| (x,a)((cid:80)n t=− 01 γtR t)
Rowland, Harutyunyan, van Hasselt, Borsa, Schaul, Munos, Dabney Proof. As in the discussion in Section 3, we have ηπ | (τ ) ρπ,µ = 0:n (x,a) 0:n . 1:n−1 ηµ | (τ ) 0:n (x,a) 0:n We then decompose E η 0µ :n|(x,a) (cid:34) η η0 0π µ: :n n| |( (x x, ,a a) )( (τ τ0 0: :n n) ) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)n (cid:88) t=− 01 γtR t(cid:35) =E η 0µ :n|(x,a)(cid:34) p pπ µ|| (( xx ,, aa )) (( (cid:80)(cid:80) n tn t == −− 00 11 γγ tt RR tt )) ηη 00 µπ :: nn || (( xx ,, aa )) (( ττ 00 :: nn || (cid:80)(cid:80) tnn t == −− 00 11 γγ tt RR tt )) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)n (cid:88) t=− 01 γtR t(cid:35) = p pπ µ|| (( xx ,, aa )) (( (cid:80)(cid:80) n tn t == −− 00 11 γγ tt RR tt )) E η 0µ :n|(x,a)(cid:34) ηη 00 µπ :: nn || (( xx ,, aa )) (( ττ 00 :: nn || (cid:80)(cid:80) n tn t == −− 00 11 γγ tt RR tt )) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)n (cid:88) t=− 01 γtR t(cid:35) p | ((cid:80)n−1 γtR ) = π (x,a) t=0 t , p | ((cid:80)n−1 γtR ) µ (x,a) t=0 t as required. Proposition 5.2. A global minimum for each of the objectives in Expressions (15) and (16) is given by (cid:20) ηπ | (τ ) (cid:12) (cid:21) f θ(Φ(τ 0:n)) = E ηµ|(x,a) η0 µ:n |(x,a) (τ0:n ) (cid:12) (cid:12) (cid:12)Φ(τ 0:n) . 0:n (x,a) 0:n Proof. We begin by restating Expression (15), and use the tower law of conditional expectation as follows: (cid:34)(cid:18) ηπ | (τ ) (cid:19)2(cid:35) E f (Φ(τ )) − 0:n (x,a) 0:n ηµ|(x,a) θ 0:n ηµ | (τ ) 0:n (x,a) 0:n (cid:34) (cid:34)(cid:18) ηπ | (τ ) (cid:19)2(cid:12) (cid:12) (cid:35)(cid:35) =E E f (Φ(τ )) − 0:n (x,a) 0:n (cid:12)Φ(τ ) . ηµ|(x,a) ηµ|(x,a) θ 0:n ηµ | (τ ) (cid:12) 0:n 0:n (x,a) 0:n (cid:12) The inner conditional expectation is of the form E [(z − Y )2]; viewed as a function of z, it is well known that the Y minimiser of such an expression is z = E[Y ]. Thus, for a fixed value of Φ(τ ), the optimal value of f (Φ(τ )) is 0:n θ 0:n given by (cid:20) ηπ | (τ ) (cid:12) (cid:21) E ηµ η0 µ:n |(x,a) (τ0:n ) (cid:12) (cid:12) (cid:12)Φ(τ 0:n) . 0:n (x,a) 0:n Therefore, the global optimiser of Expression (15) is given precisely by the function (cid:20) ηπ | (τ ) (cid:12) (cid:21) f θ(Φ(τ 0:n)) = E ηµ η0 µ:n |(x,a) (τ0:n ) (cid:12) (cid:12) (cid:12)Φ(τ 0:n) , 0:n (x,a) 0:n as required. For Expression (16), in a similar manner we can write the following: (cid:34)(cid:18) ηπ | (τ ) (cid:19)2 (cid:35) E f (Φ(τ )) − 0:n (x,a) 0:n Ψ(τ )2 ηµ|(x,a) θ 0:n ηµ | (τ ) 0:n 0:n (x,a) 0:n (cid:34) (cid:34)(cid:18) ηπ | (τ ) (cid:19)2(cid:12) (cid:12) (cid:35) (cid:35) =E E f (Φ(τ )) − 0:n (x,a) 0:n (cid:12)Φ(τ ) Ψ(τ )2 , ηµ|(x,a) ηµ|(x,a) θ 0:n ηµ | (τ ) (cid:12) 0:n 0:n 0:n (x,a) 0:n (cid:12) with the equality following from the fact that Φ is a sufficient conditioning functional for Ψ. Now we may proceed in an identical manner to that for Expression (15), and the claim follows. We also record a precise result on the form of the SCIS weights described in Section 5 below. Proposition A.1. As described in Section 5, assuming the support condition, we have E ηµ|(x,a)(cid:20) η η0π µ:n| |(x,a)( (τ τ0:n) ) (cid:12) (cid:12) (cid:12) (cid:12)X t, A t, R t(cid:21) = p pπ t µ|| (x,a) (( XX t )) × π µ(( AA t || XX t )) . 0:n (x,a) 0:n t (x,a) t t t
Conditional Importance Sampling for Off-Policy Learning Proof. The proof follows by factorising the trajectory probabilities ηπ | (τ ), ηµ | (τ ) in the following 0:n (x,a) 0:n 0:n (x,a) 0:n manner, using the Markov property of the environment: ηπ | (τ ) = pπ| (X )π(A |X )ηπ | (τ )ηπ | (τ |X ) , 0:n (x,a) 0:n t (x,a) t t t t:n (Xt,At) t:n 0:t−1 (x,a) 0:t−1 t where we write ηπ | (τ |X ) for probability mass associated with the trajectory τ under ηπ , 0:t−1 (x,a) 0:t−1 t 0:t−1 0:t conditional on the trajectory visiting the state X at time t. Using conditional independence, we therefore have t (cid:20) ηπ | (τ ) (cid:12) (cid:21) E ηµ|(x,a) η0 µ:n |(x,a) (τ0:n ) (cid:12) (cid:12) (cid:12)X t, A t, R t 0:n (x,a) 0:n pπ| (X )π(A |X ) (cid:20) ηπ | (τ )ηπ | (τ |X ) (cid:12) (cid:21) = pt µ|(x,a) (Xt )µ(At |Xt ) E ηµ|(x,a) ηt µ:n |(Xt,At) (τt:n )η0 µ:t |(x,a) (τ0:t−1 |Xt ) (cid:12) (cid:12) (cid:12)X t, A t, R t t (x,a) t t t t:n (Xt,At) t:n 0:t (x,a) 0:t−1 t pπ| (X )π(A |X ) (cid:20) ηπ | (τ ) (cid:12) (cid:21) (cid:20) ηπ | (τ |X ) (cid:12) (cid:21) = pt µ|(x,a) (Xt )µ(At |Xt ) E ηµ|(x,a) ηt µ:n |(Xt,At) (τt:n ) (cid:12) (cid:12) (cid:12)X t, A t E ηµ|(x,a) η0 µ:t |(x,a) (τ0:t−1 |Xt ) (cid:12) (cid:12) (cid:12)X t t (x,a) t t t t:n (Xt,At) t:n 0:t (x,a) 0:t−1 t pπ| (X )π(A |X ) = t (x,a) t t t , pµ| (X )µ(A |X ) t (x,a) t t t as required. The final equality follows since both of the conditional expectations are in fact expectations of Radon-Nikodym derivatives under the measure in the “denominator” of the derivative, and hence evaluate to 1 almost surely. B Experimental details B.1 Environment Chain. We use a 6-state chain environment, with absorbing states at each end of the chain. Two actions, left and right, are available at each state of the chain. Transitions corrupted with p% noise means that with probability p, a transition to a uniformly-random adjacent state (independent of the action taken) occurs. Each non-terminal step incurs a reward of +1, whilst reaching an absorbing state incurs a one-off reward of +10, and the episode then terminates. The initial state of the environment is taken to be the third state from the left. Figure 4 provides an illustration. Termination S Termination Figure 4: Illustration of the chain environment. B.2 Other experimental details: operator estimation Throughout, the discount factor is taken to be γ = 0.99, and the Q-function used to form the target (T π)nQ has its entries sampled independently from the N (0, 0.1) distribution. The policies π and µ are drawn independently, with each π(·|x) and µ(·|x) drawn independently from a Dirichlet(1, . . . , 1) distribution. Default values of parameters are taken as n = 5, the transition noise level is set to 10%, and the learning rate is set to 0.1, and 100 repetitions of each experiments are performed to compute the bootstrapped confidence intervals. B.3 Other experimental details: policy evaluation The environment and default parameters are exactly the same as in the operator estimation experiments, with the exception that the Q-function is initialised so that all coordinates are 0, and n = 3. We estimate bootstrap confidence intervals using 500 repetitions of each experiment. In the linear function approximation experiments, we use a version of tile-coding [Sutton and Barto, 2018]; the specification parametrisation we use is as follows. For a chain of length K, we take a weight vector w = (w |k ∈ [K − 1], a ∈ A) ∈ R(K−1)×|A|. Labelling the states of k,a the chain x , . . . , x , we parametrise Q(x , a) by w , Q(x , a) by w , and Q(x , a) by 1 w + 1 w , 1 K 1 1,a K K−1,a k 2 k−1,a 2 k,a for each a ∈ A and each 1 < k < K; this is illustrated in Figure 5. The weight vector is initialised with all coordinates equal to 0 in all experiments.
Rowland, Harutyunyan, van Hasselt, Borsa, Schaul, Munos, Dabney Termination Termination Figure 5: An illustration of the tile-coding scheme used in the linear function approximation scheme; the figure shows how feature weights (for each action) are allocated states. The value prediction at each state is given by averaging the weights allocated to the state. B.4 Further experimental results In this section, we give in Figure 6 the results described in Section 6.2, including also results for oracle versions of the CIS algorithms in question. We observe that the performance of the online versions of CIS algorithms generally closely track that of their oracle counterparts. Noise, tabular Off-policy, tabular Noise, linear Off-policy, linear Figure 6: Policy evaluation MSE as a function of number of trajectories for OIS, RCIS, PDIS, and SCIS, with both tabular and function approximation variants. Shaded regions indicate bootstrapped 95% confidence intervals. C Extending the CIS framework C.1 A measure-theoretic perspective on conditional importance sampling In this section, we give a measure-theoretic treatment of the conditional importance sampling framework introduced in Section 4 of the main paper. We do not provide any fundamentally new results relative to the main paper, but we believe the measure-theoretic exposition gives a useful perspective, and may be useful for future work. We begin by returning to the trajectory importance-weighted estimator given in Expression (10) in the main paper: ηπ | (τ ) 0:n (x,a) 0:n Ψ(τ ) . ηµ | (τ ) 0:n 0:n (x,a) 0:n This expression weights the target quantity Ψ(τ ) by the importance weight associated with the proposal 0:n distribution ηµ and the target distribution ηπ . A conditional importance sampling estimator is formed by 0:n 0:n taking a function Φ that in the language of the main paper, is a sufficient conditioning functional for Ψ, and
Conditional Importance Sampling for Off-Policy Learning forming the new estimator (cid:20) ηπ | (τ ) (cid:12) (cid:21) E η0 µ:n |(x,a) (τ0:n ) (cid:12) (cid:12) (cid:12)Φ(τ 0:n) Ψ(τ 0:n) . 0:n (x,a) 0:n Proposition 4.2 then shows that the variance of the conditioned estimator is no greater than that of the trajectory-weighted estimator, and, roughly speaking, in many cases it is strictly lower. Whilst this perspective of conditioning on functionals Φ of the trajectory is conceptually straightforward and clearly hints at how such techniques can be implemented in practice, as described in Section 5.3, there are some subtleties introduced by this perspective that make the analysis of the method less straightforward. One such case is illustrated by the following example: consider two sufficient conditioning functionals Φ and Φ for a target Ψ, 1 2 which happen to be related according to the identity Φ (τ ) = 2Φ (τ ) for all τ . Intuitively, Φ and Φ 1 0:n 2 0:n 0:n 1 2 encode the same information about τ , and thus the estimators they produce are identical. We might therefore 0:n like to be able to treat Φ and Φ as “identical” in our analysis, and yet this is made difficult by the focus of 1 2 the analysis on functionals of the trajectory. This is related to the need to work with preorders in Section 4.1, rather than the perhaps more familiar notion of partial orders. One route around this difficulty is to define an equivalence relation over functions of the trajectory, rigorously encoding the notion of “captures the same information about τ ”, and then to work instead with equivalence classes of trajectory functionals under this 0:n relation. However, this has the potential to be very unwieldy, and further, it turns out this is essentially equivalent to a much more familiar collection of objects from measure theory, known as sigma-algebras. For formal definitions and background on sigma-algebras, see for example Billingsley [1995]. We note that technically speaking, it is necessary to constrain functionals of the trajectory to be measurable; we do not mention this condition further in this section, but return to it in Appendix C.2 when describing the application of the conditional importance sampling framework to more general classes of MDPs. For a general random variable Z, we write F for the Z sigma-algebra generated by Z; in the discussion that follows, all random variables will be defined over the same probability space, which we therefore suppress from the notation in what follows. The counterpart to a sufficient conditioning functional Φ is a sufficient conditioning sigma-algebra (SCSA) F , which is defined as being a sigma-algebra over the same measurable space as F , with the property that τ0:n F ⊆ F . With this definition, a functional Φ is an SCF if and only if F is an SCSA. The corresponding Ψ(τ0:n) Φ(τ0:n) importance sampling estimator is then given by (cid:20) ηπ | (τ ) (cid:12) (cid:21) E ηµ|(x,a) η0 µ:n |(x,a) (τ0:n ) (cid:12) (cid:12) (cid:12)F Ψ(τ 0:n) . 0:n (x,a) 0:n The analogue of the preorder (cid:45) over conditioning functionals is the inclusion partial order ⊆ over sigma-algebras; we have Φ (cid:45) Φ if and only if F ⊆ F . Further, if for two conditioning functionals Φ and Φ 1 2 Φ1(τ0:n) Φ2(τ0:n) 1 2 we have Φ (cid:45) Φ and Φ (cid:45) Φ (that is, roughly speaking, Φ and Φ encode the same information about the 1 2 2 1 1 2 trajectory), then we have F = F . Thus, working with sigma-algebras eliminates the issue of several Φ1(τ0:n) Φ2(τ0:n) conditioning objects representing exactly the same information about the trajectory. C.2 Generalising the conditional importance sampling framework to other classes of MDPs We have restricted the presentation in the main paper to MDPs with finite state and action spaces and reward distributions with finite support for ease of exposition, and to avoid having to introduce measure-theoretic terminology such as Radon-Nikodym derivatives to deal with more general classes of MDPs. Nevertheless, the framework described in the main paper applies much more generally, such as for certain classes of MDPs with continuous state and/or action spaces. In this section, we briefly describe how the framework generalises to these settings. The aim is not to be exhaustive, but rather to indicate how key concepts change when moving away from the assumptions of the main paper; for a rigorous treatment of the measure-theoretic issues that arise in MDPs with more general state and action spaces, see Bertsekas and Shreve [2007]. Consider now an MDP with a general state space X and action space A, each equipped with a sigma-algebra, and consider R, the domain of rewards in the MDP, to be equipped with its usual Borel sigma-algebra. Given measurable transition kernel P : X × A → P(X ), reward kernel R : X × A → P(R), initial state distribution ν ∈ P(X ), and two Markov policies π, µ : X → P(A), we can straightforwardly define trajectory measures ηµ , ηπ , and conditional trajectory measures ηµ | , ηπ | over the relevant product space. The key 0:n 0:n 0:n (x,a) 0:n (x,a)
Rowland, Harutyunyan, van Hasselt, Borsa, Schaul, Munos, Dabney requirement in order to be able to carry out importance sampling in this more general case is that ηπ | is 0:n (x,a) absolutely continuous with respect to ηµ | . When this is the case, the Radon-Nikodym derivative 0:n (x,a) dηπ | 0:n (x,a) (τ ) dηµ | 0:n 0:n (x,a) exists, and has the property that for a measurable functional Ψ of the trajectory, under mild integrability conditions, we have (cid:20) dηπ | (cid:21) E η 0µ :n|(x,a) dη0 0µ: :n n|( (x x, ,a a) ) (τ 0:n)Ψ(τ 0:n) = E η 0π :n|(x,a)[Ψ(τ 0:n)] , the fundamental property we require an importance weight to satisfy. The CIS framework of the main paper can thus be extended to these more general settings by computing conditional expectations of the Radon-Nikodym derivative of the two trajectory measures. We conclude by noting that in several practical applications of interest, X and A are themselves subsets of Euclidean spaces, with π(·|x) and µ(·|x) taken to have densities with respect to Lebesgue measure for each x ∈ X ; in such circumstances, under mild assumptions, the Radon-Nikodym derivative can be expressed in the familiar form of a product of action density ratios; that is dη 0π :n| (x,a) (τ ) = n (cid:89)−1 π(A t|X t) . dηµ | 0:n µ(A |X ) 0:n (x,a) t=1 t t However, in cases where the action distribution π(·|x) is not absolutely continuous with respect to µ(·|x), such as in deterministic policy gradient algorithms [Silver et al., 2014, Lillicrap et al., 2016], the measure ηπ is not 0:n absolutely continuous with respect to ηµ , meaning that the Radon-Nikodym derivative does not exist, and so 0:n importance sampling, and in particular the CIS framework, cannot straightforwardly be applied.
