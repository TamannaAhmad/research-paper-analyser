HIGhER : Improving instruction following with Hindsight Generation for Experience Replay Geoffrey Cideron* Mathieu Seurin* Florian Strub Olivier Pietquin Université de Lille Université de Lille DeepMind Google Research CRIStAL, CNRS, Inria CRIStAL, CNRS, Inria Paris Brain Team, Paris France France France France Abstract—Language creates a compact representation of the setting, the agent is given a text description of its goal (e.g. world and allows the description of unlimited situations and "pick the red ball") and is rewarded when achieving it. The objectives through compositionality. While these characteriza- agent has thus to visually ground the language, i.e., linking and tions may foster instructing, conditioning or structuring inter- disentangling visual attributes (shape, color) from language active agent behavior, it remains an open-problem to correctly relate language understanding and supervised learning in description ("ball", "red") by using rewards to condition its even simple instruction following scenarios. This joint learning policy toward task completion. On one side, the language problem is alleviated through expert demonstrations, auxiliary compositionality allows for a high number of goals, and losses, or neural inductive biases. In this paper, we propose an offers generalization opportunities; but on the other side, it orthogonal approach called Hindsight Generation for Experience dramatically complexifies the policy search space. Besides, Replay (HIGhER) that extends the Hindsight Experience Replay approach to the language-conditioned policy setting. Whenever instruction following is a notoriously hard RL problem as the the agent does not fulfill its instruction, HIGhER learns to output training signal is very sparse since the agent is only rewarded a new directive that matches the agent trajectory, and it relabels over task completion. In practice, the navigation and language the episode with a positive reward. To do so, HIGhER learns grounding problems are often circumvented by warm-starting to map a state into an instruction by using past successful the policy with labeled trajectories [42, 1]. Although scalable, trajectories, which removes the need to have external expert interventions to relabel episodes as in vanilla HER. We show these approaches require numerous human demonstrations, the efficiency of our approach in the BabyAI environment, and whereas we here want to jointly learn the navigation policy demonstrate how it complements other instruction following and language understanding from scratch. In a seminal work, methods. [18] successfully ground language instructions, but the authors Index Terms—supervised learning, Representation Learn- used unsupervised losses and heavy curriculum to handle the ing, computer vision sparse reward challenge. In this paper, we take advantage of language composition- I. INTRODUCTION ality to tackle the lack of reward signals. To do so, we extend Language has slowly evolved to communicate intents, to Hindsight Experience Replay (HER) to language goals [3]. state objectives, or to describe complex situations [26]. It HER originally deals with the sparse reward problems in conveys information compactly by relying on composition spatial scenario; it relabels unsuccessful trajectories into suc- and highlighting salient facts. As language can express a vast cessful ones by redefining the policy goal a posteriori. As a diversity of goals and situations, it may help conditioning result, HER creates additional episodes with positive rewards the training of interactive agents over heterogeneous and and a more diverse set of goals. Unfortunately, this approach composite tasks [28] and help transfer [30]. Unfortunately, cannot be directly applied when dealing with linguistic goals. conditioning a policy on language also entails a supplementary As HER requires a mapping between the agent trajectory difficulty as the agent needs to understand linguistic cues and the goal to substitute, it requires expert supervision to to alter its behavior. The agent thus needs to ground its describe failed episodes with words. Hence, this mapping language understanding by relating the words to its obser- should either be handcrafted with synthetic bots [9], or be vations, actions, and rewards before being able to leverage learned from human demonstrations, which would both limit the language structure [25, 18]. Once the linguistic symbols HER generality. More generally, language adds a level of are grounded, the agent may then take advantage of language semantics, which allows generating textual objective that could compositionality to condition its policy on new goals. not be encoded by simple spatial observations as in regular In this work, we use instruction following as a natural HER, e.g., "fetch a ball that is not blue" or "pick any red testbed to examine this question [39, 11, 4, 28, 19]. In this object". In this work, we introduce Hindsight Generation for Experi- * Those authors contributed equally ence Replay (HIGhER), a training procedure where the agent jointly learns the language-goal mapping and the navigation 978-1-7281-2547-3/20/$31.00 ©2020 IEEE policy by solely interacting with the environment illustrated 0202 ceD 01 ]GL.sc[ 3v15490.0191:viXra
Goal: Pick the red ball Pick the purple ball. agent's trajectory (st, at, rt, st+1, g) g rT: :P 1ick the purple ball. ositive ectory (sT, g) InsS trta ut ce ti- on Update P aj Dataset Agent Replay F Tr Buffer I Train Mapper Wrong object Pick the red ball. This object is : PURPLE BALL Text Negative ajectory I Gn est nru ec rati to on r g' trR ae jela cb toe rl y g rT': : P 1ick the purple ball. F Tr agent's trajectory (st, at, rt, st+1, g) g: Pick the red ball. I rT: 0 Fig. 1. Upon positive trajectory, the agent trajectory is added to the RL replay buffer and the goal mapper dataset. Upon failed trajectory, the goal mapper is used to relabel the episode, and both trajectories are appended to the replay buffer. In the original HER paper, the mapping function is bypassed since they are dealing with spatial goals, and therefore, vanilla HER cannot be applied without external expert. in Figure 1. HIGhER leverages positive trajectories to learn sparse reward problem by taking advantage of failed trajecto- a mapping function, and HIGhER then tackles the sparse ries, relabelling them with new goals. An expert then assigns reward problem by relabeling language goals upon negative the goal that was achieved by the agent when performing its trajectories in a HER fashion. We evaluate our method on trajectory, before updating the agent memory replay buffer the BabyAI world [12], showing a clear improvement over with an additional positive trajectory. RL baselines while highlighting the robustness of HIGhER to Formally, HER assumes the existence of a predicate f : noise. S × G → {0, 1} which encodes whether the agent in a state s satisfies the goal f (s, g) = 1, and defines the reward function II. BACKGROUND AND NOTATION r(s , a, g) = f (s , g). At the beginning of an episode, a t t+1 In supervised learning, an agent interacts with the en- goal g is drawn from the space G of goals. At each time step vironment to maximize its cumulative reward [38]. At each t, the transition (s t, a t, r t, s t+1, g) is stored in the DQN replay time step t, the agent is in a state s ∈ S, where it selects buffer, and at the end of an unsuccessful episode, an expert t an action a ∈ A according its policy π : S → A. It provides an additional goal g(cid:48) that matches the trajectory. New t then receives a reward r t from the environment’s reward transitions (s t, a t, r t(cid:48), s t+1, g(cid:48)) are thus added to the replay function r : S × A → R and moves to the next state buffer for each time step t, where r(cid:48) = r(s t, a t, s t+1, g(cid:48)). DQN s with probability p(s |s , a ). The quality of the pol- update rule remains identical to [29], transitions are sampled t+1 t+1 t t icy is assessed by the Q-function defined by Qπ(s, a) = from the replay buffer, and the network is updated using one E [(cid:80) γtr(s , a )|s = s, a = a] for all (s, a) where γ ∈ step td-error minimization. π t t t 0 0 [0, 1] is the discount factor. We define the optimal Q-value as HER assumes that a mapping m between states s and Q∗(s, a) = max Qπ(s, a), from which the optimal policy π∗ goals g is given. In the original paper, this requirement is π is derived. We here use Deep Q-learning (DQN) to evaluate not restrictive as the goal space is a subset of the state the optimal Q-function with neural networks and perform off- space. Thus, the mapping m is straightforward since any policy updates by sampling transitions (s , a , r , s ) from state along the trajectory can be used as a substitution goal. t t t t+1 a replay buffer [29]. In the general case, the goal space differs from the state In this article, we augment our environment with a goal space, and the mapping function is generally unknown. In space G which defines a new reward function r : S ×A×G → the instruction following setting, there is no obvious mapping R and policy π : S × G → A by conditioning them on from visual states to linguistic instructions. It thus requires a goal descriptor g ∈ G. Similarly, the Q-function is also expert intervention to provide a new language goal given conditioned on the goal, and it is referred to as Universal the trajectory, which drastically reduces the interest of HER. Value Function Approximator (UVFA) [37]. This approach Therefore, we here explore how to learn this mapping without allows learning holistic policies that generalize over goals in any form of expert knowledge nor supervision. addition to states at the expense of complexifying the training III. HINDSIGHT GENERATION FOR EXPERIENCE REPLAY process. In this paper, we explore how language can be used for structuring the goal space, and how language composition Hindsight Generation for Experience Replay (HIGhER) eases generalization over unseen scenarios in a UVFA setting. aims to learn a mapping from past experiences that relates a a) Hindsight Experience Replay (HER): [3] is designed trajectory to a goal in order to apply HER, even when no expert to increase the sample efficiency of off-policy RL algorithms are available. The mapping function relabels unsuccessful such as DQN in the goal-conditioning setting. It reduces the trajectories by predicting a substitute goal gˆ as an expert would
do. The transitions are then appended to the replay buffer. This b) Task Complexity: It is important to underline the mapping learning is performed alongside agent policy training. complexity of this task. To get rewards over multiple episodes, Besides, we wish to discard any form of expert supervision the agent must learn to navigate and inspect objects in the to learn this mapping as it would reduce the practicability of room while simultaneously learning the meaning of each word the approach. Therefore, the core idea is to use environment and how they relate to visual characteristics. The burden signals to retrieve training mapping pairs. Instinctively, in the comes from reward sparsity as the replay buffer is filled with sparse reward setting, trajectories with positive rewards encode unsuccessful trajectories RL fails to learn. Alleviating this ground-truth mapping pairs, while trajectories with negative problem is essential and minigrid is an excellent testbed to rewards are mismatched pairs. These cues are thus collected assess algorithmic performances as an agent deals with partial to train the mapping function for HIGhER in a supervised observability, visual representation learning, and language fashion. We emphasize that such signals are inherent to the grounding only from sparse rewards signal. environment, and an external expert does not provide them. In c) Models: In this experiment, HIGhER is composed of the following, we only keep positive pairs in order to train a two separate models. The instruction generator is a neural discriminative mapping model. network outputting a sequence of words given the final state Formally, HIGhER is composed of a dataset D of (cid:104)s, g(cid:105) of a trajectory. It is trained by gradient descent using a pairs, a replay buffer R and a parametrized mapping model cross-entropy loss on the dataset D collected as described in m . For each episode, a goal g is picked, and the agent w section III. We train a DQN network following [29] with a generates transitions (s , a , r , s , g) that are appended to t t t t+1 dueling head [41], double Q-learning [17], and a prioritized the replay buffer R. The Q-function parameters are updated replay buffer [36] over trajectories. The network receives a with an off-policy algorithm by sampling minibatches from tuple < s, g > as input and output an action corresponding D. Upon episode termination, if the goal is achieved, i.e. to the argmax over states-actions values Q(s, a, g). We use f (s , g) = 1, the (cid:104)s , g(cid:105) pair is appended to the dataset D. If T T (cid:15)-greedy exploration with decaying (cid:15). the goal is not achieved, a substitute goal is sampled from the mapping model1 m (s ) = gˆ(cid:48) and the additional transitions w T {(s t, a t, r t, s t+1, gˆ(cid:48))}T t=0 are added to the replay buffer. At reg- Algorithm 1: Hindsight Generation for Experience ular intervals, the mapping model m w is optimized to predict Replay (HIGhER) the goal g given the trajectory τ by sampling mini-batches Given: from D. Noticeably, HIGhER can be extended to partially • an off-policy RL algorithm (e.g. DQN) A observable environments by replacing the predicate function • a reward function r : S × A × G → R. f (s, g) by f (τ, g), i.e., the completion of a goal depends on • a language score (e.g. parser F1-score, BLEU etc.) the full trajectory rather than one state. Although we assess 1 Initialize A , replay buffer R, dataset D train and D val of (cid:104)instruction, state(cid:105), Instruction Generator m ; HIGhER in the instruction following setting, the proposed w 2 for episode=1,M do procedure can be extended to any other goal modalities. 3 Sample a goal g and an initial state s 0; IV. EXPERIMENTS 4 t = 0; 5 repeat A. Experimental Setting 6 Execute an action a t chosen from the behavioral a) Environment: We experiment our approach on a visual policy A: a t ← π(s t||g); domain called Minigrid [12]. This environment offers a variety 7 Observe a reward r t = r(s t, a t, g) and a new state s ; of instruction-following tasks using a synthetic language for t+1 8 Store the transition (s t, a t, r t, s t+1, g) in R; grounded language learning. We use a 10x10 grid with 10 9 Update Q-network parameters using the policy A objects randomly located in the room. Each object has 4 and sampled minibatches from R; attributes (shade, size, color, and type) inducing a total of 10 t = t + 1; 300 different objects (240 objects are used for training, 60 11 until episode ends; for testing). To the best of our knowledge, the number of 12 if f(s t, g) = 1 then 13 Store the pair (cid:104)s t, g(cid:105) in D train or D val; different objects and its diversity is greater than concurrent 14 Update m w parameters by sampling minibatches works ([10] used 55 train instructions and 15 test instructions from D ; train and [22] has a total of 40 different objects). The agent has four 15 end actions {forward, left, right, pick}, and it can only see the 7x7 16 else grid in front of it. For each episode, one object’s attribute is 17 if m w language validation score is high enough and D is big enough then val randomly picked as a goal, and the text generator translates it 18 Sample gˆ(cid:48) = m w(s t); in synthetic language. , e.g., "Fetch a tiny light blue ball." The 19 Replace g by gˆ(cid:48) in the transitions of the last agent is rewarded when picking one object matching the goal episode and set rˆ = r(s t, a t, gˆ(cid:48)). description, which ends the episode; otherwise, the episode 20 end 21 end stops after 40 steps or after taking an incorrect object. 22 end 1The mapping model can be utilized with an F1-score criterion over a validation set to avoid random goal sampling.
B. Building Intuition This section examines the feasibility of HIGhER by analysing two potential issues. We first show that HER is ro- bust to a noisy mapping function (or partially incorrect goals), we then estimate the F1-score and generalisation performance of the instruction generator. 1) Noisy instruction generator and HER: We investigate how a noisy mapping m affects performance compared to a perfect mapping. As the learned instruction generator is likely to be imperfect, it is crucial to assess how a noisy mapping may alter the training of the agent. To do so, we train an agent with HER and a synthetic bot to relabel unsuccessful trajectories. We then inject noise in our mapping where each attribute has a fixed probability p to be swapped, e.g. color blue may be changed to green. For example, when p = 0.2, the probability of having the whole instruction correct is 0.84 ≈ 0.4. The resulting agent performance is depicted in Figure 2 (left). The agent performs 80% as well as an agent with perfect expert feedback even when the mapping function has a 50% noise-ratio per attribute. Surprisingly, even highly noisy map- pers, with a 80% noise-ratio, still provides an improvement over vanilla DQN-agents. Hence, HER can be applied even when relabelling trajectories with partially correct goals. We also examine whether this robustness may be induced by the environment properties (e.g. attribute redundancy) rather than HER. We thus compute the number of discriminative Fig. 2. Top: Agent performance with noisy mapping function. Bottom: features required to pick the correct object. On average, an Instruction generator F1-score over 5k pairs. Figures are averaged over 5 seeds and error bars shows one standard deviation. object can be discriminated with 1.7 features in our setting - which eases the training, but any object shares at least one property with any other object 70% of the time - which tangles section V, we here use a vanilla mapping architecture to the training. Besides, the agent does not know which features assess the generality of our HIGhER, and more advanced are noisy or important. Thus, the agent still has to disentangle architectures may drastically improve sample complexity [6]. the instructions across trajectories in the replay buffer, and this C. HIGhER for instruction following process is still relatively robust to noise. 2) Learning an instruction generator: We briefly analyze In the previous section, we observe that: (1) HER is robust the sample complexity and generalization properties of the to noisy relabeled goals, (2) an instructor generator requires instruction generator. If training the mapping function is more few positive samples to learn basic language compositional- straightforward than learning the agent policy, then we can ity. We thus here combine those two properties to execute thus use it to speed up the navigation training. HIGhER, i.e. jointly learning the agent policy and language We first split the set of missions G into two disjoint sets prediction in a online fashion for instruction following. G and G . Although all object features are present a) Baselines: We want to assess if the agent bene- train test in both sets, they contain dissimilar combinations of target fits from learning an instruction generator and using it to objects. For instance, blue, dark, key, and large are individually substitute goals as done in HER. We denote this approach present in instructions of G and G but the instruction DQN+HIGhER. We compare our approach to DQN without train test to get a large dark blue key is only in G . We therefore goal substitution (called DQN) and DQN with goal substitu- test assess whether a basic compositionality is learned. In the tion from a perfect mapping provided by an external expert following, we use train/split ratio of 80/20, i.e., 240 vs 60 (called DQN+HER) available in the BabyAI environment. We goals. emphasize again that it is impossible to have an external expert We here observe than 1000 positive episodes are necessary to apply HER in the general case. Therefore, DQN is a lower to reach around 20% F1-score with our model, and 5000 pairs bound that we expect to outperform, whereas DQN+HER is are enough to reach 70% F1-score. The instruction generator the upper bound as the learned mapping can not outperform also correctly predicts unseen instructions even with fewer the expert. Note that we only start using the parametrized than 1000 samples and the F1-score gap between seen and mapping function after collecting 1000 positive trajectories, unseen instructions slowly decrease during training, showing which is around 18% validation F1-score. Finally, we compute basic compositionality acquisition. As further discussed in an additional DQN baseline denoted DQN+reward: we reward
Fig. 3. Left: learning curves for DQN, DQN+HER, DQN+HIGhER in a 10x10 gridworld with 10 objects with 4 attributes. The instruction generator is used after the vertical bar. Right: the mapping F1-score for the prediction of instructions. mw starts being trained after collecting 1000 positive trajectories. Results are averaged over 5 seeds with one standard deviation. the agent with 0.25 for each matching properties when picking to obtain a valid policy with DQN (even after 5M environ- a object given an instruction. It enforces a hand-crafted cur- ment steps the policy has 10% success rate). Noticeably, we riculum and dramatically reduces the reward sparsity, which artificially generate a dataset in subsubsection IV-B2 to train gives a different perspective on the current task difficulty. the instruction generator, whereas we follow the agent policy b) Results: In Figure 3 (left), we show the success rate of to collect the dataset, which is a more realistic setting. For the benchmarked algorithms per environment steps. We first instance, as the instructor generator is trained on a moving observe that DQN does not manage to learn a good policy, dataset, it could overfit to the first positive samples, but in and its performance remains close to that of a random policy. practice it escapes from local minima and obtains a final high On the other side, DQN+HER and DQN+reward quickly F1-score. manage to pick the correct object 40% of the time. Finally, Different factors may also explain the learning speed dis- DQN+HIGhER sees its success rates increasing as soon as crepancy: supervised learning has less variance than rein- we use the mapping function, to rapidly perform nearly as forcement learning as it has no long-term dependency. The well as DQN+HER. Figure 3 (right) shows the performance agent instructor generator can also rely on simpler neural F1-score of the mapping generator by environment steps. We architectures than the agent. Although HIGhER thus takes observe a steady improvement of the F1-score during training advantage of those training facilities to reward the agent before reaching 78% F1-score after 5M steps. In the end, ultimately. DQN+HIGhER outperforms DQN by using the exact same c) Robustness: Finally, we observe a virtuous circle that amount of information, and even matches the conceptual upper arises. As soon as the mapping is correct, the agent success bond computed by DQN+HER. Besides, HIGhER does not rate increases, initiating the synergy. The agent then provides alter the optimal policy which can occur when reshaping the additional ground-truth mapping pairs, which increases the reward [31]. As stated in paragraph IV-C0a, a gap remains mapping F1-score, which improves the quality of substitute between DQN+HER and HIGhER as the latter is building an goals, which increases the agent success rate further more. approximate model of the instruction, thus sometimes failing As a result, there is a natural synergy that occurs between to relabel correctly as pointed out in Figure 2 language grounding and navigation policy as each module iteratively provides better training samples to the other model. D. Discussion If we ignore time-out trajectories, around 90% of the trajecto- ries are negative at the beginning of the training. As soon as a) Improvements over DQN: As observed in the pre- we start using the instruction generator, 40% the transitions vious noisy-HER experiment, the policy success rate starts are relabelled by the instructor generator, and 10% of the increasing even when the mapping F1-score is 20%, and transitions belong to positive trajectories. As training goes, DQN+HIGhER becomes nearly as good as DQN+HER despite this ratio is slowly inverted, and after 5M steps, there is only having a maximum mapping F1-score of 78%. It demonstrates 15% relabelled trajectories left while 60% are actual positive that DQN+HIGhER manages to trigger the policy learning trajectories. by better leveraging environment signals compared to DQN. E. Limitations As the instruction generator focuses solely on grounding language, it quickly provides additional training signal to the Albeit generic, HIGhER also faces some inherent limita- agent, initiating the navigation learning process. tions. From a linguistic perspective, HIGhER cannot transcribe b) Generator Analysis: We observe that the number of negative instructions (Do not pick the red ball), or alternatives positive trajectories needed to learn a non-random mapping (Pick the red ball or the blue key) in its current form. m is lower than the number of positive trajectories needed However, this problem could be alleviated by batching several w
A. Vision and Language Navigation Instruction following is sometimes coined as Vision and Language Navigation tasks in computer vision [1, 40]. Most strategies are based on imitation learning, relying on expert demonstrations and knowledge from the environment. For example, [42] relate instructions to an environment graph, requiring both demonstrations and high-level navigation infor- mation. Closer to our work, [15] also learns a navigation model and an instruction generator, but the latter is used to generate additional training data for the agent. The setup is hence fully Fig. 4. The instruction generator is triggered after collecting 0, 1000 and supervised, and requires human demonstrations. These policies 2000 positive trajectories (i.e, approximately 0%, 20%, 50% F1-score). Even are sometimes finetuned to improve navigation abilities in when the instruction generator is not accurate, the policy still makes steady unknown environments. Noticeably, [40] optimizes their agent progress and the final success rate is not impacted. Delaying the generator instructor does not provide additional benefit to find the shortest path by leveraging language information. The agent learns an instruction generator, and they derive an intrinsic reward by aligning the generator predictions over the ground truth instructions. Those approaches complete long sequences of instructions in visually rich environments but they require a substantial amount of annotated data. In this paper, we intend to discard human supervision to explore learning synergies. Besides, we needed a synthetic environ- ments with experts to evaluate HIGhER. Yet, HIGhER could be studied on natural and visually rich settings by warm- starting the instruction generator, and those two papers give a hint that HIGhER could scale up to larger environment. Recent works using pretrained language model [13, 21] could Fig. 5. Transition distributions in the replay buffer between successful, also complement HIGhER both in the generator and in the unsuccessful and relabeled trajectories. We remove time-out trajectories for instruction understanding. clarity, which accounts for 54% of the transition in average (±3% over training). B. IRL for instruction following [5] learn a mapping from <instruction, state> to a reward trajectories with the same goal. Therefore, the model would function. The method’s aim is to substitute the environment’s potentially learn to factorize trajectories into a single language reward function when instructions can be satisfied by a great objective. On the policy side, HIGhER still requires a few diversity of states, making hand-designing reward function trajectories to work, and it thus relies on the navigation policy. tedious. Similarly, [16] directly learn a reward function and In other word, historical HER could be applied in the absence assess its transferability to new environments. Those methods of reward signals, while HIGhER only alleviate the sparse are complementary to ours as they seek to transfer reward reward problem by better leveraging successful trajectories. function to new environment and we are interested in reducing A natural improvement would be to couple HIGhER with sample complexity. other exploration methods, e.g, intrinsic motivation [7] or DQN with human demonstration [20]. Finally, under-trained C. Improving language compositionality goal generators might hurt the training in some environments HIGhER heavily relies on leveraging the language struc- although we did not observe it in our setting as shown in ture in the instruction mapper toward initiating the learning Figure 4. However, a simple validation F1-score allows to synergy. For instance, [6] explore the generalization abilities circumvent this risk while activating the goal mapper (More of various neural architectures. They show that the sample details in algorithm 1). We emphasize again that the instruction efficiency of feature concatenation can be considerably im- generator can be triggered anytime to kick-start the learning proved by using feature-wise modulation [32], neural module as the it is independent of the agent. networks [2] or compositional attention networks [23]. In this spirit, [5] take advantage of these architectures to quickly V. RELATED WORK learn a dense reward model from a few human demonstrations in the instruction following setup. Differently, the instructor Instruction following have recently drawn a lot of attention generator can also be fused with the agent model to act as an following the emergence of several 2D and 3D environ- auxiliary loss, reducing further the sparse reward issue. ments [12, 8, 1]. This section first provides an overview of D. HER variants the different approaches, i.e, fully-supervised agent, reward shaping, auxiliary losses, before exploring approaches related HER has been extended to multiple settings since the orig- to HIGhER. inal paper. These extensions deal with automatic curriculum
learning [27], dynamic goals [14], or they adapt goal rela- and several Universities as well as other organizations (see belling to policy gradient methods [33]. Closer to our work, https://www.grid5000.fr). [35] train a generative adversarial network to hallucinate visual REFERENCES near-goals state over failed trajectories. However, their method requires heavy engineering as visual goals are extremely [1] Peter Anderson et al. “Vision-and-language navigation: complex to generate, and they lack the compact generalization Interpreting visually-grounded navigation instructions opportunities inherent to language. [9] also studies HER in in real environments”. In: Proc. of CVPR. 2018. the language setting, but the authors only consider the context [2] Jacob Andreas et al. “Neural module networks”. In: where a language expert is available. Proc. of CVPR. 2016. [3] Marcin Andrychowicz et al. “Hindsight experience re- E. Conditioned Language Policy play”. In: Proc. of NIPS. 2017. There have been other attempts to leverage language instruc- [4] Yoav Artzi and Luke Zettlemoyer. “Weakly supervised tion to improve the agent policy. For instance, [24] computes a learning of semantic parsers for mapping instructions to high-level language policy to give textual instruction to a low- actions”. In: TACL 1 (2013), pp. 49–62. level policy, enforcing a hierarchical learning training. The [5] Dzmitry Bahdanau et al. “Learning to Understand Goal authors manage to resolve complicated manipulating task by Specifications by Modelling Reward”. In: Proc. of decomposing the action with language operation. The language ICLR. 2019. mapper performs instruction retrieval into a predefined set [6] Dzmitry Bahdanau et al. “Systematic Generalization: of textual goals and yet, the low-level policy benefits from What Is Required and Can It Be Learned?” In: Proc. of language compositionality and is able to generalize to unseen ICLR. 2019. instructions, as mentioned by the authors. [34] train an agent [7] Marc Bellemare et al. “Unifying count-based explo- to refine its policy by collecting language corrections over ration and intrinsic motivation”. In: Proc. of NeurIPS. multiple trajectories on the same task. While the authors focus 2016. their effort on integrating language cues, it could be promising [8] Simon Brodeur et al. “HoME: a Household Multimodal to learn the correction function in a HIGhER fashion. Environment”. In: ViGIL Workshop. 2017. [9] Harris Chan et al. “ACTRCE: Augmenting Experience VI. CONCLUSION via Teacher’s Advice For Multi-Goal Reinforcement We introduce Hindsight Generation for Experience Replay Learning”. In: Goal Specifications for RL workshop (HIGhER) as an extension to HER for language. We define a (2018). protocol to learn a mapping function to relabel unsuccessful [10] Devendra Singh Chaplot et al. “Gated-attention ar- trajectories with predicted consistent language instructions. We chitectures for task-oriented language grounding”. In: show that HIGhER nearly matches HER performances despite Proc. of AAAI. 2018. only relying on signals from the environment. We provide [11] David L Chen and Raymond J Mooney. “Learning to empirical evidence that HIGhER manages to alleviate the in- interpret natural language navigation instructions from struction following task by jointly learning language grounding observations”. In: Proc. of AAAI. 2011. and navigation policy with training synergies. HIGhER has [12] Maxime Chevalier-Boisvert et al. “BabyAI: First Steps mild underlying assumptions, and it does not require human Towards Grounded Language Learning With a Human data, making it valuable to complement to other instruction In the Loop”. In: Proc. of ICLR. 2019. URL: https:// following methods. More generally, HIGhER can be extended openreview.net/forum?id=rJeXCo0cYX. to any goal modalities, and we expect similar procedures to [13] Jacob Devlin et al. “BERT: Pre-training of Deep Bidi- emerge in other setting. rectional Transformers for Language Understanding”. In: Proc. of NAACL. 2019. ACKNOWLEDGEMENTS [14] Meng Fang et al. “DHER: Hindsight Experience Replay The authors would like to acknowledge the stimulating for Dynamic Goals”. In: Proc. of ICLR. 2019. research environment of the SequeL INRIA Project-Team. [15] Daniel Fried et al. “Speaker-follower models for vision- Special thanks to Edouard Leurent, Piotr Mirowski and Flo- and-language navigation”. In: Proc. of NeurIPS. 2018. rent Altché for fruitful discussions and reviewing the final [16] Justin Fu et al. “From Language to Goals: Inverse manuscript. supervised learning for Vision-Based Instruction We acknowledge the following agencies for research fund- Following”. In: Proc of. ICLR. 2019. ing and computing support: Project BabyRobot (H2020-ICT- [17] Hado van Hasselt, Arthur Guez, and David Silver. 24-2015, grant agreement no.687831), and CPER Nord-Pas de “Deep supervised learning with double Q-Learning”. Calais/FEDER DATA Advanced data science and technologies In: Proc. of AAAI. 2016. 2015-2020. [18] Karl Moritz Hermann et al. “Grounded language learn- Experiments presented in this paper were carried out us- ing in a simulated 3d world”. In: arXiv preprint ing the Grid’5000 testbed, supported by a scientific inter- arXiv:1706.06551 (2017). est group hosted by Inria and including CNRS, RENATER
[19] Karl Moritz Hermann et al. “Learning to fol- [40] Xin Wang et al. “Reinforced cross-modal matching and low directions in street view”. In: arXiv preprint self-supervised imitation learning for vision-language arXiv:1903.00401 (2019). navigation”. In: Proc. of CVPR. 2019. [20] Todd Hester et al. “Deep q-learning from demonstra- [41] Ziyu Wang et al. “Dueling Network Architectures for tions”. In: Proc. of AAAI. 2018. Deep supervised learning”. In: Proc. of ICML. [21] Felix Hill et al. Human Instruction-Following with 2016. Deep supervised learning via Transfer-Learning [42] Xiaoxue Zang et al. “Translating Navigation Instruc- from Text. 2020. arXiv: 2005.09382 [cs.CL]. tions in Natural Language to a High-Level Plan for [22] Felix Hill et al. “Understanding grounded language Behavioral Robot Navigation”. In: Proc. of EMNLP. learning agents”. In: arXiv preprint arXiv:1710.09867 2018. (2017). [23] Drew Arad Hudson and Christopher D. Manning. “Compositional Attention Networks for Machine Rea- soning”. In: Proc. of ICLR. 2018. [24] Yiding Jiang et al. “Language as an abstraction for hierarchical deep supervised learning”. In: Proc. of NeurIPS. 2019. [25] Douwe Kiela et al. “Virtual embodiment: A scalable long-term strategy for artificial intelligence research”. In: arXiv preprint arXiv:1610.07432 (2016). [26] Simon Kirby et al. “Compression and communication in the cultural evolution of linguistic structure”. In: Cognition 141 (2015), pp. 87–102. [27] Hao Liu et al. “Competitive experience replay”. In: Proc. of ICLR. 2019. [28] Jelena Luketina et al. “A Survey of supervised learning Informed by Natural Language”. In: Proc. of IJCAI. 2019. [29] Volodymyr Mnih et al. “Human-level control through deep supervised learning”. In: Nature 518.7540 (2015), p. 529. [30] Karthik Narasimhan, Regina Barzilay, and Tommi Jaakkola. “Grounding language for transfer in deep re- inforcement learning”. In: JAIR 63 (2018), pp. 849–874. [31] Andrew Y Ng, Daishi Harada, and Stuart Russell. “Pol- icy invariance under reward transformations: Theory and application to reward shaping”. In: Proc. of ICML. 1999. [32] Ethan Perez et al. “Film: Visual reasoning with a general conditioning layer”. In: Proc. of AAAI. 2018. [33] Paulo Rauber et al. “Hindsight policy gradients”. In: Proc. of ICLR. 2019. [34] John D Co-Reyes et al. “Guiding policies with language via meta-learning”. In: Proc. of ICLR. 2018. [35] Himanshu Sahni et al. “Visual Hindsight Experience Replay”. In: Proc. of NeurIPS. 2019. [36] Tom Schaul et al. “Prioritized experience replay”. In: Proc. of ICLR. 2016. [37] Tom Schaul et al. “Universal value function approxima- tors”. In: Proc. of ICML. 2015. [38] Richard S Sutton and Andrew G Barto. supervised learning: An introduction. MIT Press, 2018. [39] Stefanie Tellex et al. “Understanding natural language commands for robotic navigation and mobile manipu- lation”. In: Proc. of AAAI. 2011.
