Improving the Gating Mechanism of Recurrent Neural Networks Albert Gu 1 Caglar Gulcehre 2 Tom Paine 2 Matt Hoffman 2 Razvan Pascanu 2 Abstract (Chung et al., 2014, GRUs). These gated RNNs have been Gating mechanisms are widely used in neural very successful in several different application areas such network models, where they allow gradients to as in supervised learning (Kapturowski et al., 2018; backpropagate more easily through depth or time. Espeholt et al., 2018) and computer vision However, their saturation property introduces (Bahdanau et al., 2014; Kocˇisky` et al., 2018). problems of its own. For example, in recurrent At every time step, gated recurrent networks use a weighted models these gates need to have outputs near 1 combination of the history summarized by the previous state, to propagate information over long time-delays, and a function of the incoming inputs, to create the next which requires them to operate in their saturation state. The values of the gates, which are the coefficients of regime and hinders gradient-based learning of the weighted combination, control the length of temporal the gate mechanism. We address this problem by dependencies that can be addressed. This weighted update deriving two synergistic modifications to the stan- can be seen as an additive or residual connection on the dard gating mechanism that are easy to implement, recurrent state, which helps signals propagate through introduce no additional hyperparameters, and time without vanishing. However, the gates themselves improve learnability of the gates when they are are prone to a saturating property which can also hamper close to saturation. We show how these changes gradient-based learning. This can be problematic for RNNs, are related to and improve on alternative recently where carrying information for very long time delays proposed gating mechanisms such as chrono requires gates to be very close to their saturated states. initialization and Ordered Neurons. Empirically, our simple gating mechanisms robustly improve We address two particular problems that arise with the the performance of recurrent models on a range standard gating mechanism of recurrent models. Firstly, of applications, including synthetic memorization learning when gates are in their saturation regime is difficult tasks, sequential image classification, language because gradients through the gates vanish as they saturate. modeling, and supervised learning, particularly We derive a modification to standard gating mechanisms when long-term dependencies are involved. that uses an auxiliary refine gate (Section 3.1) to modulate a main gate. This mechanism allows the gates to have a wider range of activations without gradients vanishing as quickly. 1. Introduction Secondly, typical initialization of the gates is relatively concentrated. This restricts the range of timescales the model Recurrent neural networks (RNNs) are an established can address at initialization, as the timescale of a particular machine learning tool for learning from sequential data. unit is dictated by its gates. We propose uniform gate initial- However, RNNs are prone to the vanishing gradient problem, ization (Section 3.2) that addresses this problem by directly which occurs when the gradients of the recurrent weights be- initializing the activations of these gates from a distribution come vanishingly small as they get backpropagated through that captures a wider spread of dependency lengths. time (Hochreiter, 1991; Bengio et al., 1994; Hochreiter et al., 2001). A common approach to alleviate the vanishing The main contribution of this paper is the refine gate mech- gradient problem is to use gating mechanisms, leading to anism. As the refine gate works better in tandem with uni- models such as the long short term memory (Hochreiter form gate initialization, we call this combination the UR & Schmidhuber, 1997, LSTM) and gated recurrent units gating mechanism. We focus on comparing the UR gating mechanism against other approaches in our experiments. 1Stanford University, USA 2DeepMind, London, UK. Corre- These changes can be applied to any gate (i.e. parameterized spondence to: Albert Gu <albertgu@stanford.edu>, Caglar Gul- bounded function) and have minimal to no overhead in terms cehre <caglarg@google.com>. of speed, memory, code complexity, parameters, or hyper Proceedings of the 37 th International Conference on Machine parameters. We apply them to the forget gate of recurrent Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by models, and evaluate on several benchmark tasks that re- the author(s). 0202 nuJ 81 ]EN.sc[ 2v09890.0191:viXra
Improving the Gating Mechanism of Recurrent Neural Networks quire long-term memory including synthetic memory tasks, pixel-by-pixel image classification, language modeling, and supervised learning. Finally, we connect our methods to other proposed gating modifications, introduce a framework that allows each component to be replaced with similar ones, and perform extensive ablations of our method. Empirically, the UR gating mechanism robustly improves on the standard forget and input gates of gated recurrent models. When ap- plied to the LSTM, these simple modifications solve synthetic Figure 1. Refine mechanism. The refine mechanism improves flow memory tasks that are pathologically difficult for the standard of gradients through a saturating gate f. As f saturates, its gradient LSTM, achieve state-of-the-art results on sequential MNIST vanishes, and its value is unlikely to change (see Figure 4). The and CIFAR-10, and show consistent improvements in lan- refine gate r is used to produce a bounded additive term φ that may guage modeling on the WikiText-103 dataset (Merity et al., push f lower or higher as necessary. The resulting effective gate g can achieve values closer to 0 and 1 and can change even when f is 2016) and supervised learning tasks (Hung et al., 2018). stuck. We apply it to the forget gate of an LSTM. The g is then used in place of f in the state update (5). 2. Gated Recurrent Neural Networks state and “cell” state. The state update equation (1) is used to Broadly speaking, RNNs are used to sweep over a sequence create the next cell state c (5). Note that the gate and update t of input data x t to produce a sequence of recurrent states activations are a function of the previous hidden state h t−1 h t ∈ Rd summarizing information seen so far. At a high instead of c t−1. Here, P (cid:63) stands for a parameterized linear level, an RNN is just a parametrized function in which each function of its inputs with bias b , e.g. (cid:63) sequential application of the network computes a state update u : (x t,h t−1) (cid:55)→ h t. Gating mechanisms were introduced to P f (x t,h t−1) = W fxx t +W fhh t−1 +b f . (8) address the vanishing gradient problem (Hochreiter, 1991; and σ(·) refers to the standard sigmoid activation function Bengio et al., 1994; Hochreiter et al., 2001), and have proven which we will assume is used for defining [0,1]-valued ac- to be crucial to the success of RNNs. This mechanism essen- tivations in the rest of this paper. The gates of the LSTM tially smooths out the update using the following equation, were initially motivated as a binary mechanism, switching on or off, allowing information and gradients to pass through. h = f (x ,h )◦h +i (x ,h )◦u(x ,h ), (1) t t t t−1 t−1 t t t−1 t t−1 However, in reality, this fails to happen due to a combination where the forget gate f and input gate i are [0,1]d-valued of two factors: initialization and saturation. This can be prob- t t functions that control how fast information is forgotten or lematic, such as when very long dependencies are present. allowed into the memory state. When the gates are tied, i.e. f t +i t = 1 as in GRUs, they behave as a low-pass filter, 3. Our Proposed Gating Mechanisms deciding the time-scale on which the unit will respond (Tallec & Ollivier, 2018). For example, large forget gate activations We present two solutions that work in tandem to address close to f = 1 are necessary for recurrent models to address the previously described issues. The first is the refine gate, t long-term dependencies.1 which allows for better gradient flow by reparameterizing a saturating gate, for example, the forget gate. The second is We will introduce our improvements to the gating mechanism uniform gate initialization, which ensures a diverse range primarily in the context of the LSTM, which is the most of gate values are captured at the start of training, which popular recurrent model. allows a recurrent model to have a multi-scale representation f t = σ(P f (x t, h t−1)), (2) of an input sequence at initialization. i = σ(P (x , h )), (3) t i t t−1 3.1. Refine Gate u = tanh(P (x , h )), (4) t u t t−1 c = f ◦c +i ◦u , (5) Formally, the full mechanism of the refine gate as applied t t t−1 t t to gated recurrent models is defined in equations (9)-(11). o = σ(P (x , h )), (6) t o t t−1 Note that it is an isolated change where the forget gate f is h = o tanh(c ). (7) t t t t modified to get the effective forget gate in (10) before apply- ing the the standard update (1). Figure 1 illustrates the refine A typical LSTM (equations (2)-(7)) is an RNN whose state gate in an LSTM cell. Figure 3 illustrates how the refine gate is represented by a tuple (h , c ) consisting of a “hidden” t t r is defined and how it changes the forget gate f to produce t t 1In this work, we use “gate” to alternatively refer to a [0,1]- an effective gate g t. The refine gate allows the effective valued function or the value (“activation”) of that function. gate g to reach much higher and lower activations than the
Improving the Gating Mechanism of Recurrent Neural Networks constituent gates f and r, bypassing the saturating gradient problem. For example, this allows the effective forget gate 1.0 to reach g = 0.99 when the forget gate is only f = 0.9. 0.8 Finally, to simplify comparisons and ensure that we always 0.6 use the same number of parameters as the standard gates, when using the refine gate we tie the input gate to the effec- 0.4 tive forget gate, i = 1 − g .2 However, we emphasize that t t 0.2 these techniques can be applied to any gate (or more broadly, any bounded function) to improve initialization distribution 0.0 0.0 0.2 0.4 0.6 0.8 1.0 Gate f and help optimization. For example, our methods can be combined in different ways in recurrent models, e.g. an inde- pendent input gate can be modified with its own refine gate. r = σ(P (x , h )), (9) t r t t−1 g = r ·(1−(1−f )2)+(1−r )·f 2, (10) t t t t t c = g c +(1−g )u . (11) t t t−1 t t 3.2. Uniform Gate Initialization Standard initialization schemes for the gates can prevent the learning of long-term temporal correlations (Tallec & Ollivier, 2018). For example, supposing that a unit in the cell state has constant forget gate value f , then the contribution t of an input x in k time steps will decay by (f )k. This gives t t the unit an effective decay period or characteristic timescale of O( 1 ).3 Standard initialization of linear layers L sets 1−ft the bias term to 0, which causes the forget gate values (2) to concentrate around 0.5. A common trick of setting the forget gate bias to b = 1.0 (Jozefowicz et al., 2015) does increase f the value of the decay period to 1 ≈ 3.7. However, 1−σ(1.0) this is still relatively small and may hinder the model from learning dependencies at varying timescales easily. We instead propose to directly control the distribution of forget gates, and hence the corresponding distribution of decay periods. In particular, we propose to simply initialize the value of the forget gate activations f according to a t uniform distribution U(0,1)4, b ∼ σ−1(U[(cid:15),1−(cid:15)]). (12) f An important difference between UGI and standard or other (e.g. Tallec & Ollivier, 2018) initializations is that negative forget biases are allowed. The effect of UGI is that all timescales are covered, from units with very high forget activations remembering information (nearly) indefinitely, to those with low activations focusing solely on the incoming 2In our experiments, we found that tying input/forget gates makes negligible difference on downstream performance, consistent with previous findings in the literature (Greff et al., 2016; Melis et al., 2017). 3This corresponds to the number of timesteps it takes to decay by 1/e. 4Since σ−1(0) = −inf, we use the standard practice of thresh- olding with a small (cid:15) for stability. )f(c egnar dnaB 1.0 0.8 0.6 0.4 0.2 0.0 4 3 2 1 0 1 2 3 4 x (a) g 1 1+e x 1 (1+e x)2 1 (1+1 ex)2 (b) Figure 2. The adjustment function. (a) An adjustment function α(f ) satisfying natural properties is chosen to define a band within t which the forget gate is refined. (b) The forget gate f (x) is conven- t tionally defined with the sigmoid function (black). The refine gate interpolates around the original gate f to yield an effective gate g t t within the upper and lower curves, g ∈ f ±α(f ). t t t input. Additionally, it introduces no additional parameters; it even can have less hyperparameters than the standard gate initialization, which sometimes tunes the forget bias b (Jozefowicz et al., 2015). Appendix B.2 and B.3 further f discuss the theoretical effects of UGI on timescales. 3.3. The URLSTM The URLSTM requires two small modifications to the vanilla LSTM. First, we present the way the biases of forget gates are initialized in Equation (12) with UGI. Second, the modifications on the standard LSTM equations to compute the refine and effective forget gates are presented in Equations (9)-(11). However, we note that these methods can be used to modify any gate (or more generally, bounded function) in any model. In this context, the URLSTM is simply defined by applying UGI and a refine gate r on the original forget gate f to create an effective forget gate g (Equation (10)). This effective gate is then used in the cell state update (11). Empirically, these small modifications to an LSTM are enough to allow it to achieve nearly binary activations and solve difficult memory problems (Figure 5). 3.4. A Formal Treatment of Refine Gates Given a gate f = σ(P (x)) ∈ [0, 1], the refine gate is an f independent gate r = σ(P (x)) that modulates f to produce r a value g ∈ [0,1] which will be used in place of f downstream. It is motivated by considering how to modify the output of a gate f in a way that promotes gradient-based learning, derived below. An additive adjustment A root cause of the saturation problem is that the gradient ∇f of a gate can be written solely as a function of the activation value as f (1 − f ), decays rapidly as f approaches to 0 or 1. Thus when the activation f is past a certain upper or lower threshold,
Improving the Gating Mechanism of Recurrent Neural Networks 3.5 3.0 2.5 2.0 1.5 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Effective gate g (a) etag dradnats .sv tneidarg ni esaercnI and gradient based optimization methods. Let us note that, f may need to be either increased or de- t creased, regardless of what value it has. This is because the gradients through the gates can vanish either when the activa- tions get closer to 0 or 1. Therefore, an additive update to f should create an effective gate activation g in the range f ±α t t for some α. We assume that the allowed adjustment range, α = α(f ), needs to be a function of f to keep the g between 0 t and 1. Since 0 and 1 are symmetrical in the gating framework, (b) our adjustment rate should also satisfy α(f ) = α(1−f ). Figure 3. Refine gate activations and gradients.: (a) Contours of Figure 2a illustrates the general appearance of α(f ) based on the effective gate g t as a function of the forget and refine gates aforementioned properties. According to the Boundedness f t, r t. High effective activations can be achieved with more modest property, the adjustment rate should be be upper-bounded by f t,r t values. (b) The gradient ∇g t as a function of effective gate min(f,1−f ) to ensure that g ∈ f ±α(f ) is bounded between activation g . [Black, blue]: Lower and upper bounds on the ratio of t 0 and 1. As a consequence of this property, its derivatives the gradient with a refine gate vs. the gradient of a standard gate. For should also satisfy, α(cid:48)(0) ≤ 1 and α(cid:48)(1) ≥ −1. Symetricity activation values near the extremes, the refine gate can significantly also implies α(cid:48)(f ) = −α(cid:48)(1−f ), and smoothness implies α(cid:48) increase the gradients. is continuous. The simplest such function satisfying all these properties is the linear α(cid:48)(f ) = 1−2f , yielding to our choice of adjustment function, α(f ) = f −f 2 = f (1−f ). However, learning effectively stops. This problem cannot be fully when f is bounded between 0 and 1, α(f ) will be positive. addressed only by modifying the input of the sigmoid, as in UGI and other techniques, as the gradient will still vanish Recall that the goal is to produce an effective activation by backpropagating through the activation function. g = f +φ(f,x) such that g ∈ f ±α(f ) (Figure 2b) given. Our final observation is that the simplest such function φ satis- Therefore to better control activations near the saturating fying this is φ(f,x) = α(f )ψ(f,x) where ψ(f,x) ∈ [−1,1] regime, instead of changing the input to the sigmoid in f = decides the sign of adjustment, and it can also change σ(P(x)), we consider modifying the output. Modifying the its magnitude as well. The standard method of defining gate with a multiplicative interaction can have unstable learn- [−1,1]-valued differentiable functions is achieved by using ing dynamics since when the gates have very small values, a tanh non-linearity, and this leads to φ(f,x) = α(f )(2r−1) the multiplicative factor may need to be very large to avoid for another gate r = σ(P(x)). The full refine update equation the gradients of the gates shrinking to zero. As a result, we can be given as in Equation (13), consider adjusting f with an input-dependent additive update φ(f,x) for some function φ, to create an effective gate g = g = f +α(f )(2r−1) = f +f (1−f )(2r−1) f +φ(f,x) that will be used in place of f downstream such as (13) in the main state update (1). This sort of additive (“residual”) = (1−r)·f 2 +r·(1−(1−f )2) connection is a common technique to increase gradient flow, and indeed was the motivation of the LSTM additive gated Equation (13) has the elegant interpretation that the gate r update (1) itself (Hochreiter & Schmidhuber, 1997). linearly interpolates between the lower band f −α(f ) = f 2 and the symmetric upper band f + α(f ) = 1 − (1 − f )2 (Figure 2b). In other words, the original gate f is the Choosing the adjustment function φ Although there coarse-grained determinant of the effective gate g, while the might be many choices that seem plausible for choosing an gate r “refines” it. appropriate additive update φ, we first identify the desirable properties of such a function and then discuss how our refine gate mechanism satisfies those properties. 4. Related Gating Mechanisms The desired properties of φ emerge considering the We highlight a few recent works that also propose small gate applications of the gating mechanisms in recurrent models: changes to address problems of long-term or variable-length dependencies. Like ours, they can be applied to any gated • Boundedness: After the additive updates, the update equation. activations still need to be bounded between 0 and 1. • Symmetricity: The resulting gating framework should Tallec & Ollivier (2018) suggest an initialization strategy be symmetric around 0, as sigmoid does. to capture long-term dependencies on the order of T , max • Smoothness: The refining mechanism should be by sampling the gate biases from b ∼ log U(1,T − 1). f max differentiable, since we will be using meta-optimization synthesis Although similar to UGI in definition, chrono initialization
Improving the Gating Mechanism of Recurrent Neural Networks (CI) has critical differences in the timescales captured, for Table 1. Summary of gate ablations. Summary of gating mecha- example, by using an explicit timescale parameter and having nisms considered in this work as applied to the forget/input gates of no negative biases. Due to its relation to UGI, we provide a recurrent models. Some of these ablations correspond to previous more detailed comparison in Appendix B.3. As mentioned work. -- standard LSTMs, C- (Tallec & Ollivier, 2018), and OM in Section 3.4, techniques such as these that only modify (Shen et al., 2018) the input to a sigmoid gate do not adequately address the Name Initialization/Activation Auxiliary Gate saturation problem. -- Standard initialization N/A The Ordered Neuron (ON) LSTM introduced by (Shen C- Chrono initialization N/A et al., 2018) aims to induce an ordering over the units in O- cumax activation N/A U- Uniform initialization N/A the hidden states such that “higher-level” neurons retain -R Standard initialization Refine gate information for longer and capture higher-level information. OM cumax activation Master gate We highlight this work due to its recent success in NLP, and UM Uniform initialization Master gate also because its novelties can be factored into introducing OR cumax activation Refine gate two mechanisms which only affect the forget and input gates, UR Uniform initialization Refine gate namely (i) the cumax := cumsum ◦ softmax activation function which creates a monotonically increasing vector bias on the forget activations, is to simply drop the auxiliary in [0,1], and (ii) a pair of “master gates” which are ordered master gates and define f ,i (2)-(3) using the cumax acti- t t by cumax and fine-tuned with another pair of gates. vation function. UM: UGI master gates. This variant of the We observe that these are related to our techniques in that ON-LSTM’s gates ablates the cumax operation on the master one controls the distribution of a gate activation, and the gates, replacing it with a sigmoid activation and UGI which other is an auxiliary gate with modulating behavior. Despite maintains the same initial distribution on the activation values. its important novelties, we find that the ON-LSTM has OR: Refine instead of master. A final variant in between the drawbacks, including speed and scaling issues of its gates. UR gates and the ON-LSTM’s gates combines cumax with We provide the formal definition and detailed analysis of the refine gates. In this formulation, as in UR gates, the refine ON-LSTM in Appendix B.4. For example, we comment on gate modifies the forget gate and the input gate is tied to the how UGI can also be motivated as a faster approximation of effective forget gate. The forget gate is ordered using cumax. the cumax activation. We also flesh out a deeper relationship Table 1 summarizes the gating modifications we consider between the master and refine gates and show how they can and their naming conventions. Note that we also denote be interchanged for each other. the ON-LSTM method as OM for mnemonic ease. Finally, We include a more thorough overview of other related works we remark that all methods here are controlled with the on RNNs in Appendix B.1. These methods are mostly same number of parameters as the standard LSTM, aside orthogonal to the isolated gate changes considered here from OM and UM which use an additional 1 -fraction 2C and are not analyzed. We note that an important drawback parameters where C is the downsize factor on the master common to all other approaches is the introduction of gates (Appendix B.4). C = 1 unless noted otherwise. substantial hyperparameters in the form of constants, training protocol, and significant architectural changes. For example, 5. Experiments even for chrono initialization, one of the less intrusive proposals, we experimentally find it to be particularly We first perform full ablations of the gating variants (Sec- sensitive to the hyperparameter T (Section 5). tion 4.1) on two common benchmarks for testing memory max models: synthetic memory tasks and pixel-by-pixel image 4.1. Gate Ablations classification tasks. We then evaluate our main method on important applications for recurrent models including Our insights about previous work with related gate compo- language modeling and supervised learning, comparing nents allow us to perform extensive ablations of our con- against baselines from literature where appropriate. tributions. We observe two independent axes of variation, The main claims we evaluate for each gating component namely, activation function/initialization (cumax, constant are (i) the refine gate is more effective than alternatives bias sigmoid, CI, UGI) and auxiliary modulating gates (mas- (the master gate, or no auxiliary gate), and (ii) UGI is more ter, refine), where different components can be replaced with effective than standard initialization for sigmoid gates. In each other. Therefore we propose several other gate combina- particular, we expect the *R gate to be more effective than tions to isolate the effects of different gating mechanisms. We *M or *- for any primary gate *, and we expect U* to be summarize a few ablations here; precise details are given in better than -* and comparable to O* for any auxiliary gate *. Appendix B.5. O-: Ordered gates. A natural simplification of the main idea of ON-LSTM, while keeping the hierarchical The standard LSTM (--) uses forget bias 1.0 (Section
Improving the Gating Mechanism of Recurrent Neural Networks Figure 5. Distribution of forget gate activations before and after training. For the Copy task. We show the distribution of activations f for four methods: -- cannot learn large enough f and makes no t t progress on the task. C- initializes with extremal activations which barely change during training. U- makes progress by encouraging a range of forget gate values, but this distribution does not change significantly during training due to saturation. UR starts with the same distribution as U- but is able to learn extreme gate values, which allows it to access the distal inputs, as necessary for this task. Appendix E.1 shows a reverse task where UR is able to un-learn from a saturated regime. Figure 4. Performance on synthetic memory: Copy task using cue token 9 is presented. We trained our models using sequences of length 500. Several methods including standard gates cross-entropy with baseline loss log(8) (Appendix D.1). fail to make any progress (overlapping flat curves at baseline). Note that methods that combine the refine gate with a range of gate values Adding task. The input consists of two sequences: 1. N (OR, UR) perform best. But the refine gate on its own does not numbers (a ,...,a ) sampled independently from U[0,1] 0 N−1 perform well. Adding task using sequences of length 2000. Most 2. an index i ∈ [0, N/2) and i ∈ [N/2, N ), together 0 1 methods eventually make progress, but again methods that combine encoded as a two-hot sequence. The target output is a +a the refine gate with a range of gate values (OR, UR) perform best. i0 i1 and models are evaluated by the precision with 2.2). When chrono initialization is used and not explicitly baseline loss 1/6. tuned, we set T to be proportional to the hidden size. max Figure 4 shows the loss of various methods on the Copy and This heuristic uses the intuition that if dependencies of Adding tasks. The only gate combinations capable of solving length T exist, then so should dependencies of all lengths Copy completely are OR, UR, O-, and C-. This confirms the ≤ T . Moreover, the amount of information that can be mechanism of their gates: these are the only methods capable remembered is proportional to the number of hidden units. of producing high enough forget gate values either through All of our benchmarks have prior work with recurrent the cumax non-linearity, the refine gate, or extremely high baselines, from which we used the same models, protocol, forget biases. U- is the only other method able to make and hyperparameters whenever possible, changing only the progress, but converges slower as it suffers from gate satu- gating mechanism without doing any additional tuning for ration without the refine gate. -- makes no progress. OM and the refine gating mechanisms. Full protocols and details for UM also get stuck at the baseline loss, despite OM’s cumax all experiments are given in Appendix D. activation, which we hypothesize is due to the suboptimal magnitudes of the gates at initialization (Appendix B.4). On 5.1. Synthetic Memory Tasks the Adding task, every method besides -- is able to eventually solve it, with all refine gate variants fastest. Our first set of experiments is on synthetic memory tasks (Hochreiter & Schmidhuber, 1997; Arjovsky et al., Figure 5 shows the distributions of forget gate activations of 2016) that are known to be hard for standard LSTMs to solve. sigmoid-activation methods, before and after training on the For these tasks, we used single layer models with 256 hidden Copy task. It shows that activations near 1.0 are important units, trained using Adam with learning rate 10−3. for a model’s ability to make progress or solve this task, and that adding the refine gate makes this significantly easier. Copy task. The input is a sequence of N +20 digits where the first 10 tokens (a ,a ,...,a ) are randomly chosen from 0 1 9 5.2. Pixel-by-pixel Image Classification {1,...,8}, the middle N tokens are set to 0, and the last ten tokens are 9. The goal of the recurrent model is to output These tasks involve feeding a recurrent model the pixels of (a ,...,a ) in order on the last 10 time steps, whenever the an image in a scanline order before producing a classification 0 9
Improving the Gating Mechanism of Recurrent Neural Networks Table 2. Comparison to prior methods for pixel-by-pixel image classification. Test acc. on pixel-by-pixel image classification benchmarks. Top: Recurrent baselines and variants. Middle: Non- recurrent sequence models with global receptive field. r-LSTM has 2-layers with an auxiliary loss. Bottom: Our methods. Method sMNIST pMNIST sCIFAR-10 LSTM (ours) 98.9 95.11 63.01 Dilated GRU (Chang et al., 2017) 99.0 94.6 - IndRNN (Li et al., 2018a) 99.0 96.0 - r-LSTM (Trinh et al., 2018) 98.4 95.2 72.2 Transformer (Trinh et al., 2018) 98.9 97.9 62.2 Temporal ConvNet (Bai et al., 2018a) 99.0 97.2 - TrellisNet (Bai et al., 2018b) 99.20 98.13 73.42 URLSTM 99.28 96.96 71.00 URLSTM + Zoneout (Krueger et al., 2016) 99.21 97.58 74.34 URGRU + Zoneout 99.27 96.51 74.4 Table 3. Language modelling results. Perplexities on the WikiText-103 dataset. Method Valid Test -- 34.3 35.8 C- 35.0 36.4 C- T = 8 34.3 36.1 max Figure 6. Performance on pixel-by-pixel image classification. C- T = 11 34.6 35.8 Performance is consistent with synthetic tasks. -- performs the max OM 34.0 34.7 worst. Other gating variants improve performance. Note that meth- U- 33.8 34.9 ods that combine the refine gate with a range of gate values (OR, UR 33.6 34.6 UR) perform best. label. We test on the sequential MNIST (sMNIST), permuted MNIST (pMNIST) (Le et al., 2015), and sequential CIFAR- 5.3. Language Modeling 10 (sCIFAR-10) tasks. Each LSTM method was ran with a We consider word-level language modeling on the WikiText- learning rate sweep with 3 seeds each. We found that many 103 dataset, where (i) the dependency lengths are much methods were quite unstable, with multiple seeds diverging. shorter than in the synthetic tasks, (ii) language has an Figure 6 shows the F1-score curves of each method at their implicit hierarchical structure and timescales of varying best stable learning rate. The basic LSTM is noticeably worse lengths. We evaluate our gate modifications against the exact than all of the others. This suggests that any of the gate mod- hyperparameters of a SOTA LSTM-based baseline (Rae et al., ifications, whether better initialization, cumax non-linearity, 2018) without additional tuning (Appendix D). Additionally, or master or refine gates, are better than standard gates espe- we compare against ON-LSTM, which was designed for cially when long-term dependencies are present. Addition- this domain (Shen et al., 2018), and chrono initialization, ally, the uniform gate initialization methods are generally bet- which addresses dependencies of a particular timescale as ter than the ordered and chrono initialization, and the refine opposed to timescale-agnostic UGI methods. In addition gate performs better than the master gate. Table 2 compares to our default hyperparameter-free initialization, we tested the test F1-score of our main model against other models from models with the chrono hyperparameter T manually set the literature. In addition, we tried variants of GRUs and the max to 8 and 11, values previously used for language modeling to addition of a generic regularization technique—we chose mimic fixed biases of about 1.0 and 2.0 respectively (Tallec Zoneout (Krueger et al., 2016) with default hyperparameters & Ollivier, 2018). (z = 0.5, z = 0.05). This combination even outperformed c h non-recurrent models on sequential MNIST and CIFAR-10. Table 3 shows Validation and Test set perplexities for various models. We find that OM, U-, and UR improve over -- with From Sections 5.1 and 5.2, we draw a few conclusions about no additional tuning. However, although OM was designed the comparative performance of different gate modifications. to capture the hierarchical nature of language with the First, the refine gate is consistently better than comparable cumax activation, it does not perform better than U- and master gates. C- solves the synthetic memory tasks but is UR. Appendix D, Figure 11 additionally shows validation worse than any other variant outside of those. We find or- perplexity curves, which indicate that UR overfits less than dered (cumax) gates to be effective, but speed issues prevent the other methods. us from using them in more complicated tasks. UR gates are consistently among the best performing and most stable. The chrono initialization using our aforementioned initial-
Improving the Gating Mechanism of Recurrent Neural Networks Figure 7. Active match. Hung et al. (2018). The agent navigates a 3D world using observations from a first person camera. The task has three phases. In phase 1, the agent must search for a colored cue. In phase 2, the agent is exposed to apples which give distractor rewards. In phase 3, the agent must correctly recall the color of the cue and pick the sensor near the corresponding color to receive the task reward. An episode lasts between 450 and 600 steps, requiring long-term memory and credit assignment. Figure 8. Performance on supervised learning tasks that re- quire memory. We evaluated the image matching tasks from Hung ization strategy makes biases far too large. While manually et al. (2018), which test memorization and credit assignment, using tweaking the T hyperparameter helps, it is still far from max an A3C agent (Mnih et al., 2016) with an LSTM policy core. We any UGI-based methods. We attribute these observations observe that general trends from the synthetic tasks (Section (5.1)) to the nature of language having dependencies on multiple transfer to this supervised learning setting. widely-varying timescales, and that UGI is enough to capture these without resorting to strictly enforced hierarchies such as in OM. task, and we found that those trends largely transferred to the RL setting even with several additional confounders present 5.4. supervised learning Memory Tasks such as agents learning via RL algorithms, being required to learn relevant features from pixels rather than being given the In most partially observable supervised learning (RL) relevant tokens, and being required to explore in the Active tasks, the agent can observe only part of the environment at Match case. a time and thus requires a memory to summarize what it has seen previously. However, designing memory architectures We found that the UR gates substantially improved the for supervised learning problems has been a challenging performance of the basic LSTM on both Passive Match and task (Oh et al., 2016; Wayne et al., 2018). Many memory Active Match tasks with distractor rewards. The URLSTM architectures for RL use an LSTM component to summarize was the was the only method able to get near optimal what an agent has seen. performance on both tasks, and achieved similar final performance to the LSTM+Mem and RMA agents reported We investigated if changing the gates of these LSTMs can in (Hung et al., 2018). improve the performance of RL agents, especially on difficult tasks involving memory and long-term credit assignment. We 5.5. Additional Results and Experimental Conclusions chose the Passive match and Active match tasks from Hung et al. (2018) using A3C agents (Mnih et al., 2016). See Figure Appendix (E.1) shows an additional synthetic experiment 7 for a description of Active match. Passive match is similar, investigating the effect of refine gates on saturation. Ap- except the agent always starts facing the colored cue. As a pendix (E.3) has results on a program execution task, which result, Passive Match only tests long term memory, not long- is interesting for having explicit long and variable-length term credit assignment. Only the final task reward is reported. dependencies and hierarchical structure. It additionally shows another very different gated recurrent model where Hung et al. (2018) evaluated agents with different recurrent the UR gates provide consistent improvement. cores: basic LSTM, LSTM+Mem (an LSTM with memory), and RMA (which also uses an LSTM core), and found the Finally, we would like to comment on the longevity of standard LSTM was not able to solve these tasks. We mod- the LSTM, which for example was frequently found to ified the LSTM agent with our gate mechanisms. Figure 8 outperform newer competitors when better tuned (Melis shows the results of different methods on the Passive match et al., 2017; Merity, 2019). Although many improvements and Active match tasks with distractors. These tasks are have been suggested over the years, none have been proven to structurally similar to the synthetic tasks (Sec. 5.1) requiring be as robust as the LSTM across an enormously diverse range retrieval of a memory over hundreds of steps to solve the of sequence modeling tasks. By experimentally starting from
Improving the Gating Mechanism of Recurrent Neural Networks well-tuned LSTM baselines, we believe our simple isolated Cho, K., Van Merrie¨nboer, B., Gulcehre, C., Bahdanau, gate modifications to actually be robust improvements. In D., Bougares, F., Schwenk, H., and Bengio, Y. Learning Appendix B.3 and B.4, we offer a few conclusions for the phrase representations using RNN encoder-decoder practitioner about the other gate components considered for statistical machine translation. arXiv preprint based on our experimental experience. arXiv:1406.1078, 2014. Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical 6. Discussion evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. In this work, we introduce and evaluate several modifications to the ubiquitous gating mechanism that appears in recurrent Chung, J., Ahn, S., and Bengio, Y. Hierarchical mul- neural networks. We describe methods that improve tiscale recurrent neural networks. arXiv preprint on the standard gating method by alleviating problems arXiv:1609.01704, 2016. with initialization and optimization. The mechanisms Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, considered include changes on independent axes, namely Q. V., and Salakhutdinov, R. Transformer-xl: Attentive initialization/activations and auxiliary gates, and we perform language models beyond a fixed-length context. arXiv extensive ablations on our improvements with previously preprint arXiv:1901.02860, 2019. considered modifications. Our main gate model robustly improves on standard gates across many different tasks and Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, recurrent cores, while requiring less tuning. Finally, we V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I., emphasize that these improvements are entirely independent et al. Impala: Scalable distributed deep-rl with importance of the large body of research on neural network architectures weighted actor-learner architectures. In International that use gates, and hope that these insights can be applied Conference on Machine Learning, pp. 1406–1415, 2018. to improve machine learning models at large. Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. References Greff, K., Srivastava, R. K., Koutn´ık, J., Steunebrink, B. R., Arjovsky, M., Shah, A., and Bengio, Y. Unitary evolution and Schmidhuber, J. LSTM: A search space odyssey. recurrent neural networks. In International Conference IEEE transactions on neural networks and learning on Machine Learning, pp. 1120–1128, 2016. systems, 28(10):2222–2232, 2016. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine Gulcehre, C., Moczulski, M., Denil, M., and Bengio, Y. translation by jointly learning to align and translate. arXiv Noisy activation functions. In International conference preprint arXiv:1409.0473, 2014. on machine learning, pp. 3059–3068, 2016. Bai, S., Kolter, J. Z., and Koltun, V. An empirical evaluation Gulcehre, C., Chandar, S., and Bengio, Y. Memory of generic convolutional and recurrent networks for augmented neural networks with wormhole connections. sequence modeling. arXiv preprint arXiv:1803.01271, arXiv preprint arXiv:1701.08718, 2017. 2018a. Henaff, M., Szlam, A., and LeCun, Y. Recurrent orthogonal Bai, S., Kolter, J. Z., and Koltun, V. Trellis networks for networks and long-memory tasks. arXiv preprint sequence modeling. arXiv preprint arXiv:1810.06682, arXiv:1602.06662, 2016. 2018b. Hochreiter, S. Untersuchungen zu dynamischen neuronalen Bengio, Y., Simard, P., Frasconi, P., et al. Learning long-term netzen. Diploma, Technische Universita¨t Mu¨nchen, 91 dependencies with gradient descent is difficult. IEEE (1), 1991. transactions on neural networks, 5(2):157–166, 1994. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. Chandar, S., Sankar, C., Vorontsov, E., Kahou, S. E., and Bengio, Y. Towards non-saturating recurrent units for Hochreiter, S., Bengio, Y., Frasconi, P., Schmidhuber, J., modelling long-term dependencies. arXiv preprint et al. Gradient flow in recurrent nets: the difficulty of arXiv:1902.06704, 2019. learning long-term dependencies, 2001. Chang, S., Zhang, Y., Han, W., Yu, M., Guo, X., Tan, W., Cui, Hung, C.-C., Lillicrap, T., Abramson, J., Wu, Y., Mirza, X., Witbrock, M., Hasegawa-Johnson, M. A., and Huang, M., Carnevale, F., Ahuja, A., and Wayne, G. Optimizing T. S. Dilated recurrent neural networks. In Advances in agent behavior over long time scales by transporting value. Neural Information Processing Systems, pp. 77–87, 2017. arXiv preprint arXiv:1810.06721, 2018.
Improving the Gating Mechanism of Recurrent Neural Networks Jang, E., Gu, S., and Poole, B. Categorical reparam- Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., eterization with Gumbel-softmax. arXiv preprint Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous arXiv:1611.01144, 2016. methods for deep supervised learning. In International conference on machine learning, pp. 1928–1937, 2016. Jozefowicz, R., Zaremba, W., and Sutskever, I. An empirical exploration of recurrent network architectures. In Oh, J., Chockalingam, V., Singh, S., and Lee, H. Control of International Conference on Machine Learning, pp. memory, active perception, and action in Minecraft. arXiv 2342–2350, 2015. preprint arXiv:1605.09128, 2016. Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Dabney, W. Recurrent experience replay in distributed Sutskever, I. Language models are unsupervised multitask supervised learning. In The International Conference learners. OpenAI Blog, 1(8), 2019. on Learning Representations (ICLR), 2018. Rae, J. W., Dyer, C., Dayan, P., and Lillicrap, T. P. Fast Kingma, D. P. and Ba, J. Adam: A method for stochastic parametric learning with activation memorization. arXiv optimization. arXiv preprint arXiv:1412.6980, 2014. preprint arXiv:1803.10049, 2018. Kocˇisky`, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski, K. M., Melis, G., and Grefenstette, E. The NarrativeQA M., Weber, T., Wierstra, D., Vinyals, O., Pascanu, R., and reading comprehension challenge. Transactions of the Lillicrap, T. Relational recurrent neural networks. In Association for Computational Linguistics, 6:317–328, Advances in Neural Information Processing Systems, pp. 2018. 7299–7310, 2018. Koutnik, J., Greff, K., Gomez, F., and Schmidhuber, J. A Shen, Y., Tan, S., Sordoni, A., and Courville, A. Ordered clockwork rnn. arXiv preprint arXiv:1402.3511, 2014. neurons: Integrating tree structures into recurrent neural networks. arXiv preprint arXiv:1810.09536, 2018. Krueger, D., Maharaj, T., Krama´r, J., Pezeshki, M., Ballas, N., Ke, N. R., Goyal, A., Bengio, Y., Courville, A., and Pal, C. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Zoneout: Regularizing RNNs by randomly preserving hid- and Salakhutdinov, R. Dropout: a simple way to prevent den activations. arXiv preprint arXiv:1606.01305, 2016. neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014. Le, Q. V., Jaitly, N., and Hinton, G. E. A simple way to initialize recurrent networks of rectified linear units. arXiv Tallec, C. and Ollivier, Y. Can recurrent neural networks preprint arXiv:1504.00941, 2015. warp time? arXiv preprint arXiv:1804.11188, 2018. Li, S., Li, W., Cook, C., Zhu, C., and Gao, Y. Independently Trinh, T. H., Dai, A. M., Luong, M.-T., and Le, Q. V. recurrent neural network (IndRNN): Building a longer and Learning longer-term dependencies in RNNs with deeper RNN. In Proceedings of the IEEE Conference on auxiliary losses. arXiv preprint arXiv:1803.00144, 2018. Computer Vision and Pattern Recognition, pp. 5457–5466, van der Westhuizen, J. and Lasenby, J. The unreason- 2018a. able effectiveness of the forget gate. arXiv preprint Li, Z., He, D., Tian, F., Chen, W., Qin, T., Wang, L., and arXiv:1804.04849, 2018. Liu, T.-Y. Towards binary-valued gates for robust LSTM Wayne, G., Hung, C.-C., Amos, D., Mirza, M., Ahuja, A., training. arXiv preprint arXiv:1806.02988, 2018b. Grabska-Barwinska, A., Rae, J., Mirowski, P., Leibo, J. Z., Maddison, C. J., Mnih, A., and Teh, Y. W. The concrete Santoro, A., et al. Unsupervised predictive memory in distribution: A continuous relaxation of discrete random a goal-directed agent. arXiv preprint arXiv:1803.10760, variables. arXiv preprint arXiv:1611.00712, 2016. 2018. Melis, G., Dyer, C., and Blunsom, P. On the state of the art Weston, J., Chopra, S., and Bordes, A. Memory networks. of evaluation in neural language models. arXiv preprint arXiv preprint arXiv:1410.3916, 2014. arXiv:1707.05589, 2017. Zaremba, W. and Sutskever, I. Learning to execute. arXiv Merity, S. Single headed attention rnn: Stop thinking with preprint arXiv:1410.4615, 2014. your head. arXiv preprint arXiv:1911.11423, 2019. Zilly, J. G., Srivastava, R. K., Koutn´ık, J., and Schmidhuber, Merity, S., Xiong, C., Bradbury, J., and Socher, R. J. Recurrent highway networks. In Proceedings of the 34th Pointer sentinel mixture models. arXiv preprint International Conference on Machine Learning-Volume arXiv:1609.07843, 2016. 70, pp. 4189–4198. JMLR. org, 2017.
Improving the Gating Mechanism of Recurrent Neural Networks A. Pseudocode from discrete instead of continuous gates. Additionally they require more involved training protocols with an additional We show how the gated update in a typical LSTM temperature hyperparameter that needs to be tuned explicitly. implementation can be easily replaced by UR- gates. Alternatively, gates can be removed entirely if strong con- The following snippets show pseudocode for the the gated straints are imposed on other parts of the model. (Li et al., state updates for a vanilla LSTM model (top) and UR-LSTM 2018a) use diagonal weight matrices and require stacked (bottom). RNN layers to combine information between hidden units. A long line of work has investigated the use of identity or orthog- forget_bias = 1.0 # hyperparameter onal initializations and constraints on the recurrent weights ... f, i, u, o = Linear(x, prev_hidden) to control multiplicative gradients unrolled through time (Le f_ = sigmoid(f + forget_bias) et al., 2015; Arjovsky et al., 2016; Henaff et al., 2016). (Chan- i_ = sigmoid(i) dar et al., 2019) proposed another RNN architecture using next_cell = f_ * prev_cell + i_ * tanh(u) additive state updates and non-saturating activation func- next_hidden = sigmoid(o) * tanh(next_cell) tions instead of gates. However, although these gate-less techniques can be used to alleviate the vanishing gradient Listing 1: LSTM problem with RNNs, unbounded activation functions can cause less stable learning dynamics and exploding gradients. As mentioned, a particular consequence of the inability of # Initialization u = np.random.uniform(low=1/hidden_size, gates to approach extrema is that gated recurrent models high=1-1/hidden_size, struggle to capture very long dependencies. These problems size=hidden_size) have traditionally been addressed by introducing new forget_bias = -np.log(1/u-1) components to the basic RNN setup. Some techniques ... include stacking layers in a hierarchy (Chung et al., 2016), # Recurrent update f, r, u, o = Linear(x, prev_hidden) adding skip connections and dilations (Koutnik et al., 2014; f_ = sigmoid(f + forget_bias) Chang et al., 2017), using an external memory (Graves et al., r_ = sigmoid(r - forget_bias) 2014; Weston et al., 2014; Wayne et al., 2018; Gulcehre g = 2*r_*f_ + (1-2*r_)*f_**2 et al., 2017), auxiliary semi-supervision (Trinh et al., 2018), next_cell = g * prev_cell + (1-g) * tanh(u) and more. However, these approaches have not been widely next_hidden = sigmoid(o) * tanh(next_cell) adopted over the standard LSTM as they are often specialized for certain tasks, are not as robust, and introduce additional Listing 2: UR-LSTM complexity. Recently the transformer model has been successful in many applications areas such as NLP (Radford et al., 2019; Dai et al., 2019). However, recurrent neural net- B. Further discussion on related methods works are still important and commonly used due their faster inference without the need to maintain the entire sequence Section 4 briefly introduced chrono initialization (Tallec in memory. We emphasize that the vast majority of proposed & Ollivier, 2018) and the ON-LSTM (Shen et al., 2018), RNN changes are completely orthogonal to the simple gate closely related methods that modify the gating mechanism of improvements in this work, and we do not focus on them. LSTMs. We provide more detailed discussion on these in Sec- tions B.3 and B.4 respectively. Section B.1 has a more thor- A few other recurrent cores that use the basic gated update (1) ough overview of related work on recurrent neural networks but use more sophisticated update functions u include the that address long-term dependencies or saturating gates. GRU, Reconstructive Memory Agent (RMA; Hung et al., 2018), and Relational Memory Core (RMC; Santoro et al., 2018), which we consider in our experiments. B.1. Related Work Several methods exist for addressing gate saturation or allow- B.2. Effect of proposed methods on timescales ing more binary activations. Gulcehre et al. (2016) proposed to use piece-wise linear functions with noise in order to allow We briefly review the connection between our methods and the gates to operate in saturated regimes. Li et al. (2018b) in- the effective timescales that gated RNNs capture. Recall stead use the Gumbel trick (Maddison et al., 2016; Jang et al., that Section 3.2 defines the characteristic timescale of a 2016), a technique for learning discrete variables within a neu- neuron with forget activation f t as 1/(1−f t), which would ral network, to train LSTM models with discrete gates. These be the number of timesteps it takes to decay that neuron by stochastic approaches can suffer from issues such as gradient a constant. estimation bias, unstable training, and limited expressivity
Improving the Gating Mechanism of Recurrent Neural Networks The fundamental principle of gated RNNs is that the A different workaround suggested by Tallec & Ollivier activations of the gates affects the timescales that the model (2018) is to sample from P(T = k) ∝ 1 and setting klog2(k+1) can address; for example, forget gate activations near 1.0 are b = log(T ). Note that such an initialization would be almost f necessary to capture long-term dependencies. equivalent to sampling the decay period from the distribution with density P(D = x) ∝ (x log2 x)−1 (since the decay Thus, although our methods were defined in terms of acti- period is (1 − f )−1 = 1 + exp(b )). This parameter-free vations g , it is illustrative to reason with their characteristic f t initialization is thus similar in spirit to the uniform gate timescales 1/(1−g ) instead, whence both UGI and refine t initialization (Proposition 1), but from a much heavier-tailed gate also have clean interpretations. distribution that emphasizes very long-term dependencies. First, UGI is equivalent to initializing the decay period from These interpretations suggest that it is plausible to define a particular heavy-tailed distribution, in contrast to standard initialization with a fixed decay period (1−σ(b ))−1. a family of Pareto-like distributions from which to draw f the initial decay periods from, with this distribution treated Proposition 1. UGI is equivalent to to sampling the decay as a hyperparameter. However, with no additional prior period D = 1/(1 − f ) from a distribution with density t information on the task, we believe the uniform gate proportional to P(D = x) ∝ d (1 − 1/x) = x−2, i.e. a dx initialization to be the best candidate, as it 1. is a simple Pareto(α = 2) distribution. distribution with easy implementation, 2. has characteristic timescale distributed as an intermediate balance between On the other hand, for any forget gate activation f with t the heavy-tailed chrono initialization and sharply decaying timescale D = 1/(1 − f ), the refine gate fine-tunes it be- t standard initialization, and 3. is similar to the ON-LSTM’s tween D = 1/(1−f 2) = 1/(1−f )(1+f ) and 1/(1−f )2. t t t t cumax activation, in particular matching the initialization Proposition 2. Given a forget gate activation with timescale distribution of the cumax activation. D, the refine gate creates an effective forget gate with Table 4 summarizes the decay period distributions at timescale in (D/2, D2). initialization using different activations and initialization strategies. B.3. Chrono Initialization In general, our experimental recommendation for CI is that The chrono initialization it can be better than standard initialization or UGI when certain conditions are met (tasks with long dependencies and b ∼ log(U([1,T −1])) (14) f max nearly fixed-length sequences as in Sections 5.1, 5.4) and/or b i = −b f . (15) when it can be explicitly tuned (both the hyperparameter T , as well as the learning rate to compensate for almost max was the first to explicitly attempt to initialize the activation all units starting in saturation). Otherwise, we recommend of gates across a distributional range. It was motivated by UGI or standard initialization. We found no scenarios where matching the gate activations to the desired timescales. it outperformed UR- gates. They also elucidate the benefits of tying the input and forget gates, leading to the simple trick (15) for approximating B.4. ON-LSTM tying the gates at initialization, which we borrow for In this section we elaborate on the connection between the UGI. (We remark that perfect tied initialization can be mechanism of (Shen et al., 2018) and our methods. We define accomplished by fully tying the linear maps L ,L , but (15) f i the full ON-LSTM and show how its gating mechanisms can is a good approximation.) be improved. For example, there is a remarkable connection However, the main drawback of CI is that the initialization between its master gates and our refine gates – independently distribution is too heavily biased toward large terms. of the derivation of refine gates in Section 3.4, we show how This leads to empirical consequences such as difficult a specific way of fixing the normalization of master gates tuning (due to most units starting in the saturation regime, becomes equivalent to a single refine gate. requiring different learning rates) and high sensitivity to the First, we formally define the full ON-LSTM. The master hyperparameter T that represents the maximum potential max gates are a cumax-activation gate length of dependencies. For example, Tallec & Ollivier (2018) set this parameter according to a different protocol f˜ = cumax(L (x ,h )) (16) t f˜ t t−1 for every task, with values ranging from 8 to 2000. Our ˜i = 1−cumax(L (x ,h )). (17) experiments used a hyperparameter-free method to initialize t ˜i t t−1 T (Section 5), and we found that chrono initialization These combine with an independent pair of forget and input max generally severely over-emphasizes long-term dependencies gates f ,i , meant to control fine-grained behavior, to create t t if T is not carefully controlled. an effective forget/input gate fˆ ,ˆi which are used to update max t t
Improving the Gating Mechanism of Recurrent Neural Networks Table 4. Distribution of the decay period D = (1−f)−1 using different initialization strategies. Initialization method Timescale distribution Constant bias b = b P(D = x) ∝ 1{x = 1+eb} f Chrono initialization (known timescale T ) P(D = x) ∝ 1{x ∈ [2,T ]} max max Chrono initialization (unknown timescale) P(D = x) ∝ 1 xlog2x Uniform gate initialization P(D = x) ∝ 1 x2 cumax activation P(D = x) ∝ 1 x2 the state (equation (1) or (5)). Master gates We observe that the magnitudes of master gates are suboptimally normalized. A nice interpretation of ω = f˜ ◦˜i (18) gated recurrent models shows that they are a discretization t t t of a continuous differential equation. This leads to the leaky fˆ = f ◦ω +(f˜ −ω ) (19) t t t t t RNN model h = (1−α)h +αu , where u is the update t+1 t t t ˆi t = i t ◦ω t +(˜i t −ω t). (20) to the model such as tanh(W xx t +W hh t +b). Learning α as a function of the current time step leads to the simplest gated recurrent model5 As mentioned in Section B.1, this model modifies the standard forget/input gates in two main ways, namely f = σ(L (x ,h )) t f t t−1 ordering the gates via the cumax activation, and supplying u = tanh(L (x ,h )) t u t t−1 an auxiliary set of gates controlling fine-grained behavior. h = f h +(1−f )u . Both of these are important novelties and together allow t t t−1 t t recurrent models to better capture tree structures. Tallec & Ollivier (2018) show that this exactly corresponds However, the UGI and refine gate can be viewed as to the discretization of a differential equation that is invariant improvements over each of these, respectively, demonstrated to time warpings and time rescalings. In the context of both theoretically (below) and empirically (Sections 5 the LSTM, this interpretation requires the values of the and E.3), even on tasks involving hierarchical sequences. forget and input gates to be tied so that f + i = 1. This t t weight-tying is often enforced, for example in the most popular LSTM variant, the GRU (Cho et al., 2014), or our Ordered gates Despite having the same parameter count UR- gates. In a large-scale LSTM architecture search, it and asymptotic efficiency as standard sigmoid gates, cumax was found that removing the input gate was not significantly gates seem noticeably slower and less stable in practice detrimental (Greff et al., 2016). for large hidden sizes. Additionally, using auxiliary master gates creates additional parameters compared to the basic However, the ON-LSTM does not satisfy this conventional LSTM. Shen et al. (2018) alleviated both of these problems wisdom that the input and forget gates should sum to close by defining a downsize operation, whereby neurons are to 1. grouped in chunks of size C, each of which share the same Proposition 3. At initialization, the expected value of the master gate values. However, this also creates an additional average effective forget gate activation fˆ is 5/6. hyperparameter. t The speed and stability issues can be fixed by just using the Let us consider the sum of the effective forget and input sigmoid non-linearity instead of cumax. To recover the most gates at initialization. Adding equations (19) and (20) yields important properties of the cumax—activations at multiple timescales—the equivalent sigmoid gate can be initialized so fˆ +ˆi = (f +i )◦ω +(f˜ +˜i −2ω ) t t t t t t t t as to match the distribution of cumax gates at initialization. = f˜ +˜i +(f +i −2)◦ω . t t t t t This is just uniform gate initialization (equation (12)). However, we believe that the cumax activation is still Note that the master gates (16), (17) sum 1 in expectation valuable in many situations if speed and instability are not at initialization, as do the original forget and input gates. issues. These include when the hidden size is small, when Looking at individual units in the ordered master gates, we extremal gate activations are desired, or when ordering 5In the literature, this is called the JANET (van der Westhuizen needs to be strictly enforced to induce explicit hierarchical & Lasenby, 2018), which is also equivalent to the GRU without a structure. For example, Section (5.1) shows that they can reset gate (Chung et al., 2014), or a recurrent highway network with solve hard memory tasks by themselves. depth L = 1 (Zilly et al., 2017).
Improving the Gating Mechanism of Recurrent Neural Networks have Efˆ(j) = j ,Eˆi(j) = 1− j . Thus the above simplifies to effective input gate ˆi (24) is also defined through a refine n n t gate mechanism, where ˜i = 1−f˜ is refined by i : t t t E[fˆ +ˆi ] = 1−Eω t t t ˆi = i·(1−(1−˜i))2 +(1−i)·˜i2. j j E[fˆ(j) +ˆi(j)] = 1− (1− ) t t n n (cid:104) (cid:105) (cid:90) 1 (cid:90) 1 Based on our experimental findings, in general we would E E j∈[n]fˆ t(j) +ˆi( tj) ≈ 1− xdx+ x2dx recommend the refine gate in place of the master gate. 0 0 5 = . B.5. Gate ablation details 6 For clarity, we formally define the gate ablations considered The gate normalization can be fixed by re-scaling equa- which mix and match different gate components. tions (19) and (20). It turns out that tying the master gates We remark that other combinations are possible, for example and re-scaling is exactly equivalent to the mechanism of a combining CI with either auxiliary gate type, which would refine gate. In this equivalence, the role of the master and lead to CR- or CM- gates. Alternatively, the master or refine forget gates of the ON-LSTM are played by our forget and gates could be defined using different activation and initial- refine gate respectively. ization strategies. We chose not to consider these methods Proposition 4. Suppose the master gates f˜ t,˜i t are tied and due to lack of interpretation and theoretical soundness. the equations (19)-(20) defining the effective gates fˆ ,ˆi are t t rescaled such as to ensure E[fˆ t + ˆi t] = 1 at initialization. O- This ablation uses the cumax activation to order the The resulting gate mechanism is exactly equivalent to that forget/input gates and has no auxiliary gates. of the refine gate. f = cumax(L (x ,h )) (25) t f t t−1 Consider the following set of equations where the master i = 1−cumax(L (x ,h )). (26) gates are tied (f˜ + ˜i = 1, f + i = 1) and (19)-(20) are t i t t−1 t t t t modified with an extra coefficient (rescaling in bold): We note that one difficulty with this in practice is the reliance on the expensive cumax, and hypothesize that this is perhaps ˜i t = 1−f˜ t (21) the ON-LSTM’s original motivation for the second set of ω = f˜ ·˜i (22) gates combined with downsizing. t t t fˆ = 2·f ·ω +(f˜ −ω ) (23) t t t t t UM- This variant of the ON-LSTM ablates the cumax ˆi = 2·i ·ω +(˜i −ω ) (24) operation on the master gates, replacing it with a sigmoid t t t t t activation initialized with UGI. Equations (16), (17) are replaced with Now we have fˆ +ˆi = f˜ +˜i +2(f +i −1)·ω u = U(0,1) (27) t t t t t t t b = σ−1(u) (28) = 1+2(f +i −1)·ω f t t t f˜ = σ(L (x ,h )+b ) (29) t f˜ t t−1 f which has the correct scaling, i.e. E[fˆ t + ˆi t] = 1 at ˜i = σ(L (x ,h )−b ) (30) initialization assuming that E[f +i ] = 1 at initialization. t ˜i t t−1 f t t But (23) can be rewritten as follows: Equations (18)-(20) are then used to define effective gates fˆ ,ˆi which are used in the gated update (1) or (5). fˆ= 2·f ·ω+(f˜−ω) t t = 2·f ·f˜·(1−f˜)+(f˜−f˜·(1−f˜)) OR- This ablation combines ordered main gates with an = 2f ·f˜−2f ·f˜2 +f˜2 auxilliary refine gate. = f ·2f˜−f ·f˜2 −f ·f˜2 +f˜2 f˜ = cumax(L (x ,h )+b ) (31) t f˜ t t−1 f = f ·(1−(1−f˜))2 +(1−f )·f˜2. r = σ(L (x , h )+b ) (32) t r t t−1 r g = r ·(1−(1−f )2)+(1−r )·f 2 (33) t t t t t This is equivalent to the refine gate, where the master gate i = 1−g (34) t t plays the role of the forget gate and the forget gate plays the role of the refine gate. It can be shown that in this case, the g ,i are used as the effective forget and input gates. t t
Improving the Gating Mechanism of Recurrent Neural Networks C. Analysis Details contrast, this task is usually defined with the model being required to output a dummy token at the first N +10 steps, The gradient analysis in Figure 3 was constructed as follows. meaning it can be hard to evaluate performance since low Let f, r, g be the forget, refine, and effective gates average losses simply indicate that the model learns to output g = 2rf +(1−2r)f 2. the dummy token. For Figure 4, the log loss curves show the median of 3 seeds, Letting x,y be the pre-activations of the sigmoids on f and and the error bars indicate 60% confidence. r, the gradient of g can be calculated as For Figure 5, each histogram represents the distribution ∇ xg = 2rf (1−f )+(1−2r)(2f )(f (1−f )) of forget gate values of the hidden units (of which there = 2f (1−f )[r+(1−2r)f ] are 256). The values are created by averaging units over time and samples, i.e., reducing a minibatch of forget ∇ g = 2f r(1−r)+(−2f 2)r(1−r) = 2f r(1−r)(1−f ) y gate activations of shape (batch size, sequence (cid:107)∇g(cid:107)2 = [2f (1−f )]2(cid:2) (r+f −2f r)2 +r2(1−r)2(cid:3) . length, hidden size) over the first two diensions, to produce the average activation value for every unit. Substituting the relation g−f 2 D.2. Image Classification r = , 2f (1−f ) All models used a single hidden layer recurrent network (LSTM or GRU). Inputs x to the model were given in batches this reduces to the Equation 35, as a sequence of shape (sequence length, num (cid:107)∇g(cid:107)2 = ((g−f 2)(1−2f )+2f 2(1−f ))2 channels), (e.g. (1024,3) for CIFAR-10), by flattening the input image left-to-right, top-to-bottom. The outputs (cid:18) g−f 2 (cid:19)2 +(g−f 2)2 1− . (35) of the model of shape (sequence length, hidden 2f (1−f ) size) were processed independently with a single ReLU hidden layer of size 256 before the final fully-connected Given the constraint f 2 ≤ g ≤ 1−(1−f )2, this function can layer outputting softmax logits. All training was performed be minimized and maximized in terms of g to produce the with the Adam optimizer, batch size 50, and gradients upper and lower bounds in Figure 3b. This was performed clipped at 1.0. MNIST trained for 150 epochs, CIFAR-10 numerically. used 100 epochs over the training set. D. Experimental Details Table 5 All models (LSTM and GRU) used hidden state size 512. Learning rate swept in To normalize the number of parameters used for models {2e−4,5e−4,1e−3,2e−3} with three seeds each. using master gates, i.e. the OM- and UM- gating mechanisms, Table 5 reports the highest validation score found. The GRU we used a downsize factor on the main gates (see Sec- model swept over learning rates {2e−4,5e−4}; all methods tion B.4). This was set to C = 16 for the synthetic and image were unstable at higher learning rates. classification tasks, and C = 32 for the language modeling and program execution tasks which used larger hidden sizes. Figure 6 shows the median validation F1-score with quartiles (25/75% confidence intervals) over the seeds, for the D.1. Synthetic Tasks best-performing stable learning rate (i.e. the one with highest average validation score on the final epoch). This was All models consisted of single layer LSTMs with 256 hidden generally 5e−4 or 1e−3, with refine gate variants tending units, trained with the Adam optimizer (Kingma & Ba, 2014) to allow higher learning rates. with learning rate 1e-3. Gradients were clipped at 1.0. The training data consisted of randomly generated sequences Table 2 The UR-LSTM and UR-GRU used 1024 hidden for every minibatch rather than iterating through a fixed units for the sequential and permuted MNIST task, and 2048 dataset. Each method ran 3 seeds, with the same training hidden units for the sequential CIFAR task. The vanilla data for every method. LSTM baseline used 512 hidden units for MNIST and 1024 for CIFAR. Larger hidden sizes were found to be unstable. Our version of the Copy task is a very minor variant of other versions reported in the literature, with the main difference Zoneout parameters were fixed to reasonable default settings being that the loss is considered only over the last 10 output based on Krueger et al. (2016), which are z = 0.5,z = 0.05 c h tokens which need to be memorized. This normalizes the for LSTM and z = 0.1 for GRU. When zoneout was used, loss so that losses approaching 0 indicate true progress. In standard Dropout (Srivastava et al., 2014) with probability
Improving the Gating Mechanism of Recurrent Neural Networks Table 5. Gate ablations on pixel-by-pixel image classification. Validation accuracies on pixel image classification. Asterisks denote divergent runs at the learning rate the best validation score was found at. Gating Method - C- O- U- R- OM- OR- UM- UR- pMNIST 94.77∗∗ 94.69 96.17 96.05 95.84∗ 95.98 96.40 95.50 96.43 sCIFAR 63.24∗∗ 65.60 67.78 67.63 71.85∗ 67.73∗ 70.41 67.29∗ 71.05 sCIFAR (GRU) 71.30∗ 64.61 69.81∗∗ 70.10 70.74∗ 70.20∗ 71.40∗∗ 69.17∗ 71.04 0.5 was also applied to the output classification hidden layer. D.3. Language Modeling Hyperparameters are taken from Rae et al. (2018) tuned for the vanilla LSTM, which consist of (chosen parameter bolded out of sweep): {1, 2} LSTM layer, {0.0, 0.1, 0.2, 0.3} embedding dropout, {yes, no} layer norm, and {shared, not shared} input/output embedding parameters. Our only divergence is using a hidden size of 3072 instead of 2048, which we found improved the performance of the vanilla LSTM. Training was performed with Adam at learning rate 1e-3, gradients clipped to 0.1, sequence length 128, and batch size 128 on TPU. The LSTM state was reset between article boundaries. Figure 11 shows smoothed validation perplexity curves show- ing the 95% confidence intervals over the last 1% of data. 10 7.5 5 2.5 0 0 1e10 2e10 3e10 4e10 Episode Steps draweR Core C-LSTM LSTM U-LSTM UR-LSTM (a) Passive Match without Distractor Rewards 5.5 5 4.5 4 3.5 3 0 1e10 2e10 3e10 4e10 Episode Steps draweR 8 6 4 0 1e10 2e10 3e10 4e10 Episode Steps Core C-LSTM LSTM U-LSTM UR-LSTM (b) Active Match without Distractor Rewards Figure 9. Performance on supervised learning Tasks that Require Memory. We evaluated the image matching tasks from Hung et al. (2018), which test memorization and credit assign- ment, using an A3C agent (Mnih et al., 2016) with an LSTM policy core. We observe that general trends from the synthetic tasks (Sec- tion (5.1)) transfer to this supervised learning setting. supervised learning The Active Match and Passive Match tasks were borrowed from Hung et al. (2018) with the same settings. For Figures 9 and 13, the discount factor in the environment was set to γ = .96. For Figure 10, the discount factor was γ = .998. Figure 13 corresponds to the full Active draweR Core C-LSTM LSTM U-LSTM UR-LSTM (a) Active Match with Distractor Rewards - LSTM 10 8 6 4 0 0.25e10 0.50e10 0.75e10 1e10 1.25e10 Episode Steps draweR Core C-RMA RMA U-RMA UR-RMA (b) Active Match with Distractor Rewards - RMA Figure 10. The addition of distractor rewards changes the task and relative performance of different gating mechanisms. For both LSTM and RMA recurrent cores, the UR- gates still perform best. Match task in Hung et al. (2018), while Figure 10 is their version with small distractor rewards where the apples in the distractor phase give 1 instead of 5 reward. Figure 8 used 5 seeds per method. D.4. Program Evaluation Protocol was taken from Santoro et al. (2018) with minor changes to the hyperparameter search. All models were trained with the Adam optimizer, the Mix curriculum strategy from Zaremba & Sutskever (2014), and batch size 128. RMC: The RMC models used a fixed memory slot size of 512 and swept over {2, 4} memories and {2, 4} attention heads for a total memory size of 1024 or 2048. They were trained for 2e5 iterations. LSTM: Instead of two-layer LSTMs with sweeps over skip connections and output concatenation, single-layer LSTMs of size 1024 or 2048 were used. Learning rate was swept in {5e-4, 1e-3}, and models were trained for 5e5 iterations. Note that training was still faster than the RMC models despite the greater number of iterations.
Improving the Gating Mechanism of Recurrent Neural Networks D.5. Additional Details E. Additional Experiments Implementation Details The inverse sigmoid func- E.1. Synthetic Forgetting tion (12) can be unstable if the input is too close to {0,1}. Figure 5 on the Copy task demonstrates that extremal gate Uniform gate initialization was instead implemented by activations are necessary to solve the task, and initializing sampling from the distribution U[1/d,1 − 1/d] instead of the activations near 1.0 is helpful. U[0, 1], where d is the hidden size, to avoid any potential numerical edge cases. This choice is justified by the fact that This raises the question: what happens if the initialization with perfect uniform sampling, the expected smallest and distribution does not match the task at hand; could the gates largest samples would be 1/(d+1) and 1−1/(d+1). learn back to a more moderate regime? We point out that such a phenomenon could occur non-pathologically on more For distributional initialization strategies, a trainable complex setups, such as a scenario where a model trains to bias vector was sampled independently from the chosen remember on a Copy-like task and then needs to “unlearn” distribution (i.e. equation (14) or (12)) and added/subtracted as part of a meta-learning or continual learning setup. to the forget and input gate ((2)-(3)) before the non-linearity. Additionally, each linear model such as W xf x t +W hf h t−1 Here, we consider such a synthetic scenario and experimen- had its own trainable bias vector, effectively doubling the tally show that the addition of a refine gate helps models train learning rate on the pre-activation bias terms on the forget much faster while in a saturated regime with extremal activa- and input gates. This was an artifact of implementation and tions. We also point to the poor performance of C- outside of not intended to affect performance. synthetic memory tasks when using our high hyperparameter- free initialization as more evidence that it is very difficult The refine gate update equation (10) can instead be for standard gates to unlearn undesired saturated behavior. implemented as For this experiment, we initialize the biases of the gates g = r ·(1−(1−f )2)+(1−r )·f 2 extremely high (effective forget activation ≈ σ(6). We then t t t t t consider the Adding task (Section 5.1) of length 500, hidden = 2r ·f +(1−2r )·f 2 t t t t size 64, learning rate 1e-4. The R-LSTM is able to solve the task, while the LSTM is stuck after 1e4 iterations. Permuted image classification In an effort to standardize the permutation used in the Permuted MNIST benchmark, 3.60 we use a particular deterministic permutation rather than a random one. After flattening the input image into a one- 3.58 dimensional sequence, we apply the bit reversal permutation. This permutation sends the index i to the index j such 3.56 that j’s binary representation is the reverse of i’s binary representation. The intuition is that if two indices i,i(cid:48) are 3.54 close, they must differ in their lower-order bits. Then the bit- reversed indices will be far apart. Therefore the bit-reversal 3.52 permutation destroys spatial and temporal locality, which 0 100000 200000 300000 400000 500000 Iteration is desirable for these sequence classification tasks meant to test long-range dependencies rather than local structure. def bitreversal_po2(n): m = int(math.log(n) / math.log(2)) perm = np.arange(n).reshape(n, 1) for i in range(m): n1 = perm.shape[0] // 2 perm = np.hstack((perm[:n1], perm[n1:])) return perm.squeeze(0) def bitreversal_permutation(n): m = int(math.ceil(math.log(n) / math.log(2))) N = 1 << m perm = bitreversal_po2(N) return np.extract(perm < n, perm) Listing 3: Bit-reversal permutation for permuted MNIST. ytixelpreP goL Method C-LSTM C11-LSTM C8-LSTM LSTM OM-LSTM U-LSTM UR-LSTM Figure 11. Validation learning curves, illustrating training speed and generalization (i.e. overfitting) behavior. E.2. supervised learning Figures 9 and 10 evaluated our gating methods with the LSTM and RMA models on the Passive Match and Active Match tasks, with and without distractors. We additionally ran the agents on an even harder version of the Active Match task with larger distractor rewards (the full Active Match from Hung et al. (2018)). Learning curves are shown in Figure 13. Similarly to the other results, the UR- gated core is noticeably better than the others. For the DNC model, it is the only one that performs better than random chance.
Improving the Gating Mechanism of Recurrent Neural Networks 12 20 10 15 8 6 10 4 5 2 0 0 0.90 0.92 0.94 0.96 0.98 1.00 0.955 0.960 0.965 0.970 0.975 0.980 0.985 0.990 0.995 1.000 (a) R-LSTM (b) LSTM Figure 12. Distribution of forget gate activations after extremal ini- tialization, and training on the Adding task. The UR-LSTM is able to learn much faster in this saturated gate regime while the LSTM does not solve the task. The smallest forget unit for the UR-LSTM after training has characteristic timescale over an order of magnitude smaller than that of the LSTM. 5 4.5 4 3.5 3 0 1e10 2e10 3e10 4e10 Episode Steps draweR CoCr-eLSTM LSTM U-LSTM UR-LSTM 3.9 3.6 3.3 0 1e10 2e10 3e10 4e10 Episode Steps draweR • The task has highly variable sequence lengths, wherein the standard training procedure randomly samples inputs of varying lengths (called the ”Mix” curriculum in Zaremba & Sutskever (2014)). Additionally, the Control and Program tasks contain complex control flow and nested structure. They are thus a measure of a sequence model’s ability to model dependencies of differing lengths, as well as hierarchical information. Thus we are interested in comparing the effects of UGI methods, as well as the full OM- gates which are designed for hierarchical structures (Shen et al., 2018). Finally, this task has prior work using a different type of recurrent core, the Relational Memory Core (RMC), that we also use as a baseline to evaluate our gates on different models (Santoro et al., 2018). Both the LSTM and RMC were found to outperform other recurrent baselines such as the Differential Neural Computer (DNC) and EntNet. Training curves are shown in Figure 14, which plots the median F1-score with confidence intervals. We point out a few observations. First, despite having a T value on max the right order of magnitude, the C- gated methods have very CoCr-eLSTM+Mem poor performance across the board, reaffirming the chrono LSTM+Mem U U- RL US -T LM ST+ MM +e Mm em initialization’s high sensitivity to this hyperparameter. Second, the U-LSTM and U-RMC are the best methods on the Addition task. Additionally, the UR-RMC vs. RMC on Figure 13. The full Active Match task with large distractor rewards, Addition is one of the very few tasks we have found where using agents with LSTM or DNC recurrent cores. a generic substitution of the UR- gate does not improve on the basic gate. We have not investigated what property of this task caused these phenomena. E.3. Program Execution Aside from the U-LSTM on addition, the UR-LSTM The Learning to Execute (Zaremba & Sutskever, 2014) outperforms all other LSTM cores. The UR-RMC is also the dataset consists of algorithmic snippets from a programming best core on both Control and Program, the tasks involving language of pseudo-code. An input is a program from this hierarchical inputs and longer dependencies. For the most language presented one character at a time, and the target part, the improved mechanisms of the UR- gates seem to output is a numeric sequence of characters representing the transfer to this recurrent core as well. We highlight that this is execution output of the program. There are three categories not true of similar gating mechanisms. In particular, the OM- of tasks: Addition, Control, and Program, with distinctive LSTM, which is supposed to model hierarchies, has good types of input programs. We use the most difficult setting performance on Control and Program as expected (although from Zaremba & Sutskever (2014), which uses the param- not better than the UR-LSTM). However, the OM- gates’ eters nesting=4, length=9, referring to the nesting performance plummets when transferred to the RMC core. depth of control structure and base length of numeric literals, Interestingly, the -LSTM cores are consistently better than respectively. Examples of input programs are shown in previ- the -RMC versions, contrary to previous findings on easier ous works (Zaremba & Sutskever, 2014; Santoro et al., 2018). versions of this task using similar protocol and hyperparam- We are interested in this task for several reasons. First, we eters (Santoro et al., 2018). We did not explore different are interested in comparing against the C- and OM- gate hyperparameter regimes on this more difficult setting. methods, because • The maximum sequence length is fairly long (several hundred tokens), meaning our T heuristic for max C- gates is within the right order of magnitude of dependency lengths.
Improving the Gating Mechanism of Recurrent Neural Networks 0.5 0.4 0.3 0 2.5e5 5.0e5 7.5e5 1.0e6 ycaruccA Addition Control Program 1 0.5 0.75 Method -LSTM 0.4 C-LSTM OM-LSTM 0.50 0.3 U-LSTM UR-LSTM 0.2 0.25 0 2.5e5 5.0e5 7.5e5 1.0e6 0 2.5e5 5.0e5 7.5e5 1.0e6 Iteration (a) LSTM - Learning to Execute (nesting=4, length=9) 0.6 0.5 0.4 0.3 0 1.0e5 2.0e5 3.0e5 4.0e5 5.0e5 ycaruccA Addition Control Program 0.4 0.6 Method -RMC C-RMC 0.3 OM-RMC 0.4 U-RMC UR-RMC 0.2 0.2 0 1.0e5 2.0e5 3.0e5 4.0e5 5.0e5 0 1.0e5 2.0e5 3.0e5 4.0e5 5.0e5 Iteration (b) RMC - Learning to Execute (nesting=4, length=9) Figure 14. Program Execution evaluation accuracies.
