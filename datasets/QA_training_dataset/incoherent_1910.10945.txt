Fixed-Confidence Guarantees for Bayesian Best-Arm Identification Xuedong Shang1,2 Rianne de Heide3,4 Emilie Kaufmann1,2,5 Pierre Ménard1 Michal Valko1,6 1Inria Lille Nord Europe 2Université de Lille 3Leiden University 4CWI 5CNRS 6DeepMind Paris Abstract wards Y = (Y , · · · , Y ) is generated for all arms n n,1 n,K independently from past observations, but only Y is n,In revealed to the learner. Let F be the σ-algebra gen- We investigate and provide new insights on n erated by (U , I , Y , · · · , U , I , Y ), then I is the sampling rule called Top-Two Thompson 1 1 1,I1 n n n,In n F -measurable, i.e., it can only depend on the past Sampling (TTTS). In particular, we justify its n−1 n − 1 observations, and some exogeneous randomness, use for fixed-confidence best-arm identifica- materialized into U ∼ U([0, 1]). The second com- tion. We further propose a variant of TTTS n−1 ponent is a F -measurable recommendation rule J , called Top-Two Transportation Cost (T3C), n n which returns a guess for the best arm, and thirdly, which disposes of the computational burden the stopping rule τ , a stopping time with respect to of TTTS. As our main contribution, we pro- (F ) , decides when the exploration is over. vide the first sample complexity analysis of n n∈N TTTS and T3C when coupled with a very nat- BAI has already been studied within several theo- ural Bayesian stopping rule, for bandits with retical frameworks. In this paper we consider the Gaussian rewards, solving one of the open fixed-confidence setting, introduced by Even-dar et al. questions raised by Russo (2016). We also (2003), in which given a risk parameter δ, the goal is provide new posterior convergence results for to ensure that probability to stop and recommend a TTTS under two models that are commonly wrong arm, P [J (cid:54)= I(cid:63)], is smaller than δ, while mini- τ used in practice: bandits with Gaussian and mizing the expected total number of samples to make Bernoulli rewards and conjugate priors. this accurate recommendation, E [τ ]. The most stud- ied alternative is the fixed-budget setting for which the stopping rule τ is fixed to some (known) maximal bud- 1 Introduction get n, and the goal is to minimize the error probability P [J (cid:54)= I(cid:63)] (Audibert and Bubeck, 2010). Note that n In multi-armed bandits, a learner repeatedly chooses these two frameworks are very different in general and an arm to play, and receives a reward from the as- do not share transferable regret bounds (see Carpen- sociated unknown probability distribution. When the tier and Locatelli 2016 for an additional discussion). task is best-arm identification (BAI), the learner is not Most of the existing sampling rules for the fixed- only asked to sample an arm at each stage, but is also confidence setting depend on the risk parameter δ. asked to output a recommendation (i.e., a guess for Some of them rely on confidence intervals such as the arm with the largest mean reward) after a certain LUCB (Kalyanakrishnan et al., 2012), UGapE (Gabil- period. Unlike in another well-studied bandit setting, lon et al., 2012), or lil’UCB (Jamieson et al., the learner is not interested in maximizing the sum 2014); others are based on eliminations such as of rewards gathered during the exploration (or mini- SuccessiveElimination (Even-dar et al., 2003) and mizing regret), but only cares about the quality of her ExponentialGapElimination (Karnin et al., 2013). recommendation. As such, BAI is a particular pure The first known sampling rule for BAI that does not exploration setting (Bubeck et al., 2009). depend on δ is the tracking rule proposed by Garivier Formally, we consider a finite-arm bandit model, which and Kaufmann (2016), which is proved to achieve the is a collection of K probability distributions, called minimal sample complexity when combined with the arms A (cid:44) {1, . . . , K}, parametrized by their means Chernoff stopping rule when δ goes to zero. Such an µ 1, . . . , µ K. We assume the (unknown) best arm is anytime sampling rule (neither depending on a risk δ unique and we denote it by I(cid:63) (cid:44) arg max i µ i. A best- or a budget n) is very appealing for applications, as arm identification strategy (I n, J n, τ ) consists of three advocated by Jun and Nowak (2016), who introduce components. The first is a sampling rule, which selects the anytime best-arm identification framework. In this an arm I at round n. At each round n, a vector of re- n 9102 tcO 82 ]GL.sc[ 3v54901.0191:viXra
Preprint paper, we investigate another anytime sampling rule prove that optimal posterior convergence rates can be for BAI: Top-Two Thompson Sampling (TTTS), and obtained for those two as well. propose a second anytime sampling rule: Top-Two Transportation Cost (T3C). Outline In Section 2, we give a reminder of TTTS and introduce T3C along with our proposed recom- Thompson Sampling (Thompson, 1933) is a Bayesian mendation and stopping rules. Then, in Section 3, algorithm well known for regret minimization, for we describe in detail two important notions of opti- which it is now seen as a major competitor to UCB- mality that are invoked in this paper. The main fixed- typed approaches (Burnetas and Katehakis, 1996; confidence analysis follows in Section 4, and further Auer et al., 2002; Cappé et al., 2013). However, it is Bayesian optimality results are given in Section 5. Nu- also well known that regret minimizing algorithms can- merical illustrations are given in Section 6. not yield optimal performance for BAI (Bubeck et al., 2011; Kaufmann and Garivier, 2017) and as we opt 2 Bayesian BAI Strategies Thompson Sampling for BAI, then its adaptation is necessary. Such an adaptation, TTTS, was given by Russo (2016) along with the other top-two sampling In this section, we give an overview of the sampling rules TTPS and TTVS. By choosing between two dif- rule TTTS and introduce T3C. We provide details for ferent candidate arms in each round, these sampling Bayesian updating for Gaussian and Bernoulli models rules enforce the exploration of sub-optimal arms, that respectively, and introduce associated Bayesian stop- would be under-sampled by vanilla Thompson sam- ping and recommendation rules. pling due to its objective of maximizing rewards. 2.1 Sampling rules While TTTS appears to be a good anytime sampling rule for the fixed-confidence BAI when coupled with an Both TTTS and T3C employ a Bayesian machinery appropriate stopping rule, so far there is no theoretical and make use of a prior distribution Π over a set 1 support for this employment. Indeed, the (Bayesian- of parameters Θ, that contains the unknown true flavored) asymptotic analysis of Russo (2016) shows parameter vector µ. Upon acquiring observations that under TTTS, the posterior probability that I(cid:63) is (Y , · · · , Y ), we update our beliefs accord- the best arm converges almost surely to 1 at the best 1,I1 n−1,In−1 ing to Bayes’ rule and obtain a posterior distribu- possible rate. However, this property does not by it- tion Π which we assume to have density π w.r.t. the n n self translate into sample complexity guarantees. Since Lebesgue measure. Russo’s analysis is restricted to the result of Russo (2016), Qin et al. (2017) proposed strong regularity properties on the models and priors and analyzed TTEI, another Bayesian sampling rule, that exclude two important useful cases we consider in both in the fixed-confidence setting and in terms of this paper: (1) the observations of each arm i follow a posterior convergence rate. Nonetheless, similar guar- Gaussian distribution N (µ , σ2) with common known i antees for TTTS have been left as an open question by variance σ2, with imposed Gaussian prior N (µ , σ2 ), 1,i 1,i Russo (2016). In the present paper, we answer this (2) all arms receive Bernoulli rewards with unknown open question. In addition, we propose T3C, a compu- means, with a uniform prior on each arm. tationally more favorable variant of TTTS and extend the fixed-confidence guarantees to T3C as well. Gaussian model For Gaussian bandits with a N (0, κ2) prior on each mean, the posterior distribution Contributions (1) We propose a new Bayesian sam- of µ at round n is Gaussian with mean and variance i pling rule, T3C, which is inspired by TTTS but easier to that are respectively given by implement and computationally advantageous (2) We investigate two Bayesian stopping and recommenda- (cid:80)n (cid:96)=− 11 1{I (cid:96) = i}Y (cid:96),I(cid:96) and σ2 , tion rules and establish their δ-correctness for a ban- T + σ2/κ2 T + σ2/κ2 n,i n,i dit model with Gaussian rewards.1 (3) We provide the first sample complexity analysis of TTTS and T3C where T n,i (cid:44) (cid:80)n (cid:96)=− 11 1{I (cid:96) = i} is the number of selec- for a Gaussian model and our proposed stopping rule. tions of arm i before round n. For the sake of simplic- (4) Russo’s posterior convergence results for TTTS were ity, we consider improper Gaussian priors with µ 1,i = 0 obtained under restrictive assumptions on the mod- and σ 1,i = +∞ for all i ∈ A, for which els and priors, which exclude the two mostly used in practice: Gaussian bandits with Gaussian priors and µ = 1 n (cid:88)−1 1{I = i}Y and σ2 = σ2 . bandits with Bernoulli rewards2 with Beta priors. We n,i T n,i (cid:96) (cid:96),I(cid:96) n,i T n,i (cid:96)=1 1hereafter ‘Gaussian bandits’ or ‘Gaussian model’ Observe that in that case the posterior mean µ n,i co- 2hereafter ‘Bernoulli bandits’ incides with the empirical mean.
Preprint Beta-Bernoulli model For Bernoulli bandits with Algorithm 1 Sampling rule (TTTS/T3C) a uniform (Beta(1, 1)) prior on each mean, the poste- 1: Input: β rior distribution of µ i at round n is a Beta distribution 2: for n ← 1, 2, · · · do with shape parameters α = (cid:80)n−1 1{I = i}Y + 1 3: sample θ ∼ Π n,i (cid:96)=1 (cid:96) (cid:96),I(cid:96) n and β n,i = T n,i − (cid:80)n (cid:96)=− 11 1{I (cid:96) = i}Y (cid:96),I(cid:96) + 1. 4 5: : I sa(1 m) p← lea brg ∼m Ba ex ri n∈ (A βθ )i Now we briefly recall TTTS and introduce T3C. 6: if b = 1 then 7: evaluate arm I(1) 8: else Description of TTTS At each time step n, TTTS has 9: repeat sample θ(cid:48) ∼ Π n two potential actions: (1) with probability β, a param- 10: I(2) ← arg max θ(cid:48) TTTS i∈A i eter vector θ is sampled from Π n, and TTTS chooses to 11: until I(2) (cid:54)= I(1) play I n(1) (cid:44) arg max i∈A θ i, (2) and with probability 12: I(2) ← arg min i(cid:54)=I(1) W n(I(1), i), cf. (1) T3C 1 − β, the algorithm continues sampling new θ(cid:48) until 13: evaluate arm I(2) we obtain a challenger I(2) (cid:44) arg max θ(cid:48) that is dif- 14: end if n i∈A i ferent from I(1), and TTTS then selects the challenger. 15: update mean and variance n 16: t = t + 1 17: end for Description of T3C One drawback of TTTS is that, in practice, when the posteriors become concentrated, it takes many Thompson samples before the challenger Optimal action probability The optimal action I(2) is obtained. We thus propose a variant of TTTS, n probability a is defined as the posterior probability called T3C, which alleviates this computational burden. n,i that arm i is optimal. Formally, letting Θ be the Instead of re-sampling from the posterior until a differ- i subset of Θ such that arm i is the optimal arm, ent candidate appears, we define the challenger as the a wr im tht rh ea st ph eca ts t th oe tl ho ewe fis rt sttra cn ansp do idrt aa tt eio (n wc io thst tW iesn( bI rn( o1) k, ei n) Θ i (cid:44) (cid:26) θ ∈ Θ (cid:12) (cid:12) (cid:12) (cid:12) θ i > m j(cid:54)=a ix θ j(cid:27) , uniformly at random). then we define Let µ n,i be the empirical mean of arm i and µ n,i,j (cid:44) (cid:90) (T µ + T µ )/(T + T ), then we define a (cid:44) Π (Θ ) = π (θ)dθ. n,i n,i n,j n,j n,i n,j n,i n i n Θi (cid:26) 0 if µ ≥ µ , W (i, j) (cid:44) n,j n,i (1) With this notation, one can show that under TTTS, n W + W otherwise, n,i,j n,j,i (cid:16) (cid:17) a Π I(2) = j|I(1) = i = n,j . (2) where W n,i,j (cid:44) T n,id (µ n,i, µ n,i,j) for any i, j and n n n (cid:80) k(cid:54)=i a n,k d(µ; µ(cid:48)) denotes the Kullback-Leibler between the dis- Furthermore, when i coincides with the empirical best tribution with mean µ and that of mean µ(cid:48). In the mean (and this will often be the case for I(1) when n Gaussian case, d(µ; µ(cid:48)) = (µ − µ(cid:48))2/(2σ2) while in the n is large due to posterior convergence) one can write Bernoulli case d(µ; µ(cid:48)) = µ ln(µ/µ(cid:48)) + (1 − µ) ln(1 − µ)/(1 − µ(cid:48)). In particular, for Gaussian bandits a (cid:39) Π (θ ≥ θ ) (cid:39) exp (−W (i, j)) , n,j n j i n (µ − µ )2 where the last step is justified in Lemma 2 in the W (i, j) = n,i n,j 1{µ < µ }. n 2σ2(1/T + 1/T ) n,j n,i Gaussian case (and Lemma 26 in Appendix H.3 in the n,i n,j Bernoulli case). Hence, T3C replaces sampling from the distribution (2) by an approximation of its mode The pseudo-code of TTTS and T3C are shown in Algo- which is easy to compute. Note that directly comput- rithm 1. Note that under the Gaussian model with ing the mode would require to compute a , which is improper priors, one should pull each arm once at the n,j much more costly than the computation of W (i, j)3. beginning for the sake of obtaining proper posteriors. n W n in Line 12 of Algorithm 1 is the transportation 2.3 Stopping and recommendation rules cost defined in (1). In order to use TTTS or T3C as sampling rule for fixed- confidence BAI, we need to additionally define stop- 2.2 Rationale for T3C ping and recommendation rules. While Qin et al. In order to explain how T3C can be seen as an approxi- 3the TTPS sampling rule (Russo, 2016) also requires the mation of the re-sampling performed by TTTS, we first computation of a , thus we do not report simulations for n,i need to define the optimal action probabilities. this Bayesian sampling rule in Section 6
Preprint (2017) suggest to couple TTEI with the “frequen- In particular, for TTTS we have tist” Chernoff stopping rule (Garivier and Kaufmann, 2016), we propose in this section natural Bayesian ψ = βa + (1 − β)a (cid:88) a n,j , n,i n,i n,i 1 − a stopping and recommendation rule. They both rely j(cid:54)=i n,j on the optimal action probabilities defined above. while for T3C aBa ny ae tus ria aln cr ae nc do idm atm e e fon rd ta ht eio bn esr tu al re m A ist thti em ae rmste wp itn h, ψ n,i = βa n,i+(1−β) (cid:88) a n,j 1{W n (cid:12)(j, i) = min k(cid:54)=j W n(j (cid:12), k)} . largest optimal action probability, hence we define j(cid:54)=i # (cid:12)arg min k(cid:54)=j W n(j, k)(cid:12) J (cid:44) arg max a . n n,i i∈A 3 Two Related Optimality Notions Bayesian stopping rule In view of the recommen- In the fixed-confidence setting, we aim for building δ- dation rule, it is natural to stop when the posterior correct strategies, i.e. strategies that identify the best probability that the recommended action is optimal is arm with high confidence on any problem instance. large, and exceeds some threshold c which gets close n,δ to 1. Hence our Bayesian stopping rule is Definition 1. A strategy (I , J , τ ) is δ-correct if for n n all bandit models µ with a unique optimal arm, it holds (cid:26) (cid:27) τ δ (cid:44) inf n ∈ N : max a n,i ≥ c n,δ . (3) that P µ [J τ (cid:54)= I(cid:63)] ≤ δ. i∈A Among δ-correct strategies, seek the one with the Links with frequentist counterparts Using the smallest sample complexity E [τ ]. So far, TTTS has not δ transportation cost W n(i, j) defined in (1), the Cher- been analyzed in terms of sample complexity; Russo noff stopping rule of Garivier and Kaufmann (2016) (2016) focusses on posterior consistency and optimal can actually be rewritten as convergence rates. Interestingly, both the smallest (cid:26) (cid:27) possible sample complexity and the fastest rate of pos- τ Ch. (cid:44) inf n ∈ N : max min W (i, j) > d . (4) terior convergence can be expressed in terms of the δ n n,δ i∈A j∈A\{i} following quantities. This stopping rule coupled with the recommendation Definition 2. Let Σ = {ω : (cid:80)K ω = 1} and K k=1 k rule J n = arg max i µ n,i. define for all i (cid:54)= I(cid:63) As explained in that paper, W (i, j) can be interpreted n C (ω, ω(cid:48)) (cid:44) min ωd(µ ; x) + ω(cid:48)d(µ ; x), as a (log) Generalized Likelihood Ratio statistic for re- i x∈I I(cid:63) i jecting the hypothesis H : (µ < µ ). Through our 0 i j where d(µ, µ(cid:48)) is the KL-divergence defined above and Bayesian lens, we rather have in mind the approxi- I = R in the Gaussian case and I = [0, 1] in the mation Π (θ > θ ) (cid:39) exp {−W (i, j)}, valid when n j i n Bernoulli case. We define µ > µ , which permits to analyze the two stop- n,i n,j ping rules using similar tools, as will be seen in the Γ(cid:63) (cid:44) max min C (ω , ω ), proof of Theorem 2. ω∈ΣK i(cid:54)=I(cid:63) i I(cid:63) i A fas irls yho sw imn ill aa rter foi rn sS oe mct eio cn or4 r, esτ pδ oa nn dd inτ gδC ch h. op icro ev se ot fo tb he e Γ(cid:63) β (cid:44) ωωm I∈ (cid:63)a Σ =x K β im (cid:54)=i In (cid:63) C i(ω I(cid:63), ω i). (5) thresholds c and d . This endorses the use of the n,δ n,δ Chernoff stopping rule in practice, which does not re- The quantity C (ω , ω ) can be interpreted as a i I(cid:63) i quire the (heavy) computation of optimal action prob- “transportation cost”4 from the original bandit in- abilities. Still, our sample complexity analysis applies stance µ to an alternative instance in which the mean to the two stopping rules, and we believe that a fre- of arm i is larger than that of I(cid:63), when the proportion quentist sample complexity analysis of a fully Bayesian of samples allocated to each arm is given by the vec- BAI strategy is a nice theoretical contribution. tor ω ∈ Σ . As shown by Russo (2016), the ω that K maximizes (5) is unique, which allows us to define the Useful notation We follow the notation of Russo β-optimal allocation ωβ in the following proposition. (2016) and define the following measures of effort al- Proposition 1. There is a unique solution ωβ to the located to arm i up to time n, optimization problem (5) satisfying ωβ = β, and for I(cid:63) ψ (cid:44) P [I = i|F ] and Ψ (cid:44) (cid:88)n ψ . all i, j (cid:54)= I(cid:63), C i(β, ω iβ) = C j(β, ω jβ). n,i n n−1 n,i l,i l=1 4for which W n(I(cid:63), i) is an empirical counterpart
Preprint For models with more than two arms, there is no closed lim sup τ / ln(1/δ) ≤ (Γ(cid:63) )−1 almost surely, when δ→0 δ β form expression for Γ(cid:63) or Γ(cid:63), even for Gaussian bandits coupled with the Chernoff stopping rule. The fixed β with variance σ2 for which we have confidence optimality that we define above is stronger as it provides guarantees on E [τ ]. (µ − µ )2 δ Γ(cid:63) = max min I(cid:63) i . β ω:ωI(cid:63)=β i(cid:54)=I(cid:63) 2σ2(1/ω i + 1/β) 4 Fixed-Confidence Analysis Bayesian β-optimality Russo (2016) proves that any sampling rule allocating a fraction β to the optimal In this section, we consider Gaussian bandits and the arm (Ψ n,I(cid:63)/n → β) satisfies 1 − a n,I(cid:63) ≥ e−n(Γ(cid:63) β+o(1)) Bayesian rules using an improper prior on the means. (a.s.) for large values of n. We define a Bayesian We state our main result below, showing that TTTS β-optimal sampling rule as a sampling rule matching and T3C are asymptotically β-optimal in the fixed con- this lower bound, i.e. satisfying Ψ /n → β and fidence setting, when coupled with appropriate stop- n,I(cid:63) 1 − a n,I(cid:63) ≤ e−n(Γ(cid:63) β+o(1)). ping and recommendation rules. Russo (2016) proves that TTTS with parameter β is Theorem 1. With CgG the function defined by Kauf- Bayesian β-optimal. However, the result is valid mann and Koolen (2018), which satisfies CgG(x) (cid:39) x + ln(x), we introduce the threshold only under strong regularity assumptions, exclud- ing the two practically important cases of Gaussian (cid:18) (cid:19) ln((K − 1)/δ) and Bernoulli bandits. In this paper, we complete d n,δ = 4 ln(4 + ln(n)) + 2CgG 2 . (6) the picture by establishing Bayesian β-optimality for those models in Section 5. For the Gaussian bandit, The TTTS and T3C sampling rules coupled with either Bayesian β-optimality was established for TTEI by Qin • the Bayesian stopping rule (3) with threshold et al. (2017) with Gaussian priors, but this remained an open problem for TTTS. c n,δ = 1 − √1 e−(cid:16)√ dn,δ+ √1 2 (cid:17)2 A fundamental ingredient of these proofs is to establish 2π the convergence of the allocation of measurement effort and the recommendation rule J = arg max a to the β-optimal allocation: Ψ /n → ωβ for all i, t i n,i n,i i • or the Chernoff stopping rule (4) with threshold which is equivalent to T /n → ωβ (cf. Lemma 4). n,i i d and recommendation rule J = arg max µ , n,δ t i n,i β-optimality in the fixed-confidence setting In form a δ-correct BAI strategy. Moreover, if all the the fixed confidence setting, the performance of an arms means are distinct, it satisfies algorithm is evaluated in terms of sample complex- E [τ ] 1 ity. A lower bound given by Garivier and Kauf- lim sup δ ≤ . log(1/δ) Γ(cid:63) mann (2016) states that any δ-correct strategy satisfies δ→0 β E [τ ] ≥ (Γ(cid:63))−1 ln (1/(3δ)). δ We now give the proof of Theorem 1, which is divided Observe that Γ(cid:63) = max Γ(cid:63) . Using the same β∈[0,1] β into three parts. The first step of the analysis is to lower bound techniques, one can also prove that under prove the δ-correctness of the studied BAI strategies. any δ-correct strategy satisfying T /n → β, n,I(cid:63) Theorem 2. Regardless of the sampling rule, the stop- E [τ ] 1 lim inf δ ≥ . ping rule (3) with the threshold c n,δ and the Chernoff δ→0 ln(1/δ) Γ(cid:63) stopping rule with threshold d defined in Theorem 1 β n,δ satisfy P [τ < ∞ ∧ J (cid:54)= I(cid:63)] ≤ δ. This motivates the relaxed optimality notion that we δ τδ introduce in this paper: A BAI strategy is called To prove that TTTS and T3C allow to reach a β-optimal asymptotically β-optimal if it satisfies sample complexity, one needs to quantify how fast the T E [τ ] 1 measurement effort for each arm is concentrating to n,I(cid:63) → β and lim sup δ ≤ . n ln(1/δ) Γ(cid:63) its corresponding optimal weight. For this purpose, δ→0 β we introduce the random variable In the paper, we provide the first sample complexity (cid:26) (cid:27) analysis of a BAI algorithm based on TTTS (with the T ε (cid:44) inf N ∈ N : max |T /n − ωβ| ≤ ε, ∀n ≥ N . stopping and recommendation rules described in Sec- β i∈A n,i i tion 2), establishing its asymptotic β-optimality. The second step of our analysis is a sufficient condi- As already observed by Qin et al. (2017), any sampling tion for β-optimality, stated in Lemma 1. Its proof is rule converging to the β-optimal allocation (i.e. satis- given in Appendix E. The same result was proven for fying T /n → wβ for all i) can be shown to satisfy the Chernoff stopping rule by Qin et al. (2017). n,i i
Preprint Lemma 1. Let δ, β ∈ (0, 1). For any sampling rule remains to check that (cid:104) (cid:105) which satisfies E T βε < ∞ for all ε > 0, we have P (cid:34) ∃n∈N :µn,i ≥ µn,I(cid:63), (µn 2,i σ− n2 ,µ i,n I, (cid:63)I(cid:63))2 ≥dn,δ(cid:35) ≤ Kδ −1 . (9) E [τ ] 1 lim sup δ ≤ , δ→0 log(1/δ) Γ(cid:63) β To prove this, we observe that for µ n,i ≥ µ n,I(cid:63), if the sampling rule is coupled with stopping rule (3), (µ n 2,i σ− n2 ,µ i,In (cid:63),I(cid:63))2 = θii <n θf I(cid:63) T n,id(µ n,i; θ i) + T n,I(cid:63)d(µ n,I(cid:63); θ I(cid:63)) Finally, it remains to show that TTTS and T3C meet ≤ T n,id(µ n,i; µ i) + T n,I(cid:63)d(µ n,I(cid:63); µ I(cid:63)). the sufficient condition, and therefore the last step, which is the core component and the most technical Corollary 10 of Kaufmann and Koolen (2018) then al- part our analysis, consists of showing the following. lows us to upper bound the probability (cid:104) (cid:105) Theorem 3. Under TTTS or T3C, E T ε < +∞. β P [∃n ∈ N : T n,id(µ n,i; µ i) + T n,I(cid:63)d(µ n,I(cid:63), µ I(cid:63)) ≥ d n,δ] In the rest of this section, we prove Theorem 2 and by δ/(K − 1) for the choice of threshold given in (6), sketch the proof of Theorem 3. But we first highlight which completes the proof that the stopping rule (3) some important ingredients for these proofs. is δ-correct. The fact that the Chernoff stopping rule with the above threshold d given above is δ-correct n,δ 4.1 Core ingredients straightforwardly follows from (9). Our analysis hinges on properties of the Gaussian pos- 4.3 Sketch of the proof of Theorem 3 teriors, in particular on the following tails bounds, which follow from Lemma 1 of Qin et al. (2017). We present a unified proof sketch of Theorem 3 for Lemma 2. For any i, j ∈ A, if µ ≤ µ n,i n,j TTTS and T3C. While the two analyses follow the same 1 (cid:26) (µ − µ )2 (cid:27) steps, some of the lemmas given below have different Π n [θ i ≥ θ j] ≤ 2 exp − n, 2j σ2 n,i , (7) proofs for TTTS and T3C, which can be found in Ap- n,i,j pendix C and Appendix D respectively. 1 (cid:26) (µ − µ + σ )2 (cid:27) Π [θ ≥ θ ] ≥ √ exp − n,j n,i n,i,j , (8) n i j 2π 2σ2 We first state two important concentration results, n,i,j that hold under any sampling rule. where σ2 (cid:44) σ2/T + σ2/T . n,i,j n,i n,j Lemma 3. [Lemma 5 of Qin et al. 2017] There exists a random variable W , such that for all i ∈ A, This lemma is crucial to control a and ψ , the op- 1 n,i n,i timal action and selection probabilities. (cid:115) log(e + T ) ∀n ∈ N, |µ − µ | ≤ σW n,i a.s., n,i i 1 1 + T 4.2 Proof of Theorem 2 n,i We upper bound the desired probability as follows and E (cid:2) eλW1(cid:3) < ∞ for all λ > 0. P [τ δ < ∞ ∧ J τδ (cid:54)= I(cid:63)] ≤ (cid:88) P [∃n ∈ N : α i,n > c n,δ] Lemma 4. There exists a random variable W 2, such i(cid:54)=I(cid:63) that for all i ∈ A, (cid:88) ≤ P [∃n ∈ N : Π n(θ i ≥ θ I(cid:63)) > c n,δ, µ n,I(cid:63) ≤ µ n,i] ∀n ∈ N, |T − Ψ | ≤ W (cid:112) (n + 1) log(e2 + n) a.s., i(cid:54)=I(cid:63) n,i n,i 2 (cid:88) ≤ P [∃n ∈ N : 1 − c n,δ > Π n(θ I(cid:63)> θ i), µ n,I(cid:63) ≤ µ n,i] . and E (cid:2) eλW2(cid:3) < ∞ for any λ > 0. i(cid:54)=I(cid:63) Lemma 3 controls the concentration of the posterior The second step uses the fact that as c ≥ 1/2, a n,δ means towards the true means and Lemma 4 estab- necessary condition for Π (θ ≥ θ ) ≥ c is that n i I(cid:63) n,δ lishes that T and Ψ are close. Both results rely µ ≥ µ . Now using the lower bound (8), if µ ≤ n,i n,i n,i n,I(cid:63) n,I(cid:63) on uniform deviation inequalities for martingales. µ , the inequality 1 − c > Π (θ > θ ) implies n,i n,δ n I(cid:63) i Our analysis uses the same principle as that of TTEI: (cid:32)(cid:115) (cid:33)2 (µ − µ )2 1 1 We establish that T ε is upper bounded by some ran- n,i n,I(cid:63) ≥ ln √ − √ = d , β 2σ2 2π(1 − c ) 2 n,δ dom variable N which is a polynomial of the random n,i,I(cid:63) n,δ variables W and W introduced in the above lemmas, 1 2 where the equality follows from the expression of c n,δ denoted by Poly(W 1, W 2) (cid:44) O(W 1c1W 2c2), where c 1 as function of d . Hence to conclude the proof it and c are two constants (that may depend on arms’ n,δ 2
Preprint means and the constant hidden in the O). As all ex- The major step in the proof of Lemma 8 for each sam- ponential moments of W and W are finite, N has a pling rule, is to establish that if some arm is over- 1 2 finite expectation as well, which concludes the proof. sampled, then its probability to be selected is expo- nentially small. Formally, we show that for n larger The first step to exhibit such an upper bound N is to than some Poly(1/ε, W , W ), establish that every arm is pulled sufficiently often. 1 2 PL oe lm y(m Wa , W5. )U sn .td .er ∀nTT ≥TS No ,r foT r3C a, llt ih ,ere T exi ≥sts (cid:112)N n1 /K= , Ψ nn,i ≥ ω iβ + ξ ⇒ ψ n,i ≤ exp {−f (n, ξ)} , 1 2 1 n,i almost surely. for some function f (n, ξ) to be specified for each sam- √ pling rule, satisfying f (n) ≥ C n (a.s.). This result Due to the randomized nature of TTTS and T3C, the ξ leads to the concentration of Ψ /n, thus can be easily proof of Lemma 5 is significantly more involved than n,i converted to the concentration of T /n by Lemma 4. for a deterministic rule like TTEI. Intuitively, the pos- n,i terior of each arm would be well concentrated once the Finally, Lemma 7 and Lemma 8 show that T ε is upper β arm is sufficiently pulled. If the optimal arm is under- bounded by N (cid:44) max(N , N ), which yields E[T ε] ≤ sampled, then it would be chosen as the first candidate 3 4 β max(E [N ] , E [N ]) < ∞. 3 4 with large probability. If a sub-optimal arm is under- sampled, then its posterior distribution would possess 5 Optimal Posterior Convergence a relatively wide tail that overlaps with or cover the somehow narrow tails of other overly-sampled arms. Recall that a denotes the posterior mass assigned The probability of that sub-optimal arm being chosen n,I(cid:63) to the event that action I(cid:63) (i.e. the true optimal arm) as the challenger would be large enough then. is optimal at time n. As the number of observations Combining Lemma 5 with Lemma 3 straightforwardly tends to infinity, we desire that the posterior distribu- leads to the following result. tion converges to the truth. In this section we show Lemma 6. Under TTTS or T3C, fix a constant ε > 0, equivalently that the posterior mass on the comple- there exists N = Poly(1/ε, W , W ) s.t. ∀n ≥ N , mentory event, 1 − a , the event that arm I(cid:63) is not 2 1 2 2 n,I(cid:63) optimal, converges to zero at an exponential rate, and ∀i ∈ A, |µ − µ | ≤ ε. n,i i that it does so at optimal rate Γ(cid:63) . β We can then deduce a very nice property about the Russo (2016) proves a similar theorem under three con- optimal action probability for sub-optimal arms from fining boundedness assumptions (cf. Russo 2016, Ass- the previous two lemmas. Indeed, we can show that sumption 1) on the parameter space, the prior den- sity and the (first derivative of the) log-normalizer of (cid:26) ∆2 (cid:114) n (cid:27) ∀i (cid:54)= I(cid:63), a ≤ exp − min the exponential family. Hence, the theorems in Russo n,i 16σ2 K (2016) do not apply to the two bandit models most used in practise, which we consider in this paper: the for n larger than some Poly(W , W ). In the previous 1 2 Gaussian and Bernoulli model. inequality, ∆ is the smallest mean difference among min all the arms. In the first case, the parameter space is unbounded, in the latter model, the derivative of the log-normalizer Plugging this in the expression of ψ , one can easily n,i (which is eη/(1 + eη)) is unbounded. Here we provide quantify how fast ψ converges to β, which eventu- n,I(cid:63) two theorems, proving that under TTTS, the optimal, ally yields the following result. exponential posterior convergence rates are obtained Lemma 7. Under TTTS or T3C, fix ε > 0, then there for the Gaussian model with uninformative (improper) exists N = Poly(1/ε, W , W ) s.t. ∀n ≥ N , 3 1 2 3 Gaussian priors (proof given in Appendix G), and the (cid:12) (cid:12) (cid:12) (cid:12) T n,I(cid:63) − β(cid:12) (cid:12) ≤ ε. Bernoulli model with Beta(1, 1) priors (proof given in (cid:12) n (cid:12) Appendix H). Theorem 4. Under TTTS, for Gaussian bandits with The last, more involved, step is to establish that improper Gaussian priors, it holds almost surely that the fraction of measurement allocation to every sub- 1 optimal arm i is indeed similarly close to its optimal lim − log(1 − a ) = Γ(cid:63) . proportion ωβ. n→∞ n n,I(cid:63) β i Lemma 8. Under TTTS or T3C, fix a constant ε > 0, Theorem 5. Under TTTS, for Bernoulli bandits and there exists N 4 = Poly(1/ε, W 1, W 2) s.t. ∀n ≥ N 4, uniform priors, it holds almost surely that (cid:12) (cid:12) ∀i (cid:54)= I(cid:63), (cid:12) (cid:12) (cid:12) T nn,i − ω iβ(cid:12) (cid:12) (cid:12) ≤ ε. nl →im ∞ − n1 log(1 − a n,I(cid:63)) = Γ(cid:63) β.
Preprint 2000 Problem 1, Gaussian bandits, =0.01 7000 Problem 2, Gaussian bandits, =0.01 450 Problem 1, Bernoulli bandits, =0.01 400 Problem 2, Bernoulli bandits, =0.01 6000 400 350 1500 5000 350 300 4000 300 250 1000 3000 250 200 2000 200 150 500 1000 150 100 0 0 100 50 T3C TTTS TTEI BC D-Tracking Uniform UGapE T3C TTTS TTEI BC D-Tracking Uniform UGapE T3C TTTS BC D-Tracking Uniform UGapE T3C TTTS BC D-Tracking Uniform UGapE Figure 1: black dots represent means and oranges lines represent medians. Sampling rule T3C TTTS TTEI BC D-Tracking Uniform UGapE Execution time (s) 1.6 × 10−5 2.3 × 10−4 1 × 10−5 1.4 × 10−5 1.3 × 10−3 6 × 10−6 5 × 10−6 Table 1: average execution time in seconds for different sampling rules. 6 Numerical Illustrations setting. (3) The fact that D-Tracking performs best is not surprising, since it converges to ωβ(cid:63) and achieves minimal sample complexity. However, in terms of com- This section is aimed at illustrating our theoretical putation time, D-Tracking is much worse than other results and supporting the practical use of Bayesian sampling rules, as can be seen in Table 1, which re- sampling rules for fixed-confidence BAI. ports the average execution time of one step of each We experiment with three different Bayesian sam- sampling rule for µ in the Gaussian case. (4) TTTS 1 pling rules: T3C, TTTS and TTEI, and we also in- also suffers from computational costs, whose origins clude the Direct Tracking (D-Tracking) rule of Gariv- are explained in Section 2, unlike T3C and TTEI. Al- ier and Kaufmann (2016) (which is adaptive to β), the though TTEI is already computationally more attrac- UGapE (Gabillon et al., 2012) algorithm, and a uni- tive than TTTS, its practical benefits are limited to the form baseline. In order to make a fair comparison, Gaussian case, since the Expected Improvement (EI) we use the Chernoff stopping rule (4) and associated does not have a closed form beyond this case and its recommendation rule for all of the sampling rules, in- approximation would be costly. In contrast, T3C can cluding the uniform one, except for UGapE which has be applied for other distributions. its own stopping rule. Furthermore, we include a top- two variant of the Best Challenger (BC) heuristic (see, 7 Conclusion e.g., Ménard, 2019). BC selects the empirical best arm I(cid:98)n with probability We have advocated the use of a Bayesian sampling β and the maximizer of W n(I(cid:98)n, j) with probability 1 − rule for BAI. In particular, we proved that TTTS and a β, but also performs forced exploration (selecting any computationally advantageous approach T3C, are both √ arm sampled less than n times at round n). T3C can β-optimal in the fixed-confidence setting, for Gaussian thus be viewed as a variant of BC in which no forced bandits. We further extended the Bayesian optimality exploration is needed to converge to ωβ, due to the properties established by Russo (2016) to more prac- noise added by replacing I(cid:98)n with I n(1). tical choices of models and prior distributions. In order to be optimal, the sampling rules studied We consider two simple instances with arms means in this paper would need to use the oracle tuning given by µ = [0.5 0.9 0.4 0.45 0.44999], and µ = 1 2 β(cid:63) = arg max Γ(cid:63) , which is not feasible. In future [1 0.8 0.75 0.7] respectively. We run simulations for β∈[0,1] β work, we will investigate an efficient online tuning of β both Gaussian (with σ = 1) and Bernoulli bandits to circumvent this issue. We also plan to investigate with a risk parameter δ = 0.01. Figure 1 reports the the extension of T3C to more general pure exploration empirical distribution of τ under the different sam- δ problems, as an alternative to approaches recently pro- pling rules, estimated over 1000 independent runs. posed by Ménard (2019); Degenne et al. (2019). These figures provide several insights: (1) T3C is com- Finally, it is also important to study Bayesian sam- petitive with, and sometimes slightly better than TTTS pling rules in the fixed-budget setting which is more and TTEI in terms of sample complexity. (2) The plausible in many application scenarios such as ap- UGapE algorithm has a larger sample complexity than plying BAI for automated machine learning (Hoffman the uniform sampling rule, which highlights the im- et al., 2014; Li et al., 2017; Shang et al., 2019). portance of the stopping rule in the fixed-confidence
Preprint Acknowledgement The research presented was Garivier, A. and Kaufmann, E. (2016). Optimal best supported by European CHIST-ERA project DELTA, arm identification with fixed confidence. In Pro- French National Research Agency projects BADASS ceedings of the 29th Conference on Learning Theory (ANR-16-CE40-0002) and BOLD (ANR-19-CE23- (CoLT). 0026-04), the Inria-CWI associate team 6PAC and Hoffman, M. W., Shahriari, B., and de Freitas, N. LUF travel grant number W19204-1-35. (2014). On correlation and budget constraints in model-based bandit optimization with application References to automatic machine learning. In Proceedings of the 17th International Conference on Artificial In- Abbasi-Yadkori, Y., Pál, D., and Szepesvári, C. telligence and Statistics (AIStats). (2012). Online-to-confidence-set conversions and ap- Jamieson, K., Malloy, M., Nowak, R., and Bubeck, S. plication to sparse stochastic bandits. In Proceedings (2014). lil’UCB: An optimal exploration algorithm of the 15th International Conference on Artificial for multi-armed bandits. In Proceedings of the 27th Intelligence and Statistics (AIStats). Conference on Learning Theory (CoLT). Audibert, J.-Y. and Bubeck, S. (2010). Best arm iden- Jun, K.-S. and Nowak, R. (2016). Anytime exploration tification in multi-armed bandits. In Proceedings of for multi-armed bandits using confidence informa- the 23rd Conference on Learning Theory (CoLT). tion. In Proceedings of the 33rd International Con- Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). ference on Machine Learning (ICML). Finite-time analysis of the multi-armed bandit prob- Kalyanakrishnan, S., Tewari, A., Auer, P., and Stone, lem. Machine Learning Journal, 47(2-3):235–256. P. (2012). PAC subset selection in stochastic multi- armed bandits. In Proceedings of the 29th Interna- Bubeck, S., Munos, R., and Stoltz, G. (2009). Pure tional Conference on Machine Learning (ICML). exploration in multi-armed bandits problems. In Proceedings of the 20th International Conference on Karnin, Z., Koren, T., and Somekh, O. (2013). Al- Algorithmic Learning Theory (ALT). most optimal exploration in multi-armed bandits. In Proceedings of the 30th International Conference Bubeck, S., Munos, R., and Stoltz, G. (2011). on Machine Learning (ICML). Pure exploration in finitely-armed and continuous- armed bandits. Theoretical Computer Science, Kaufmann, E. and Garivier, A. (2017). Learning the 412(19):1832–1852. distribution with largest mean: two bandit frame- works. ESAIM: Proceedings and Surveys, 60:114– Burnetas, A. N. and Katehakis, M. N. (1996). Optimal 131. adaptive policies for sequential allocation problems. Kaufmann, E. and Koolen, W. (2018). Mixture Advances in Applied Mathematics, 17(2):122–142. martingales revisited with applications to sequen- Cappé, O., Garivier, A., Maillard, O. A., Munos, R., tial tests and confidence intervals. arXiv preprint and Stoltz, G. (2013). Kullback-Leibler upper confi- arXiv:1811.11419. dence bounds for optimal sequential allocation. An- Li, L., Jamieson, K., DeSalvo, G., Talwalkar, A., nals of Statistics, 41(3):1516–1541. and Rostamizadeh, A. (2017). Hyperband: Bandit- Carpentier, A. and Locatelli, A. (2016). Tight (lower) based configuration evaluation for hyperparameter bounds for the fixed budget best arm identification optimization. In Proceedings of the 5th International bandit problem. In Proceedings of the 29th Confer- Conference on Learning Representations (ICLR). ence on Learning Theory (CoLT). Ménard, P. (2019). Gradient ascent for active ex- Degenne, R., Koolen, W., and Ménard, P. (2019). Non- ploration in bandit problems. arXiv preprint asymptotic pure exploration by solving games. In arXiv:1905.08165. Advances in Neural Information Processing Systems Qin, C., Klabjan, D., and Russo, D. (2017). Improv- 33 (NeurIPS). ing the expected improvement algorithm. In Ad- Even-dar, E., Mannor, S., and Mansour, Y. (2003). vances in Neural Information Processing Systems 30 Action elimination and stopping conditions for rein- (NIPS). forcement learning. In Proceedings of the 20th Inter- Russo, D. (2016). Simple Bayesian algorithms for best national Conference on Machine Learning (ICML). arm identification. In Proceedings of the 29th Con- ference on Learning Theory (CoLT). Gabillon, V., Ghavamzadeh, M., and Lazaric, A. (2012). Best arm identification: A unified approach Shang, X., Kaufmann, E., and Valko, M. (2019). to fixed budget and fixed confidence. In Advances in A simple dynamic bandit algorithm for hyper- Neural Information Processing Systems 25 (NIPS). parameter tuning. In 6th Workshop on Automated
Preprint Machine Learning at International Conference on Machine Learning (ICML-AutoML). Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285.
Preprint A Outline The appendix of this paper is organized as follows: (cid:3) Appendix C provides the complete fixed-confidence analysis of TTTS (Gaussian case). (cid:3) Appendix D provides the complete fixed-confidence analysis of T3C (Gaussian case). (cid:3) Appendix E is dedicated to Lemma 1. (cid:3) Appendix F is dedicated to crucial technical lemmas. (cid:3) Appendix G is the proof to the posterior convergence Theorem 4 (Gaussian case). (cid:3) Appendix H is the proof to the posterior convergence Theorem 5 (Beta-Bernoulli case). B Useful Notation for the Appendices In this section, we provide a list of useful notation that is applied in appendices (including reminders of previous notation in the main text and some new ones). • Recall that d(µ ; µ ) denotes the KL-divergence between two distributions parametrized by their means µ 1 2 1 and µ . For Gaussian distributions, we know that 2 (µ − µ )2 d(µ ; µ ) = 1 2 . 1 2 2σ2 When it comes to Bernoulli distributions, we denote this with kl, i.e. (cid:18) (cid:19) (cid:18) (cid:19) µ 1 − µ kl(µ ; µ ) = µ ln 1 + (1 − µ ) ln 1 . 1 2 1 µ 1 1 − µ 2 2 • Beta(·, ·) denotes a Beta distribution. • Bern(·) denotes a Bernoulli distribution. • B(·) denotes a Binomial distribution. • N (·, ·) denotes a normal distribution. • Y is the reward of arm i at time n. n,i • Y is the observation of the sampling rule at time n. n,In • F (cid:44) σ(I , Y , I , Y , · · · , I , Y ) is the filtration generated by the first n observations. n 1 1,I1 2 2,I2 n n,In • ψ (cid:44) P [I = i|F ]. n,i n n−1 • Ψ (cid:44) (cid:80)n ψ . n,i l=1 l,i • For the sake of simplicity, we further define ψ (cid:44) Ψn,i . n,i n • T is the number of pulls of arm i before round n. n,i • T denotes the vector of the number of arm selections. n • I(cid:63) (cid:44) arg max µ denotes the empirical best arm at time n. n i∈A n,i • For any a, b > 0, define a function C s.t. ∀y, a,b a − 1 C (y) (cid:44) (a + b − 1)kl( ; y). a,b a + b − 1 • We define the minimum and the maximum means gap as ∆ (cid:44) min |µ − µ |; ∆ (cid:44) max |µ − µ |. min i j max i j i(cid:54)=j i(cid:54)=j • We introduce two indices J(1) (cid:44) arg max a , J(2) (cid:44) arg max a . n n,j n n,j j j(cid:54)=Jn(1) Note that J(1) coincides with the Bayesian recommendation index J . n n • Two real-valued sequences (a ) and (b ) are are said to be logarithmically equivalent if n n (cid:18) (cid:19) 1 a lim log n = 0, n→∞ n b n . and we denote this by a = b . n n
Preprint C Fixed-Confidence Analysis for TTTS This section is entirely dedicated to TTTS. C.1 Sufficient exploration of all arms, proof of Lemma 5 under TTTS To prove this lemma, we introduce the two following sets of indices for a given L > 0: ∀n ∈ N we define √ U L (cid:44) {i : T < L}, n n,i V L (cid:44) {i : T < L3/4}. n n,i It is seemingly non trivial to manipulate directly TTTS’s candidate arms, we thus start by connecting TTTS with TTPS (top two probability sampling). TTPS is another sampling rule presented by Russo (2016) for which the two candidate samples are defined as in Appendix B, we recall them in the following. J(1) (cid:44) arg max a , J(2) (cid:44) arg max a . n n,j n n,j j j(cid:54)=Jn(1) Lemma 5 is proved via the following sequence of lemmas. Lemma 9. There exists L = Poly(W ) s.t. if L > L , for all n, U L (cid:54)= ∅ implies J(1) ∈ V L or J(2) ∈ V L. 1 1 1 n n n n n Proof. If J(1) ∈ V L, then the proof is finished. Now we assume that J(1) ∈ V L, and we prove that J(2) ∈ V L. n n n n n n Step 1 According to Lemma 3, there exists L = Poly(W ) s.t. ∀L > L , ∀i ∈ U L, 2 1 2 n (cid:115) log(e + T ) |µ − µ | ≤ σW n,i n,i i 1 1 + T n,i (cid:115) √ log(e + L) ≤ σW √ 1 1 + L ∆ ∆ ≤ σW min = min . 1 4σW 4 1 The second inequality holds since x (cid:55)→ log(e+x) is a decreasing function. The third inequality holds for a large 1+x L > L with L = . . .. 2 2 Step 2 We now assume that L > L , and we define 2 J(cid:63) (cid:44) arg max µ = arg max µ . n n,j j j∈UL j∈UL n n The last equality holds since ∀j ∈ U L, |µ − µ | ≤ ∆ /4. We show that there exists L = Poly(W ) s.t. n n,i i min 3 1 ∀L > L , 3 J(cid:63) = J(1). n n We proceed by contradiction, and suppose that J(cid:63) (cid:54)= J(1), then µ < µ , since J(1) ∈ V L ⊂ U L. However, n n n,Jn(1) n,J n(cid:63) n n n we have (cid:34) (cid:35) a = Π θ > max θ n,Jn(1) n Jn(1) j(cid:54)=Jn(1) j (cid:104) (cid:105) ≤ Π θ > θ n Jn(1) J n(cid:63) ≤ 1 exp (cid:40) − (µ n,Jn(1) − µ n,J n(cid:63) )2 (cid:41) . 2 2σ2(1/T + 1/T ) n,Jn(1) n,J n(cid:63)
Preprint The last inequality uses the Gaussian tail inequality (7) of Lemma 2. On the other hand, |µ − µ | = |µ − µ + µ − µ + µ − µ | n,Jn(1) n,J n(cid:63) n,Jn(1) Jn(1) Jn(1) J n(cid:63) J n(cid:63) n,J n(cid:63) ≥ |µ − µ | − |µ − µ + µ − µ | Jn(1) J n(cid:63) n,Jn(1) Jn(1) J n(cid:63) n,J n(cid:63) ∆ ∆ ≥ ∆ − ( min + min ) min 4 4 ∆ = min , 2 and 1 1 2 + ≤ √ . T T L n,Jn(1) n,J n(cid:63) Thus, if we take L s.t. 3 √ (cid:26) L ∆2 (cid:27) 1 exp − 3 min ≤ , 16σ2 2K then for any L > L , we have 3 1 1 a ≤ < , n,Jn(1) 2K K which contradicts the definition of J(1). We now assume that L > L , thus J(1) = J(cid:63). n 3 n n Step 3 We finally show that for L large enough, J(2) ∈ V L. First note that ∀j ∈ V L, we have n n n (cid:104) (cid:105) (cid:26) L3/4∆2 (cid:27) a ≤ Π θ ≥ θ ≤ exp − min . (10) n,j n j J n(cid:63) 16σ2 This last inequality can be proved using the same argument as Step 2. Now we define another index J(cid:63) (cid:44) n arg max µ and the quantity c (cid:44) max(µ , µ ). We can lower bound a as follows: j∈U nL n,j n n,J n(cid:63) n,J n(cid:63) n,J n(cid:63) (cid:2) (cid:3) (cid:89) a ≥ Π θ ≥ c Π [θ ≤ c ] n,J(cid:63) n J(cid:63) n n j n n n j(cid:54)=J(cid:63) n (cid:2) (cid:3) (cid:89) (cid:89) = Π θ ≥ c Π [θ ≤ c ] Π [θ ≤ c ] n J(cid:63) n n j n n j n n j(cid:54)=J n(cid:63);j∈U nL j∈U nL (cid:2) (cid:3) 1 ≥ Π θ ≥ c . n J n(cid:63) n 2K−1 Now there are two cases: • If µ > µ , then we have n,J n(cid:63) n,J n(cid:63) (cid:2) (cid:3) (cid:2) (cid:3) 1 Π θ ≥ c = Π θ ≥ µ ≥ . n J n(cid:63) n n J n(cid:63) n,J n(cid:63) 2 • If µ < µ , then we can apply the Gaussian tail bound (8) of Lemma 2, and we obtain n,J n(cid:63) n,J n(cid:63) (cid:2) (cid:3) (cid:104) (cid:105) (cid:104) (cid:105) Π θ ≥ c = Π θ ≥ µ = Π θ ≥ µ + (µ − µ ) n J n(cid:63) n n J n(cid:63) n,J n(cid:63) n J n(cid:63) n,J n(cid:63) n,J n(cid:63) n,J n(cid:63)   (cid:32) (cid:112) (cid:33)2 1  1 T n,J(cid:63)  ≥ √ exp − 1 − n (µ − µ ) 2π  2 σ n,J n(cid:63) n,J n(cid:63)    (cid:32) (cid:112) (cid:33)2 1  1 T n,J(cid:63)  = √ exp − 1 + n (µ − µ ) . 2π  2 σ n,J n(cid:63) n,J n(cid:63) 
Preprint On the other hand, by Lemma 3, we know that |µ − µ | = |µ − µ + µ − µ + µ − µ | n,J n(cid:63) n,J n(cid:63) n,J n(cid:63) J n(cid:63) J n(cid:63) J n(cid:63) J n(cid:63) n,J n(cid:63) (cid:115) log(e + T n,J(cid:63) ) (cid:118) (cid:117) (cid:117) log(e + T n,J(cid:63) ) ≤ |µ J n(cid:63) − µ J n(cid:63) | + σW 1 1 + T n,J n(cid:63) n + σW 1(cid:116) 1 + T n,J n(cid:63) n (cid:115) log(e + T ) n,J(cid:63) ≤ |µ − µ | + 2σW n J n(cid:63) J n(cid:63) 1 1 + T n,J(cid:63) n (cid:115) log(e + T ) n,J(cid:63) ≤ ∆ + 2σW n . max 1 1 + T n,J(cid:63) n Therefore,  (cid:32) (cid:112) (cid:32) (cid:115) (cid:33)(cid:33)2 (cid:2) (cid:3) 1  1 T n,J(cid:63) log(e + T n,J(cid:63) )  Π θ ≥ c ≥ √ exp − 1 + n ∆ + 2σW n n J n(cid:63) n 2π  2 σ max 1 1 + T n,J(cid:63)  n   1  1  (cid:112)√ L  (cid:115) log(e + √ L) 2  ≥ √ 2π exp − 2 1 + σ ∆ max + 2σW 1 1 + √ L   1 (cid:40) 1 (cid:18) L1/4∆ (cid:113) √ (cid:19)2(cid:41) ≥ √ exp − 1 + max + 2W log(e + L) . 2π 2 σ 1 Now we have (cid:32)(cid:18) 1 (cid:19)K (cid:18) 1 (cid:19)K−1 1 (cid:40) 1 (cid:18) L1/4∆ (cid:113) √ (cid:19)2(cid:41)(cid:33) a ≥ max , √ exp − 1 + max + 2W log(e + L) , n,J n(cid:63) 2 2 2π 2 σ 1 and we have ∀j ∈ V L, a ≤ exp (cid:8) −L3/4∆2 /(16σ2)(cid:9) , thus there exists L = Poly(W ) s.t. ∀L > L , ∀j ∈ V L, n n,j min 4 1 4 n a n,J(cid:63) a ≤ n , n,j 2 and by consequence, J(2) ∈ V L. n n Finally, taking L = max(L , L , L ), we have ∀L > L , either J(1) ∈ V L or J(2) ∈ V L. 1 2 3 4 1 n n n n Next we show that there exists at least one arm in V L for whom the probability of being pulled is large enough. n More precisely, we prove the following lemma. Lemma 10. There exists L = Poly(W ) s.t. for L > L and for all n s.t. U L (cid:54)= ∅, then there exists J ∈ V L 1 1 1 n n n s.t. min(β, 1 − β) ψ ≥ (cid:44) ψ . n,Jn K2 min Proof. Using Lemma 9, we know that J(1) or J(2) ∈ V L. On the other hand, we know that n n n   ∀i ∈ A, ψ n,i = a n,i β + (1 − β) (cid:88) 1 −a n a,j  . n,j j(cid:54)=i Therefore we have β ψ ≥ βa ≥ , n,Jn(1) n,Jn(1) K
Preprint (cid:80) since a = 1, and i∈A n,i a ψ ≥ (1 − β)a n,Jn(1) n,Jn(2) n,Jn(2) 1 − a n,Jn(1) a = (1 − β)a n,Jn(2) n,Jn(1) 1 − a n,Jn(1) 1 − β ≥ , K2 (cid:80) since a ≥ 1/K and a /(1 − a ) = 1, thus a /(1 − a ) ≥ 1/K. n,Jn(1) i(cid:54)=Jn(1) n,i n,Jn(1) n,Jn(2) n,Jn(1) The rest of this subsection is quite similar to that of Qin et al. (2017). Indeed, with the above lemma, we can show that the set of poorly explored arms U L is empty when n is large enough. n Lemma 11. Under TTTS, there exists L = Poly(W , W ) s.t. ∀L > L , U L = ∅. 0 1 2 0 (cid:98)KL(cid:99) Proof. We proceed by contradiction, and we assume that U L is not empty. Then for any 1 ≤ (cid:96) ≤ (cid:98)KL(cid:99), U L (cid:98)KL(cid:99) (cid:96) and V L are non empty as well. (cid:96) There exists a deterministic L s.t. ∀L > L , 5 5 (cid:98)L(cid:99) ≥ KL3/4. Using the pigeonhole principle, there exists some i ∈ A s.t. T ≥ L3/4. Thus, we have |V L | ≤ K − 1. (cid:98)L(cid:99),i (cid:98)L(cid:99) Next, we prove |V L | ≤ K − 2. Otherwise, since U L is non-empty for any (cid:98)L(cid:99) + 1 ≤ (cid:96) ≤ (cid:98)2L(cid:99), thus by (cid:98)2L(cid:99) (cid:96) Lemma 10, there exists J ∈ V L s.t. ψ ≥ ψ . Therefore, (cid:96) (cid:96) (cid:96),J(cid:96) min (cid:88) ψ ≥ ψ , (cid:96),i min i∈V L (cid:96) and (cid:88) ψ ≥ ψ (cid:96),i min i∈V L (cid:98)L(cid:99) since V L ⊂ V L . Hence, we have (cid:96) (cid:98)L(cid:99) (cid:98)2L(cid:99) (cid:88) (cid:88) (cid:88) (Ψ − Ψ ) = ψ ≥ ψ (cid:98)L(cid:99) . (cid:98)2L(cid:99),i (cid:98)L(cid:99),i (cid:96),i min i∈V L (cid:96)=(cid:98)L(cid:99)+1 i∈V L (cid:98)L(cid:99) (cid:98)L(cid:99) Then, using Lemma 4, there exists L = Poly(W ) s.t. ∀L > L , we have 6 2 6 (cid:88) (cid:88) (cid:112) (T − T ) ≥ (Ψ − Ψ − 2W (cid:98)2L(cid:99) log(e2 + (cid:98)2L(cid:99))) (cid:98)2L(cid:99),i (cid:98)L(cid:99),i (cid:98)2L(cid:99),i (cid:98)L(cid:99),i 2 i∈V L i∈V L (cid:98)L(cid:99) (cid:98)L(cid:99) (cid:88) (cid:112) ≥ (Ψ − Ψ ) − 2KW (cid:98)2L(cid:99) log(e2 + (cid:98)2L(cid:99)) (cid:98)2L(cid:99),i (cid:98)L(cid:99),i 2 i∈V L (cid:98)L(cid:99) ≥ ψ (cid:98)L(cid:99) − 2KW C (cid:98)L(cid:99)3/4 min 2 2 ≥ KL3/4, where C is some absolute constant. Thus, we have one arm in V L that is pulled at least L3/4 times between 2 (cid:98)L(cid:99) (cid:98)L(cid:99) + 1 and (cid:98)2L(cid:99), thus |V L | ≤ K − 2. (cid:98)2L(cid:99) By induction, for any 1 ≤ k ≤ K, we have |V L | ≤ K − k, and finally if we take L = max(L , L , L ), then (cid:98)kL(cid:99) 0 1 5 6 ∀L > L , U L = ∅. 0 (cid:98)KL(cid:99) We can finally conclude the proof of Lemma 5 for TTTS.
Preprint Proof of Lemma 5 Let N = KL where L = Poly(W , W ) is chosen according to Lemma 11. For all 1 0 0 1 2 n > N , we let L = n/K, then by Lemma 11, we have U L = U n/K is empty, which concludes the proof. 1 (cid:98)KL(cid:99) n C.2 Concentration of the empirical means, proof of Lemma 6 under TTTS As a corollary of the previous section, we can show the concentration of µ to µ for TTTS5. n,i i By Lemma 3, we know that ∀i ∈ A and n ∈ N, (cid:115) log(e + T ) |µ − µ | ≤ σW n,i . n,i i 1 T + 1 n,i (cid:112) According to the previous section, there exists N = Poly(W , W ) s.t. ∀n ≥ N and ∀i ∈ A, T ≥ n/K. 1 1 2 1 n,i Therefore, (cid:115) (cid:112) log(e + n/K) |µ − µ | ≤ , n,i i (cid:112) n/K + 1 since x (cid:55)→ log(e + x)/(x + 1) is a decreasing function. There exists N (cid:48) = Poly(ε, W ) s.t. ∀n ≥ N (cid:48) , 2 1 2 (cid:115) (cid:112) (cid:115) log(e + n/K) 2(n/K)1/4 ε ≤ ≤ . (cid:112) (cid:112) n/K + 1 n/K + 1 σW 1 Therefore, ∀n ≥ N (cid:44) max{N , N (cid:48) }, we have 2 1 2 ε |µ − µ | ≤ σW . n,i i 1 σW 1 C.3 Measurement effort concentration of the optimal arm, proof of Lemma 7 under TTTS In this section we show that the empirical arm draws proportion of the true best arm for TTTS concentrates to β when the total number of arm draws is sufficiently large. The proof is established upon the following lemmas. First, we prove that the empirical best arm coincides with the true best arm when the total number of arm draws goes sufficiently large. Lemma 12. Under TTTS, there exists M = Poly(W , W ) s.t. ∀n > M , we have I(cid:63) = I(cid:63) = J(1) and ∀i (cid:54)= I(cid:63), 1 1 2 1 n n (cid:26) ∆2 (cid:114) n (cid:27) a ≤ exp − min . n,i 16σ2 K Proof. Using Lemma 6 with ε = ∆ /4, there exists N (cid:48) = Poly(4/∆ , W , W ) s.t. ∀n > N (cid:48) , min 1 min 1 2 1 ∆ ∀i ∈ A, |µ − µ | ≤ min , n,i i 4 which implies that starting from a known moment, µ > µ for all i (cid:54)= I(cid:63), hence I(cid:63) = I(cid:63). Thus, ∀i (cid:54)= I(cid:63), n,I(cid:63) n,i n (cid:20) (cid:21) a = Π θ > max θ n,i n i j j(cid:54)=i ≤ Π [θ > θ ] n i I(cid:63) 1 (cid:26) (µ − µ )2 (cid:27) ≤ exp − n,i n,I(cid:63) . 2 2σ2(1/T + 1/T ) n,i n,I(cid:63) 5this proof is the same as Proposition 3 of Qin et al. (2017)
Preprint The last inequality uses the Gaussian tail inequality of (7) Lemma 2. Furthermore, (µ − µ )2 = (|µ − µ |)2 n,i n,I(cid:63) n,i n,I(cid:63) = (|µ − µ + µ − µ + µ − µ |)2 n,i i i I(cid:63) I(cid:63) n,I(cid:63) ≥ (|µ − µ | − |µ − µ + µ − µ |)2 i I(cid:63) n,i i I(cid:63) n,I(cid:63) (cid:18) (cid:18) ∆ ∆ (cid:19)(cid:19)2 ∆2 ≥ ∆ − min + min = min , min 4 4 4 and according to Lemma 5, we know that there exists M = Poly(W , W ) s.t. ∀n > M , 2 1 2 2 1 1 2 + ≤ . (cid:112) T n,i T n,I(cid:63) n/K Thus, ∀n > max{N (cid:48) , M }, we have 1 2 (cid:26) ∆2 (cid:114) n (cid:27) ∀i (cid:54)= I(cid:63), a ≤ exp − min . n,i 16σ2 K Then, we have (cid:88) (cid:26) ∆2 (cid:114) n (cid:27) a = 1 − a ≥ 1 − (K − 1) exp − min . n,I(cid:63) n,i 16σ2 K i(cid:54)=I(cid:63) There exists M (cid:48) s.t. ∀n > M (cid:48) , a > 1/2, and by consequence I(cid:63) = J(1). Finally taking M (cid:44) 2 2 n,I(cid:63) n 1 max{N (cid:48) , M , M (cid:48) } concludes the proof. 1 2 2 Before we prove Lemma 7, we first show that Ψ /n concentrates to β. n,I(cid:63) Lemma 13. Under TTTS, fix a constant ε > 0, there exists M = Poly(ε, W , W ) s.t. ∀n > M , we have 3 1 2 3 (cid:12) (cid:12) (cid:12) (cid:12) Ψ n,I(cid:63) − β(cid:12) (cid:12) ≤ ε. (cid:12) n (cid:12) Proof. By Lemma 12, we know that there exists M (cid:48) = Poly(W , W ) s.t. ∀n > M (cid:48) , we have I(cid:63) = I(cid:63) = J(1) and 1 1 2 1 n n ∀i (cid:54)= I(cid:63), (cid:26) ∆2 (cid:114) n (cid:27) a ≤ exp − min . n,i 16σ2 K Note also that ∀n ∈ N, we have   ψ n,I(cid:63) = a n,I(cid:63) β + (1 − β) (cid:88) 1 −a n a,j  . n,j j(cid:54)=I(cid:63) We proceed the proof with the following two steps.
Preprint Step 1 We first lower bound Ψ for a given ε. Take M > M (cid:48) that we decide later, we have ∀n > M , n,I(cid:63) 4 1 4 Ψ n,I(cid:63) = 1 (cid:88)n ψ = 1 (cid:88)M4 ψ + 1 (cid:88)n ψ n n l,I(cid:63) n l,I(cid:63) n l,I(cid:63) l=1 l=I(cid:63) l=M4+1 n n 1 (cid:88) 1 (cid:88) ≥ ψ ≥ a β n l,I(cid:63) n l,I(cid:63) l=M4+1 l=M4+1   n β (cid:88) (cid:88) = n 1 − a l,j l=M4+1 j(cid:54)=I(cid:63) β (cid:88)n (cid:32) (cid:40) ∆2 (cid:114) l (cid:41)(cid:33) ≥ 1 − (K − 1) exp − min n 16σ2 K l=M4+1 = β − M 4 β − β (cid:88)n (K − 1) exp (cid:40) − ∆2 min (cid:114) l (cid:41) n n 16σ2 K l=M4+1 (cid:40) (cid:114) (cid:41) M (n − M ) ∆2 M ≥ β − 4 β − 4 β(K − 1) exp − min 4 n n 16σ2 K (cid:40) (cid:114) (cid:41) M ∆2 M ≥ β − 4 β − β(K − 1) exp − min 4 . n 16σ2 K For a given constant ε > 0, there exists M s.t. ∀n > M , 5 5 (cid:26) ∆2 (cid:114) n (cid:27) ε β(K − 1) exp − min < . 16σ2 K 2 Furthermore, there exists M = Poly(ε/2, M ) s.t. ∀n > M , 6 5 6 M ε 5 β < . n 2 Therefore, if we take M (cid:44) max{M (cid:48) , M , M }, we have ∀n > M , 4 1 5 6 4 Ψ n,I(cid:63) ≥ β − ε. n Step 2 On the other hand, we can also upper bound Ψ . We have ∀n > M , n,I(cid:63) 3 n Ψ n,I(cid:63) = 1 (cid:88) ψ n n l,I(cid:63) l=1   n = n1 (cid:88) a l,I(cid:63) β + (1 − β) (cid:88) 1 −a l, aj  l,j l=1 j(cid:54)=I(cid:63) n n ≤ 1 (cid:88) a β + 1 (cid:88) a (1 − β) (cid:88) a l,j n l,I(cid:63) n l,I(cid:63) 1 − a l,j l=1 l=1 j(cid:54)=I(cid:63) n ≤ β + 1 (cid:88) (1 − β) (cid:88) a l,j n 1 − a l,j l=1 j(cid:54)=I(cid:63) (cid:26) (cid:113) (cid:27) 1 (cid:88)n (cid:88) exp − ∆ 162 m σi 2n Kl ≤ β + (1 − β) . n l=1 j(cid:54)=I(cid:63) 1 − exp (cid:26) − ∆2 min (cid:113) l (cid:27) 16σ2 K
Preprint Since, for a given ε > 0, there exists M s.t. ∀n > M , 8 8 (cid:26) ∆2 (cid:114) n (cid:27) 1 exp − min < , 16σ2 K 2 and there exists M s.t. ∀n > M , 9 9 (cid:26) ∆2 (cid:114) n (cid:27) ε (1 − β)(K − 1) exp − min < . 16σ2 K 4 Thus, ∀n > M (cid:44) max{M , M }, 10 8 9  (cid:26) (cid:113) (cid:27) (cid:26) (cid:113) (cid:27)  Ψ n n,I(cid:63) ≤ β + 1 − n β   M (cid:88) l=1 10 j(cid:88) (cid:54)=I(cid:63) 1 −ex ep xp− (cid:26) −∆ 162 m ∆σ 1i 62n 2 m σi 2n (cid:113)Kl Kl (cid:27) + l=M(cid:88)n 10+1 j(cid:88) (cid:54)=I(cid:63) 1 −ex ep xp− (cid:26) −∆ 162 m ∆σ 1i 62n 2 m σi 2n (cid:113)Kl Kl (cid:27)    (cid:26) (cid:113) (cid:27) ≤ β + 1 − β M (cid:88)10 (cid:88) exp − ∆ 162 m σi 2n Kl + 2(1 − β)(K − 1) exp (cid:40) − ∆2 min (cid:114) M 10 (cid:41) n l=1 j(cid:54)=I(cid:63) 1 − exp (cid:26) − ∆2 min (cid:113) l (cid:27) 16σ2 K 16σ2 K (cid:26) (cid:113) (cid:27) 1 − β M (cid:88)10 (cid:88) exp − ∆ 162 m σi 2n Kl ε ≤ β + + . n l=1 j(cid:54)=I(cid:63) 1 − exp (cid:26) − ∆2 min (cid:113) l (cid:27) 2 16σ2 K There exists M = Poly(ε/2, M ) s.t. ∀n > M , 11 10 11 (cid:26) (cid:113) (cid:27) 1 − β M (cid:88)10 (cid:88) exp − ∆ 162 m σi 2n Kl ε < . n l=1 j(cid:54)=I(cid:63) 1 − exp (cid:26) − ∆2 min (cid:113) l (cid:27) 2 16σ2 K Therefore, ∀n > M (cid:44) max{M , M }, we have 7 3 11 Ψ n,I(cid:63) ≤ β + ε. n Conclusion Finally, combining the two steps and define M (cid:44) max{M , M }, we have ∀n > M , 3 4 7 3 (cid:12) (cid:12) (cid:12) (cid:12) Ψ n,I(cid:63) − β(cid:12) (cid:12) ≤ ε. (cid:12) n (cid:12) With the help of the previous lemma and Lemma 4, we can finally prove Lemma 7. Proof of Lemma 7 Fix an ε > 0. Using Lemma 4, we have ∀n ∈ N, (cid:12) (cid:12) (cid:112) (cid:12) (cid:12) T n,I(cid:63) − Ψ n,I(cid:63) (cid:12) (cid:12) ≤ W 2 (n + 1) log(e2 + n) . (cid:12) n n (cid:12) n Thus there exists M s.t. ∀n > M , 12 12 (cid:12) (cid:12) (cid:12) (cid:12) T n,I(cid:63) − Ψ n,I(cid:63) (cid:12) (cid:12) ≤ ε . (cid:12) n n (cid:12) 2 And using Lemma 13, there exists M (cid:48) = Poly(ε/2, W , W ) s.t. ∀n > M (cid:48) , 3 1 2 3 (cid:12) (cid:12) (cid:12) (cid:12) Ψ n,I(cid:63) − β(cid:12) (cid:12) ≤ ε . (cid:12) n (cid:12) 2
Preprint Again, according to Lemma 10, there exists M (cid:48) s.t. ∀n > M (cid:48) , 3 3 Ψ ε n,I(cid:63) ≤ β + . n 2 Thus, if we take N (cid:44) max{M (cid:48) , M }, then ∀n > N , we have 3 3 12 3 (cid:12) (cid:12) (cid:12) (cid:12) T n,I(cid:63) − β(cid:12) (cid:12) ≤ ε. (cid:12) n (cid:12) C.4 Measurement effort concentration of other arms, proof of Lemma 8 under TTTS In this section, we show that, for TTTS, the empirical measurement effort concentration also holds for other arms than the true best arm. We first show that if some arm is overly sampled at time n, then its probability of being picked is reduced exponentially. Lemma 14. Under TTTS, for every ξ ∈ (0, 1), there exists S = Poly(1/ξ, W , W ) such that for all n > S , for 1 1 2 1 all i (cid:54)= I(cid:63), Ψ n,i ≥ ωβ + ξ ⇒ ψ ≤ exp {−ε (ξ)n} , n i n,i 0 where ε is defined in (11) below. 0 Proof. First, by Lemma 12, there exists M (cid:48)(cid:48) = Poly(W , W ) s.t. ∀n > M (cid:48)(cid:48), 1 1 2 1 I(cid:63) = I(cid:63) = J(1). n n Then, following the similar argument as in Lemma 25, one can show that for all i (cid:54)= I(cid:63) and for all n > M (cid:48)(cid:48), 1   ψ n,i = a n,i β + (1 − β) (cid:88) 1 −a n a,j  n,j j(cid:54)=i (cid:80) a ≤ a β + a (1 − β) j(cid:54)=i n,j n,i n,i 1 − a n,Jn(1) (cid:80) a = a β + a (1 − β) j(cid:54)=i n,j n,i n,i 1 − a n,I(cid:63) 1 ≤ a β + a (1 − β) n,i n,i 1 − a n,I(cid:63) a ≤ n,i 1 − a n,I(cid:63) Π [θ ≥ θ ] ≤ n i I(cid:63) Π [∪ θ ≥ θ ] n j(cid:54)=I(cid:63) j I(cid:63) Π [θ ≥ θ ] ≤ n i I(cid:63) . max Π [θ ≥ θ ] j(cid:54)=I(cid:63) n j I(cid:63)
Preprint Using the upper and lower Gaussian tail bounds from Lemma 2, we have (cid:26) (µ − µ )2 (cid:27) exp − n,I(cid:63) n,i 2σ2 (1/T + 1/T ) ψ n,i ≤  (cid:32) n,I(cid:63) n,i (cid:33)2  1 (µ − µ )  exp − min n,I(cid:63) n,j − 1 (cid:112)  j(cid:54)=I(cid:63) 2 σ (1/T n,I(cid:63) + 1/T n,j)  (cid:26) (µ − µ )2 (cid:27) exp −n n,I(cid:63) n,i 2σ2 (n/T + n/T ) =  n,I(cid:63) n,i  , (cid:32) (cid:33)2  (µ − µ ) 1  exp −n min n,I(cid:63) n,j − √ (cid:112)  j(cid:54)=I(cid:63) 2σ2 (n/T n,I(cid:63) + n/T n,j) 2n  where we assume that n > S = Poly(W , W ) for which 2 1 2 (µ − µ )2 n,I(cid:63) n,i ≥ 1 σ2 (1/T + 1/T ) n,I(cid:63) n,i according to Lemma 5. From there we take a supremum over the possible allocations to lower bound the denominator and write (cid:26) (µ − µ )2 (cid:27) exp −n n,I(cid:63) n,i 2σ2 (n/T + n/T ) ψ n,i ≤  (cid:32) n,I(cid:63) n,i (cid:33)2  (µ − µ ) 1  exp −n sup min n,I(cid:63) n,i − √ (cid:112)  ω:ωI(cid:63)=Tn,I(cid:63)/n j(cid:54)=I(cid:63) 2σ2 (1/ω I(cid:63) + 1/ω j) 2n  (cid:26) (µ − µ )2 (cid:27) exp −n n,I(cid:63) n,i 2σ2 (n/T + n/T ) = n,I(cid:63) n,i , (cid:40) (cid:18) (cid:113) 1 (cid:19)2(cid:41) exp −n Γ(cid:63) (µ ) − √ Tn,I(cid:63)/n n 2n where µ (cid:44) (µ , · · · , µ ), and (β, µ) (cid:55)→ Γ(cid:63) (µ) represents a function that maps β and µ to the parameterized n n,1 n,K β optimal error decay that any allocation rule can reach given parameter β and a set of arms with means µ. Note that this function is continuous with respect to β and µ respectively. Now, assuming Ψ /n ≥ ωβ +ξ yields that there exists S(cid:48) (cid:44) Poly(2/ξ, W ) s.t. for all n > S(cid:48) , T /n ≥ ωβ +ξ/2, n,i i 2 2 2 n,i i and by consequence,   ψ n,i ≤ exp  −n (cid:124)  2σ2 (cid:16) n/( Tµ nn ,I,I (cid:63)(cid:63) +− 1µ /n (ω,i iβ)2 + ξ/2)(cid:17) − Γ(cid:63) T (cid:123)n (cid:122),I(cid:63)/n (µ n) − 21 n + (cid:115) 2Γ(cid:63) Tn,I(cid:63) n/n (µ n)   (cid:125)  . εn(ξ) Using Lemma 7, we know that for any ε, there exists S = Poly(1/ε, W , W ) s.t. ∀n > S , |T /n − β| ≤ ε, 3 1 2 3 n,I(cid:63) and ∀j ∈ A, |µ − µ | ≤ ε. Furthermore, (β, µ) (cid:55)→ Γ(cid:63) (µ) is continuous with respect to β and µ, thus for a n,j j β given ε , there exists S(cid:48) = Poly(1/ε , W , W ) s.t. ∀n > S(cid:48) , we have 0 3 0 1 2 3 (cid:12)  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)ε n(ξ) −  2σ2 (cid:16) 1/( βµ +I(cid:63) 1− /(µ ωi β)2 + ξ/2)(cid:17) − Γ(cid:63) β(cid:12) (cid:12) (cid:12) (cid:12) ≤ ε 0. (cid:12) i (cid:12) Finally, define S (cid:44) max{S , S(cid:48) , S(cid:48) }, we have ∀n > S , 1 2 2 3 1 ψ ≤ exp {−ε (ξ)n} , n,i 0
Preprint where (µ − µ )2 ε (ξ) = I(cid:63) i − Γ(cid:63) + ε . (11) 0 (cid:16) (cid:17) β 0 2σ2 1/β + 1/(ωβ + ξ/2) i Next, starting from some known moment, no arm is overly allocated. More precisely, we show the following lemma. Lemma 15. Under TTTS, for every ξ, there exists S = Poly(1/ξ, W , W ) s.t. ∀n > S , 4 1 2 4 Ψ ∀i ∈ A, n,i ≤ ωβ + ξ. n i Proof. From Lemma 14, there exists S(cid:48) = Poly(2/ξ, W , W ) such that for all n > S(cid:48) and for all i (cid:54)= I(cid:63), 1 1 2 1 Ψ ξ n,i ≥ ωβ + ⇒ ψ ≤ exp {−ε (ξ/2)n} . n i 2 n,i 0 Thus, for all i (cid:54)= I(cid:63), n (cid:18) (cid:19) n (cid:18) (cid:19) (cid:88) ψ 1 Ψ (cid:96),i ≥ ωβ + ξ (cid:88) ψ 1 Ψ (cid:96),i ≤ ωβ + ξ (cid:96),i n i 2 (cid:96),i n i 2 Ψ n,i ≤ S 1(cid:48) + (cid:96)=S 1(cid:48) +1 + (cid:96)=S 1(cid:48) +1 n n n n (cid:88)n exp {−ε 0(ξ/2)n} (cid:96) (cid:88)n(ξ) ψ (cid:96),i1(cid:18) Ψ n(cid:96),i ≤ ω iβ + 2ξ (cid:19) ≤ S 1(cid:48) + (cid:96)=1 + (cid:96)=S 1(cid:48) +1 , n n n (cid:110) (cid:111) where we let (cid:96) (ξ) = max (cid:96) ≤ n : Ψ /n ≤ ωβ + ξ/2 . Then n (cid:96),i i n (cid:88) exp {−ε (ξ/2)n} 0 Ψ S(cid:48) n,i ≤ 1 + (cid:96)=1 + Ψ n n n (cid:96)n(ξ),i S(cid:48) + (1 − exp(−ε (ξ/2))−1 ξ ≤ 1 0 + ωβ + n i 2 Then, there exists S such that for all n ≥ S , 5 5 S(cid:48) + (1 − exp(−ε (ξ/2))−1 ξ 1 0 ≤ . n 2 Therefore, for any n > S (cid:44) max{S(cid:48) , S }, Ψ ≤ ωβ + ξ holds for all i (cid:54)= I(cid:63). For i = I(cid:63), it is already proved for 4 1 5 n,i i the optimal arm. We now prove Lemma 8 under TTTS. Proof of Lemma 8 From Lemma 15, there exists S(cid:48) = Poly((K − 1)/ξ, W , W ) such that for all n > S(cid:48) , 4 1 2 4 Ψ ξ ∀i ∈ A, n,i ≤ ωβ + . n i K − 1 Using the fact that Ψ /n and ωβ all sum to 1, we have ∀i ∈ A, n,i i Ψ n,i = 1 − (cid:88) Ψ n,j n n j(cid:54)=i (cid:18) (cid:19) (cid:88) ξ ≥ 1 − ωβ + j K − 1 j(cid:54)=i = ωβ − ξ. i
Preprint Thus, for all n > S(cid:48) , we have 4 (cid:12) (cid:12) ∀i ∈ A, (cid:12) (cid:12) Ψ n,i − ωβ(cid:12) (cid:12) ≤ ξ. (cid:12) n i (cid:12) And finally we use the same reasoning as the proof of Lemma 7 to link T and Ψ . Fix an ε > 0. Using n,i n,i Lemma 4, we have ∀n ∈ N, (cid:12) (cid:12) (cid:112) ∀i ∈ A, (cid:12) (cid:12) T n,i − Ψ n,i (cid:12) (cid:12) ≤ W 2 (n + 1) log(e2 + n) . (cid:12) n n (cid:12) n Thus there exists S s.t. ∀n > S , 5 5 (cid:12) (cid:12) (cid:12) (cid:12) T n,I(cid:63) − Ψ n,I(cid:63) (cid:12) (cid:12) ≤ ε . (cid:12) n n (cid:12) 2 And using the above result, there exists S(cid:48)(cid:48) = Poly(2/ε, W , W ) s.t. ∀n > S(cid:48)(cid:48), 4 1 2 4 (cid:12) (cid:12) (cid:12) (cid:12) Ψ n,i − ωβ(cid:12) (cid:12) ≤ ε . (cid:12) n i (cid:12) 2 Thus, if we take N (cid:44) max{S(cid:48)(cid:48), S }, then ∀n > N , we have 4 4 5 4 (cid:12) (cid:12) ∀i ∈ A, (cid:12) (cid:12) T n,i − ωβ(cid:12) (cid:12) ≤ ε. (cid:12) n i (cid:12) D Fixed-Confidence Analysis for T3C This section is entirely dedicated to T3C. Note that the analysis to follow share the same proof line with that of TTTS, and some parts even completely coincide with those of TTTS. For the sake of simplicity and clearness, we shall only focus on the parts that differ and skip some redundant proofs. D.1 Sufficient exploration of all arms, proof of Lemma 5 under T3C To prove this lemma, we still need the two sets of indices for under-sampled arms like in Appendix C.1. We recall that for a given L > 0: ∀n ∈ N we define √ U L (cid:44) {i : T < L}, n n,i V L (cid:44) {i : T < L3/4}. n n,i For T3C however, we investigate the following two indices, J(1) (cid:44) arg max a , J(cid:103)(2) (cid:44) arg min W (J(1), j). n n,j n n n j j(cid:54)=Jn(1) Lemma 5 is proved via the following sequence of lemmas. Lemma 16. There exists L = Poly(W ) s.t. if L > L , for all n, U L (cid:54)= ∅ implies J(1) ∈ V L or J(cid:103)(2) ∈ V L. 1 1 1 n n n n n Proof. If J(1) ∈ V L, then the proof is finished. Now we assume that J(1) ∈ V L ⊂ U L, and we prove that n n n n n J(2) ∈ V L. n n Step 1 Following the same reasoning as Step 1 and Step 2 of the proof of Lemma 9, we know that there exists L = Poly(W ) s.t. if L > L , then 2 1 2 J(cid:63) (cid:44) arg max µ = arg max µ = J(1). n n,j j n j∈UL j∈UL n n
Preprint Step 2 Now assuming that L > L , and we show that for L large enough, J(cid:103)(2) ∈ V L. In the same way that 2 n n we proved (10) one can show that for all ∀j ∈ V L, n (µ − µ )2 L3/4∆2 W (J(1), j) = n,I(cid:63) n,j ≥ min . n n (cid:18) 1 1 (cid:19) 16σ2 2σ2 + T T n,I(cid:63) n,j Again, denote J(cid:63) (cid:44) arg max µ , we obtain n j∈UL n,j n  0 if µ ≥ µ , W n(J n(1), J n(cid:63)) =  (µ (cid:32)n,Jn(1) − µ n,J n(cid:63) )2 (cid:33) elsen . ,J n(cid:63) n,Jn(1)  2σ2 T 1 + T 1 n,Jn(1) n,J n(cid:63) In the second case, as already shown in Step 3 of Lemma 9 we have that (cid:115) log(e + T ) n,J(cid:63) |µ − µ | ≤ ∆ + 2σW n n,J n(cid:63) n,J n(cid:63) max 1 1 + T n,J(cid:63) n (cid:115) √ log(e + L) ≤ ∆ + 2σW √ , max 1 1 + L since J(cid:63) ∈ U L. We also know that n n (cid:32) (cid:33) 1 1 2σ2 2σ2 2σ2 + ≥ ≥ √ . T n,Jn(1) T n,J n(cid:63) T n,J n(cid:63) L Therefore, we get √  (cid:115) √ 2 L log(e + L) W n(J n(1), J n(cid:63)) ≤ 2σ2 ∆ max + 2σW 1 1 + √ L  . On the other hand, we know that for all j ∈ V L, n L3/4∆2 W (J(1), j) ≥ min . n n 16σ2 Thus, there exists L s.t. if L > L , then 3 3 ∀j ∈ V L, W (J(1), j) ≥ 2W (J(1), J(cid:63)). n n n n n n That means J(cid:103)(2) ∈/ V L and by consequence, J(cid:103)(2) ∈ V L. n n n n Finally, taking L = max(L , L ), we have ∀L > L , either J(1) ∈ V L or J(cid:103)(2) ∈ V L. 1 2 3 1 n n n n Next we show that there exists at least one arm in V L for whom the probability of being pulled is large enough. n More precisely, we prove the following lemma. Lemma 17. There exists L = Poly(W ) s.t. for L > L and for all n s.t. U L (cid:54)= ∅, then there exists J ∈ V L 1 1 1 n n n s.t. min(β, 1 − β) ψ ≥ (cid:44) ψ . n,Jn K2 min Proof. Using Lemma 16, we know that J(1) or J(cid:103)(2) ∈ V L. We also know that under T3C, for any arm i, ψ can n n n n,i be written as ψ n,i = βa n,i + (1 − β) (cid:88) a n,j 1{W (cid:12)n(j, i) = min k(cid:54)=j W n( (cid:12)j, k)} . j(cid:54)=i (cid:12) arg min k(cid:54)=j W n(j, k)(cid:12)
Preprint Note that (ψ ) sums to 1, n,i i (cid:88) ψ n,i = β + (1 − β) (cid:88) a n,j (cid:88) 1{W (cid:12)n(j, i) = min k(cid:54)=j W n( (cid:12)j, k)} i j i(cid:54)=j (cid:12) arg min k(cid:54)=j W n(j, k)(cid:12) (cid:88) = β + (1 − β) a = 1 . n,j j Therefore, we have β ψ ≥ βa ≥ n,Jn(1) n,Jn(1) K (cid:80) on one hand, since a = 1. On the other hand, we have i∈A n,i a ψ ≥ (1 − β) n,Jn(1) n,J(cid:103) n(2) K 1 − β ≥ , K2 which concludes the proof. The rest of this subsection is exactly the same to that of TTTS. Indeed, with the above lemma, we can show that the set of poorly explored arms U L is empty when n is large enough. n Lemma 18. Under T3C, there exists L = Poly(W , W ) s.t. ∀L > L , U L = ∅. 0 1 2 0 (cid:98)KL(cid:99) Proof. See proof of Lemma 11 in Appendix C.1. We can finally conclude the proof of Lemma 5 for T3C in the same way as for TTTS in Appendix C.1. D.2 Concentration of the empirical means, proof of Lemma 6 under T3C As a corollary of the previous section, we can show the concentration of µ to µ , and the proof remains the n,i i same as that of TTTS in Appendix C.2. D.3 Measurement effort concentration of the optimal arm, proof of Lemma 7 under T3C Next, we show that the empirical arm draws proportion of the true best arm for T3C concentrates to β when the total number of arm draws is sufficiently large. This proof also remains the same as that of TTTS in Appendix C.3. D.4 Measurement effort concentration of other arms, proof of Lemma 8 under T3C In this section, we show that, for T3C, the empirical measurement effort concentration also holds for other arms than the true best arm. Note that this part differs from that of TTTS. We again establish first an over-allocation implies negligible probability result as follow. Lemma 19. Under T3C, for every ξ ≤ ε with ε problem dependent, there exists S = Poly(1/ξ, W , W ) such 0 0 1 1 2 that for all n > S , for all i (cid:54)= I(cid:63), 1 Ψ (cid:26) ∆2 (cid:114) n (cid:27) n,i ≥ ωβ + 2ξ ⇒ ψ ≤ (K − 1) exp − min . n i n,i 16σ2 K Proof. Fix i (cid:54)= I(cid:63) s.t. Ψ /n ≥ ωβ + 2ξ, then using Lemma 4, there exists S = Poly(1/ξ, W ) such that for any n,i i 2 2 n > S , we have 2 T n,i ≥ ωβ + ξ. n i
Preprint Then, (cid:88) ψ ≤ βa + (1 − β) a 1{W (j, i) = min W (j, k)} n,i n,i n,j n n k(cid:54)=j j(cid:54)=i   (cid:88) ≤ βa n,i + (1 − β)  a n,j + a n,I(cid:63)1{W n(I(cid:63), i) = min W n(I(cid:63), k)} k(cid:54)=I(cid:63) j(cid:54)=i,I(cid:63) (cid:88) ≤ a + 1{W (I(cid:63), i) = min W (I(cid:63), k)}. n,j n n k(cid:54)=I(cid:63) j(cid:54)=I(cid:63) Next we show that the indicator function term in the previous inequality equals to 0. Using Lemma 3 and Lemma 7 for T3C, there exists S = Poly(1/ξ, W , W ) such that for any n > S , 3 1 2 3 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) T n n,I(cid:63) − β(cid:12) (cid:12) (cid:12) ≤ ξ2 and ∀j ∈ A, |µ n,j − µ j| ≤ ξ2. Now if ∀j (cid:54)= I(cid:63), i, we have T /n > ωβ, then n,j j n − 1 = (cid:88) T n,j n n j∈A = T n,I(cid:63) + T n,i + (cid:88) T n,j n n n j(cid:54)=I(cid:63),i (cid:88) > β − ε2 + ωβ + ε + ωβ ≥ 1, i j j(cid:54)=I(cid:63),i which is a contradiction. Thus there exists at least one j (cid:54)= I(cid:63), i, such that T /n ≤ ωβ. Assuming n > max(S , S ), we have 0 n,j0 j 2 3 (µ − µ )2 (µ − µ )2 W (I(cid:63), i) − W (I(cid:63), j ) = n,I(cid:63) n,i − n,I(cid:63) n,j0 n n 0 (cid:18) 1 1 (cid:19) (cid:18) 1 1 (cid:19) 2σ2 + 2σ2 + T T T T n,I(cid:63) n,i n,I(cid:63) n,j0 (µ − µ − 2ξ2)2 (µ − µ + 2ξ2)2 ≥ I(cid:63) i − I(cid:63) j0 . (cid:32) (cid:33) (cid:32) (cid:33) 1 1 1 1 2σ2 + 2σ2 + β − ξ2 ωβ + ξ β + ξ2 ωβ i j0 (cid:124) (cid:123)(cid:122) (cid:125) W ξ i,j0 According to Proposition 1, W ξ converges to 0 when ξ goes to 0, more precisely we have i,j0 (cid:32) (cid:33)2 (µ − µ )2 β W ξ = I(cid:63) i ξ + O(ξ2) , i,j0 2σ2 β + ωβ i thus there exists a ε such that for all ξ < ε it holds for all i, j (cid:54)= I(cid:63), W ξ > 0. It follows then 0 0 0 i,j0 W (I(cid:63), i) − min W (I(cid:63), k) ≥ W (I(cid:63), i) − W (I(cid:63), j ) > 0, n n n n 0 k(cid:54)=I(cid:63) and 1{W (I(cid:63), i) = min W (I(cid:63), k)} = 0. n k(cid:54)=I(cid:63) n Knowing that Lemma 12 is also valid for T3C, thus there exists M = Poly(4/∆ , W , W ) such that for all 1 min 1 2 n > M , 1 (cid:26) ∆2 (cid:114) n (cid:27) ∀j (cid:54)= I(cid:63), a ≤ exp − min , n,j 16σ2 K which then concludes the proof by taking S (cid:44) max(M , S , S ). 1 1 2 3
Preprint The rest of this subsection almost coincides with that of TTTS. We first show that, starting from some known moment, no arm is overly allocated. More precisely, we show the following lemma. Lemma 20. Under T3C, for every ξ, there exists S = Poly(1/ξ, W , W ) s.t. ∀n > S , 4 1 2 4 Ψ ∀i ∈ A, n,i ≤ ωβ + 2ξ. n i Proof. See proof of Lemma 15 in Appendix C.4. Note that the previous step does not match exactly that of TTTS, so the proof would be slightly different. However, the difference is only a matter of constant, we thus still choose to skip this proof. It remains to prove Lemma 8 for T3C, which stays the same as that of TTTS. Proof of Lemma 8 for T3C See proof of Lemma 8 for TTTS in Appendix C.4. E Proof of Lemma 1 Finally, it remains to prove Lemma 1 under the Gaussian case before we can conclude for Theorem 1 for TTTS or T3C. (cid:104) (cid:105) Lemma 1. Let δ, β ∈ (0, 1). For any sampling rule which satisfies E T ε < ∞ for all ε > 0, we have β E [τ ] 1 lim sup δ ≤ , log(1/δ) Γ(cid:63) δ→0 β if the sampling rule is coupled with stopping rule (3), For the clarity, we recall the definition of generalized likelihood ratio. For any pair of arms i, j, We first define a weighted average of their empirical means, T T µ (cid:44) n,i µ + n,j µ . (cid:98)n,i,j T + T (cid:98)n,i T + T (cid:98)n,j n,i n,j n,i n,j And if µ ≥ µ , then the generalized likelihood ratio Z for Gaussian noise distributions has the following (cid:98)n,i (cid:98)n,j n,i,j analytic expression, Z (cid:44) T d(µ ; µ ) + T d(µ ; µ ). n,i,j n,i (cid:98)n,i (cid:98)n,i,j n,j (cid:98)n,j (cid:98)n,i,j We further define a statistic Z as n Z (cid:44) max min Z . n n,i,j i∈A j∈A\{i} The following lemma stated by Qin et al. (2017) is needed in our proof. Lemma 21. For any ζ > 0, there exists ε s.t. ∀n ≥ T ε, Z ≥ (Γ(cid:63) − ζ)n. β n β To prove Lemma 1, we need the Gaussian tail inequality (7) of Lemma 2. Proof. We know that (cid:88) 1 − a = a n,I(cid:63) n,i i(cid:54)=I(cid:63) (cid:88) ≤ Π [θ > θ ] n i I(cid:63) i(cid:54)=I(cid:63) (cid:88) = Π [θ − θ > 0] n i I(cid:63) i(cid:54)=I(cid:63) ≤ (K − 1) max Π [θ − θ > 0] . n i I(cid:63) i(cid:54)=I(cid:63)
Preprint We can further rewrite Π [θ − θ > 0] as n i I(cid:63) Π [θ − θ > µ − µ + µ − µ ] . n i I(cid:63) n,i n,I(cid:63) n,I(cid:63) n,i We choose ε sufficiently small such that the empirical best arm I(cid:63) = I(cid:63). Then, for all n ≥ T n and for any i (cid:54)= I(cid:63), n β µ ≥ µ . Thus, fix any ζ ∈ (0, Γ(cid:63) /2) and apply inequality (7) of Lemma 2 with µ and µ , we have for n,I(cid:63) n,i β n,I(cid:63) n,i any n ≥ T ε, β (cid:40) (cid:41) 1 (µ − µ )2 1 − a ≤ (K − 1) max exp − n,I(cid:63) n,i n,I(cid:63) i(cid:54)=I(cid:63) 2 2σ n2 ,i,I(cid:63) (K − 1) exp {−Z } = n 2 (cid:110) (cid:111) (K − 1) exp −(Γ(cid:63) − ζ)n β ≤ . 2 The last inequality is deduced from Lemma 21. By consequence, K − 1 ∀n ≥ T ε, ln (1 − a ) ≤ ln − (Γ(cid:63) − ζ)n. β n,I(cid:63) 2 β On the other hand, we have for any n, δ 1 − c = . n,δ (cid:40)(cid:114) (cid:41) √ 2n(K − 1) 2n(K − 1) 2πe exp 2 ln δ Thus, there exists a deterministic time N s.t. ∀n ≥ N , (cid:114) δ 2n(K − 1) ln (1 − c ) = ln √ − ln n − 2 ln n,δ (K − 1) 8πe δ δ ≥ ln √ − ζn. 2(K − 1) 2πe √ Let C (cid:44) (K − 1)2 2πe, we have for any n ≥ N (cid:44) T ε + N , 3 0 β C ln (1 − a ) − ln (1 − c ) ≤ ln 3 − (Γ(cid:63) − 2ζ)n, (12) n,I(cid:63) n,δ δ β and it is clear that E [N ] < ∞. 0 Let us consider the following two cases: Case 1 There exists n ∈ [1, N ] s.t. a ≥ c , then by definition, 0 n,I(cid:63) n,δ τ ≤ n ≤ N . δ 1 Case 2 For any n ∈ [1, N ], we have a < c , then τ ≥ N + 1, thus by Equation 12, 0 n,I(cid:63) n,δ δ 0 0 ≤ ln (1 − a ) − ln (1 − c ) τδ−1,I(cid:63) τδ−1,δ C ≤ ln 3 − (Γ(cid:63) − 2ζ)(τ − 1), δ β δ and we obtain ln(C /δ) τ ≤ 3 + 1. δ Γ(cid:63) − 2ζ β
Preprint Combining the two cases, and we have for any ζ ∈ (0, Γ(cid:63) /2), β (cid:40) (cid:41) ln(C /δ) τ ≤ max N , 3 + 1 δ 0 Γ(cid:63) − 2ζ β ln(C ) ln(1/δ) ≤ N + 1 + 3 + . 0 Γ(cid:63) − 2ζ Γ(cid:63) − 2ζ β β Since E [N ] < ∞, therefore 1 E [τ ] 1 lim sup δ ≤ , ∀ζ ∈ (0, Γ(cid:63) /2), log(1/δ) Γ(cid:63) − 2ζ β δ β which concludes the proof. F Technical Lemmas The whole fixed-confidence analysis for the two sampling rules are both substantially based on two lemmas: Lemma 5 of Qin et al. (2017) and Lemma 4. We prove Lemma 4 in this section. Lemma 4. There exists a random variable W , such that for all i ∈ A, 2 (cid:112) ∀n ∈ N, |T − Ψ | ≤ W (n + 1) log(e2 + n) a.s., n,i n,i 2 and E (cid:2) eλW2(cid:3) < ∞ for any λ > 0. Proof. The proof shares some similarities with that of Lemma 6 of Qin et al. (2017). For any arm i ∈ A, define ∀n ∈ N, D (cid:44) T − Ψ , n n,i n,i d (cid:44) 1{I = i} − ψ . n n n,i It is clear that D = (cid:80)n−1 d and E [d |F ] = 0. Indeed, n l=1 l n n−1 E [d |F ] = E [1{I = i} − ψ |F ] n n−1 n n,i n−1 = P [I = i|F ] − E [P [I = i|F ] |F ] n n−1 n n−1 n−1 = P [I = i|F ] − P [I = i|F ] = 0. n n−1 n n−1 The second last equality holds since P [I = i|F ] is F -measurable. Thus D is a martingale, whose n n−1 n−1 n increment are 1 sub-Gaussian as d ∈ [−1, 1] for all n. n Applying Corollary 8 of Abbasi-Yadkori et al. (2012)6, it holds that, with probability larger than 1 − δ, for all n, (cid:115) √ (cid:18) (cid:19) 1 + n |D | ≤ 2 (1 + n) ln n δ which yields the first statement of Lemma 4. We now introduce the random variable |T − Ψ | W (cid:44) max max n,i n,i . 2 (cid:112) n∈N i∈A (n + 1) ln(e2 + n) Applying the previous inequality with δ = e−x2/2 yields P (cid:104) ∃n ∈ N(cid:63) : |D | > (cid:112) (1 + n) (ln (1 + n) + x2)(cid:105) ≤ e−x2/2, n P (cid:104) ∃n ∈ N(cid:63) : |D | > (cid:112) (1 + n) ln (e2 + n) x2(cid:105) ≤ e−x2/2, n 6but we could actually use several deviation inequalities that hold uniformly over time for martingales with sub- Gaussian increments
Preprint where the last inequality uses that for all a, b ≥ 2, we have ab ≥ a + b. Consequently ∀x ≥ 2, for all i ∈ A (cid:34) (cid:35) |T − Ψ | P max n,i n,i ≥ x ≤ e−x2/2. (cid:112) n∈N (n + 1) log (e2 + n) Now taking a union bound over i ∈ A, we have ∀x ≥ 2, (cid:34) (cid:35) |T − Ψ | P [W ≥ x] ≤ P max max n,i √n,i ≥ x 2 (cid:0) (cid:1) i∈A n∈N (n + 1) log e2 + n (cid:34) (cid:35) ≤ P (cid:91) max |T n,i − Ψ √n,i| ≥ x (cid:0) (cid:1) n∈N (n + 1) log e2 + n i∈A (cid:34) (cid:35) ≤ (cid:88) P max |T n,i − Ψ √n,i| ≥ x (cid:0) (cid:1) n∈N (n + 1) log e2 + n i∈A ≤ Ke−x2/2. (cid:112) The previous inequalities imply that ∀i ∈ A and ∀n ∈ N, we have |T − Ψ | ≤ W (n + 1) log(e2 + n) almost n,i n,i 2 surely. Now it remains to show that ∀λ > 0, E (cid:2) eλW2(cid:3) < ∞. Fix some λ > 0. (cid:90) ∞ (cid:90) ∞ E (cid:2) eλW (cid:3) = P (cid:2) eλW ≥ x(cid:3) dx = P (cid:2) eλW ≥ e2λy(cid:3) 2λe2λy dy x=1 y=0 (cid:90) 2 (cid:90) ∞ = 2λ P [W ≥ 2y] e2λy dy + 2λ P [W ≥ 2y] e2λy dy y=0 y=2 (cid:90) 2 (cid:90) ∞ ≤ 2λ P [W ≥ 2y] e2λy dy + 2λC e−y2/2e2λy dy < ∞, 1 y=0 y=2 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =e4λ−1 <∞ where C is some constant. 1 G Proof of Posterior Convergence for the Gaussian Bandit G.1 Proof of Theorem 4 Theorem 4. Under TTTS, for Gaussian bandits with improper Gaussian priors, it holds almost surely that 1 lim − log(1 − a ) = Γ(cid:63) . n→∞ n n,I(cid:63) β From Theorem 2 in Qin et al. (2017), any allocation rule satisfying T /n → ωβ for each i ∈ A, satisfies n,i i 1 lim − log(1 − a ) = Γ(cid:63) . n→∞ n n,I(cid:63) β Therefore, to prove Theorem 4, it is sufficient to prove that under TTTS, T ∀i ∈ {1, . . . , K}, lim n,i a =.s ωβ. (13) n→∞ n i Due to the concentration result in Lemma 4 that we restate below (and proved in Appendix C), which will be useful at several places in the proof, observe that T Ψ lim n,i a =.s ωβ ⇔ lim n,i a =.s ωβ, n→∞ n i n→∞ n i therefore it suffices to establish the convergence of ψ = Ψ /n to ωβ, which we do next. For that purpose, we n,i n,i i need again the following maximality inequality lemma.
Preprint Lemma 4. There exists a random variable W , such that for all i ∈ A, 2 (cid:112) ∀n ∈ N, |T − Ψ | ≤ W (n + 1) log(e2 + n) a.s., n,i n,i 2 and E (cid:2) eλW2(cid:3) < ∞ for any λ > 0. Step 1: TTTS draws all arms infinitely often and satisfies T /n → β. More precisely, we prove the n,I(cid:63) following lemma. Lemma 22. Under TTTS, it holds almost surely that 1. for all i ∈ A, lim T = ∞. n→∞ n,i 2. a → 1. n,I(cid:63) 3. T /n → β. n,I(cid:63) Proof. Our first ingredient is a lemma showing the implications of finite measurement, and consistency when all arms are sampled infinitely often. Its proof follows standard posterior concentration arguments and is given in Appendix G.2. Lemma 23 (Consistency and implications of finite measurement). Denote with I the arms that are sampled only a finite amount of times: I = {i ∈ {1, . . . , k} : ∀n, T < ∞}. n,i If I is empty, a converges almost surely to 1 when i = I(cid:63) and to 0 when i (cid:54)= I(cid:63). If I is non-empty, then for n,i every i ∈ I, we have lim inf a > 0 a.s. n→∞ n,i (cid:80) First we show that T = ∞ for each arm j. Suppose otherwise. Let I again be the set of arms to which n∈N n,j only finite measurement effort is allocated. Under TTTS, we have   ψ n,i = a n,i β + (1 − β) (cid:88) 1 −a n a,j  , n,j j(cid:54)=i (cid:80) so ψ ≥ βa . Therefore, by Lemma 23, if i ∈ I, then lim inf a > 0 implies that ψ = ∞. By Lemma 4, n,i n,i n,i n n,i we then must have that lim T = ∞ as well: contradiction. Thus, lim T = ∞ for all i, and we n→∞ n,i n→∞ n,i conclude that a → 1, by Lemma 23. n,I(cid:63) For TTTS with parameter β this implies that ψ → β, and since we have a bound on |T /n−ψ | in Lemma 4, n,I(cid:63) n,i n,i we have T /n → β as well. n,I(cid:63) Step 2: Controlling the over-allocation of sub-optimal arms. The convergence of T /n to β leads n,I(cid:63) to following interesting consequence, expressed in Lemma 24: if an arm is sampled more often than its optimal proportion, the posterior probability of this arm to be optimal is reduced compared to that of other sub-optimal arms. Lemma 24 (Over-allocation implies negligible probability). 7 Fix any ξ > 0 and j (cid:54)= I(cid:63). With probability 1, under any allocation rule, if T /n → β, there exist ξ(cid:48) > 0 and a sequence ε with ε → 0 such that for any n,I(cid:63) n n n ∈ N, T a n,j ≥ ωβ + ξ ⇒ n,j ≤ e−n(ξ(cid:48)+εn). n j max a i(cid:54)=I(cid:63) n,i (cid:80) Proof. We have Π (Θ ) = a = 1 − a , therefore max a ≤ 1 − a . By Theorem 2 of Qin n ∪i(cid:54)=I(cid:63) i(cid:54)=I(cid:63) n,i n,I(cid:63) i(cid:54)=I(cid:63) n,i n,I(cid:63) et al. (2017) we have, as T /n → β, n,I(cid:63) (cid:18) (cid:19) 1 lim sup − log max a ≤ Γ(cid:63) . n→∞ n i(cid:54)=I(cid:63) n,i β 7analogue of Lemma 13 of Russo (2016)
Preprint We also have the following from the standard Gaussian tail inequality, for n ≥ τ after which µ ≥ µ , using n,I(cid:63) n,i that θ − θ ∼ N (µ − µ , σ2 + σ2 ) and σ2 + σ2 = σ2(1/T + 1/T ), i I(cid:63) n,i n,I(cid:63) n,i n,I(cid:63) n,i n,I(cid:63) n,i n,I(cid:63) (cid:18) −(µ − µ )2 (cid:19) (cid:18) (µ − µ )2 (cid:19) a ≤ Π (θ ≥ θ ) ≤ exp n,i n,I(cid:63) = exp −n n,i n,1 . n,i n i I(cid:63) 2σ2(1/T + 1/T ) 2σ2(n/T + n/T ) n,I(cid:63) n,i n,I(cid:63) n,i Thus, there exists a sequence ε → 0, for which n (cid:26) (cid:18) (µ − µ )2 (cid:19)(cid:27) exp −n n,j n,I(cid:63) − ε /2 a 2σ2(n/T + n/T ) n (cid:26) (cid:18) (µ − µ )2 (cid:19)(cid:27) n,j ≤ n,I(cid:63) n,j = exp −n n,j n,I(cid:63) − Γ(cid:63) − ε . max i(cid:54)=I(cid:63) a n,i exp (cid:8) −n (cid:0) Γ(cid:63) β + ε n/2(cid:1)(cid:9) ) 2σ2(n/T n,I(cid:63) + n/T n,j) β n Now we take a look at the two terms in the middle: (µ − µ )2 n,j n,I(cid:63) − Γ(cid:63) . 2σ2(n/T + n/T ) β n,I(cid:63) n,j Note that the first term is increasing in T /n. We have the definition from Qin et al. (2017), for any j (cid:54)= I(cid:63), n,j (µ − µ )2 Γ(cid:63) = j I(cid:63) , β (cid:16) (cid:17) 2σ2 1/ωβ + 1/ωβ I(cid:63) j and we have the premise T n,j ≥ ωβ + ξ. n j Combining these with the convergence of the empirical means to the true means (consistency, see Lemma 23), we can conclude that for all ε > 0, there exists a time n such that for all later times n ≥ n , we have 0 0 (µ − µ )2 (µ − µ )2 (µ − µ )2 n,j n,I(cid:63) ≥ j I(cid:63) − ε ≥ j I(cid:63) − ε > Γ(cid:63) , 2σ2(n/T n,I(cid:63) + n/T n,j) 2σ2 (1/β + n/T n,j) 2σ2 (cid:16) 1/β + 1/(ωβ + ξ)(cid:17) β j where the first inequality follows from consistency, the second from monotonicity in T /n. That means that n,j there exist a ξ(cid:48) > 0 such that (µ − µ )2 n,j n,I(cid:63) − Γ(cid:63) > ξ(cid:48), 2σ2(n/T + n/T ) β n,I(cid:63) n,j and thus the claim follows that when Tn,j ≥ ωβ + ξ, we have n j a (cid:26) (cid:18) (µ − µ )2 (cid:19)(cid:27) n,j ≤ exp −n n,j n,I(cid:63) − Γ(cid:63) − ε ≤ e−n(ξ(cid:48)+εn). max a 2σ2(n/T + n/T ) β n i(cid:54)=I(cid:63) n,i n,I(cid:63) n,j Step 3: ψ converges to ωβ for all arms. To establish the convergence of the allocation effort of all arms, n,i i we rely on the same sufficient condition used in the analysis of Russo (2016), that we recall below. Lemma 25 (Sufficient condition for optimality). 8 Consider any adaptive allocation rule. If we have (cid:88) (cid:110) (cid:111) ψ → β, and ψ 1 ψ ≥ ωβ + ξ < ∞, ∀j (cid:54)= I(cid:63), ξ > 0, (14) n,I(cid:63) n,j n,j j n∈N then ψ → ψβ. n 8Lemma 12 of Russo (2016)
Preprint First, note that from Lemma 22 we know that T /n → β, an by Lemma 4 this implies ψ → β, hence we n,I(cid:63) n,I(cid:63) can use Lemma 25 to prove convergence to the optimal proportions. Thus, we now show that (14) holds under TTTS. Recall that J(1) = arg max a and J(2) = arg max a . Since a → 1 by Lemma 22, there is n j n,j n j(cid:54)=Jn(1) n,j n,I(cid:63) some finite time τ after which for all n > τ , J(1) = I(cid:63). Under TTTS, n   (cid:80) (cid:80) ψ n,i = a n,i β + (1 − β) (cid:88) 1 −a n a,j  ≤ a n,iβ + a n,i(1 − β) 1 −j(cid:54)= ai a n,j ≤ a n,iβ + a n,i(1 − β) aj(cid:54)=i a n,j j(cid:54)=i n,j n,Jn(1) n,Jn(2) 1 a ≤ a β + a (1 − β) ≤ n,i , n,i n,i a a n,Jn(2) n,Jn(2) where we use the fact that for j (cid:54)= J(1), we have a ≥ a and a ≤ 1 − a . For n ≥ τ this means n n,Jn(1) n,j n,Jn(2) n,Jn(1) that ψ ≤ a / max a for any i (cid:54)= I(cid:63). n,i n,i j(cid:54)=I(cid:63) n,i By Lemma 24, there is a constant ξ(cid:48) > 0 such and a sequence ε → 0 such that n a T /n ≥ wβ + ξ ⇒ n,i ≤ e−n(ξ(cid:48)−εn). n,i i max a j(cid:54)=I(cid:63) n,j Now take a time τ large enough, such that for n ≥ τ we have |T /n − ψ | ≤ ξ (which can be found by n,j n,j Lemma 4). Then we have (cid:110) (cid:111) (cid:26) T (cid:27) 1 ψ ≥ ψβ + ξ ≤ 1 n,j ≥ ωβ + 2ξ n,j j n j Therefore, for all i (cid:54)= I(cid:63), we have (cid:88) ψ 1(cid:110) ψ ≥ ψβ + ξ(cid:111) ≤ (cid:88) ψ 1(cid:26) T n,j ≥ ωβ + 2ξ(cid:27) ≤ (cid:88) e−n(ξ(cid:48)−εn) < ∞. n,i n,j j n,i n j n≥τ n≥τ n≥τ Thus (14) holds and the convergence to the optimal proportions follows by Lemma 25. G.2 Proof of auxiliary lemmas Proof of Lemma 23 Let I be nonempty. Define µ (cid:44) lim µ , and σ2 (cid:44) lim σ2 , ∞,n n,i ∞,i n,i n→∞ n→∞ and recall that for i ∈ A for which T = 0, we have µ = µ = 0 and σ2 = σ2 = ∞, and if T > 0, we n,i ni 1,i n,i 1,i n,i have 1 n (cid:88)−1 σ2 µ = 1{I = i}Y , and σ2 = . n,i T (cid:96) (cid:96),I(cid:96) n,i T n,i n,i (cid:96)=1 For all arms that are sampled infinitely often, we therefore have µ = µ and σ2 = 0. For all arms that are ∞,i i ∞,i sampled only a finite number of times, i.e. i ∈ I, we have σ2 > 0, and there exists a time n after which for all ∞,i 0 n ≥ n and i ∈ I, we have T = T . Define 0 n,i n0,i (cid:79) (cid:79) Π (cid:44) N (µ , σ2 ) ⊗ N (µ , σ2 ) ⊗ . . . ⊗ N (µ , σ2 ) = δ ⊗ Π . ∞ ∞,1 ∞,1 ∞,2 ∞,2 ∞,k ∞,k µi n0 i(cid:54)∈I i∈I Then for each i ∈ A we define (cid:18) (cid:19) a (cid:44) Π θ > max θ . ∞,i ∞ i j j(cid:54)=i Then we have for all i ∈ I, a ∈ (0, 1), since σ2 > 0, and thus a < 1. ∞,i ∞,i ∞,I(cid:63) (cid:78) When I is empty, we have a = Π (θ > max θ ), but since Π = δ , we have a = 1 and n,I(cid:63) n I(cid:63) i(cid:54)=I(cid:63) i ∞ i∈A µi ∞,I(cid:63) a = 0 for all i (cid:54)= I(cid:63). ∞,i
Preprint H Proof of Posterior Convergence for the Bernoulli Bandit H.1 Preliminaries We first introduce a crucial Beta tail bound inequality. Let F Beta denote the cdf of a Beta distribution with a,b parameters a and b, and F B the cdf of a Binomial distribution with parameters c and d, then we have the c,d following relationship, often called the ‘Beta-Binomial trick’, F Beta(y) = 1 − F B (a − 1), a,b a+b−1,y so that we have P [X ≥ x] = P [B ≤ a − 1] = P [B ≥ b] . a+b−1,x a+b−1,1−x We can bound Binomial tails with Sanov’s inequality: e−nd(k/n,x) ≤ P [B ≥ k] ≤ e−nd(k/n,x), n + 1 n,x where the last inequalities hold when k ≥ nx. Lemma 26. Let X ∼ Beta(a, b) and Y ∼ Beta(c, d) with 0 < a−1 < c−1 . Then we have P [X > Y ] ≤ De−C a+b−1 c+d−1 where C = inf C (y) + C (y), a,b c,d a−1 ≤y≤ c−1 a+b−1 c+d−1 and (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19) c − 1 a − 1 D = 3 + min C , C . a,b c + d − 1 c,d a + b − 1 Note that this lemma is the Bernoulli version of Lemma 2. Theorem 6. Consider the Beta-Bernoulli setting. For β ∈ (0, 1), under any allocation rule satisfying T /n → n,I(cid:63) ωβ , I(cid:63) 1 lim − log(1 − a ) ≤ Γ(cid:63) , n→∞ n n,I(cid:63) β and under any allocation rule satisfying T /n → ωβ for each i ∈ A, n,i i 1 lim − log(1 − a ) = Γ(cid:63) . n→∞ n n,I(cid:63) β Proof. Denote again with I again the set of arms sampled only finitely many times. For I empty, we thus have µ (cid:44) lim µ = µ . The posterior variance is ∞,i n→∞ n,i i α β (1 + (cid:80)n−1 1{I = i}Y )(1 + T − (cid:80)n−1 1{I = i}Y ) σ2 = n,i n,i = (cid:96)=1 (cid:96) (cid:96),I(cid:96) n,i (cid:96)=1 (cid:96) (cid:96),I(cid:96) . n,i (α + β )2(α + β + 1) (2 + T )2(2 + T + 1) n,i n,i n,i n,i n,i n,i We see that when I is empty, we have σ2 (cid:44) lim σ2 = 0, i.e., the posterior is concentrated. ∞,i n→∞ n,i Step 1: A lower bound when some arms are sampled only finitely often. First, note that when T = 0 for some i ∈ A, the empirical mean for that arm equals the prior mean µ = α /(α + β ), and the n,i n,i 0,i 0,i 0,i variance is strictly positive: σ2 = (α β )/ (cid:0) (α + β )2(α + β + 1)(cid:1) > 0. When I is not empty, then n,i 0,i 0,i 0,i 0,i 0,i 0,i for every i ∈ I we have σ2 > 0, and a ∈ (0, 1), implying a < 1, and thus ∞,i ∞,i ∞,I(cid:63) 1 1 lim − log (1 − a ) = − log (1 − a ) = 0. n→∞ n n,I(cid:63) n ∞,I(cid:63)
Preprint Step 2: A lower bound when every arm is sampled infinitely often. Suppose now that I is empty, then we have (cid:88) max Π (θ ≥ θ ) ≤ 1 − a ≤ Π (θ ≥ θ ) ≤ (k − 1) max Π (θ ≥ θ ). n i I(cid:63) n,I(cid:63) n i I(cid:63) n i I(cid:63) i(cid:54)=I(cid:63) i(cid:54)=I(cid:63) i(cid:54)=I(cid:63) . Thus, we have 1 − a ≤ (k − 1) max Π (θ ≥ θ ) and also 1 − a = max Π (θ ≥ θ ). We have n,I(cid:63) i(cid:54)=I(cid:63) n i I(cid:63) n,I(cid:63) i(cid:54)=I(cid:63) n i I(cid:63) Γ(cid:63) = max min C (ω , ω ), i I(cid:63) i w∈W i(cid:54)=I(cid:63) Γ(cid:63) = max min C (β, ω ), with β i i w∈W ;ωI(cid:63)=β i(cid:54)=I(cid:63) C (ω , ω ) = min ω d(θ ; x) + ω d(θ ; x) = ω d(θ ; θ) + ω d(θ ; θ), i I(cid:63) i I(cid:63) I(cid:63) i i I(cid:63) I(cid:63) i i x∈R where θ ∈ [θ , θ ] is the solution to i I(cid:63) ω A(cid:48)(θ ) + ω A(cid:48)(θ ) A(cid:48)(θ) = I(cid:63) I(cid:63) i i . ω + ω I(cid:63) i Since every arm is sampled infinitely often, when n is large, we have µ > µ . Define S (cid:44) n,I(cid:63) n,i n,i (cid:80)n−1 1{I = i}Y . Recall that the posterior is a Beta distribution with parameters a = S + 1 and (cid:96)=1 (cid:96) (cid:96),I(cid:96) n,i n,i β = T − S + 1. Let τ ∈ N be such that for every n ≥ τ , we have S /(T + 1) < S /(T + 1). For n,i n,i n,i n,i n,i n,I(cid:63) n,I(cid:63) the sake of simplicity, we define for any i ∈ A the interval (cid:20) (cid:21) S S I (cid:44) n,i , n,I(cid:63) . i,I(cid:63) T + 1 T + 1 n,i n,I(cid:63) Then using Lemma 26 with a = S + 1, b = T − S + 1, c = S + 1, d = T − S + 1, we have n,i n,i n,i n,I(cid:63) n,I(cid:63) n,I(cid:63) (cid:26) (cid:27) Π (θ − θ ≥ 0) ≤ D exp − inf C (y) + C (y) . n i I(cid:63) y∈Ii,I(cid:63) Sn,i+1,Tn,i−Sn,i+1 Sn,I(cid:63)+1,Tn,I(cid:63)−Sn,I(cid:63)+1 This implies (cid:32) (cid:33) 1 Π (θ ≥ θ ) 1 log n i I(cid:63) ≤ log(D), (cid:8) (cid:9) n exp − inf C (y) + C (y) n y∈Ii,I(cid:63) Sn,i+1,Tn,i−Sn,i+1 Sn,I(cid:63)+1,Tn,I(cid:63)−Sn,I(cid:63)+1 which goes to zero as n goes to infinity. Indeed replacing a, b, c, d by their values in the definition of D we get (cid:18) (cid:19) S S D ≤ 3 + (T − 1)kl n,i ; n,I(cid:63) n,i T + 1 T + 1 n,i n,I(cid:63) (cid:18) (cid:19) n ≤ 3 + (n + 1)kl 0; = (n + 1) log(n + 1) . n + 1 Hence, (cid:26) (cid:27) . Π (θ ≥ θ ) = exp − inf C (y) + C (y) . n i I(cid:63) y∈Ii,I(cid:63) Sn,i+1,Tn,i−Sn,i+1 Sn,I(cid:63)+1,Tn,I(cid:63)−Sn,I(cid:63)+1 We thus have for any i, . 1 − a = max Π [θ ≥ θ ] n,i n j I(cid:63) j(cid:54)=I(cid:63) (cid:26) (cid:27) . = max exp − inf C (y) + C (y) j(cid:54)=I(cid:63) y∈Ij,I(cid:63) Sn,j+1,Tn,j−Sn,j+1 Sn,I(cid:63)+1,Tn,I(cid:63)−Sn,I(cid:63)+1 (cid:26) (cid:18) (cid:19) (cid:18) (cid:19)(cid:27) =. exp −n min inf T n,j + 1 kl S n,j ; y + T n,I(cid:63) + 1 kl S n,I(cid:63) ; y j(cid:54)=I(cid:63) y∈Ij,I(cid:63) n T n,j + 1 n T n,I(cid:63) + 1 (cid:26) (cid:18) (cid:19) (cid:18) (cid:19)(cid:27) S S ≥ exp −n max min inf ω kl n,j ; y + ω kl n,I(cid:63) ; y . ω j(cid:54)=I(cid:63) y∈Ij,I(cid:63) i T n,j + 1 I(cid:63) T n,j + 1
Preprint Fix some ε > 0, then there exists some n (ε) such that for all n ≥ n (ε), we have for any j, 0 0 (cid:20) (cid:21) S S I = n,j , n,I(cid:63) , ⊂ [µ + ε, µ − ε] (cid:44) I(cid:63) , j,I(cid:63) T + 1 T + 1 j I(cid:63) j,ε n,j n,I(cid:63) and because KL-divergence is uniformly continuous on the compact interval I(cid:63) , there exists an n such that for j,ε 1 every n ≥ n we have 1 (cid:18) (cid:19) S kl n,j ; y ≥ (1 − ε)kl (µ ; y) , T + 1 j n,j for any y and for all j ∈ A. Therefore, we have (cid:26) (cid:18) (cid:19) (cid:18) (cid:19)(cid:27) 1 − a =. exp −n max min inf ω kl S n,j ; y + ω kl S n,I(cid:63) ; y n,i ω j(cid:54)=I(cid:63) y∈Ij,I(cid:63) j T n,j + 1 I(cid:63) T n,I(cid:63) + 1 (cid:26) (cid:27) ≥ exp −n max min inf ω kl(µ ; y) + ω kl(µ ; y) . i j I(cid:63) I(cid:63) ω i(cid:54)=I(cid:63) y∈I(cid:63) j,ε Therefore, we have 1 lim sup − log(1 − a ) ≤ Γ(cid:63). n n,i n→∞ If T /n → ω(cid:63) for each i ∈ A, we have n,i i (cid:18) (cid:19) (cid:18) (cid:19) T + 1 S T + 1 S lim inf n,i kl n,i ; y + n,I(cid:63) kl n,I(cid:63) ; y n→∞ y∈Ii,I(cid:63) n T n,i + 1 n T n,i + 1 = inf ω(cid:63)kl(µ ; y) + ω(cid:63) kl(µ ; y) i i I(cid:63) I(cid:63) y∈[µi, µI(cid:63)] = Γ(cid:63), and thus (cid:26) (cid:27) . 1 − a = exp −n max min inf ω kl(µ ; y) + ω kl(µ ; y) n,i i j I(cid:63) I(cid:63) ω j(cid:54)=I(cid:63) y∈I(cid:63) ε . = exp {−nΓ(cid:63)} , implying 1 lim − log (1 − a ) = Γ(cid:63). n→∞ n n,i Everything goes similarly when ω = β ∈ (0, 1), so under any sampling rule satisfying T /n → β we have I(cid:63) n,I(cid:63) 1 lim sup − log(1 − a ) ≤ Γ(cid:63) n n,i β n→∞ and under any sampling rule satisfying T /n → ωβ for each i ∈ A, we have n,i i 1 lim − log(1 − a ) = Γ(cid:63) . n→∞ n n,i β H.2 Proof of Theorem 5 Theorem 5. Under TTTS, for Bernoulli bandits and uniform priors, it holds almost surely that 1 lim − log(1 − a ) = Γ(cid:63) . n→∞ n n,I(cid:63) β
Preprint From Theorem 6 we know that under any allocation rule satisfying T /n → ωβ for every i ∈ A, we have n,i i 1 lim − log (1 − a ) = Γ(cid:63) . n→∞ n n,I(cid:63) β Thus, we only need to prove that under TTTS, for all i ∈ A, we have T lim n,i a =.s ωβ. n→∞ n i Just as for the proof of the Gaussian case, we can use Lemma 4 (proof in Appendix G.2), which implies T Ψ lim n,i a =.s ωβ ⇔ lim n,i a =.s ωβ. n→∞ n i n→∞ n i Therefore, it suffices to show convergence for ψ = Ψ /n to ωβ, which we will do next, following the same n,i n,i i steps as in the proof for the Gaussian case. Step 1: TTTS draws all arms infinitely often and satisfies T /n → β. We prove the following lemma. n,I(cid:63) Lemma 27. Under TTTS, it holds almost surely that 1. for all i ∈ A, lim T = ∞. n→∞ n,i 2. a → 1. n,I(cid:63) 3. Tn,I(cid:63) → β. n Proof. First, we give a lemma showing the implications of finite measurement, and consistency when all arms are sampled infinitely often, which provides a proof for 2. The proof of this lemma follows from the proof of Theorem 6, and is given in Appendix H.3. Lemma 28 (Consistency and implications of finite measurement). Denote with I the arms that are sampled only a finite amount of times: I = {i ∈ {1, . . . , k} : ∀n, T < ∞}. n,i If I is empty, a converges almost surely to 1 when i = I(cid:63) and to 0 when i (cid:54)= I(cid:63). If I is non-empty, then for n,i every i ∈ I, we have lim inf a > 0 a.s. n→∞ n,i (cid:80) Now we can show 1. of Lemma 27: we show that under TTTS, for each j ∈ A, we have T = ∞. The n∈N n,j proof is exactly equal to the proof for Gaussian arms. Under TTTS, we have   ψ n,i = a n,i β + (1 − β) (cid:88) 1 −a n a,j  , n,j j(cid:54)=i (cid:80) so ψ ≥ βa , therefore, by Lemma 23, if i ∈ I, then lim inf a > 0 implies that ψ = ∞. By Lemma 4, n,i n,i n,i n n,i we then must have that lim T = ∞ as well: contradiction. Thus, lim T = ∞ for all i, and we n→∞ n,i n→∞ n,i conclude that a → 1, by Lemma 23. n,I(cid:63) Lastly we prove point 3. of Lemma 27. For TTTS with parameter β, the above implies that ψ → β, and since n,I(cid:63) we have a bound on |T /n − ψ | in Lemma 4, we have T /n → β as well. n,i n,i n,I(cid:63)
Preprint Step 2: Controlling the over-allocation of sub-optimal arms. Following the proof for the Gaussian case again, we can establish a consequence of the convergence of T /n to β : if an arm is sampled more often than n,I(cid:63) its optimal proportion, the posterior probability of this arm to be optimal is reduced compared to that of other sub-optimal arms. We can prove this by using ingredients from the proof of the lower bound in Theorem 6. Lemma 29 (Over-allocation implies negligible probability). 9 Fix any ξ > 0 and j (cid:54)= I(cid:63). With probability 1, under any allocation rule, if T /n → β, there exist ξ(cid:48) > 0 and n,I(cid:63) a sequence ε with ε → 0 such that for any n ∈ N, n n T a n,j ≥ ωβ + ξ =⇒ n,j ≤ e−n(ξ(cid:48)+εn). n j max a i(cid:54)=I(cid:63) n,i Proof. By Theorem 6, we have, as T /n → β, n,I(cid:63) (cid:18) (cid:19) 1 lim sup − log max a ≤ Γ(cid:63) , n→∞ n i(cid:54)=I(cid:63) n,i β since max a ≤ 1 − a . We also have from Lemma 26 a deviation inequality, so that we can establish the i(cid:54)=I(cid:63) n,i n,I(cid:63) following logarithmic equivalence: . . a ≤ Π (θ ≥ θ ) = exp {−nC (w , ω )} = exp {−nC (β, ω )} , n,j n j I(cid:63) j n,I(cid:63) n,j j n,j where we denote ω (cid:44) Tn,j . We can combine these results, which implies that there exists a non-negative n,j n sequence ε → 0 such that n a n,j ≤ exp {−nC j (β, ω n,j) − ε n/2} = exp (cid:8) −n (cid:0) C (β, ω ) − Γ(cid:63) (cid:1) − ε (cid:9) . max i(cid:54)=I(cid:63) a n,i exp (cid:110) −n(Γ(cid:63) + ε/2)(cid:111) j n,j β n β (cid:16) (cid:17) (cid:16) (cid:17) We know that C β, ωβ is strictly increasing in ωβ, and C β, ωβ = Γ(cid:63) , thus, there exists some ξ(cid:48) > 0 such j j j j j β that ω ≥ ωβ + ξ =⇒ C (β, ω ) − Γ(cid:63) > ξ(cid:48). n,j j j n,j β Step 3: ψ converges to ωβ for all arms. To establish the convergence of the allocation effort of all arms, n,i i we rely on the same sufficient condition used in the analysis of Russo (2016), restated above in Lemma 25, and we will restate it here again for convenience. Lemma 30 (Sufficient condition for optimality). Consider any adaptive allocation rule. If (cid:88) (cid:110) (cid:111) ψ → β, and ψ 1 ψ ≥ ωβ + ξ < ∞, ∀j (cid:54)= I(cid:63), ξ > 0, (15) n,I(cid:63) n,j n,j j n∈N then ψ → ψβ. n First, note that from Lemma 27 we know that Tn,I(cid:63) → β, and by Lemma 4 this implies ψ → β, hence we n n,I(cid:63) can use the lemma above to prove convergence to the optimal proportions. This proof is already given in Step 3 of the proof for the Gaussian case, and since it does not depend on the specifics of the Gaussian case, except for invoking Lemma 23 (consistency), which for the Bernoulli case we replace by Lemma 28, it gives a proof for the Bernoulli case as well. We conclude that (14) holds, and the convergence to the optimal proportions follows by Lemma 25. 9analogue of Lemma 13 of Russo (2016)
Preprint H.3 Proof of auxiliary lemmas Proof of Lemma 26 (cid:20) (cid:21) a − 1 a − 1 P [X > Y ] = E [P [X > Y |Y ]] ≤ E 1{Y < } + 1{Y ≥ }P [X > Y |Y ] a + b − 1 a + b − 1 (cid:26) (cid:18) (cid:19)(cid:27) c − 1 a − 1 ≤ exp −(c + d − 1)kl ; c + d − 1 a + b − 1 (cid:20) (cid:26) (cid:18) (cid:19)(cid:27) (cid:21) a − 1 a − 1 + E exp −(a + b − 1)kl ; Y 1{Y ≥ } , a + b − 1 a + b − 1 Using the Beta-Binomial trick in the second inequality. Then we have (call the second half A) (cid:20) (cid:21) (cid:26) (cid:18) (cid:19)(cid:27) a − 1 c − 1 a − 1 A ≤ E 1{ ≤ Y ≤ } exp −(a + b − 1)kl ; Y a + b − 1 c + d − 1 a + b − 1 (cid:26) (cid:18) (cid:19)(cid:27) a − 1 c − 1 + exp −(a + b − 1)kl ; a + b − 1 c + d − 1 (call the first half B). Denote with f the density of Y , then (cid:90) c+c− d−1 1 (cid:26) (cid:18) a − 1 (cid:19)(cid:27) B = exp −(a + b − 1)kl ; y f (y) dy. a + b − 1 a−1 a+b−1 Via integration by parts we obtain (cid:20) (cid:26) (cid:18) (cid:19)(cid:27) (cid:21) c−1 a − 1 c+d−1 B = exp −(a + b − 1)kl ; y P [Y ≤ y] a + b − 1 a−1 a+b−1 (cid:90) c+c− d−1 1 d (cid:18) a − 1 (cid:19) + (a + b − 1) kl ; y exp {−C (y)} P (Y ≤ y) dy dy a + b − 1 a,b a−1 a+b−1 (cid:90) c+c− d−1 1 d (cid:18) a − 1 (cid:19) ≤ (a + b − 1) kl ; y exp {−(C (y) + C (y))} dy dy a + b − 1 a,b c,d a−1 a+b−1 (cid:26) (cid:18) (cid:19)(cid:27) a − 1 c − 1 + exp −(a + b − 1)kl ; , a + b − 1 c + d − 1 where the first inequality uses the Binomial trick again. Let (cid:18) (cid:19) (cid:18) (cid:19) a − 1 c − 1 C = inf (a + b − 1)kl ; y + (c + d − 1)kl ; y = inf C (y) + C (y), a−1 ≤y≤ c−1 a + b − 1 c + d − 1 a−1 ≤y≤ c−1 a,b c,d a+b−1 c+d−1 a+b−1 c+d−1 then note that in particular we have (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19) a − 1 c − 1 c − 1 a − 1 C ≤ min (a + b − 1)kl ; , (c + d − 1)kl ; a + b − 1 c + d − 1 c + d − 1 a + b − 1 (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19) c − 1 a − 1 = min C , C . a,b c + d − 1 c,d a + b − 1 Then B ≤ e−C (cid:90) c+c− d−1 1 (a + b − 1) d kl( a − 1 ; y) dy + e−C = (cid:20) (a + b − 1)kl (cid:18) a − 1 ; c − 1 (cid:19) + 1(cid:21) e−C. dy a + b − 1 a + b − 1 c + d − 1 a−1 a+b−1 Thus we have (cid:18) (cid:18) (cid:19)(cid:19) a − 1 c − 1 P [X > Y ] ≤ 3 + (a + b − 1)kl ; e−C. a + b − 1 c + d − 1
Preprint By symmetry, we have (cid:18) (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)(cid:19) c − 1 a − 1 P [X > Y ] ≤ 3 + min C , C e−C, a,b c + d − 1 c,d a + b − 1 where (cid:18) (cid:19) (cid:18) (cid:19) a − 1 c − 1 C = inf (a + b − 1)kl ; y + (c + d − 1)kl ; y . a−1 ≤y≤ c−1 a + b − 1 c + d − 1 a+b−1 c+d−1 Proof of Lemma 28 Let I be empty, then we have µ (cid:44) lim µ = µ . The posterior variance is ∞,i n→∞ n,i i α β (1 + (cid:80)n−1 1{I = i}Y )(1 + T − (cid:80)n−1 1{I = i}Y ) σ2 = n,i n,i = (cid:96)=1 (cid:96) (cid:96),I(cid:96) n,i (cid:96)=1 (cid:96) (cid:96),I(cid:96) , n,i (α + β )2(α + β + 1) (2 + T )2(2 + T + 1) n,i n,i n,i n,i n,i n,i We see that when I is empty, we have σ2 (cid:44) lim σ2 = 0, i.e., the posterior is concentrated. ∞,i n→∞ n,i When T = 0 for some i ∈ A, the empirical mean for that arm equals to the prior mean µ = α /(α + β ), n,i n,i 1,i 1,i 1,i and the variance is strictly positive: σ2 = (α β )/ (cid:0) (α + β )2(α + β + 1)(cid:1) > 0. When I is not n,i n,i n,i 1,i 1,i 1,i 1,i empty, then for every i ∈ I we have σ2 > 0, and α ∈ (0, 1), implying α < 1, hence the posterior is not ∞,i ∞,i ∞,I(cid:63) concentrated.
