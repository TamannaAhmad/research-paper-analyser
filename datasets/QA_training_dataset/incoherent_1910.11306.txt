Controllable Attention for Structured Layered Video Decomposition Jean-Baptiste Alayrac1∗ Joa˜o Carreira1∗ Relja Arandjelovic´1 Andrew Zisserman1,2 {jalayrac,joaoluis}@google.com 1DeepMind 2VGG, Dept. of Engineering Science, University of Oxford Abstract The objective of this paper is to be able to separate a video into its natural layers, and to control which of the separated layers to attend to. For example, to be able to separate reflections, transparency or object motion. We make the following three contributions: (i) we in- troduce a new structured neural network architecture that explicitly incorporates layers (as spatial masks) into its de- sign. This improves separation performance over previous general purpose networks for this task; (ii) we demonstrate that we can augment the architecture to leverage external cues such as audio for controllability and to help disam- biguation; and (iii) we experimentally demonstrate the ef- fectiveness of our approach and training procedure with controlled experiments while also showing that the pro- Figure 1: We propose a model, C3, able to decompose a video posed model can be successfully applied to real-word ap- into meaningful layers. This decomposition process is controllable plications such as reflection removal and action recognition through external cues such as audio, that can select the layer to in cluttered scenes. output. sion is modelled by superimposing opaque layers in a depth 1. Introduction ordering. Given a a layered decomposition, attention can “The more you look the more you see”, is generally true switch between the various layers as necessary for the task for our complex, ambiguous visual world. Consider the ev- at hand. eryday task of cleaning teeth in front of a mirror. People Our objective in this paper is to separate videos into performing this task may first attend to the mirror surface to their constituent layers, and to select the layers to attend identify any dirty spots, clean them up, then switch atten- to as illustrated in Figure 1. A number of recent works tion to their mouth reflected in the mirror. Or they may hear have used deep learning to separate layers in images and steps behind them and switch attention to a new face now videos [3, 12, 16, 18, 26, 58], with varying success, but the reflecting in the mirror. Not all visual possibilities can be selection of the layers has either had to be hard coded into investigated at once given a fixed computational budget and the architecture, or the layers are arbitrarily mapped to the this creates the need for such controllable attention mecha- outputs. For example, [3] considers the problem of sepa- nisms. rating blended videos into component videos, but because the mapping between input videos and outputs is arbitrary, Layers offer a simple but useful model for handling this training is forced to use a permutation invariant loss, and complexity of the visual world [51]. They provide a com- there is no control over the mapping at inference time. How positional model of an image or video sequence, and cover can this symmetry between the composed input layers and a multitude of scenarios (reflections, shadows, occlusions, output layers be broken? haze, blur, ...) according to the composition rule. For exam- The solution explored here is based on the simple fact ple, an additive composition models reflections, and occlu- that videos do not consist of visual streams alone, they also ∗Equal contribution. have an audio stream; and, significantly, the visual and au- 1 9102 tcO 42 ]VC.sc[ 1v60311.0191:viXra
dio streams are often correlated. The correlation can be teresting aspect is the types of representations that are sub- strong (e.g. the synchronised sound and movement of beat- ject to attention, often categorized into location-based [42], ing on a drum), or quite weak (e.g. street noise that separates object-based or feature-based [6]: examples of the latter in- an outdoor from indoor scene), but this correlation can be clude attending to anything that is red, or to anything that employed to break the symmetry. This symmetry breaking moves. Another relevant stream of research relates to the is related to recent approaches to the cocktail party audio role of attention in multisensory integration [45, 47]. Note separation problem [2, 15] where visual cues are used to also that attention does not always require eye movement – select speakers and improve the quality of the separation. this is called covert (as opposed to overt) attention. In this Here we use audio cues to select the visual layers. paper we consider covert attention as we will not be consid- ering active vision approaches, and focus on feature-based Contributions: The contributions of this paper are three- visual attention control. fold: (i) we propose a new structured neural network archi- tecture that explicitly incorporates layers (as spatial masks) Cross-modal attention control. The idea of using one into its design; (ii) we demonstrate that we can augment modality to control attention in the other has a long history, the architecture to leverage external cues such as audio for one notable application being informed audio source sepa- controllability and to help disambiguation; and (iii) we ex- ration and denoising [7, 21, 39, 52]. Visual information has perimentally demonstrate the effectiveness of our approach been used to aid audio denoising [21, 39], solve the cock- and training procedure with controlled experiments while tail party problem of isolating sound coming from different also showing that the proposed model can be successfully speakers [2, 15, 37, 52] or musical instruments [7, 19, 59]. applied to real-word applications such as reflection removal Other sources of information used for audio source separa- and action recognition in cluttered scenes. tion include text to separate speech [32] and score to sepa- We show that the new architecture leads to improved rate musical instruments [25]. layer separation. This is demonstrated both qualitatively More relevant to this paper where audio is used for con- and quantitatively by comparing to recent general purpose trol, [4, 37, 40, 59] learn to attend to the object that is mak- models, such as the visual centrifuge [3]. For the quanti- ing the sound. However, unlike in this work, they do not tative evaluation we evaluate how the downstream task of directly output the disentangled video nor can they be used human action recognition is affected by reflection removal. to remove reflections as objects are assumed to be perfectly For this, we compare the performance of a standard action opaque. classification network on sequences with reflections, and Other examples of control across modalities include tem- with reflections removed using the layer architecture, and porally localizing a moment in a video using language [24], demonstrate a significant improvement in the latter case. video summarization guided by titles [44] or query object labels [41], object localization from spoken words [23], 2. Related work image-text alignment [29], and interactive object segmen- tation via user clicks [9]. Attention control. Attention in neural network modelling Layered video representations. Layered image and video has had a significant impact in computer vision, representations have a long history in computer vision [50] such as machine translation, [5, 49] and vision [54], where and are an appealing framework for modelling 2.1D depth it is implemented as a soft masking of features. In these set- relationships [43, 56], motion segmentation [50], reflec- tings attention is often not directly evaluated, but is just used tions [8, 12, 16, 17, 22, 26, 31, 35, 46, 55, 58], trans- as an aid to improve the end performance. In this paper we parency [3, 18], or even haze [18]. There is also evidence investigate models of attention in isolation, aiming for high that the brain uses multi-layered visual representations for consistency and controllability. By consistency we mean modelling transparency and occlusion [53]. the ability to maintain the focus of attention on a particular target. By controllability we mean the ability to switch to a 3. Approach different target on command. Visual attentional control is actively studied in psychol- This section describes the two technical contributions of ogy and neuroscience [14, 20, 28, 36, 48, 57] and, when this work. First, in Section 3.1, a novel architecture for malfunctioning, is a potentially important cause of condi- decomposing videos into layers. This architecture is built tions such as ADHD, autism or schizophrenia [33]. One of upon the visual centrifuge [3], a generic U-Net like encoder- the problems studied in these fields is the relationship be- decoder, but extends it with two structural changes tailored tween attention control based on top-down processes that towards the layered video decomposition task. Second, in are voluntary and goal-directed, and bottom-up processes Section 3.2, the decomposition model is endowed with con- that are stimulus-driven (e.g. saliency) [27, 48]. Another in- trollability – the ability of the network to use external cues
Layer I3D Generator Composition Module Mask Generator Encoder Decoder (a) Overview of the Compositional Centrifuge (C2) architecture (b) Composition module Figure 2: Network architecture for layer decomposition (3.1). to control what it should focus on reconstructing. Here, we the features produced by the I3D are gated with multiple propose to use a natural video modality, namely audio, to (m) masks, also produced by the encoder itself. The gated select layers. Given this external cue, different mechanisms features therefore already encode information about the un- for controlling the outputs are investigated. Finally, in Sec- derlying layers and this helps the decoder’s task. tion 3.3, we describe how this model can be trained for suc- In order to avoid gating all features with all m masks, cessful controllable video decomposition. which would be prohibitively expensive in terms of compu- In the following, V stands for an input video. Formally, tation and memory usage, feature channels are divided into V ∈ RT ×W ×H×3 where T is the number of frames, W m mutually-exclusive groups and each mask is applied only and H are the width and height of the frames, and there to the corresponding group. are 3 standard RGB channels. The network produces an More formally, the mask generator produces M ∈ T × W × H × (n × 3) tensor, interpreted as n output videos [0, 1]T ×W ×H×m which is interpreted as a set of m spatio- O, where each O i is of the same size as V . temporal masks M = (M c)m c=1. M is constrained to sum to 1 along the channel dimension by using a softmax non- 3.1. Architecture for layer decomposition linearity. Denote F the output feature taken at level l in l We start from the visual centrifuge [3], a U-Net [38] the I3D. We assume that F l ∈ RTl×Wl×Hl×(m×dl), i.e. the encoder-decoder architecture, which separates an input number of output channels of F is a multiple of m. Given l video into n output videos. The encoder consists of an this, F can be grouped into m features (F c)m where l l c=1 I3D network [11] and the decoder is composed by stacking F c ∈ RTl×Wl×Hl×dl. The following transformation is ap- l 3D up convolutions. However, the U-Net architecture used plied to each F c: l there is generic and not tailored to the layered video decom- position task (this is verified experimentally in Section 4.1). F˜c = M c (cid:12) F c, (1) l l l Therefore, we propose two structural modifications specifi- cally designed to achieve layered decomposition, forming a where M c is obtained by downsampling M c to the shape l new network architecture, Compositional Centrifuge (C2), [T l × W l × H l], (cid:12) refers to the Hadamard matrix product shown in Figure 2a. Firstly, a bespoke gating mechanism is with a slight abuse of notation as the channel dimension is used in the encoder, which enables selection of scene seg- broadcast, i.e. the same mask is used across the channels. ments across space/time, thereby making the decoder’s task This process is illustrated in Figure 2a. Appendix B gives easier. Secondly, layer compositionality is imposed by con- details on which feature levels are used in practice. straining how the output videos are generated – the layer Imposing compositionality. In order to bias the decoder generator outputs multiple layers L and their composing towards constructing layered decompositions, we split it coefficients β such that the output videos O are produced into two parts – the layer generator produces m layers L as a linear combination of the layers. These modifications and composing coefficients β which are then combined by are described in detail next. the composition module to form the final n output videos Encoder. We aim to recover layers in the presence of occlu- O. The motivation is that individual layers should ideally sions and transparent surfaces. In such cases there are win- represent independent scene units, such as moving objects, dows of opportunity when objects are fully visible and their reflections or shadows, that can be composed in different appearance can be modelled, and periods when the objects ways into full scene videos. The proposed model architec- are temporarily invisible or indistinguishable and hence can ture is designed to impose the inductive bias towards this only be tracked. We incorporate this intuition into a novel type of compositionality. spatio-temporal encoder architecture. The core idea is that More formally, the layer generator outputs a set of m
layers L = (L )m , where L ∈ RT ×H×W ×3, and a set of j j=1 j n × m composing coefficients β = (β ) . ij (i,j)∈[[1,n]]×[[1,m]] These are then combined in the composition module (Fig- ure 2b) to produce the final output videos O: log spectrogram (cid:88) O = β (cid:12) L . (2) i ij j j 3.2. Controllable symmetry breaking Encoder [T, W, H, C] The method presented in the previous section is inher- ently symmetric – the network is free to assign videos to output slots in any order. In this section, we present a strat- egy for controllable attention that is able to break the sym- Mean F. Time Space metry by making use of side-information, a control signal, Concat. Pool Sample broadcast provided as an additional input to the network. Audio is Audio [T',C'] [T,C'] [T,W,H,C'] used as a natural control signal since it is readily available Network with the video. In our mirror example from the introduction, hearing speech indicates the attention should be focused on Decoder the person in the mirror, not the mirror surface itself. For the rest of this section, audio is used as the control signal, but the proposed approach remains agnostic to the control signal nature. Next, we explain how to compute audio features, fuse them with the visual features, and finally, how to obtain the output video which corresponds to the input audio. The ar- chitecture, named Controllable Compositional Centrifuge (C3), is shown in Figure 3. Audio network. The audio first needs to be processed be- fore feeding it as a control signal to the video decomposition model. We follow the strategy employed in [4] to process the audio. Namely, the log spectrogram of the raw audio signal is computed and treated as an image, and a VGG-like network is used to extract the audio features. The network is trained from scratch jointly with the video decomposition model. Audio-visual fusion. To feed the audio signal to the video model, we concatenate audio features to the outputs of the encoder before they get passed to the decoder. Since visual and audio features have different shapes – their sampling rates differ and they are 3-D and 4-D tensors for audio and vision, respectively – they cannot be concatenated naively. We make the two features compatible by (1) average pool- ing the audio features over frequency dimension, (2) sam- pling audio features in time to match the number of tem- poral video feature samples, and (3) broadcasting the audio feature in the spatial dimensions. After these operations the audio tensor is concatenated with the visual tensor along the channel dimension. This fusion process is illustrated in Figure 3. We provide the full details of this architecture in Appendix B. Attention control. We propose two strategies for obtain- ing the output video which corresponds to the input audio. ]'C ,F ,'T[ Figure 3: The Controllable Compositional Centrifuge (C3). The Encoder-Decoder components are the same as in C2 (Fig- ure 2a). Audio features are extracted from the audio control signal and fused with the visual features before entering the decoder. One is to use deterministic control where the desired video is forced to be output in a specific pre-defined output slot, without loss of generality O is used. While simple, this 1 strategy might be too rigid as it imposes too many con- straints onto the network. For example, a network might naturally learn to output guitars in slot 1, drums in slot 2, etc., while deterministic control is forcing it to change this ordering at will. This intuition motivates our second strat- egy – internal prediction – where the network is free to produce output videos in any order it sees fit, but it also provides a pointer to the output slot which contains the de- sired video. Internal prediction is trained jointly with the rest of the network, full details of the architecture are given in Appendix B. The training procedure and losses for the two control strategies are described in the next section. 3.3. Training procedure Training data. Since it is hard to obtain supervised training data for the video decomposition problem, we adopt and extend the approach of [3] and synthetically generate the training data. This by construction provides direct access to one meaningful ground truth decomposition. Specifically, we start from two real videos V , V ∈ RT ×W ×H×3. These 1 2 videos are mixed together to generate a training video V ∈ RT ×W ×H×3: V = α (cid:12) V + (1 − α) (cid:12) V , (3) 1 2
where α ∈ [0, 1]T ×W ×H is a composing mask. Strategy 1: deterministic control We explore two ways to generate the composing mask α. The first one is transparent blending, used by [3], where α = 1 1. While attractive because of its simplicity, it does 2 not capture the full complexity of the real world composi- tions we wish to address, such as occlusions. For this rea- son, we also explore a second strategy, referred to as oc- clusion blending, where α is allowed to vary in space and Strategy 2: internal pred. control takes values 0 or 1. In more detail, we follow the proce- dure of [13] where spatio-temporal SLIC superpixels [1] are extracted from V , and one is chosen at random. The 1 compositing mask α is set to 1 inside the superpixel and 0 elsewhere; this produces mixtures of completely transparent or completely opaque spatio-temporal regions. The impact of the α sampling strategy on the final performance is ex- plored in Section 4.1. Control Training loss: without control. By construction, for an Regressor input training video V we know that one valid decompo- sition is into V and V . However, when training without 1 2 Figure 4: Audio control strategies for video decomposition. control, there is no easy way to know beforehand the order In this example, the inputs are the video V , a composition of V 1 in which output videos are produced by the network. We showing a violin and V showing drums, and an audio control sig- 2 therefore optimize the network weights to minimize the fol- nal, A , being the sound of the violin. With deterministic control, 1 lowing permutation invariant reconstruction loss [3]: V is forced to be put in output slot O (and therefore V in O ). 1 1 2 2 With internal prediction control, the network can freely order the L pil ({V 1, V 2}, O) = min (cid:96)(V 1, O i) + (cid:96)(V 2, O j), (4) output videos, so is trained with the permutation invariant loss, but (i,j)|i(cid:54)=j it contains an additional control regressor module which is trained to point to the desired output. where (cid:96) is a video reconstruction loss, e.g. a pixel wise error loss (see Section 4 for our particular choice). desired video and output videos, and the attended output is Training loss: with control. When training with audio as chosen as arg min s . This module is trained with the fol- i i the control signal, the audio of one video (V 1 without loss lowing regression loss: of generality) is also provided. This potentially removes the need for the permutation invariant loss required in the no- n (cid:88) L (V , s) = |s − (cid:96)(V , sg(O ))|, (6) control case, but the loss depends on the choice of control reg 1 i 1 i strategy. The two proposed strategies are illustrated in Fig- i=1 ure 4 and described next. where sg is the stop gradient operator. Stopping the gradient flow is important as it ensures that the only effect Deterministic control loss. Here, the network is forced to of training the module is to learn to point to the desired output the desired video V as O so a natural loss is: 1 1 video. Its training is not allowed to influence the output L ({V , V }, O) = (cid:96)(V , O ) + (cid:96)(V , O ). (5) videos themselves, as if it did, it could sacrifice the recon- det 1 2 1 1 2 2 struction quality in order to set an easier regression problem Note that for this loss the number of output videos has to for itself. be restricted to n = 2. This limitation is another drawback of deterministic control as it allows less freedom to propose 4. Experiments multiple output video options. Internal prediction loss. In this strategy, the network freely This section evaluates the merits of the proposed Com- decomposes the input video into outputs, and therefore the positional Centrifuge (C2) compared to previous work, per- training loss is the same permutation invariant loss as for forms ablation studies, investigates attention control via the the no-control case (4). In addition, the network also points audio control signal and the effectiveness of the two pro- to the output which corresponds to the desired video, where posed attention control strategies of the Controllable Com- the pointing mechanism is implemented as a module which positional Centrifuge (C3), followed by qualitative decom- outputs n real values s = (s )n , one for each output position examples on natural videos, and evaluation on the i i=1 video. These represent predicted dissimilarity between the downstream task of action recognition.
Model Loss (Transp.) Loss (Occl.) Size Identity 0.364 0.362 – Centrifuge [3] 0.149 0.253 22.6M CentrifugePC [3] 0.135 0.264 45.4M C2 w/o masking 0.131 0.200 23.4M C2 0.120 0.190 27.1M Table 1: Model comparison in terms of average validation loss for synthetically generated videos with transp(arency) and occl(usions), as well as size in millions of parameters. All the results are obtained using models with n = 4 output layers. Cen- trifugePC is the predictor-corrector centrifuge [3], Identity is a baseline where the output videos are just copies of the input. Implementation details. Following [3, 34], in all experi- ments we use the following video reconstruction loss, de- fined for videos U and V as: (cid:32) (cid:33) 1 (cid:88) (cid:96)(U, V ) = (cid:107)U − V (cid:107) + (cid:107)∇(U ) − ∇(V )(cid:107) , 2T t t 1 t t 1 t where (cid:107) · (cid:107) is the L1 norm and ∇(·) is the spatial gradient 1 operator. All models are trained and evaluated on the blended ver- sions of the training and validation sets of the Kinetics-600 dataset [10]. Training is done using stochastic gradient de- scent with momentum for 124k iterations, using batch size 128. We employed a learning rate schedule, dividing by 10 the initial learning rate of 0.5 after 80k, 100k and 120k iter- ations. In all experiments we randomly sampled 64-frame clips at 128x128 resolution by taking random crops from videos whose smaller size being resized to 148 pixels. 4.1. Quantitative analysis In this section, we evaluate the effectiveness of our ap- proaches through quantitative comparisons on synthetically generated data using blended versions of the Kinetics-600 videos. Effectiveness of the C2 architecture for video decomposi- tion. The baseline visual centrifuge achieves a slightly bet- ter performance (lower loss) than originally reported [3] by training on clips which are twice as long (64 vs 32 frames). As can be seen in Table 1, our proposed architecture outper- forms both the Centrifuge baseline [3], as well as the twice as large predictor-corrector model of [3]. Furthermore, both of our architectural improvements – the masking and the composition module – improve the performance (recall that the baseline Centrifuge is equivalent to C2 without the two improvements). The improvements are especially apparent for occlusion blending since our architecture is explicitly designed to account for more complicated real-world blend- ing than the simple transparency blending used in [3]. tnerapsnarT noisulccO Figure 5: Outputs of C2 on blended Kinetics validation clips. Each row shows one example via a representative frame, with columns showing the input blended clip V , two output videos O 1 and O , and the two ground truth clips V and V . Top three rows 2 1 2 show the network is able to successfully decompose videos with transparencies. Bottom three rows show synthetic occlusions – this is a much harder task where, apart from having to detect the occlusions, the network also has to inpaint the occluded parts of each video. C2 performs satisfactory in such a challenging sce- nario. Model Loss (Transp.) Control Acc. C2 0.120 50% (chance) C3 w/ deterministic control 0.191 79.1% C3 w/ internal prediction 0.119 77.7% Table 2: Model comparison on average validation reconstruction loss and control F1-score. The controllable models, C3, use audio as the control signal. Attention control. The effectiveness of the two proposed attention control strategies using the audio control signal is evaluated next. Apart from comparing the reconstruc- tion quality, we also contrast the methods in terms of their control F1-score, i.e. their ability to output the desired video into the correct output slot. For a given video V (composed of videos V and V ) and audio control signal 1 2 A , the output is deemed to be correctly controlled if the 1 chosen output slot O reconstructs the desired video V c 1 well. Recall that the ‘chosen output slot’ is simply slot O = O for the deterministic control, and predicted by c 1 the control regressor as O for the internal pre- arg min i(si)
tnerapsnarT noisulccO Figure 6: Visualization of the internals of the compositional model. Recall that the C2 model produces the output videos via the composition module (Figure 2b) which multiplies the layers L with composing coefficients β. Here we visualize the individual β (cid:12) L terms which when added together form the output videos. It can be observed that the layers and composing coefficient indeed decompose the input video V into its constituent parts, for both the transparent and occlusion blending. diction control. The chosen output video O is deemed c to reconstruct the desired video well if its reconstruction loss is the smallest out of all outputs (up to a threshold t = 0.2 ∗ (max (cid:96)(V , O ) − min (cid:96)(V , O )) to account for i 1 i i 1 i potentially nearly identical outputs when outputing more than 2 layers): (cid:96)(V , O ) < min (cid:96)(V , O ) + t. 1 c i 1 i Table 2 evaluates control performance across different models with the transparency blending. It shows that the non-controllable C2 network, as expected, achieves control F1-score equal to random chance, while the two control- lable variants of C3 indeed exhibit highly controllable be- haviour. The two strategies are comparable on control ac- curacy, while internal prediction control clearly beats deter- ministic control in terms of reconstruction loss, confirming our intuition that deterministic control imposes overly tight constraints on the network. 4.2. Qualitative analysis Here we perform qualitative analysis of the performance of our decomposition networks and investigate the internal layered representations. Figure 7: Qualitative results of C3 with internal prediction. Figure 5 shows the video decompositions obtained from For visualization purposes, as it is hard to display sound, we show our C2 network for transparent and occlusion blending. The a frame of the video from which we use the audio as control on network is able to almost perfectly decompose the videos the left most column (A ). V (second column) represents the vi- 1 with transparencies, while it does a reasonable job of re- sual input to the model. The right 4 columns are the outputs of C3. constructing videos in the much harder case where strong All examples exhibit good reconstruction error. The first four rows occlusions are present and it needs to inpaint parts of the illustrate accurate control behaviour, where C3 has correctly pre- videos it has never seen. dicted the output that corresponds to the control signal (illustrated by a green marker under the frame). The last row illustrates an The internal representations produced by our layer gen- incorrect control (specified with a red marker under the wrongly erator, which are combined in the composition module to chosen frame), where C3 was fooled by a liquid sound that is plau- produce the output videos, are visualized in Figure 6. Our sible in the two scenarios. architecture indeed biases the model towards learning com- positionality as the internal layers show a high degree of video, making few mistakes which are often reasonable due independence and specialize towards reconstructing one of to the inherent noisiness and ambiguity in the sound. the two constituent videos. Finally, Figure 7 shows qualitative results for the best 4.3. Downstream tasks controllable network, C3 with internal prediction, where au- dio is used as the control signal. The network is able to ac- In the following, we investigate the usefulness of lay- curately predict which output slot corresponds to the desired ered video decomposition as a preprocessing step for other
downstream tasks. Graphics. Layered video decomposition can be used in var- ious graphics applications, such as removal of reflections, specularities, shadows, etc. Figure 8 shows some examples of decompositions of real videos. Compared with previous work of [3], as expected from the quantitative results, the decompositions are better as the produced output videos are more pure. Action recognition. A natural use case for video decom- position is action recognition in challenging scenarios with transparencies, reflections and occlusions. Since there are no action recognition datasets focused on such difficult set- tings, we again resort to using blended videos. A pre-trained I3D action recognition network [11] is used and its per- formance is measured when the input is pure unblended video, blended video, and decomposed videos, where the Figure 8: Comparison of our C2 model against [3] on real- decomposition is performed using the best baseline model world videos. The input video is shown on the left, and the output (predictor-corrector centrifuge, CentrifugePC [3]) or our videos of C2 and [3] are interleaved in the remaining columns for Compositional Centrifuge (C2). For the pure video perfor- easier comparison. While both models manage to decompose the mance, we report the standard top-1 F1-score. videos reasonably well, C2 achieves less leakage of one video into For transparency blended videos, the desired outputs another. For example, C2 versus [3] output O 1 (first row) removes the reflections of branches on the right side better, (second row) are both ground truth labels of the two constituent videos. has fewer yellow circles of light, and (third row) makes the large Therefore, the models make two predictions and are scored circular reflection in the top half of the image much fainter. 1, 0.5 and 0 depending on whether both predictions are cor- rect, only one or none is, respectively. When I3D is applied Mode Acc. (Transp.) Acc. (Occl.) directly on the blended video, the two predictions are natu- I3D – pure video 59.5 59.5 rally obtained as the two classes with the largest scores. For I3D 22.1 21.3 the decomposition models, each of the two output videos CentrifugePC [3] + I3D 34.4 21.5 contributes their highest scoring prediction. C2 + I3D 40.1 24.7 In the case of occlusion blended videos, the desired out- put is the ground truth label of V 2 because there is not Table 3: Action recognition F1-score on the Kinetics-600 vali- enough signal to reconstruct V 1 as the blended video only dation set when the input to a pre-trained I3D classifier is a pure contains a single superpixel from V 1. When I3D is applied – non-blended – video (top row), a blended video directly passed directly on the blended video, the top prediction is used. through I3D, or a blended video that is first unblended using a The decomposition models tend to consistently reconstruct layer decomposition model. The two columns show accuracies for V in one particular output slot, so we apply the I3D net- two different blending processes: transparent and occluding. 2 work onto the relevant output and report the top-1 F1-score. this case sound. We showed that the proposed model can Table 3 shows that decomposition significantly improves the action recognition performance, while our C2 strongly better endure automatically generated transparency and es- pecially occlusions, compared to previous work, and that outperforms the baseline CentrifugePC [3] for both blend- ing strategies. There is still a gap between C2 and the pure the layers are selected based on sound cues with accuracies of up to 80% on the blended Kinetics dataset. As future video performance, but this is understandable as blended work we would like to train our model on more naturally- videos are much more challenging. looking occlusions, possibly by generating the composing mask using supervised segmentations instead of unsuper- 5. Conclusion vised superpixels. General vision systems, that can serve a variety of pur- poses, will probably require controllable attention mecha- References nisms. There are just too many possible visual narratives to [1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien investigate in natural scenes, for a system with finite com- Lucchi, Pascal Fua, and Sabine Su¨sstrunk. SLIC superpix- putational power to pursue them all at once, always. In this els compared to state-of-the-art superpixel methods. TPAMI, paper we proposed a new compositional model for layered 2012. video representation and introduced techniques to make the [2] Triantafyllos Afouras, Joon Son Chung, and Andrew Zis- resulting layers selectable via an external control signal – in serman. The Conversation: Deep Audio-Visual Speech En-
hancement. In Interspeech, 2018. Chuang, Antonio Torralba, and James Glass. Jointly dis- [3] Jean-Baptiste Alayrac, Joa˜o Carreira, and Andrew Zisser- covering visual objects and spoken words from raw sensory man. The visual centrifuge: Model-free layered video repre- input. In ECCV, 2018. sentations. In CVPR, 2019. [24] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef [4] Relja Arandjelovic´ and Andrew Zisserman. Objects that Sivic, Trevor Darrell, and Bryan Russell. Localizing mo- sound. In ECCV, 2018. ments in video with natural language. In ICCV, 2017. [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. [25] Romain Hennequin, Bertrand David, and Roland Badeau. Neural machine translation by jointly learning to align and Score informed audio source separation using a parametric translate. arXiv preprint arXiv:1409.0473, 2014. model of non-negative spectrogram. In ICASSP, 2011. [6] Daniel Baldauf and Robert Desimone. Neural mechanisms [26] Daniel Heydecker, Georg Maierhofer, Angelica I. Aviles- of object-based attention. Science, 2014. Rivero, Qingnan Fan, Carola-Bibiane Scho¨nlieb, and Sabine [7] Zohar Barzelay and Yoav Y. Schechner. Harmony in motion. Su¨sstrunk. Mirror, mirror, on the wall, who’s got the clearest In CVPR, 2007. image of them all? – A tailored approach to single image [8] Efrat Be’ery and Arie Yeredor. Blind Separation of Super- reflection removal. arXiv preprint arXiv:1805.11589, 2018. imposed Shifted Images Using Parameterized Joint Diago- [27] Laurent Itti, Christof Koch, and Ernst Niebur. A model nalization. Transactions on Image Processing, 2008. of saliency-based visual attention for rapid scene analysis. [9] Yuri Y. Boykov and Marie-Pierre Jolly. Interactive graph PAMI, 1998. cuts for optimal boundary & region segmentation of objects [28] Laurent Itti, Geraint Rees, and John Tsotsos. Neurobiology in N-D images. In ICCV, 2001. of attention. Academic Press, 2005. [10] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe [29] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align- Hillier, and Andrew Zisserman. A short note about kinetics- ments for generating image descriptions. In CVPR, 2015. 600. In arXiv preprint arXiv:1808.01340, 2018. [30] Bruno Korbar, Du Tran, and Lorenzo Torresani. Coopera- [11] Joao Carreira and Andrew Zisserman. Quo vadis, action tive learning of audio and video models from self-supervised recognition? a new model and the kinetics dataset. In CVPR, synchronization. In NeurIPS, 2018. 2017. [31] Kun Gai, Zhenwei Shi, and Changshui Zhang. Blind Sepa- [12] Zhixiang Chi, Xiaolin Wu, Xiao Shu, and Jinjin Gu. Sin- ration of Superimposed Moving Images Using Image Statis- gle Image Reflection Removal Using Deep Encoder-Decoder tics. PAMI, 2012. Network. In arXiv preprint arXiv:1802.00094, 2018. [32] Luc Le Magoarou, Alexey Ozerov, and Ngoc Q.K. Duong. [13] Carl Doersch and Andrew Zisserman. Sim2real transfer Text-informed audio source separation. Example-based ap- learning for 3D pose estimation: motion to the rescue. In proach using non-negative matrix partial co-factorization. arXiv preprint arXiv:1907.02499, 2019. Journal of Signal Processing Systems, 2015. [14] Howard E. Egeth and Steven Yantis. Visual attention: Con- [33] Eric Mash and David Wolfe. Abnormal child psychology. trol, representation, and time course. Annual review of psy- Cengage Learning, 2012. chology, 1997. [34] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep [15] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin multi-scale video prediction beyond mean square error. In Wilson, Avinatan Hassidim, William T. Freeman, and ICLR, 2016. Michael Rubinstein. Looking to Listen at the Cocktail Party: [35] Ajay Nandoriya, Mohamed Elgharib, Changil Kim, Mo- A Speaker-Independent Audio-Visual Model for Speech hamed Hefeeda, and Wojciech Matusik. Video Reflection Separation. In SIGGRAPH, 2018. Removal Through Spatio-Temporal Optimization. In ICCV, [16] Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, and 2017. David Wipf. A Generic Deep Architecture for Single Image [36] Aude Oliva, Antonio Torralba, Monica S Castelhano, and Reflection Removal and Image Smoothing. In ICCV, 2017. John M Henderson. Top-down control of visual attention [17] Hany Farid and Edward H. Adelson. Separating reflections in object detection. In ICIP, volume 1, pages I–253. IEEE, and lighting using independent components analysis. In 2003. CVPR, 1999. [37] Andrew Owens and Alexei A. Efros. Audio-visual scene [18] Yossi Gandelsman, Assaf Shocher, and Michal Irani. analysis with self-supervised multisensory features. In “Double-DIP”: Unsupervised image decomposition via cou- ECCV, 2018. pled deep-image-priors. In CVPR, 2019. [38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- [19] Ruohan Gao, Rogeris Feris, and Kristen Grauman. Learning Net: Convolutional Networks for Biomedical Image Seg- to separate object sounds by watching unlabeled video. In mentation. In MICCAI, 2015. CVPR, 2018. [39] Dana Segev, Yoav Y. Schechner, and Michael Elad. [20] Michael Gazzaniga and Richard B Ivry. Cognitive Neuro- Example-based cross-modal denoising. In CVPR, 2012. science: The Biology of the Mind: Fourth International Stu- [40] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan dent Edition. WW Norton, 2013. Yang, and In So Kweon. Learning to localize sound source [21] Laurent Girin, Jean-Luc Schwartz, and Gang Feng. Audio- in visual scenes. In CVPR, 2018. visual enhancement of speech in noise. The Journal of the [41] Aidean Sharghi, Boqing Gong, and Mubarak Shah. Query- Acoustical Society of America, 2001. focused extractive video summarization. In ECCV, 2016. [22] Xiaojie Guo, Xiaochun Cao, and Yi Ma. Robust Separation [42] Markus Siegel, Tobias H Donner, Robert Oostenveld, Pas- of Reflection from Multiple Images. In CVPR, 2014. cal Fries, and Andreas K Engel. Neuronal synchronization [23] David Harwath, Adria` Recasens, D´ıdac Sur´ıs, Galen along the dorsal visual pathway reflects the focus of spatial
attention. Neuron, 2008. [43] Paul Smith, Tom Drummond, and Roberto Cipolla. Layered motion segmentation and depth ordering by tracking edges. PAMI, 26(4):479–494, 2004. [44] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. TVSum: Summarizing web videos using titles. In CVPR, 2015. [45] Charles Spence and Jon Driver. Audiovisual links in endoge- nous covert spatial attention. Journal of Experimental Psy- chology: Human Perception and Performance, 22(4):1005, 1996. [46] Richard Szeliski, Shai Avidan, and P. Anandan. Layer extraction from multiple images containing reflections and transparency. In CVPR, 2000. [47] Durk Talsma, Daniel Senkowski, Salvador Soto-Faraco, and Marty G Woldorff. The multifaceted interplay between at- tention and multisensory integration. Trends in cognitive sci- ences, 2010. [48] John Tsotsos, Scan M. Culhane, Winky Yan Kei Wai, Yuzhong Lai, Neal Davis, and Fernando Nuflo. Modeling visual attention via selective tuning. Artificial Intelligence, 1995. [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. [50] John YA Wang and Edward H Adelson. Representing mov- ing images with layers. IEEE Transactions on Image Pro- cessing, 3(5):625–638, 1994. [51] John Y. A. Wang and Edward H. Adelson. Representing moving images with layers. Transactions on image process- ing, 1994. [52] Wenwu Wang, Darren Cosker, Yulia Hicks, Saeid Sanei, and Jonathon. Chambers. Video assisted speech source separa- tion. In ICASSP, 2005. [53] Michael A Webster. Color vision: Appearance is a many- layered thing. Current Biology, 2009. [54] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption gen- eration with visual attention. In ICML, pages 2048–2057, 2015. [55] Tianfan Xue, Michael Rubinstein, Ce Liu, and William T. Freeman. A Computational Approach for Obstruction-Free Photography. In SIGGRAPH, 2015. [56] Yi Yang, Sam Hallman, Deva Ramanan, and Charless Fowlkes. Layered object detection for multi-class segmenta- tion. In CVPR, 2010. [57] Steven Yantis. Control of visual attention. Attention, 1998. [58] Xuaner Zhang, Ren Ng, and Qifeng Chen. Single image reflection separation with perceptual losses. CVPR, pages 4786–4794, 2018. [59] Hang Zhao, Can Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound of pixels. In ECCV, 2018.
Overview In this appendix, we cover three additional aspects: (i) in Section A we include an additional qualitative comparison on real videos, for which there was not space in the original manuscript; (ii) Section B provides details for the network architecture; and finally (iii) in Section C we study how the network uses the audio for control, by perturbing the audio. A. Additional comparison of C2 with previous work We compare to previous work [3, 55] on the task of re- flection removal in Figure 9. One of the baselines [55] uses geometrical modelling and optimization but under strict as- sumptions (e.g. rigid motion). The second baseline [3] is trained on the same data as our model. The proposed model generates a sharp video with little reflection left. B. Architecture details Figure 11 illustrates the architecture employed for C2 while Figure 12 provides full details about the architecture employed for C3 with internal prediction control strategy. C. Additional quantitative study for C3 What aspect of the audio control signal is used for the controlled decomposition? One hypothesis is that the net- work latches onto low-level synchronization cues, so that the desired output video is identified as the one that is in sync with the audio. An alternative is that the desired video is the one whose semantic content matches the audio. To answer this question, we use the best trained C3 net- work with internal prediction control and evaluate its perfor- mance with respect to varying degrees of audio offset. The experiment is performed on the validation set of Kinetics- 600. Reconstruction loss remains completely unaffected by shifting audio, while control F1-score deteriorates slightly as the offset is increased, as shown in Figure 10. The re- sults suggest that the network predominantly uses the se- mantic information contained in the audio signal as control F1-score only decreases by 1.4 percentage points with the largest offsets where the audio does not overlap with the vi- sual stream. However, some synchronization information is probably used as audio offset does have an adverse effect on control F1-score, and there is a sharp drop at relatively small offsets of 0.5-1s. There is scope for exploiting the synchronization signal further as it might provide a boost in control F1-score. A potential approach includes using a training curriculum analogous to [30].
Figure 9: Qualitative comparison of C2 with other works. 77.0 76.8 76.6 76.4 76.2 76.0 75.8 75.6 75.4 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Audio offset (s) )%( ycrucca lortnoC Figure 10: Effect of shifting the control audio signal on the control F1-score. Note that the network was trained and tested on 2.56 second clips, so a 2.56 second offset corre- sponds to no overlap between the audio and visual streams.
Conv3D s=1, k=4, C=m [64, 224, 224, 32] Conv3DTranspose s=2, k=4, C=32 Concat. BatchNorm + ReLu BatchNorm + ReLu Conv3D Conv3D s=1, k=1, C=16 s=1, k=4, C=16 BatchNorm + ReLu [32, 112, 112, 32] Conv3DTranspose s=[1,2,2], k=4, C=32 Concat. BatchNorm + ReLu BatchNorm + ReLu Conv3D Conv3D s=1, k=1, C=32 s=1, k=4, C=32 BatchNorm + ReLu [32, 56, 56, 32] Conv3DTranspose s=[1,2,2], k=4, C=32 Concat. I3D network BatchNorm + ReLu BatchNorm + ReLu Conv3D Conv3D s=1, k=1, C=64 s=1, k=4, C=64 BatchNorm + ReLu [32, 28, 28, 64] Conv3DTranspose s=2, k=4, C=64 Concat. BatchNorm + ReLu BatchNorm + ReLu Conv3D Conv3D s=1, k=1, C=64 s=1, k=[2,4,4], C=64 BatchNorm + ReLu [16, 14, 14, 64] Conv3DTranspose s=2, k=[2,4,4], C=64 rotareneG ksaM Input video Output Masks [64, 224, 224, 3] [64, 224, 224, m + m*n] Conv3D s=1, k=4, C=m*n+3*m [64, 224, 224, 32] Conv3DTranspose s=2, k=4, C=32 Concat. AvgPool3D k=3, s=2 BatchNorm + ReLu BatchNorm + ReLu [32, 112, 112, m] Conv3D Conv3D s=1, k=1, C=16 s=1, k=4, C=16 BatchNorm + ReLu Conv3d_1a_7x7 [32, 112, 112, 64] [32, 112, 112, 32] Conv3DTranspose s=[1,2,2], k=4, C=32 Concat. AvgPool3D k=3, s=[1,2,2] BatchNorm + ReLu BatchNorm + ReLu [32, 56, 56, m] Conv3D Conv3D s=1, k=1, C=32 s=1, k=4, C=32 BatchNorm + ReLu [32C ,o n 5v 63 ,d _ 52 6c_ ,3 x 13 92] Channel Masking [32, 56, 56, 32] Conv3DTranspose s=[1,2,2], k=4, C=32 Concat. AvgPool3D k=3, s=[1,2,2] BatchNorm + ReLu BatchNorm + ReLu [32, 28, 28, m] Conv3D Conv3D s=1, k=1, C=64 s=1, k=4, C=64 BatchNorm + ReLu Mixed_3c [32, 28, 28, 480] Channel Masking [32, 28, 28, 64] Conv3DTranspose s=2, k=4, C=64 Concat. AvgPool3D k=3, s=2 [16, 14, 14, m] BatchNorm + ReLu BatchNorm + ReLu Conv3D Conv3D s=1, k=1, C=64 s=1, k=[2,4,4], C=64 BatchNorm + ReLu Mixed_4f [16, 14, 14, 832] Channel Masking [16, 14, 14, 64] AvgPool3D [8, M 7,ix e 7d ,_ 5 1c 024] [8,k = 73 ,, s 7= ,2 m] sC =2o ,n v k3 =[D 2,T 4r ,a 4n ]s ,p Co =s 6e 4 Channel Masking rotareneG reyaL [64, 224, 224, m*n+3*m] Channel Masking Figure 11: Details of the architecture used for C2. The ‘Channel Masking‘ block corresponds to the masking procedure described in equation (1) of the main paper.
oidua zHk84 tupnI margortceps gol Conv3D s=1, k=4, C=m [64, 224, 224, 32] Conv3DTranspose s=2, k=4, C=32 Concat. BatchNorm + ReLu BatchNorm + ReLu Conv3D Conv3D s=1, k=1, C=16 s=1, k=4, C=16 BatchNorm + ReLu [32, 112, 112, 32] Conv3DTranspose s=[1,2,2], k=4, C=32 Concat. BatchNorm + ReLu BatchNorm + ReLu Conv3D Conv3D s=1, k=1, C=32 s=1, k=4, C=32 BatchNorm + ReLu [32, 56, 56, 32] Conv3DTranspose s=[1,2,2], k=4, C=32 I3D network Concat. BatchNorm + ReLu BatchNorm + ReLu Conv3D Conv3D s=1, k=1, C=64 s=1, k=4, C=64 BatchNorm + ReLu [32, 28, 28, 64] Conv3DTranspose s=2, k=4, C=64 Concat. BatchNorm + ReLu BatchNorm + ReLu Conv3D Conv3D s=1, k=1, C=64 s=1, k=[2,4,4], C=64 BatchNorm + ReLu [16, 14, 14, 64] Conv3DTranspose s=2, k=[2,4,4], C=64 rotareneG ksaM Input video Output Masks [64, 224, 224, 3] [64, 224, 224, m + m*n] Conv3D s=1, k=4, C=m*n+3*m [64, 224, 224, 32] Conv3DTranspose s=2, k=4, C=32 Concat. AvgPool3D k=3, s=2 BatchNorm + ReLu BatchNorm + ReLu [32, 112, 112, m] Conv3D Conv3D s=1, k=1, C=16 s=1, k=4, C=16 BatchNorm + ReLu Conv3d_1a_7x7 [32, 112, 112, 64] [32, 112, 112, 32] Conv3DTranspose s=[1,2,2], k=4, C=32 Concat. AvgPool3D k=3, s=[1,2,2] BatchNorm + ReLu BatchNorm + ReLu [32, 56, 56, m] Conv3D Conv3D s=1, k=1, C=32 s=1, k=4, C=32 BatchNorm + ReLu [32C ,o n 5v 63 ,d _ 52 6c_ ,3 x 13 92] Channel Masking [32, 56, 56, 32] Conv3DTranspose s=[1,2,2], k=4, C=32 Concat. AvgPool3D k=3, s=[1,2,2] BatchNorm + ReLu BatchNorm + ReLu [32, 28, 28, m] Conv3D Conv3D s=1, k=1, C=64 s=1, k=4, C=64 BatchNorm + ReLu Mixed_3c [32, 28, 28, 480] Channel Masking [32, 28, 28, 64] Conv3DTranspose s=2, k=4, C=64 Concat. AvgPool3D k=3, s=2 [16, 14, 14, m] BatchNorm + ReLu BatchNorm + ReLu Conv3D Conv3D s=1, k=1, C=64 s=1, k=[2,4,4], C=64 BatchNorm + ReLu Mixed_4f [16, 14, 14, 832] Channel Masking [16, 14, 14, 64] AvgPool3D [8, M 7,ix e 7d ,_ 5 1c 024] [8,k = 73 ,, s 7= ,2 m] sC =2o ,n v k3 =[D 2,T 4r ,a 4n ]s ,p Co =s 6e 4 Channel Masking rotareneG reyaL [64, 224, 224, m*n+3*m] Audio/Visual Fusion Channel Masking Audio/Visual Fusion Audio/Visual Fusion Audio/Visual Fusion Audio Network ]1 ,652 ,215[ D2vnoC 46=C ,3=k ,2=s uLeR + mroNhctaB D2vnoC 46=C ,3=k ,2=s uLeR + mroNhctaB looPxaM 2=k ,2=s ]46 ,23 ,46[:1vnoc D2vnoC 821=C ,3=k ,1=s uLeR + mroNhctaB D2vnoC 821=C ,3=k ,1=s uLeR + mroNhctaB looPxaM 2=k ,2=s ]821 ,61 ,23[:2vnoc D2vnoC 652=C ,3=k ,1=s uLeR + mroNhctaB D2vnoC 652=C ,3=k ,1=s uLeR + mroNhctaB looPxaM 2=k ,2=s ]652 ,8 ,61[:3vnoc D2vnoC 215=C ,3=k ,1=s uLeR + mroNhctaB D2vnoC 215=C ,3=k ,1=s uLeR + mroNhctaB ]215 ,8 ,61[:4vnoc xaMecudeR ycneuqerF ,emiT 821=C:CF uLeR 821=C:CF ]821[:2cf rossergeR lortnoC Audio/Visual Fusion AvgPool3D k=[8, 7, 7], s=1 [1152] FC: C=n Figure 12: Details of the architecture used for C3 with internal prediction control. The ‘Channel Masking‘ block corresponds to the masking procedure described in equation (1) of the main paper. The ‘Audio/Visual Fusion‘ block matches the Audio- visual fusion procedure described in Section 3.2 of the main paper and illustrated in Figure 3.
