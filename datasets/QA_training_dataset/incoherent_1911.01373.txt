Gradient-based Adaptive Markov Chain Monte Carlo Michalis K. Titsias Petros Dellaportas DeepMind Department of Statistical Science London, UK University College of London, UK mtitsias@google.com Department of Statistics, Athens Univ. of Econ. and Business, Greece and The Alan Turing Institute, UK Abstract We introduce a gradient-based learning method to automatically adapt Markov chain Monte Carlo (MCMC) proposal distributions to intractable targets. We define a maximum entropy regularised objective function, referred to as generalised speed measure, which can be robustly optimised over the parameters of the proposal dis- tribution by applying stochastic gradient optimisation. An advantage of our method compared to traditional adaptive MCMC methods is that the adaptation occurs even when candidate state values are rejected. This is a highly desirable property of any adaptation strategy because the adaptation starts in early iterations even if the initial proposal distribution is far from optimum. We apply the framework for learning multivariate random walk Metropolis and Metropolis-adjusted Langevin proposals with full covariance matrices, and provide empirical evidence that our method can outperform other MCMC algorithms, including Hamiltonian Monte Carlo schemes. 1 Introduction Markov chain Monte Carlo (MCMC) is a family of algorithms that provide a mechanism for gen- erating dependent draws from arbitrarily complex distributions. The basic set up of an MCMC algorithm in any probabilistic (e.g. Bayesian) inference problem, with an intractable target density π(x), is as follows. A discrete time Markov chain {X }∞ with transition kernel P , appropriately t t=0 θ chosen from a collection of π-invariant kernels {P (·, ·)} , is generated and the ergodic averages θ θ∈Θ µ (F ) = t−1 (cid:80)t−1 F (X ) are used as approximations to E (F ) for any real-valued function F . Al- t i=0 i π though in principle this sampling setup is simple, the actual implementation of any MCMC algorithm requires careful choice of P because the properties of µ depend on θ. In common kernels that θ t lead to random walk Metropolis (RWM), Metropolis-adjusted Langevin (MALA) or Hamiltonian Monte Carlo (HMC) algorithms the kernels P are specified through an accept-reject mechanism in θ which the chain moves from time t to time t + 1 by first proposing candidate values y from a density q (y|x) and accepting them with some probability α(x , y) and setting x = y, or rejecting them θ t t+1 and setting x = x . Since θ directly affects this acceptance probability, it is clear that one should t+1 t choose θ so that the chain does not move too slowly or rejects too many proposed values y because in both these cases convergence to the stationary distribution will be slow. This has been recognised as early as in [22] and has initiated exciting research that has produced optimum average acceptance probabilities for a series of algorithms; see [30, 31, 32, 15, 6, 8, 34, 7, 35, 9]. Such optimal average acceptance probabilities provide basic guidelines for adapting single step size parameters to achieve certain average acceptance rates. More sophisticated adaptive MCMC algorithms that can learn a full set of parameters θ, such as a covariance matrix, borrow information from the history of the chain to optimise some criterion reflecting the performance of the Markov chain [14, 5, 33, 13, 2, 1, 4]. Such methods are typically 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. 0202 naJ 6 ]LM.tats[ 2v37310.1191:viXra
non gradient-based and the basic strategy they use is to sequentially fit the proposal q (y|x) to the θ history of states x , x , . . . , by ignoring also the rejected state values. This can result in very slow t−1 t adaptation because the initial Markov chain simulations are based on poor initial θ and the generated states, from which θ is learnt, are highly correlated and far from the target. The authors in [34] call such adaptive strategies ‘greedy’ in the sense that they try to adapt too closely to initial information from the output and take considerable time to recover from misleading initial information. In this paper, we develop faster and more robust gradient-based adaptive MCMC algorithms that make use of the gradient of the target, ∇ log π(x), and they learn from both actual states of the chain and proposed (and possibly rejected) states. The key idea is to define and maximise w.r.t. θ an entropy regularised objective function that promotes high acceptance rates and high values for the entropy of the proposal distribution. This objective function, referred to as generalised speed measure, is inspired by the speed measure of the infinite-dimensional limiting diffusion process that captures the notion of speed in which a Markov chain converges to its stationary distribution [32]. We maximise this objective function by applying stochastic gradient variational inference techniques such as those based on the reparametrisation trick [19, 29, 40]. An advantage of our algorithm compared to traditional adaptive MCMC methods is that the adaptation occurs even when candidate state values are rejected. In fact, the adaptation can be faster when candidate values y are rejected since then we make always full use of the gradient ∇ log π(y) evaluated at the rejected y. This allows the adaptation to start in early iterations even if the initial proposal distribution is far from optimum and the chain is not moving. We apply the method for learning multivariate RWM and MALA proposals where we adapt full covariance matrices parametrised efficiently using Cholesky factors. In the experiments we demonstrate our algorithms to multivariate Gaussian targets and Bayesian logistic regression and empirically show that they outperform several other baselines, including advanced HMC schemes. 2 Gradient-based adaptive MCMC Assume a target distribution π(x), known up to some unknown normalising constant, where x ∈ Rn is the state vector. To sample from π(x) we consider the Metropolis-Hastings (M-H) algorithm that generates new states by sampling from a proposal distribution q (y|x), that depends on parameters θ, θ and accepts or rejects each proposed state by using the standard M-H acceptance probability (cid:26) (cid:27) π(y)q (x|y) α(x, y; θ) = min 1, θ . (1) π(x)q (y|x) θ While the M-H algorithm defines a Markov chain that converges to the target distribution, the efficiency of the algorithm heavily depends on the choice of the proposal distribution q (x|y) and the θ setting of its parameters θ. Here, we develop a framework for stochastic gradient-based adaptation or learning of q (x|y) that θ maximises an objective function inspired by the concept of speed measure that underlies the theoretical foundations of MCMC optimal tuning [30, 31]. Given that the chain is at state x we would like: (i) to propose big jumps in the state space and (ii) accept these jumps with high probability. By assuming for now that the proposal has the standard random walk isotropic form, such that q (y|x) = N (y|x, σ2I), σ the speed measure is defined as s (x) = σ2 × α(x; σ), (2) σ where σ2 denotes the variance, also called step size, of the proposal distribution, while α(x; σ) is (cid:82) the average acceptance probability when starting at x, i.e. α(x; σ) = α(x, y; σ)q (y|x)dy. To σ learn a good value for the step size we could maximise the speed measure in Eq. 2, which intuitively promotes high variance for the proposal distribution together with high acceptance rates. In the theory of optimal MCMC tuning, s (x) is averaged under the stationary distribution π(x) to obtain a σ (cid:82) global speed measure value s = π(x)s (x)dx. For simple targets and with increasing dimension σ σ this measure is maximised so that σ2 is set to a value that leads to the acceptance probability 0.234 [30, 31]. This subsequently leads to the popular heuristic for tuning random walk proposals: tune σ2 so that on average the proposed states are accepted with probability 1/4. Similar heuristics have been obtained for tuning the step sizes of more advanced schemes such as MALA and HMC, where 0.574 is considered optimal for MALA [32] and 0.651 for HMC [24, 9]. While the current notion of speed measure from Eq. 2 is intuitive, it is only suitable for tuning proposals having a single step size. Thus, in order to learn arbitrary proposal distributions q (y|x), θ 2
where θ is a vector of parameters, we need to define suitable generalisations of the speed measure. Suppose, for instance, that we wish to tune a Gaussian with a full covariance matrix, i.e. q (y|x) = Σ N (y|x, Σ). To achieve this we need to generalise the objective in Eq. 2 by replacing σ2 with some functional F(Σ) that depends on the full covariance. An obvious choice is to consider the average distance ||y − x||2 given by the trace tr(Σ) = (cid:80) σ2. However, this is problematic since it can lead to i i learning proposals with very poor mixing. To see this note that since the trace is the sum of variances it can obtain high values even when some of the components of x have very low variance, e.g. for some x it holds σ2 ≈ 0, which can result in very low sampling efficiency or even non-ergodicity. In order i i to define better functionals F(Σ) we wish to exploit the intuition that for MCMC all components of x need to jointly perform (relative to their scale) big jumps, a requirement that is better captured by the determinant |Σ| or more generally by the entropy of the proposal distribution. Therefore, we introduce a generalisation of the speed measure having the form, (cid:90) s (x) = exp{βH } × α(x; θ) = exp{βH } × α(x, y; θ)q (y|x)dy, (3) θ qθ(y|x) qθ(y|x) θ (cid:82) where H = − q (y|x) log q (y|x)dy denotes the entropy of the proposal distribution and qθ(y|x) θ θ β > 0 is an hyperparameter. Note that when the proposal distribution is a full Gaussian q (y|x) = Σ N (y|x, Σ) then exp{βH q(y|x)} = const × |Σ| β 2 which depends on the determinant of Σ. s θ(x), referred to as generalised speed measure, trades off between high entropy of the proposal distribution and high acceptance probability. The hyperparameter β plays the crucial role of balancing the relative strengths of these terms. As discussed in the next section we can efficiently optimise β in order to achieve a desirable average acceptance rate. In the following we make use of the above generalised speed measure to derive a variational learning algorithm for adapting the parameters θ using stochastic gradient-based optimisation. 2.1 Maximising the generalised speed measure using variational inference During MCMC iterations we collect the pairs of vectors (x , y ) where x is the state of the chain t t t>0 t at time t and y the corresponding proposed next state, which if accepted then x = y . When t t+1 t the chain has converged each x follows the stationary distribution π(x), otherwise it follows some t distribution that progressively converges to π(x). In either case we view the sequence of pairs (x , y ) t t as non-iid data based on which we wish to perform gradient-based updates of the parameters θ. In practice such updates can be performed with diminishing learning rates, or more safely completely stop after some number of burn-in iterations to ensure convergence. Specifically, given the current state x we wish to take a step towards maximising s (x ) in Eq. 3 or equivalently its logarithm, t θ t (cid:90) log s (x ) = log α(x, y; θ)q (y|x )dy + βH . (4) θ t θ t qθ(y|xt) The second term is just the entropy of the proposal distribution, which typically will be analytically tractable, while the first term involves an intractable integral. To approximate the first term we work similarly to variational inference [18, 10] and we lower bound it using Jensen’s inequality, (cid:90) (cid:26) π(y)q (x |y) (cid:27) log s (x ) ≥ F (x ) = q (y|x ) log min 1, θ t dy + βH (5) θ t θ t θ t π(x )q (y|x ) qθ(y|xt) t θ t (cid:90) (cid:26) π(y) q (x |y) (cid:27) = q (y|x )min 0, log + log θ t dy + βH . (6) θ t π(x ) q (y|x ) qθ(y|xt) t θ t To take a step towards maximising F we can apply standard stochastic variational inference tech- θ niques such as the score function method or the reparametrisation trick [11, 26, 28, 19, 29, 40, 20]. Here, we focus on the case q (y|x ) is a reparametrisable distribution such that y = T (x , (cid:15)) where θ t θ t T is a deterministic transformation and (cid:15) ∼ p((cid:15)). F (x ) can be re-written as θ θ t (cid:90) (cid:26) π(T (x , (cid:15))) q (x |T (x , (cid:15))) (cid:27) F (x ) = p((cid:15))min 0, log θ t + log θ t θ t d(cid:15) + βH . θ t π(x ) q (T (x , (cid:15))|x ) qθ(y|xt) t θ θ t t Since MCMC at the t-th iteration proposes a specific state y constructed as (cid:15) ∼ p((cid:15) ), y = t t t t T (x , (cid:15) ), an unbiased estimate of the exact gradient ∇ F (x ) can be obtained by θ t t θ θ t (cid:26) (cid:27) π(T (x , (cid:15) )) q (x |T (x , (cid:15) )) ∇ F (x , (cid:15) ) = ∇ min 0, log θ t t + log θ t θ t t + β∇ H , θ θ t t θ π(x ) q (T (x , (cid:15) )|x ) θ qθ(y|xt) t θ θ t t t 3
Algorithm 1 Gradient-based Adaptive MCMC Input: target π(x); reparametrisable proposal q (y|x) s.t. y = T (x, (cid:15)), (cid:15) ∼ p((cid:15)); initial x ; θ θ 0 desired average acceptance probability α . ∗ Initialise θ, β = 1. for t = 1, 2, 3, . . . , do : Propose (cid:15) ∼ p((cid:15) ), y = T (x , (cid:15) ). t t t θ t t : Adapt θ: θ ← θ + ρ ∇ F (x , (cid:15) ). t θ θ t t : Accept or reject y using the standard M-H ratio to obtain x . t t+1 : Set α = 1 if y was accepted and α = 0 otherwise. t t t : Adapt hyperparameter β: β ← β[1 + ρ (α − α )] # default value for ρ = 0.02. β t ∗ β end for which is used to make a gradient update for the parameters θ. Note that the first term in the stochastic gradient is analogous to differentiating through a rectified linear hidden unit (ReLu) in neural networks, i.e. if log π(yt) + log qθ(xt|yt) ≥ 0 the gradient is zero (this is the case when y is π(xt) qθ(yt|xt) t accepted with probability one), while otherwise the gradient of the first term reduces to q (x |T (x , (cid:15) )) ∇ log π(T (x , (cid:15) )) + ∇ log θ t θ t t . θ θ t t θ q (T (x , (cid:15) )|x ) θ θ t t t The value of the hyperparameter β can allow to trade off between large acceptance probability and large entropy of the proposal distribution. Such hyperparameter cannot be optimised by maximising the variational objective F (this typically will set β to a very small value so that the acceptance θ probability becomes close to one but the chain is not moving since the entropy is very low). Thus, β needs to be updated in order to control the average acceptance probability of the chain in order to achieve a certain desired value α . The value of α can be determined based on the specific MCMC ∗ ∗ proposal we are using and by following standard recommendations in the literature, as discussed also in the previous section. For instance, when we use RWM α can be set to 1/4 (see Section 2.2), ∗ while for gradient-based MALA (see Section 2.3) α can be set to 0.55. ∗ Pseudocode for the general procedure is given by Algorithm 1. We set the learning rate ρ using √ t RMSProp [39]; at each iteration t we set ρ = η/(1 + G ), where η is the baseline learning rate, and t t the updates of G depend on the gradient estimate ∇ F (x , (cid:15) ) as G = 0.9G +0.1 [∇ F (x , (cid:15) )]2. t θ θ t t t t θ θ t t 2.2 Fitting a full covariance Gaussian random walk proposal We now specialise to the case the proposal distribution is a random walk Gaussian q (y|x) = L N (y|x, LL(cid:62)) where the parameter L is a positive definite lower triangular matrix, i.e. a Cholesky factor. This distribution is reparametrisable since y ≡ T (x, (cid:15)) = x + L(cid:15), (cid:15) ∼ N ((cid:15)|0, I). At the t-th L iteration when the state is x the lower bound becomes t (cid:90) n (cid:88) F (x ) = N ((cid:15)|0, I)min {0, log π(x + L(cid:15)) − log π(x )} d(cid:15) + β log L + const. (7) L t t t ii i=1 Here, the proposal distribution has cancelled out from the M-H ratio, since it is symmetric, while H = log |L| + const and log |L| = (cid:80)n log L . By making use of the MCMC proposed qθ(y|xt) i=1 ii state y = x + L(cid:15) we can obtain an unbiased estimate of the exact gradient ∇ F (x ), t t t L L t (cid:40)(cid:2) ∇ log π(y ) × (cid:15)(cid:62)(cid:3) + βdiag( 1 , . . . , 1 ), if log π(y ) < log π(x ) ∇ LF L(x t, (cid:15) t) = βdiy at g( 1 , . .t . , 1t ),lower L11 Lnn otherwiset t L11 Lnn where y = x + L(cid:15) , the operation [A] zeros the upper triangular part (above the main diagonal) t t t lower of a squared matrix and diag( 1 , . . . , 1 ) is a diagonal matrix with elements 1/L . The first case L11 Lnn ii of this gradient, i.e. when log π(y ) < log π(x ), has a very similar structure with the stochastic t t reparametrisation gradient when fitting a variational Gaussian approximation [19, 29, 40] with the difference that here we centre the corresponding approximation, i.e. the proposal q (y |x ), at the L t t current state x instead of having a global variational mean parameter. Interestingly, this first case t when MCMC rejects many samples (or even it gets stuck at the same value for long time) is when 4
learning can be faster since the term ∇ log π(y ) × (cid:15)(cid:62) transfers information about the curvature yt t t of the target to the covariance of the proposal. When we start getting high acceptance rates the second case, i.e. log π(y ) ≥ log π(x ), will often be activated so that the gradient will often reduce t t to only having the term βdiag( 1 , . . . , 1 ) that encourages the entropy of the proposal to become L11 Lnn large. The ability to learn from rejections is in sharp contrast with the traditional non gradient-based adaptive MCMC methods which can become very slow when MCMC has high rejection rates. This is because these methods typically learn from the history of state vectors x by ignoring the information t from the rejected states. The algorithm for learning the full random walk Gaussian follows precisely the general structure of Algorithm 1. For the average acceptance rate α we use the value 1/4. ∗ 2.3 Fitting a full covariance MALA proposal Here, we specialise to a full covariance, also called preconditioned, MALA of the form q (y|x) = L N (y|x + (1/2)LL(cid:62)∇ log π(x), LL(cid:62)) where the covariance matrix is parametrised by the x Cholesky factor L. Again this distribution is reparametrisable according to y ≡ T (x, (cid:15)) = L x + (1/2)LL(cid:62)∇ log π(x) + L(cid:15), (cid:15) ∼ N ((cid:15)|0, I). At the t-th iteration when the state is x the t reparametrised lower bound simplifies significantly and reduces to, (cid:90) (cid:110) (cid:16) (cid:17) F (x ) = N ((cid:15)|0, I)min 0, log π x + (1/2)LL(cid:62)∇ log π(x ) + L(cid:15) − log π(x ) L t t t t − 1 (cid:16) ||(1/2)L(cid:62)[∇ log π(x ) + ∇ log π(y)] + (cid:15)||2 − ||(cid:15)||2(cid:17)(cid:111) d(cid:15) + β (cid:88)n log L + const, 2 t ii i=1 where ||·|| denotes Euclidean norm and in the term ∇ log π(y), y = x +(1/2)LL(cid:62)∇ log π(x )+L(cid:15). t t Then, based on the proposed state y = T (x , (cid:15) ) we can obtain the unbiased gradient estimate t L t t ∇F (x , (cid:15) ) similarly to the previous section. In general, such an estimate can be very expensive L t t because the existence of L inside ∇ log π(y ) means that we need to compute the matrix of second t derivatives or Hessian ∇∇ log π(y ). We have found that an alternative procedure which stops t the gradient inside ∇ log π(y ) (i.e. it views ∇ log π(y ) as a constant w.r.t. L) has small bias and t t works well in practice. In fact, as we will show in the experiments this approximation not only is computationally much faster but remarkably also it leads to better adaptation compared to the exact Hessian procedure, presumably because by not accounting for the gradient inside ∇ log π(y ) reduces t the variance. Furthermore, the expression of the gradient w.r.t. L used by this fast approximation can be computed very efficiently with a single O(n2) operation (an outer vector product; see Supplement), while each iteration of the algorithm requires overall at most four O(n2) operations. For these gradient-based adaptive MALA schemes, β in Algorithm 1 is adapted to obtain an average acceptance rate roughly α = 0.55. ∗ 3 Related Work Connection of our method with traditional adaptive MCMC methods has been discussed in Section 1. Here, we analyse additional related works that make use of gradient-based optimisation and specialised objective functions or algorithms to train MCMC proposal distributions. The work in [21] proposed a criterion to tune MCMC proposals based on maximising a modified version of the expected squared jumped distance, (cid:82) q (y|x )||y − x ||2α(x , y; θ)dy, previously θ t t t considered in [27]. Specifically, the authors in [21] firstly observe that the expected squared jumped distance may not encourage mixing across all dimensions of x1 and then try to resolve this by including a reciprocal term (see Section 4.2 in their paper). The generalised speed measure proposed in this paper is rather different from such criteria since it encourages joint exploration of all dimensions of x by applying maximum entropy regularisation, which by construction penalises "dimensions that do not move" since the entropy becomes minus infinity in such cases. Another important difference is that in our method the optimisation is performed in the log space by propagating gradients through the logarithm of the M-H acceptance probability, i.e. through log α(x , y; θ) and not through α(x , y; θ). t t This is exactly analogous to other numerically stable objectives such as variational lower bounds and log likelihoods, and as those our method leads to numerically stable optimisation for arbitrarily large dimensionality of x and complex targets π(x). 1Because the additive form of ||y − x ||2 = (cid:80) (y − x )2 implies that even when some dimensions might t i i ti not be moving at all (the corresponding distance terms are zero), the overall sum can still be large. 5
In another related work, the authors in [25] considered minimising the KL divergence KL[π(x )q (y |x )||π(y )q (x |y )]. However, this loss for standard proposal schemes, such as t θ t t t θ t t RWM and MALA, leads to degenerate deterministic solutions where q (y |x ) collapses to a delta θ t t function. Therefore, [25] maximised this objective for the independent M-H sampler where the collapsing problem does not occur. The entropy regularised objective we introduced is different and it can adapt arbitrary MCMC proposal distributions, and not just the independent M-H sampler. There has been also work to learn flexible MCMC proposals using neural networks [38, 21, 16, 36]. For instance, [38] use volume preserving flows and an adversarial objective, [21] use the modified expected jumped distance, discussed earlier, to learn neural network-based extensions of HMC, while [16, 36] use auxiliary variational inference. The need to train neural networks can add a significant computational cost, and from the end-user point of view these neural adaptive samplers might be hard to tune especially in high dimensions. Notice that the generalised speed measure we proposed in this paper could possibly be used to train neural adaptive samplers as well. However, to really obtain practical algorithms we need to ensure that training has small cost that does not overwhelm the possible benefits in terms of effective sample size. Finally, the generalised speed measure that is based on entropy regularisation shares similarities with other used objectives for learning probability distributions, such as in variational Bayesian inference, where the variational lower bound includes an entropy term [18, 10] and supervised learning (RL) where maximum-entropy regularised policy gradients are able to estimate more explorative policies [37, 23]. Further discussion on the resemblance of our algorithm with RL is given in the Supplement. 4 Experiments We test the gradient-based adaptive MCMC methods in several simulated and real data. We investigate the performance of two instances of the framework: the gradient-based adaptive random walk (gadRWM) detailed in Section 2.2 and the corresponding MALA (gadMALA) detailed in Section 2.3. For gadMALA we consider the exact reparametrisation method that requires the evaluation of the Hessian (gadMALAe) and the fast approximate variant (gadMALAf). These schemes are compared against: (i) standard random walk Metropolis (RWM) with proposal N (y|x, σ2I), (ii) an adaptive MCMC (AM) that fits a proposal of the form N (y|x, Σ) (we consider a computational efficient version based on updating the Cholesky factor of Σ; see Supplement), (iii) a standard MALA proposal N (y|x + (1/2)σ2∇ log π(x), σ2I), (iv) an Hamiltonian Monte Carlo (HMC) with a fixed number of leap frog steps (either 5, or 10, or 20) (v) and the state of the art no-U-turn sampler (NUTS) [17] which arguably is the most efficient adaptive HMC method that automatically determines the number of leap frog steps. We provide our own MALTAB implementation2 of all algorithms, apart from NUTS for which we consider a publicly available implementation. 4.1 Illustrative experiments To visually illustrate the gradient-based adaptive samplers we consider a correlated 2-D Gaussian target with covariance matrix Σ = [1 0.99; 0.99 1] and a 51-dimensional Gaussian target obtained by evaluating the squared exponential kernel plus small white noise, i.e. k(x , x ) = exp{− 1 (xi−xj)2 }+ i j 2 0.16 0.01δ , on the regular grid [0, 4]. The first two panels in Figure 1 show the true covariance i,j together with the adapted covariances obtained by gadRWM for two different settings of the average acceptance rate α in Algorithm 1, which illustrates also the adaptation of the entropy-regularisation ∗ hyperparameter β that is learnt to obtain a certain α . The remaining two plots illustrate the ability to ∗ learn a highly correlated 51-dimensional covariance matrix (with eigenvalues ranging from 0.01 to 12.07) by applying our most advanced gadMALAf scheme. 4.2 Quantitative results Here, we compare all algorithms in some standard benchmark problems, such as Bayesian logistic regression, and report effective sample size (ESS) together with other quantitative scores. Experimental settings. In all experiments for AM and gradient-based adaptive schemes the Cholesky √ factor L was initialised to a diagonal matrix with values 0.1/ n in the diagonal where n is the dimensionality of x. For the AM algorithm the learning rate was set to 0.001/(1 + t/T ) where t is the number of iterations and T (the value 4000 was used in all experiments) serves to keep the learning rate nearly constant for the first T training iterations. For the gradient-based adaptive algorithms 2https://github.com/mtitsias/gadMCMC. 6
Figure 1: The green contours in the first two panels (from left to right) show the 2-D Gaussian target, while the blue contours show the learned covariance, LL(cid:62), after adapting for 2 × 104 iterations using gadRWM and targeting acceptance rates α = 0.25 and α = 0.4, respectively. For α = 0.25 the adapted blue contours ∗ ∗ ∗ show that the proposal matches the shape of the target but it has higher entropy/variance and the hyperparameter β obtained the value 7.4. For α = 0.4 the blue contours shrink a bit and β is reduced to 2.2 (since higher ∗ acceptance rate requires smaller entropy). The third panel shows the exact 51 × 51 covariance matrix and the last panel shows the adapted one, after running our most efficient gadMALAf scheme for 2 × 105 iterations. In √ both experiments L was initialised to diagonal matrix with 0.1/ n in the diagonal. Figure 2: Panels in the first row show trace plots, obtained by different schemes, across the last 2×104 sampling iterations for the most difficult to sample x dimension. The panels in the second row show the estimated 100 values of the diagonal of L obtained by different adaptive schemes. Notice that the real Gaussian target has diagonal covariance matrix Σ = diag(s2, . . . , s2 ) where s are uniform in the range [0.01, 1]. 1 100 i we use RMSprop (see Section 2.1) where η was set to 0.00005 for gadRWM and to 0.00015 for the gadMALA schemes. NUTS uses its own fully automatic adaptive procedure that determines both the step size and the number of leap frog steps [17]. For all experiments and algorithms (apart from NUTS) we consider 2 × 104 burn-in iterations and 2 × 104 iterations for collecting samples. This adaptation of L or σ2 takes place only during the burn-in iterations and then it stops, i.e. at collection of samples stage these parameters are kept fixed. For NUTS, which has its own internal tuning procedure, 500 burn-in iterations are sufficient before collecting 2 × 104 samples. The computational time for all algorithms reported in the tables corresponds to the overall running time, i.e. the time for performing jointly all burn-in and collection of samples iterations. Neal’s Gaussian target. We first consider an example used in [24] where the target is a zero-mean multivariate Gaussian with diagonal covariance matrix Σ = diag(s2, . . . , s2 ) where the stds s 1 100 i take values in the linear grid 0.01, 0.02, . . . , 1. This is a challenging example because the different scaling of the dimensions means that the schemes that use an isotropic step σ2 will be adapted to the smallest dimension x while the chain at the higher dimensions, such as x , will be moving slowly 1 100 exhibiting high autocorrelation and small effective sample size. The first row of Figure 3 shows the trace plot across iterations of the dimension x for some of the adaptive schemes including 100 an HMC scheme that uses 20 leap frog steps. Clearly, the gradient-based adaptive methods show much smaller autocorrelation that AM. This is because they achieve a more efficient adaptation of the Cholesky factor L which ideally should become proportional to a diagonal matrix with the linear grid 0.01, 0.02, . . . , 1 in the main diagonal. The second row of Figure 3 shows the diagonal elements of L from which we can observe that all gradient-based schemes lead to more accurate adaptation with gadMALAf being the most accurate. Furthermore, Table 1 provides quantitative results such as minimum, median and maximum ESS computed across all dimensions of the state vector x, running times and an overall efficiency score 7
Table 1: Comparison in Neal’s Gaussian example (dimensionality was n = 100; see panel above) and Caravan binary classification dataset where the latter consists of 5822 data points (dimensionality was n = 87; see panel below). All numbers are averages across ten repeats where also one-standard deviation is given for the Min ESS/s score. From the three HMC schemes we report only the best one in each case. Method Time(s) Accept Rate ESS (Min, Med, Max) Min ESS/s (1 st.d.) (Neal’s Gaussian) gadMALAf 8.7 0.556 (1413.4, 1987.4, 2580.8) 161.70 (15.07) gadMALAe 10.0 0.541 (922.2, 2006.3, 2691.1) 92.34 (7.11) gadRWM 7.0 0.254 (27.5, 66.9, 126.9) 3.95 (0.66) AM 2.3 0.257 (8.7, 48.6, 829.1) 3.71 (0.87) RWM 2.2 0.261 (2.9, 8.4, 2547.6) 1.31 (0.06) MALA 3.1 0.530 (2.9, 10.0, 12489.2) 0.95 (0.03) HMC-20 49.7 0.694 (306.1, 1537.8, 19732.4) 6.17 (3.35) NUTS 360.5 >0.7 (18479.6, 20000.0, 20000.0) 51.28 (1.64) (Caravan) gadMALAf 23.1 0.621 (228.1, 750.3, 1114.7) 9.94 (2.64) gadMALAe 95.1 0.494 (66.6, 508.3, 1442.7) 0.70 (0.16) gadRWM 22.6 0.234 (5.3, 34.3, 104.5) 0.23 (0.06) AM 20.0 0.257 (3.2, 11.8, 62.5) 0.16 (0.01) RWM 15.3 0.242 (3.0, 9.3, 52.5) 0.20 (0.03) MALA 22.8 0.543 (4.4, 28.3, 326.0) 0.19 (0.05) HMC-10 225.5 0.711 (248.3, 2415.7, 19778.7) 1.10 (0.12) NUTS 1412.1 >0.7 (7469.5, 20000.0, 20000.0) 5.29 (0.38) Min ESS/s (i.e. ESS for the slowest moving component of x divided by running time – last column in the Table) which allows to rank the different algorithms. All results are averages after repeating the simulations 10 times under different random initialisations. From the table it is clear that the gadMALA algorithms give the best performance with gadMALAf being overall the most effective. Bayesian logistic regression. We consider binary classification where given a set of training exam- ples {y , s }n we assume a logistic regression likelihood p(y|w, s) = (cid:80)n y log σ(s ) + (1 − i i i=1 i=1 i i y ) log(1 − σ(s )), where σ(s ) = 1/(1 + exp(−w(cid:62)s )), s is the input vector and w the parame- i i i i i ters. We place a Gaussian prior on w and we wish to sample from the posterior distribution over w. We considered six binary classification datasets (Australian Credit, Heart, Pima Indian, Ripley, German Credit and Caravan) with a number of examples ranging from n = 250 to n = 5822 and dimensionality of the state/parameter vector ranging from 3 to 87. Table 1 shows results for the most challenging Caravan dataset where the dimensionality of w is 87, while the remaining five tables are given in the Supplement. From all tables we observe that the gadMALAf is the most effective and it outperforms all other methods. While NUTS has always very high ESS is still outperformed by gadMALAf because of the high computational cost, i.e. NUTS might need to use a very large number of leap frog steps (each requiring re-evaluating the gradient of the log target) per iteration. Further results, including a higher 785-dimensional example on MNIST, are given in the Supplement. 5 Conclusion We have presented a new framework for gradient-based adaptive MCMC that makes use of an entropy- regularised objective function that generalises the concept of speed measure. We have applied this method for learning RWM and MALA proposals with full covariance matrices. Some topics for future research are the following. Firstly, to deal with very high dimensional spaces it would be useful to consider low rank parametrisations of the covariance matrices in RWM and MALA proposals. Secondly, it would be interesting to investigate whether our method can be used to tune the so-called mass matrix in HMC samplers. However, in order for this to lead to practical and scalable algorithms we have to come up with schemes that avoid the computation of the Hessian, as we successfully have done for MALA. Finally, in order to reduce the variance of the stochastic gradients and speed up further the adaptation, especially in high dimensions, our framework could be possibly combined with parallel computing as used for instance in deep supervised learning [12]. 8
References [1] Christophe Andrieu and Yves Atchade. On the efficiency of adaptive mcmc algorithms. Elec- tronic Communications in Probability, 12:336–349, 2007. [2] Christophe Andrieu and Éric Moulines. On the ergodicity properties of some adaptive mcmc algorithms. The Annals of Applied Probability, 16(3):1462–1505, 2006. [3] Christophe Andrieu and Johannes Thoms. A tutorial on adaptive mcmc. Statistics and Comput- ing, 18(4):343–373, December 2008. [4] Yves Atchade, Gersende Fort, Eric Moulines, and Pierre Priouret. Adaptive markov chain monte carlo: theory and methods. Preprint, 2009. [5] Yves F Atchadé and Jeffrey S Rosenthal. On adaptive markov chain monte carlo algorithms. Bernoulli, 11(5):815–828, 2005. [6] Mylène Bédard. Weak convergence of metropolis algorithms for non-iid target distributions. The Annals of Applied Probability, pages 1222–1244, 2007. [7] Mylène Bédard. Efficient sampling using metropolis algorithms: Applications of optimal scaling results. Journal of Computational and Graphical Statistics, 17(2):312–332, 2008. [8] Mylene Bedard. Optimal acceptance rates for metropolis algorithms: Moving beyond 0.234. Stochastic Processes and their Applications, 118(12):2198–2222, 2008. [9] Alexandros Beskos, Natesh Pillai, Gareth Roberts, Jesus-Maria Sanz-Serna, and Andrew Stuart. Optimal tuning of the hybrid monte carlo algorithm. Bernoulli, 19(5A):1501–1534, 2013. [10] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006. [11] P. Carbonetto, M. King, and F. Hamze. A stochastic approximation method for inference in probabilistic graphical models. In Advances in Neural Information Processing Systems, 2009. [12] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Ma- chine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1407–1416, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR. [13] Paolo Giordani and Robert Kohn. Adaptive independent metropolis–hastings by fast estimation of mixtures of normals. Journal of Computational and Graphical Statistics, 19(2):243–259, 2010. [14] Heikki Haario, Eero Saksman, and Johanna Tamminen. An adaptive metropolis algorithm. Bernoulli, 7(2):223–242, 2001. [15] Heikki Haario, Eero Saksman, and Johanna Tamminen. Componentwise adaptation for high dimensional mcmc. Computational Statistics, 20(2):265–273, 2005. [16] Raza Habib and David Barber. Auxiliary variational mcmc. To appear at ICLR 2019, 2019. [17] Matthew D Hoffman and Andrew Gelman. The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo. Journal of Machine Learning Research, 15(1):1593–1623, 2014. [18] M. I. Jordan, editor. Learning in Graphical Models. MIT Press, Cambridge, MA, USA, 1999. [19] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014. [20] A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and D. M. Blei. Automatic differentiation variational inference. Journal of Machine Learning Research, 18(14):1–45, 2017. 9
[21] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing hamiltonian monte carlo with neural networks. In International Conference on Learning Representations, 2018. [22] Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 21(6):1087–1092, 1953. [23] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli- crap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep supervised learning. In International Conference on Machine Learning, 2016. [24] Radford M. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 54:113–162, 2010. [25] Kirill Neklyudov, Pavel Shvechikov, and Dmitry Vetrov. Metropolis-hastings view on variational inference and adversarial training. arXiv preprint arXiv:1810.07151, 2018. [26] J. W. Paisley, D. M. Blei, and M. I. Jordan. Variational Bayesian inference with stochastic search. In International Conference on Machine Learning, 2012. [27] Cristian Pasarica and Andrew Gelman. Adaptively scaling the metropolis algorithm using expected squared jumped distance. Statistica Sinica, pages 343–364, 2010. [28] R. Ranganath, S. Gerrish, and D. M. Blei. Black box variational inference. In Artificial Intelligence and Statistics, 2014. [29] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic meta-optimization synthesis and approximate inference in deep generative models. In International Conference on Machine Learning, 2014. [30] Gareth O Roberts, Andrew Gelman, and Walter R Gilks. Weak convergence and optimal scaling of random walk metropolis algorithms. The annals of applied probability, 7(1):110–120, 1997. [31] Gareth O Roberts and Jeffrey S Rosenthal. Optimal scaling of discrete approximations to langevin diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(1):255–268, 1998. [32] Gareth O Roberts and Jeffrey S Rosenthal. Optimal scaling for various metropolis-hastings algorithms. Statistical Science, pages 351–367, 2001. [33] Gareth O Roberts and Jeffrey S Rosenthal. Coupling and ergodicity of adaptive markov chain monte carlo algorithms. Journal of applied probability, 44(2):458–475, 2007. [34] Gareth O Roberts and Jeffrey S Rosenthal. Examples of adaptive mcmc. Journal of Computa- tional and Graphical Statistics, 18(2):349–367, 2009. [35] Jeffrey S Rosenthal. Optimal proposal distributions and adaptive mcmc. In Handbook of Markov Chain Monte Carlo, pages 114–132. Chapman and Hall/CRC, 2011. [36] T. Salimans, D. P. Kingma, and M. Welling. Markov chain Monte Carlo and variational inference: Bridging the gap. In International Conference on Machine Learning, 2015. [37] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [38] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for mcmc. In Advances in Neural Information Processing Systems, pages 5140–5150, 2017. [39] T. Tieleman and G. Hinton. Lecture 6.5-RMSPROP: Divide the gradient by a running average of its recent magnitude. Coursera: Neural Networks for Machine Learning, 4, 2012. [40] M. K. Titsias and M. Lázaro-Gredilla. Doubly stochastic variational Bayes for non-conjugate inference. In International Conference on Machine Learning, 2014. 10
Supplement: Gradient-based Adaptive Markov Chain Monte Carlo A Gradient-based adaptive MCMC as supervised learning A MDP is a tuple (X , Y, P, R) where X is the state space, Y is the action space, P is the transition distribution with density p(x |x , y ) that describes how the next state x is generated given that t+1 t t t+1 currently we are at state x and we take action y . Further, the reward function R(x , y ) provides t t t t some instantaneous or local signal about how good the action y was when being at x . Furthermore, t t in a MDP we have also a policy π(y |x ) which is a distribution over actions given states and it fully t t describes the behaviour of the agent. Given that we start at x we wish to specify the policy so that to 0 maximise future reward, such as the expected accumulated discounted reward (cid:34) ∞ (cid:35) (cid:88) E γtR(x , y ) , γ ∈ [0, 1]. π t t t=0 Suppose now a MCMC procedure targeting π(x), where x ∈ X is the state vector. Consider a proposal distribution q (y|x), such that the standard Metropolis-Hastings algorithm accepts each θ proposed state y ∼ q (y |x ) with probability t θ t t (cid:18) (cid:19) π(y ) q (x |y ) α(x , y ; θ) = min 1, t θ t t , (8) t t π(x ) q (y |x ) t θ t t so that x = y , while if the proposal is rejected, x = x . To reformulate MCMC as an MDP t+1 t t+1 t we make the following correspondences. Firstly, both the state x and the action y will live in the t t same space which will be the state space X of the target distribution. The MCMC proposal q (y |x ) θ t t will correspond to the policy π(y |x ), while the environmental transition dynamics will be stochastic t t and given by the two-component mixture, p(x |x , y ) = α(x , y ; θ)δ + (1 − α(x , y ; θ))δ , t+1 t t t t xt+1,yt t t xt+1,xt where δ denotes the delta function. This transition density simply says that the new state x with x,y t+1 probability α(x , y ; θ) will be equal to the proposed action y , while with the remaining probability t t t will be set to the previous state, i.e. x = x . Notice that the standard MCMC transition kernel t+1 t K (x , x ) is obtained by integrating out the action y , i.e. θ t t+1 t (cid:90) K (x , x ) = p(x |x , y )q (y |x )dy θ t t+1 t+1 t t θ t t t (cid:18) (cid:90) (cid:19) = α(x , x )q(x |x ) + 1 − α(x , y )q (y |x )dy δ . (9) t t+1 t+1 t t t θ t t t xt+1,xt The final ingredient we need to reformulate MCMC as MDP is the reward function R(x , y ). The t t gradient-based adaptive MCMC method essentially assumes as reward R(y , x ; θ) = log α(x , y ; θ) − β log q (y |x ), t t t t θ t t which is an entropy-regularised reward that promotes high exploration with the entropic term −β log q (y |x ). Gradient-based adaptive MCMC essentially at each step stochastically maximises θ t t the expected reward starting from state x , i.e. t (cid:90) q (y |x )R(y , x ; θ)dy . θ t t t t t While the above reformulates MCMC as a supervised learning (RL) problem, there are clearly also some differences with standard RL problems. Given that the reward R(y , x ; θ) is very informative t t (we are not facing the delayed-reward problem commonly encountered in standard RL) gradient- based MCMC sets γ = 0 in order to maximise immediate reward. Further, the transition dynamics p(x |x , y ) are known in MCMC, while this typically is not the case in standard RL. Finally, t+1 t t notice that the reward R(y , x ; θ) as well as the transition dynamics p(x |x , y ) all depend on the t t t+1 t t parameter θ and the policy q (y |x ), i.e. they depend on the MCMC proposal distribution. θ t t 11
B Further details about the algorithms For the standard adaptive MCMC method (AM) we implemented a computational efficient version that requires no matrix decompositions (which are expensive due to the O(n3) scaling) by parametrising the proposal as N (y|x, LL(cid:62)) and updating the Cholesky factor in each iteration according to the updates µ ← µ + ρ (x − µ), t t+1 L ← L + ρ L (cid:2) L−1(x − µ)(x − µ)(cid:62)L−(cid:62) − I(cid:3) , t t+1 t+1 lower where µ tracks the global mean of the state vector. Further details about this scheme can be found in Section 5.1.1 in [3]. For our most efficient gadMALAf scheme the stochastic gradient in each iteration is (cid:110) (cid:16) (cid:17) ∇F (x , (cid:15) ) = ∇ min 0, log π x + (1/2)LL(cid:62)∇ log π(x ) + L(cid:15) − log π(x ) L t t L t t t t − 1 (cid:16) ||(1/2)L(cid:62)[∇ log π(x ) + ∇ log π(y )] + (cid:15) ||2 − ||(cid:15) ||2(cid:17)(cid:111) + β∇ (cid:88)n log L , 2 t t t t L ii i=1 where, as discussed in the main paper, ∇ log π(y ) is taken as constant w.r.t. L. Then the gradient of t the M-H log ratio (when this log ratio is negative, since otherwise its gradient is zero) simplifies as (cid:16) (cid:17) 1 ∇ log π x + (1/2)LL(cid:62)∇ log π(x ) + L(cid:15) − ∇ ||(1/2)L(cid:62)[∇ log π(x ) + ∇ log π(y )] + (cid:15) ||2 L t t t 2 L t t t 1 (cid:16) (cid:17) (cid:16) (cid:17)(cid:62) = − ∇ log π(x ) − ∇ log π(y ) (1/2)L(cid:62)[∇ log π(x ) − ∇ log π(y )] + (cid:15) 2 t t t t t and then take the lower triangular part. This is just an outer vector product that scales as O(n2). Overall each iteration of the algorithm can be implemented (plus the extra overhead of a single gradient evaluation ∇ log π(y ) of the log target at the proposed state y ) by using at most four O(n2) t t operations during adaptation and exactly two O(n2) operations after burn-in, as shown in the released code. C Extra results on the Neal’s Gaussian target Figure 3 shows trace plot for the log density of the target for all different algorithms. 12
Figure 3: The evolution of the log-target across iterations for all algorithms in Neal’s Gaussian example. Table 2: Comparison of sampling methods in Australian Credit dataset consisted of 690 data points. The size of the state/parameter vector from which we draw samples was n = 15. Method Time(s) Accept Rate ESS (Min, Med, Max) Min ESS/s (1 st.d.) gadMALAf 8.1 0.569 (3485.9, 4262.9, 4784.0) 443.97 (76.13) gadMALAe 13.5 0.540 (3034.9, 4234.3, 4836.3) 227.61 (29.87) gadRWM 7.7 0.253 (288.0, 423.0, 515.0) 38.68 (9.53) AM 4.4 0.261 (310.9, 410.1, 507.2) 70.21 (6.23) RWM 3.4 0.252 (31.3, 312.6, 495.2) 9.16 (3.12) MALA 7.0 0.524 (138.4, 2388.1, 3818.8) 20.22 (5.14) HMC-5 37.0 0.700 (1048.1, 3510.3, 14809.7) 28.06 (11.69) NUTS 41.3 >0.7 (2995.2, 20000.0, 20000.0) 72.86 (7.31) D Extra results on the binary classification datasets Tables 2-6 show the results for the remaining five binary classification datasets not reported in the main article. Figures 4 and 5 show trace plot of log density of the target acrorss all different algorithms for the German Credit and Caravan datasets. For the remaining datasets the corresponding plots are similar. 13
Table 3: Comparison of sampling methods in Ripley dataset consisted of 250 data points. The size of the state/parameter vector from which we draw samples was n = 3. Method Time(s) Accept Rate ESS (Min, Med, Max) Min ESS/s (1 st.d.) gadMALAf 3.3 0.536 (8328.4, 8913.2, 9442.4) 2506.04 (143.47) gadMALAe 4.9 0.543 (8446.7, 9006.6, 9595.6) 1713.44 (44.91) gadRWM 3.1 0.068 (638.0, 736.9, 803.2) 205.99 (17.85) AM 3.0 0.257 (1702.8, 1792.2, 1902.0) 570.19 (49.32) RWM 2.1 0.252 (1129.2, 1627.8, 1979.8) 534.21 (43.54) MALA 2.8 0.542 (2976.0, 5683.0, 9726.5) 1046.48 (54.86) HMC-5 14.7 0.678 (9205.3, 10818.1, 16136.5) 626.55 (196.48) NUTS 7.5 >0.7 (9436.2, 17463.5, 20000.0) 1265.99 (73.01) Table 4: Comparison of sampling methods in Pima Indian dataset consisted of 532 data points. The size of the state/parameter vector from which we draw samples was n = 8. Method Time(s) Accept Rate ESS (Min, Med, Max) Min ESS/s (1 st.d.) gadMALAf 4.6 0.545 (5407.6, 5810.3, 6467.6) 1176.12 (79.54) gadMALAe 6.8 0.547 (5469.6, 5963.6, 6421.1) 801.03 (16.07) gadRWM 4.2 0.267 (635.6, 760.0, 866.2) 150.70 (9.73) AM 4.1 0.273 (612.7, 729.1, 854.8) 149.18 (10.40) RWM 3.2 0.246 (354.6, 496.4, 709.6) 111.81 (6.16) MALA 4.0 0.509 (1524.9, 2457.2, 3853.6) 377.17 (25.80) HMC-5 20.3 0.711 (7295.7, 12798.7, 18267.4) 359.22 (103.55) NUTS 15.2 >0.7 (15343.3, 18606.0, 20000.0) 1008.97 (42.33) Table 5: Comparison of sampling methods in Heart dataset consisted of 270 data points. The size of the state/parameter vector from which we draw samples was n = 14. Method Time(s) Accept Rate ESS (Min, Med, Max) Min ESS/s (1 st.d.) gadMALAf 4.1 0.551 (3892.9, 4362.7, 4784.2) 946.98 (56.10) gadMALAe 6.4 0.560 (3832.4, 4372.3, 4845.6) 599.51 (30.00) gadRWM 3.8 0.288 (342.5, 440.9, 536.1) 88.94 (10.29) AM 3.2 0.238 (342.5, 425.5, 535.4) 106.97 (7.18) RWM 2.3 0.266 (196.9, 314.3, 472.7) 86.57 (11.33) MALA 3.5 0.530 (1429.7, 2310.6, 3260.4) 404.96 (18.57) HMC-5 18.4 0.699 (1913.2, 5600.3, 11883.0) 103.81 (39.38) NUTS 15.4 >0.7 (20000.0, 20000.0, 20000.0) 1295.13 (15.74) Table 6: Comparison of sampling methods in German Credit dataset consisted of 1000 data points. The size of the state/parameter vector from which we draw samples was n = 25. Method Time(s) Accept Rate ESS (Min, Med, Max) Min ESS/s (1 st.d.) gadMALAf 11.0 0.560 (2734.9, 3414.5, 3928.6) 252.91 (37.86) gadMALAe 22.4 0.549 (2808.2, 3384.9, 3883.5) 126.00 (14.68) gadRWM 10.4 0.248 (179.1, 252.5, 323.1) 17.92 (4.18) AM 12.6 0.262 (121.9, 207.6, 308.0) 9.72 (1.25) RWM 8.4 0.233 (45.0, 153.8, 298.7) 5.48 (1.83) MALA 9.2 0.535 (420.1, 1313.2, 2573.7) 47.37 (10.22) HMC-5 43.4 0.706 (3020.2, 10294.4, 20000.0) 71.62 (49.62) NUTS 47.4 >0.7 (7737.2, 20000.0, 20000.0) 166.93 (30.57) 14
Figure 4: The evolution of the log-target across iterations for all algorithms in German Credit dataset. E Results on a higher dimensional example We all tried a much larger Bayesian binary classification problem by taking all 11339 training examples of "5" and "6" MNIST digits which are 28 × 28 images and therefore the dimensionality of the parameter vector w was 785 (the plus one accounts for the bias term). For this larger example from the baselines we applied the gradient-based schemes, MALA, HMC and NUTS since the other methods become very inefficient. From the proposed schemes we applied the most efficient algorithm which is gadMALAf. Also because of the much higher dimensionality of this problem, which makes the stochastic optimisation over L harder, we had to decrease the baseline learning rate in the RMSprop schedule from 0.00015 to 0.00001. We also considered a larger adaptation phase consisted of 5 × 104 instead of 2 × 104. All other algorithms use the same experimental settings as described in the main paper. Figure 7 shows the evolution of the log-target densities for all sampling schemes while Table 7 gives ESS, computation times and other statistics. We can observe that the performance of gadMALAf is reasonably good: it outperforms all methods apart form NUTS. NUTS is better in this example, but it takes around 22 hours to run (since it performs on average 550 gradient evaluations per sampling iteration). Finally, to visualise some part of the learned L found by gadMALAf, Figure 7 depicts the 784 diagonal elements of L as an 28 × 28 grey-scale image. Clearly, gadMALAf manages to perform a sort of feature selection, i.e. to discover that the border pixels in MNIST digits do not really take part in the classification, so it learns a much higher variance for those dimensions (close to the variance of the prior). 15
Figure 5: The evolution of the log-target across iterations for all algorithms in Caravan dataset. Figure 6: The evolution of the log-target across iterations for all algorithms in binary MNIST classification over "5" versus "6". 16
Table 7: Comparison of sampling methods in binary MNIST dataset, of "5" versus "6", consisted of 11339 data points. The size of the state/parameter vector from which we draw samples was n = 785. All numbers are averages across five repeats where also one-standard deviation is given for the Min ESS/s score. Method Time(s) Accept Rate ESS (Min, Med, Max) Min ESS/s (1 st.d.) gadMALAf 779.3 0.575 (46.0, 128.7, 282.8) 0.059 (0.00) MALA 311.8 0.530 (2.8, 5.9, 28.4) 0.009 (0.00) HMC-5 1847.4 0.733 (4.5, 23.1, 162.7) 0.002 (0.00) HMC-10 3381.3 0.589 (13.9, 66.5, 576.0) 0.004 (0.00) HMC-20 6449.1 0.666 (77.8, 240.1, 2060.9) 0.012 (0.00) NUTS 83232.1 >0.7 (18514.1, 20000.0, 20000.0) 0.223 (0.01) Figure 7: The first 784 diagonal elements (i.e. excluding the bias component of x) of the full 785 × 785 Cholesky factor L found after 5 × 104 adapting iterations by gadMALAf. Brighter/white colour means larger values. 17
