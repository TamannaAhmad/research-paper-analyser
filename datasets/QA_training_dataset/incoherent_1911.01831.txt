2018-12-08 Quinoa: a Q-function You Infer Normalized Over Actions Jonas Degrave1, Abbas Abdolmaleki1, Jost Tobias Springenberg1, Nicolas Heess1 and Martin Riedmiller1 1DeepMind We present an algorithm for learning an approximate action-value soft Q-function in the relative entropy regularised supervised learning setting, for which an optimal improved policy can be recovered in closed form. We use recent advances in normalising flows for parametrising the policy together with a learned value-function; and show how this combination can be used to implicitly represent Q-values of an arbitrary policy in continuous action space. Using simple temporal difference learning on the Q-values then leads to a unified objective for policy and value learning. We show how this approach considerably simplifies standard Actor-Critic off-policy algorithms, removing the need for a policy optimisation step. We perform experiments on a range of established supervised learning benchmarks, demonstrating that our approach allows for complex, multimodal policy distributions in continuous action spaces, while keeping the process of sampling from the policy both fast and exact. 1. Introduction Off-policy actor-critic algorithms, in combination with deep neural networks, hold promise for solving problems in continuous control, as they can be used to learn complex non-linear policies in a data-efficient manner [11]. Typically deep actor-critic approaches consist of two steps. First, a neural network is used to fit the Q-values of the current policy. After that, a parametric policy – often a conditional Gaussian distribution – is learned by maximising these learned Q-values. These two steps are then iterated to convergence. Ideally, the second policy optimisation step would not be needed. After all, optimising a policy against a learned Q-function just transforms action-preferences into a normalised distribution. This optimisation step cannot produce new information which was not already encoded in the Q-function. It can however introduce sub-optimal behaviour through approximation errors; either due to the choice in the parametric policy distribution or due to numerical fitting errors. As a consequence, the idea of learning a Q-function from which an improved policy can be obtained without additional optimisation, has been previously considered by learning normalised advantage functions (NAF) [2, 5] or using compatible function approximation with Gaussian policies [16]. While appealing in theory, these approaches come with the caveat that they put additional constraints on the Q-function, such as being locally quadratic in action space. This limits the expressiveness of the Q-function, making it no longer able to correctly fit any set of Q-values. In this work, we propose an algorithm which learns a soft Q-function globally while providing the optimal policy in closed form. We call this algorithm Quinoa, a Q-function you Infer Normalised Over Actions as we can directly perform inference on the optimal policy, which is the soft Q-function normalised over the action dimensions. We find that the key to allowing unrestricted Q-functions that allow for inference of the optimal policy, is to use a richer class of policy parametrisations. In particular, we use normalising flows as they can be universal density function approximators. In the next section, we will explain how we derive our soft Q-function, starting from a relative entropy regularised RL objective, as considered in REPS [12], TRPO [15], MPO [1] and SAC [8]. Corresponding author(s): grave@deepmind.com 9102 voN 5 ]GL.sc[ 1v13810.1191:viXra
Quinoa: a Q-function You Infer Normalized Over Actions 2. Background We consider the standard discounted supervised learning (RL) problem defined by a Markov decision process (MDP). The MDP consists of continuous states s, actions a, transition probabilities p(s t +1|s t , a t ) which specify the probability of transitioning from state s t to s t +1 under action a t , a reward function r (s, a) ∈ (cid:82) and the discount factor γ ∈ [0, 1). The policy π (a|s) with parameters θ is a probability θ distribution over actions a given a state s. For brevity, we drop the subscript θ in the following. Together with the transition probabilities, these give rise to a state-visitation distribution µ (s). We consider the π relative entropy regularised RL setting that encourages the policy to trade off reward with policy entropy. Unlike the regular expected reward objective, this can provide advantages when it is desirable to learn multiple solutions for a given task. More generally, it can help to regularise the policy by preventing collapse of the state-conditional action distribution. We define the relative entropy of policy π compared (cid:104) (cid:16) (cid:17) (cid:105) to a reference policy π˜ as: D [π , π˜ |s] = (cid:69) log π (a |s) , noting that if π˜ is uniform we recover KL a∼π (·|s) π˜(a |s) the entropy H [π ]. The goal for the RL algorithm is to maximise the expected sum of discounted future returns, regularised with this relative entropy: (cid:34) ∞ (cid:12) (cid:35) (cid:213) (cid:12) J (π ) = (cid:69) γ ir t (s i , a i ) − α D KL[π , π˜ |s i ](cid:12) (cid:12)s 0, a 0, a i ∼ π (·|s), s i ∼ p(·|s i−1, a i−1) . π,p i=0 (cid:12) We define the soft action-value function associated with policy π as the expected cumulative dis- counted return when choosing action a in state s and acting subsequently according to policy π factor γ , as (cid:34) ∞ (cid:12) (cid:35) (cid:213) (cid:12) Q πs (a, s) = J (π ) {s 0=s,a 0=a } = π(cid:69) ,p i=0 γ ir t (s i , a i ) − α D KL[π , π˜ |s i ](cid:12) (cid:12) (cid:12)s 0 = s, a 0 = a . Observing that the action at time t does not influence the KL at t , this action value function can be expressed recursively as Q πs (a t , s t ) = (cid:69) (cid:2) r (s t , a t ) + γV πs (s t +1)(cid:3) , st+1∼p(·|st,at ) where V s (s) = (cid:69) [Qs (s, a)] − α D [π , π˜ |s] is known as the soft value function of π . π π π KL 3. Quinoa We would like to find a soft-optimal policy π , which in every state maximises the soft Q-function Qs (a, s). π This in turn would locally maximise our objective J (π ) at each state under the assumption that Qs π is sufficiently accurate. Solving for π comes with one caveat: finding the multiplier α trading the regularisation with the reward is hard, as the magnitude of the reward can differ significantly over the course of the training process. We therefore optimise Qs subject to a hard constraint on the relative entropy between the policy π and the prior π˜, as the parameters through that approach are easier to set in practice [1, 12]: π = argmax (cid:69) (cid:2) Qs (a, s)π (a|s)(cid:3) subject to (cid:69) (cid:2) D [π , π˜ |s](cid:3) < ϵ and ∀s : (cid:69) (cid:2) π (a|s)(cid:3) = 1, π KL π s∼µπ s∼µπ a where the last constraint ensures that π is normalised. We solve this constrained optimisation problem using the method of Lagrange multipliers, automatically obtaining an optimal α for a given ϵ. The details of this procedure are given in the Appendix. 2
Quinoa: a Q-function You Infer Normalized Over Actions Figure 1 | The performance of Quinoa compared with SVG(0) [9]. Shown are the median performance across 5 seeds, together with the minimum and maximum performance. We show these performances for three domains from the DeepMind Control Suite as illustrated on the bottom right; from left to right: Cheetah, Walker and Hopper. Taking into account the constraint that π (a|s) is a distribution, we obtain that the optimal policy π (a|s) for a given Qs (a, s) is given by π (cid:16) (cid:17) π˜(a|s) exp Q πs (a,s) α π (a|s) = (1) ∫ π˜(a(cid:48)|s) exp (cid:16) Q πs (a(cid:48),s) (cid:17) da(cid:48) α this not only defines an improved policy, but also establishes a relation between the soft action-value Qs (a, s) and the policy π . To act according to π , we need a way to infer actions from Q-values. There π are three main viable approaches. Firstly, we could learn a parametric Q-function and then project the exponentiated Q-values onto a parametric π . This approach has been considered in Haarnoja et al. [8] and was extended to use rich parametric policies in Haarnoja et al. [7]. Secondly, we could aim to sample from π directly, for instance via importance sampling based on samples from π˜(a|s) reweighed with exp(Qs (a, s)/α ) [6]. This approach is known to have high-variance and is compute intensive. Finally, we π could parameterise the Q-function, restricting its expressiveness, such that we can obtain π in closed form. Using a Gaussian distribution for the policy would recover the NAF setting [5], but this restricts Qs (a, s) to be quadratic in action space. In this paper we follow the third approach yet make use of a rich π policy class of normalising flows, allowing the soft action-value function Qs to be a universal function π approximator. To achieve this we first solve Equation 1 for Qs (a, s) to find the following equation. π π (a|s) ∫ Qs (a, s) = V s (s) + α log where V s (s) = α log π˜(a|s) exp(Qs (a, s)/α ) da (2) π π π˜(a|s) π π 3
Quinoa: a Q-function You Infer Normalized Over Actions It makes sense to call the first term the soft value function, since taking the expectation of both sides of the equation gives (cid:69) (cid:2) Qs (a, s)(cid:3) = V s (s) + α D [π , π˜ |s], which corresponds to the definition of the soft π π π KL value function. Additionally, the second term in Equation 2 can be interpreted as a soft version of the advantage function A(a, s) = α log(π (a|s)/π˜(a|s)), where differences in log-likelihoods are interpreted as advantages. Given this sum, a natural way to parameterise Q becomes apparent. We can choose to parameterise V s (s) as a deep neural network, and π (a|s) as a density modelled by a normalising π flow [13]. These can be universal density estimators [10] and hence allow Qs to model arbitrary π functions. In the following, we chose to use a Real NVP architecture [4] for our policy, as we can both sample and infer the probability density function efficiently1. In order to condition the Real NVP on the state s, we concatenate s to the input of every neural network inside the Real NVP. Using this parametrisation, we can fit Qs (a, s) directly by minimising the squared temporal difference π error (cid:20) (cid:21) (cid:16) (cid:17)2 min (cid:69) r (s, a) + γV s (s (cid:48) ; ϕ (cid:48)) − Qs (a, s; θ , ϕ) | s (cid:48) ∼ p(s (cid:48)|s, a) , (3) θ,ϕ µπ (s),p π π where θ denote policy parameters, ϕ are value function parameters and ϕ (cid:48) are the parameters of a target value function, that are periodically copied from ϕ; and Qs , V s are given as in Equation 2. We π approximate the expectation over transition and state visitation distribution by samples from a replay buffer. A full algorithm listing of the procedure is given in Algorithms 1 and 2. Figure 2 | An illustration of the distribution of the policy π (a|s) in the state of the walker s illustrated on the left. In the three scatter plots on the right we plotted 1000 randomly drawn a ∼ π (·|s), where respectively action dimension a is scattered against a , a against a and a against a . To make the 1 2 3 4 5 6 density differences clearer, we added a kernel density estimation on these samples. Note the finite support of the policy. As can be seen, some properties of the richer policy class are utilised, such as having high-skew and non-linearly correlated exploration noise. In the first scatter plot the policy also displays multimodal behaviour, with a mode in at least three corners of the action domain of the marginalised distribution. It is apparent that the resulting policy is not normally distributed. 4. Results We ran experiments across three domains from the DeepMind control suite [17], the walker, the cheetah and the hopper, as depicted in Figure 1. Our neural networks were initialised such that Q(a, s) is identically zero in all states and actions, which means that our initial policy π (a|s) is exactly uniform. All neural networks have weight normalisation with an initialisation based on the statistics of the first batch [14]. In order to deal with the gradients of the squashing operations in the Real NVP, we clip the gradient norm to 1. We set the learning rate to 0.001 and update the target network every 1000 steps. 1We note that to the best of our knowledge there is no formal proof that Real NVP’s are universal density function approximators, nor any counterexamples of why they would not be. Other flows such as Neural Autoregressive Flows [10] could be used when a formal proof is required. 4
Quinoa: a Q-function You Infer Normalized Over Actions As we can see, the performance of Quinoa is similar to the one obtained by SVG(0) [9] for the cheetah and the walker tasks. On the hopper task, the performance is slightly lacking behind. When analysing the policies obtained, we find that using this richer class of distributions for a policy shows a distinct behaviour which is hard to obtain using a Gaussian policy. First of all, the actions samples from this policy have limited support. As shown in Figure 2, we can observe that in some states the policy has high-skew and non-linearly correlated exploration noise during the training process. Therefore, it is clearly not following a normal distribution. Moreover, the policy shows some multimodal behaviour during training. This can be explained by the fact that the converged policy for the walker domain has actions in the extremities for most states. The policy depicted has not converged yet, but it has learned that it prefers to take actions in the extremities. In this state however, it does not know which one yet, resulting in a multimodal distribution. The scatter plots also show how some dimensions of the action space have already collapsed, while others remain high in variance in order to keep the entropy large and keep exploring the action space. 5. Conclusion In this paper, we describe a new parametrisation of the soft Q-function, such that the optimal policy can be obtained in closed form. This approach removes the need for a policy optimisation step from the learning process, simplifying standard actor-critic algorithms. We show that our algorithm is able to work across a range of tasks. We have illustrated that the policy is able to have an arbitrary distribution for its exploration noise. Moreover, we have shown that given this additional degree of freedom, the resulting policy does not show a Gaussian behaviour, with long tails and non-linearly correlated noise. We find in some states the actions of the policy are distributed multimodally. In the future, we will work on expanding this approach to harder tasks. References [1] A. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess, and M. Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018. [2] L. C. Baird. supervised learning in continuous time: Advantage updating. In Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, volume 4, pages 2448–2453. IEEE, 1994. [3] J. Degrave, A. Abdolmaleki, J. Springenberg, N. Heess, and M. Riedmiller. Quinoa: a Q-function you infer normalized over actions. In Deep RL Workshop/NeurIPS, 2018. [4] L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016. [5] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine. Continuous deep q-learning with model-based acceleration. In International Conference on Machine Learning, pages 2829–2838, 2016. [6] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. supervised learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017. [7] T. Haarnoja, K. Hartikainen, P. Abbeel, and S. Levine. Latent space policies for hierarchical supervised learning. arXiv preprint arXiv:1804.02808, 2018. [8] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep supervised learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018. [9] N. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. Learning continuous control 5
Quinoa: a Q-function You Infer Normalized Over Actions policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pages 2944–2952, 2015. [10] C.-W. Huang, D. Krueger, A. Lacoste, and A. Courville. Neural autoregressive flows. arXiv preprint arXiv:1804.00779, 2018. [11] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180–1190, 2008. [12] J. Peters, K. Mülling, and Y. Altun. Relative entropy policy search. In AAAI, pages 1607–1612. Atlanta, 2010. [13] D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015. [14] T. Salimans and D. P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901–909, 2016. [15] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889–1897, 2015. [16] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for supervised learning with function approximation. In Advances in neural information processing systems, pages 1057–1063, 2000. [17] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. Citing this work This is a free, open access paper provided by DeepMind. The final version of this work was published on workshops at NeurIPS on 2018-12-08. Cite as: J. Degrave, A. Abdolmaleki, J. Springenberg, N. Heess, and M. Riedmiller. Quinoa: a Q-function you infer normalized over actions. In Deep RL Workshop/NeurIPS, 2018 Funding This research was funded by DeepMind. The authors declare no competing financial interests. 6
Quinoa: a Q-function You Infer Normalized Over Actions 6. Appendix 6.1. Solving the Q-function in closed form We want to find the optimal policy π (a|s) which optimises Qs (a, s) subject to a hard constraint on the π relative entropy between the policy π and the prior π˜. π = argmax (cid:69) (cid:2) Qs (a, s)π (a|s)(cid:3) subject to (cid:69) (cid:2) D [π , π˜ |s](cid:3) < ϵ and ∀s : (cid:69) (cid:2) π (a|s)(cid:3) = 1, π KL π s∼µπ s∼µπ a where the last constraint ensures that π is normalised. We solve this constrained optimisation problem using the method of Lagrange multipliers, obtaining an optimal α for a given ϵ automatically. We solve this constrained optimisation problem using the method of Lagrange multipliers. Here, the Lagrange function we construct is L = (cid:69) (cid:2) Qs (a, s)π (a|s)(cid:3) + α (cid:0) ϵ − (cid:69) (cid:2) D [π , π˜ |s](cid:3) (cid:1) + β(s)(cid:0) 1 − (cid:69) (cid:2) π (a|s)(cid:3) (cid:1) π KL s∼µπ s∼µπ a with Lagrange multipliers α ≥ 0 and β(s). Next we maximise the Lagrangian L w.r.t the primal variable π . The derivative w.r.t π (a|s) for an action a in a state s, making the approximation that µ (s) and π Qs (a, s) are independent of π is the following: π ∂ L π (a|s) = Qs (a, s) − α log − α − β(s). ∂π π π˜(a|s) Setting this derivative to zero, we find the policy which satisfies the Lagrangian in every state s. π (a|s) = π˜ exp(Qs (a, s)/α ) exp(−1 − β(s)/α ) π Taking into account the constraint that π (a|s) is a distribution, we obtain that the optimal policy π (a|s) for a given Qs (a, s) is given by softmax (log π˜(a|s) + Qs (a, s)/α )2, from which Quinoa derives its name. π a π Note that π (a|s) is always positive, so we have fulfilled the two conditions for it to be a probability density function. At this point we can derive the dual function, by substituting the parametrisation for Qs (a, s) in the π Lagrangian L. (cid:34) (cid:34) (cid:35) (cid:35) (cid:16)V s (s) π (a|s) (cid:17) L(α ) = αϵ + α (cid:69) log (cid:69) exp π + log s∼µπ a∼π α π˜(a|s) When we minimise this convex function in α , we find the optimal temperature for our distribution. Even though there is no analytic solution to this equation, the temperature can be computed efficiently using regula falsi to find the zero in the derivative under the constraint α ≥ 0: (cid:34) (cid:34) (cid:35) (cid:34) (cid:35) (cid:35) ∂ L(α ) (cid:16)Qs (a, s) (cid:17) Qs (a, s) (cid:16)Qs (a, s) (cid:17) = ϵ + (cid:69) log (cid:69) exp π − (cid:69) π softmax π ∂α s∼µπ a∼π α a∼π α a α Finally, we have all the elements to write the algorithm for both the actor and the learner, which run asynchronously in parallel. These are written out in Algorithm 1 and Algorithm 2. 2Here, we define a continuous softmax operation as softmaxx (y) = ∫ exp(y) exp(y) dx 7
Quinoa: a Q-function You Infer Normalized Over Actions Algorithm 1 Actor algorithm Input: policy π (a|s) with parameters θ shared with the learner Input: replay buffer ρ shared with the learner Input: environment e 1: loop while π (a|s) not converged 2: loop while e not terminated 3: get state s from environment e 4: sample a from policy π (·|s) 5: send a to environment e 6: end loop 7: send trajectory to replay buffer ρ 8: end loop Algorithm 2 Learner algorithm Input: policy π (a|s) with parameters θ shared with the actor Input: replay buffer ρ shared with the actor Input: prior policy π˜(a|s) with parameters θ˜ Input: soft value V s (s) with parameters ϕ π Input: KL-constraint ϵ Input: Discount γ 1: loop while π (a|s) not converged 2: sample (s, a, r , s (cid:48)) from ρ 3: D KL = log π (a|s) − log π˜(a|s) 4: find optimal α minimising α ϵ + α log (cid:69) a[exp (V πs (s)/α + D KL)] 5: q = α D KL +V πs (s) 6: q(cid:48) = r + γV s (s (cid:48)) and stop gradient π 7: optimise ϕ and θ to minimise (q − q(cid:48))2 with gradient descent 8: every 1000 iterations: π˜(a|s) ← π (a|s) 9: end loop 8
