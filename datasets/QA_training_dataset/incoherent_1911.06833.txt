Improved Exploration through Latent Trajectory Optimization in Deep Deterministic Policy Gradient Kevin Sebastian Luck1, Mel Vecerik2, Simon Stepputtis1, Heni Ben Amor1 and Jonathan Scholz2 Abstract— Model-free supervised learning algorithms such as Deep Deterministic Policy Gradient (DDPG) often require additional exploration strategies, especially if the actor is of deterministic nature. This work evaluates the use of model- based trajectory optimization methods used for exploration in Deep Deterministic Policy Gradient when trained on a latent image embedding. In addition, an extension of DDPG is derived using a value function as critic, making use of a learned deep dynamics model to compute the policy gradient. This approach leads to a symbiotic relationship between the deep supervised learning algorithm and the latent trajectory optimizer. The trajectory optimizer benefits from the critic learned by the RL algorithm and the latter from the enhanced exploration generated by the planner. The developed methods are evaluated on two continuous control tasks, one in simulation Fig. 1: A Baxter robot learning a visuo-motor policy for an and one in the real world. In particular, a Baxter robot is insertion task using efficient exploration in latent spaces. The trained to perform an insertion task, while only receiving sparse rewards and images as observations from the environment. peg is suspended from a string. I. INTRODUCTION supervised learning (RL) methods enabled the devel- Returns (PoWER) [8] into Policy Search with Probabilistic opment of autonomous systems that can autonomously learn Principal Component Exploration (PePPEr) [14]. Instead of and master a task when provided with an objective function. using a fixed Gaussian distribution for exploration, the noise RL has been successfully applied to a wide range of tasks generating process in PePPEr is based on Probabilistic Prin- including flying [24], [17], manipulation [26], [9], [12], [3], cipal Component Analysis (PPCA) and generates samples [1], locomotion [10], [13], and even autonomous driving [6], along the latent space of high-reward actions. Generating [7]. The vast majority of RL algorithms can be classified explorative noise from PPCA and sampling along the latent into the two categories of (a) inherently stochastic or (b) space was shown to outperform the previously fixed Gaussian deterministic methods. While inherently stochastic methods exploration. Alternatively, one can choose to optimize the have their exploration typically built-in [4], [18], their de- exploration strategy itself. Examples of this methodology are terministic counterparts require an, often independent, ex- count-based exploration strategies [22], novelty search [21] ploration strategy for the acquisition of new experiences or curiosity-driven approaches [16] which can be transferred within the task domain [11], [8]. In deep reinforcement with ease to other algorithms or frameworks. Typically, when learning, simple exploration strategies such as Gaussian noise incorporating these techniques into supervised learning, or Ornstein-Uhlenbeck (OU) processes [25], which model they are limited to local exploration cues based on the Brownian motion, are standard practice and have been found current state. This paper aims to combine the model-free to be effective [11]. However, research has shown that deep deterministic policy gradient method with a model- advanced exploration strategies can lead to a higher sample- based exploration technique for increased sample-efficiency efficiency and performance of the underlying RL algorithm in real world task domains. The proposed method generates [14]. In practice, there are two ways to incorporate advanced exploratory noise by optimizing a (latent) trajectory from the exploration strategies into deterministic policy search meth- current state to ideal future states, based on value functions ods. Where possible, one can reformulate the deterministic learned by an RL algorithm. This experience is, in turn, used approach within a stochastic framework, such as by modeling by the RL algorithm to optimize policy and value functions the actions to be sampled as a distribution. Parameters of the in an off-policy fashion, providing an improved objective distribution can then be trained and are tightly interconnected function for the trajectory optimizer. We investigate whether with the learning framework. One example for this method- this strategy of formulating exploration as a latent trajectory ology, is the transformation of Policy Search with Weighted optimization problem leads to an improved learning process both in simulation, as well as in a robotic insertion task 1Interactive Robotics Lab, Arizona State University, Tempe, AZ, USA executed solely in the real world. In particular, we apply our {ksluck, sstepput, hbenamor} ¨at asu.edu approach to a challenging, flexible insertion task as seen in 2Google DeepMind, London, UK. {vec, jscholz} ¨at google.com Fig. 1. 9102 voN 51 ]GL.sc[ 1v33860.1191:viXra
and mostly flat region in the state space with a reward of zero. In [2], Chua et al. introduce the model-based probabilistic ensembles with trajectory sampling method. This work builds upon [15], but also makes use of a reward function. It makes use of a probabilistic formulation of the deep dynamics func- tion by using an ensemble of bootstrapped models encoding distributions to improve the sample complexity and improves the properties of the trajectory planner. Both approaches do not explicitly train an actor or a critic network. Similarly to us, Universal planning networks [20] intro- duced by Srinivas et al. use a latent, gradient-based trajectory optimization method. However, the planner requires a goal (a) Rand. initial position (b) Insertion started state for the trajectory optimization. In certain tasks such as Fig. 2: The experimental setup in which a Baxter robot has walking or running, it might be hard to acquire such a goal to insert a blue cylinder into a white tube (b). The cylinder is state to use in place of a velocity-based reward function. It is with a string attached to the end-effector of the robot. Camera mentioned in [20] that to achieve walking, it was necessary images are recorded with the integrated end-effector camera. to re-render images or reformulate the objective function by The sensor detecting the state of insertion is integrated into including an accessible dense reward function. the white tube. Experiments on this platform were run fully In contrast to previous work, we focus explicitly on the autonomously without human intervention or simulations. impact of using trajectory optimization as an additional tech- nique for exploration and its impact on the learning process when used by a deep supervised learning algorithm such as Deep Deterministic Policy Gradient. Furthermore, using II. RELATED WORK an actor-critic architecture is a key element in our work to The advancement of deep supervised learning in recent allow off-policy updates in a fast manner during the training years has lead to the development of a number of methods process and to inform the trajectory optimization process combining model-free and model-based learning techniques, initially. in particular to improve the sample complexity of deep re- inforcement learning methods. Nagabandi et al. [15] present III. METHOD a model-based deep supervised learning approach which The following sections introduce the different compo- learns a deep dynamic function mapping a state and action nents used to generate explorative actions via trajectory pair (s , a ) to the next state s . The dynamics function is optimization. We first describe the image embedding used, t t t+1 used to unroll a trajectory and to create an objective function then the training process of the dynamics function and based on the cumulative reward along the trajectory. This Deep Deterministic Policy Gradient (DDPG) [11], as well objective function is, then, used to optimize the actions along as its extension for the use of a value function. The section the trajectory and thereafter the first action is executed. The ends with a description of our trajectory optimization based procedure is repeated whenever the next state is reached. exploration for DDPG. After a dataset of executed trajectories is collected by the A. Image Embedding planning process, the policy of a model-free supervised learning algorithm is initialized in a supervised fashion by All tasks used throughout this paper are setup such that training it to match the actions produced by the planner. they use only images as observations, which have to be This technique is different to our approach in that we do projected into a latent image embedding. This serves two not force the actor to match the executed action, but rather main purposes: First, the number of parameters is greatly see it as an exploration from which we generate off-policy reduced since the actor, critic, and the dynamics network updates. Furthermore, it is implicitly assumed in [15] that can be trained directly in the low dimensional latent space. a reward function is available for each state during the Second, it is desirable to enforce temporal constraints within planning process. This can be a rather strong assumption, the latent image embedding, namely that subsequent images especially when learning in the real world without access to are close to each other after being projected into the latent a simulation of the task and only providing minimal human space. Therefore, we make use of the recently introduced supervision. Using executions in the environment during the approach of time-contrastive networks [19]: the loss function planning process would be too costly since each change in enforces that the distance between latent representations of state would require a re-execution of the whole trajectory. two subsequent images are small but the distance between Since our insertion task provides only sparse rewards during two randomly chosen images is above a chosen threshold α. execution, the trajectory planning algorithm would fail when Enforcing a temporal constraint in the latent space improves relying only on rewards due to flat regions with zero reward the learning process of a consistent deep dynamics function and require additional reward engineering. This leaves a large in the latent space [19]. Time-contrastive networks make use
of two losses. The first is defined on the output of the decoder network and the input image as found in most autoencoder implementations. The second loss, the triplet loss, takes the latent representation z and z of two temporally close t t+1 images and the latent representation z of a randomly chosen r image. Thus, given two temporal images Im and Im and a t t+1 randomly chosen image Im , the loss functions for each r element in the batch is given by L(Im , Im , Im ) = L (Im ) + L (Im , Im , Im ). t t+1 r ae t contr t t+1 r (1) (a) Q-Value based actor update (b) Value based actor update The classical autoencoder loss L and the contrastive loss ae Fig. 3: The original DDPG algorithm (a) can be reformulated L are here defined as contr such that a value function (b) is used. In the case of a value L =(cid:107) Im − D(E(Im )) (cid:107), function the policy gradient (red arrow) is computed via a ae t t neural dynamics function. L (Im , Im , Im ) =(cid:107) E(Im ) − E(Im ) (cid:107) (2) contr t t+1 r t t+1 + max(α− (cid:107) E(Im ) − E(Im ) (cid:107), 0), t r with E being the encoder and D being the decoder network. between the predicted latent state and the actual latent state. The scalar value α defines the desired minimum distance Therefore, the loss is given as between two random images in the latent embedding. Thus L (Im , a , Im ) = (cid:107)Ψ(z , a ) − E(Im ) (cid:107), dyn t−2:t t t+1 t t t+1 the classic autoencoder loss L trains both the encoder and ae (5) decoder network to learn a reconstructable image embedding. The contrastive loss L contr, on the other hand, generates for each state-action-state triple (Im t−2:t, a t, Im t−1:t+1) ob- only a learning signal for the encoder network and places served during execution. The dynamics networks is con- a temporal constraint on the image embedding. The encoder structed out of 3 fully connected layers of size 400, 400 and decoder consist of three convolutional networks with a and 20 with ReLUs as nonlinear activation functions. kernel shape of (3, 3) and a stride of (2, 2), followed by C. Deep supervised learning a linear layer of size 20 and an l2-normalized embedding which projects the states on a unit sphere [19]. All activation We make use of the Deep Deterministic Policy Gradient functions are rectified linear units (ReLU). (DDPG) algorithm since action and state/latent space are continuous. DDPG is based on the actor-critic model which B. Latent Dynamics is characterized by the idea to generate a training signal for the actor (network) from the critic (network). In turn, the Using a trajectory optimization algorithm in latent space critic utilizes the actor to achieve an off-policy update and requires a dynamics function which maps a latent state z and t models usually a Q-value function. In DDPG, the actor is a an action a to a subsequent latent state z . This allows us t t+1 network mapping (latent) states to an action with the goal to unroll trajectories into the future. In the case of a single of choosing optimal actions under a reward function. Hence, image with z = E(Im ), we learn a dynamics mapping of t t the loss function for the actor is given by Ψ(z , a ) = z˜ . In the other case, when our latent state is t t t+1 derived from several stacked images, then we project each L (z ) = −Q(z , π(z )), (6) actor t t t image into the latent space, for example by  E(Im )  zt−2 where only the parameters of the actor π(z t) are optimized t−2 t (see Eq. 6 in [11]). In the case of classical DDPG, the critic E(Im t−1) = zt t−1  = z t. (3) is a Q-function network, which maps state and action pairs E(Im ) zt t t to a Q-value: Q(z , a ) = r(z , a ) + γQ(z , π(z )). t t t t t+1 t+1 To predict the next latent state, the dynamics function simply The scalar gamma is a discount factor and r(z t, a t) is the has to rotate the state and only predict the third latent sub- reward. The loss function of the critic network is based on state. This function can be described with the Bellman equation:  zt−2  zt−1  L (z , a , r , z ) = (cid:107) Q(z , a )− t t critic t t t+1 t+1 t t z t = zt t−1  (cid:55)→  zt t  = z˜ t+1, (4) (r t+1 + γQ(cid:48)(z t+1, π(cid:48)(z t+1))) (cid:107), zt t Ψ(z t, a t) (7) where Ψ is the output of the neural network while we will where Q(cid:48) and π(cid:48) are target networks. For more details on use the notation Ψ(z , a ) = z˜ for the whole operation, DDPG we refer the interested reader to [11]. It is worth t t t+1 and z˜ is the predicted next latent state. The loss function noting that DDPG can be reformulated such that the critic i+1 for the dynamics network is then simply the difference resembles a value function instead of a Q-value function (Fig.
usually chosen by intuition or have to be optimized as hyper- parameter, for example with grid-search. In preliminary experiments we found Ornstein-Uhlenbeck processes with σ = 0.5 and θ = 0.15 most effective on the chosen simulated task. In the presented approach we make use of the fact that we can access a dynamics function and therefore unroll trajectories throughout the latent space. The basic idea is to first unroll a trajectory using the actor network a number of steps into the future from the current point in time. We then optimize the actions a , · · · , a such t t+n Fig. 4: The proposed exploration strategy unrolls the tra- that we maximize the Q-values/rewards along the latent jectory in the latent space and uses the Value/Q-Value to trajectory. We characterize a latent trajectory, given a start optimize the actions of the trajectory. Dotted connections state z = E(Im ), as a sequence of state-action pairs t t might not be used when using a Value function as critic. (z , a , · · · , z , a , z ). We can then formulate a t t t+H t+H t+H+1 scalar function to be maximized by the trajectory optimizer based on the Q-value or reward-functions available. This pro- 3, see also [5]). A naive reformulation of the loss function cess is visualized in Fig. 4. The Q-function in the following given above is equations can be substituted with a learned value function. An intuitive objective function to optimize is to simply sum L (z , a , r , z ) = (cid:107) V(z ) − (r + γV(cid:48)(z )) (cid:107), critic t t t t+1 t t+1 t+1 up all Q-values for each state-action pair of the trajectory (8) H (cid:88) given an experience (z t, a t, r t+1, z t+1). But this reformula- f Q(a t:t+H , z t) = w 0Q(z t, a t) + w jQ(z t+j, a t+j), tion updates only on-policy and lacks the off-policy update j=1 ability of classical DDPG. Even worse, we would fail to use (11) such a critic to update the actor since no action gradient with z = Ψ(z , a ) and z being the current can be computed due to the sole dependency on the state. t+j t+j−1 t+j−1 t state from which we start unrolling the trajectory. The time- However, since we have access to a dynamics function we dependent weight w determines how much actions are going reformulate for our extension of DDPG the loss function and i to be impacted by future states and their values and can be incorporate off-policy updates with uniform, linearly increasing or exponential. We consider in L (z , a , r , z ) = (cid:107) V(z ) our experiments the special case of w = 1 . Alternatively, critic t t t t+1 t i H − (r + γV(cid:48)(Ψ(z , π(cid:48)(z ))) (cid:107) . if one has access to a rewards function or learns a state-to- t t t reward mapping simultaneously, then an objective function (9) can be used which accumulates all rewards along the latent This formulation allows for off-policy updates given the trajectory and adds only the final q-value: experience (z , a , r , z ), for which we assume that the t t t t+1 H−1 reward r(z) is only state-dependent. While this might appear (cid:88) f (a , z ) = w r(z ) + w Q(z , a ). to be a strong assumption at first, it holds true for most tasks r+Q t:t+H t j t+j H t+H t+H j=1 in robotics. The insertion task presented in the remainder (12) of this paper is such a case in which the reward is fully described by the current position of both end-effector and Clearly, this objective function is especially useful in the the object to be inserted. context of tasks with dense rewards. Both objective functions will be evaluated on the simulated cheetah task, which The loss function for the actor is then given with provides such dense rewards. While executing policies in the L (z ) = −V(Ψ(z , π(z ))), (10) real world, we unroll a planning trajectory from the current actor t t t state for n steps into the future. Then, the actions a are t:t+H which is fully differentiable and, again, only used to optimize optimized under one of the introduced objectives from above the parameters of the actor network. We use for both actor with a gradient-based optimization method such as L-BFGS and critic two fully connected hidden layers of size 400 and [27]. After a number of iterations of trajectory optimization, 300 with ReLUs as nonlinear activation functions. here 20, the first action of the trajectory, namely a , is t executed in the real world (Alg. 1). D. Optimized Exploration Due to the deterministic nature of the actor network in IV. EXPERIMENTS DDPG and similar algorithms, the standard approach for We compare in our experiments the classical approach of exploration is to add random noise to actions. Random noise exploration in DDPG with an optimized Ornstein-Uhlenbeck is usually generated from an Ornstein-Uhlenbeck process process against the introduced approach of exploration or a Gaussian distribution with fixed parameters. Such pa- through optimization. First, an experiment in simulation was rameters, like the variance for a Gaussian distribution, are conducted using the DeepMind Control Suite [23]. The
Algorithm 1 Exploration through trajectory optimization in TABLE I: The average success rate of insertion for poli- DDPG cies trained by DDPG with standard Ornstein-Uhlenbeck Require: Horizon H, Encoder network exploration or trajectory optimization with varying planning for number of episodes do horizons. The individual success rates for each experiment while end of episode not reached do were computed over a window of 50 subsequent episodes of Compute latent state z from images with encoder 500 executions total. The average success rates and standard t Initialize action with a = π(z ) deviations were then computed with the highest success rate t t if training then achieved in each experiment. A total of five experiments were for k = t + 1 : t + H do executed for each method. Initialize action with a k = π(z k) Method Avg. Success rate (±std) Predict latent state z k+1 = Ψ(z k, a k) Ornstein-Uhlenbeck Exploration 75.2% (±11.7%) end for 1 Step Planning Horizon 93.2% (±5.2%) 3 Steps Planning Horizon 91.6% (±1.5%) Optimize max f (a , z ) at:t+H t:t+H t 5 Steps Planning Horizon 84.0% (±14.1%) end if 15 Steps Planning Horizon 84.4% (±9%) Execute step in environment with action a t Store (z , a , r , z ) in replay buffer t t t t+1 end while rameter σ of DDPG and found that an Ornstein-Uhlenbeck Optimize dynamics network process with σ = 0.5 and θ = 0.15 achieve a better result for Optimize actor network DDPG on this task than the variance of σ = 0.2 proposed Optimize critic network in [11], especially in the early stages of the training process. Update target networks A planning horizon of ten steps was used to generate the end for optimized noise. We make comparisons between the training process, in which we use the exploration strategies, and the test case, in which we execute the deterministic actions cheetah task, in which a two-dimensional bipedal agent has produced by the actor without noise. Throughout the training to learn to walk, is especially interesting because it involves process we evaluate the current policy of the actor after each contacts with the environment that makes the dynamics episode. The results are presented in Fig. 5. hard to model. In the second experiment, we evaluate the 2) Comparison between different planning horizons: The algorithms directly on a robot and aim to solve an insertion main hyperparameter for optimized noise is the length of task in the real world. the planning horizon. If it is too short, actions are optimized greedily for immediate or apparent short-term success; if it A. Evaluation in Simulation on the Cheetah Task is too long, the planning error becomes too large. Figure 7 The cheetah environment of the DeepMind control suite shows the optimized exploration strategy with three different [23] has six degrees-of-freedom in its joints and we only use step-sizes: one step, ten steps and 20 steps into the future camera images as state information. The actions are limited from the current state. to the range of [−1, 1] and camera images are of the size 3) Comparison between different objectives: We intro- 320 × 240 px in RGB and were resized to 64 × 64 px. Each duced two potential objective functions, based on Q-values episode consists of 420 time steps and actions are repeated (Eq. 11) and a mix of reward- and Q-function (Eq. 12). We two times per time step. First, a dataset of 50 representative compare both of these against another objective where we episodes was collected through the use of DDPG on the only optimize for the q-value of the very last state-action original state space of joint positions, joint velocities, relative pair of the unrolled trajectory (Fig. 8). body pose and body velocity of cheetah. This dataset was B. Insertion in the real world used to train the time-contrastive autoencoder as described Fast exploration is especially important when tasks have above. The same parameters for the neural encoder were to be solved in a real world environment and training needs use for all exploration strategies. This was done to allow the to be executed on the real robot. An insertion task was set sole evaluation of the exploration strategies independently of up in which a Baxter robot had to insert a cylinder into the used embedding. Since cheetah is a quite dynamic task a tube where both training and testing were performed in and rewards depend on the forward velocity, this velocity the real world environment, without the use of simulation must be inferable from each state. Hence, we project three (Fig. 2) 1. Cylinder and tube were 3D-printed. The cylinder subsequent images (Im , Im , Im ) down by using the t−2 t−1 t was attached to the right end-effector of the robot with a encoder network and define the current state z as the three t string. The position control mode was used because there is stacked latent states z = [z , z , z ]T . For each of the t t−2 t−1 t a variable delay in the observations. Image observation were presented evaluations 25 experiments were executed and the acquired from the end-effector camera of the Baxter robot mean and standard deviations of the episodic cumulative via ethernet. The six dimensional actions are in the range rewards are shown in Figures 5-8. 1) Comparison between Ornstein-Uhlenbeck and opti- 1A video of the experiment can be found here: https://youtu.be/ mized exploration: As a first step we optimized the hyperpa- rfZcUWnut5I
(a) Deterministic Policy (Q-Value) (b) Exploration (Q-Value) (c) Deterministic Policy (Value) (d) Exploration (Value) Fig. 5: Comparison between DDPG using exploration with optimization (orange) and classical exploration using an Ornstein- Uhlenbeck process (blue) on the simulated cheetah task. The exploitation graph shows the evaluation of actions produced by the deterministic actor while exploration strategies are applied during training. Fig. 6: Comparison between DDPG using exploration with Fig. 7: Exploration through optimization evaluated with optimization (orange) and classical exploration using an different horizons for the planning trajectory on the simulated Ornstein-Uhlenbeck process (blue) on the simulated cheetah cheetah task. task while using a value function as critic. The number of training iterations per episode were raised from 1000 (Fig. 5-d) to 3000 for this evaluation. of [−0.05, 0.05] radians and represent the deviation for each joint of the arm at a point in time. This restriction ensures a strong correlation between subsequent camera images throughout the execution and allows the task to be solved in 20 steps. The initial position (radians) of the robot arm was randomized by sampling from a normal distribution with mean µ = (0.48, −1.23, −0.15, 1.42, 0.025, 1.35) and 1:6 variances σ = (0.05, 0.05, 0.05, 0.05, 0.05, 0.1), ensuring 1:6 that the tube is in the image. As a simplification of the Fig. 8: Comparison between three different objective func- task, we excluded the last rotational wrist joint of the robot tions for optimized exploration on the simulated cheetah task. arm. Because of the adynamic nature of this task and the
7 6 5 4 3 2 1 0 1 0 100 200 300 400 500 Episodes draweR Ornstein-Uhlenbeck Trajectory Optimization (a) Deterministic Policy 7 6 5 4 3 2 1 0 1 0 100 200 300 400 500 Episodes draweR terms of successful insertions. Each experiment was repeated five times and the cumulative reward for each episode is used to compute the mean shown in Figure 9. For better interpretability, the figures show, in bold lines, additionally a smoothed version of the mean where a Savitzky-Golay filter was applied with a window size of 21 and polynomials of order one. The autoencoder network as well as the dynamics network were trained with a demonstration dataset of 50 tra- jectories. Of these, 19 were positive demonstrations, in which the cylinder was successfully inserted. At the beginning of each training process, 5 of these 19 trajectories were added to the replay buffer to ensure convergence of the training Ornstein-Uhlenbeck Trajectory Optimization process due to the difficulty of the task caused by using sparse reward. V. DISCUSSION We start with a discussion of the results from the simulated bipedal cheetah task which uses a dense reward function: The first insight is that both actors seem to perform equally well after 20 episodes, with the actor trained with optimized (b) Exploration noise outperforming classical DDPG throughout the first 20 Fig. 9: Comparison between exploration with an Ornstein- episodes (Fig. 5 (a)). However, during training the optimized Uhlenbeck (blue) and exploration through optimization (red) exploration does not only perform better than exploration on the insertion task in the real world. The planning horizon with an Ornstein-Uhlenbeck process (Fig. 5 (b)) but also is three steps. The figures show the cumulative rewards performs better than the actions produced by both actors averaged over five experiments in light colours and in bold during test time (Fig. 5 (a)). colours, for better interpretability due to the sparse reward, We found that using a critic network modelling the Q- the mean smoothed with a Savitzky-Golay filter with window function (Fig. 5 (b)) outperformed the formulation of DDPG size 21 and 1st order polynomials. using a value network when using optimized exploration (Fig. 5 (d)), while DDPG with Ornstein-Uhlenbeck noise performs slightly better with a Value network (Fig. 5 (a,d)). One could argue, that the effects of using optimized noise necessity to use position control mode it is sufficient to use could vanish when increasing the number of trainings per the latent representation of the current image versus a stack episode, giving DDPG more time to find an optimal actor of images as in simulation. Larger movements of the cylinder given the current training set. Following this line of thought appear as blur in the images. Each episode consists of 20 we increased the number of training iterations per episode time steps and a sparse reward is used: For safety reasons, three times to 3000 (Fig. 6). The evaluation shows that if the end-effector left the designated workspace area, the while DDPG with OU noise improves in the later stages episode ended and a reward of −1 is assigned. When the of the learning process, the trajectory optimization uncovers cylinder is inserted into the tube, the extent of insertion valuable training experience now much faster early on. This is transformed into a reward from [0, 1.0] and an episode strongly indicates that the data distribution generated by stops if a reward of 0.9 or higher is assigned. The state of the exploration strategy has an impact on the performance insertion is measured with a laser-based time-of-flight sensor of DDPG. Evaluating the step-lengths we could find that (VL6180). The reward for all other possible states is zero. trajectory optimization improved up to a planning horizon Five experiments were conducted on the robot: DDPG with a of 20 steps, although we opted for our experiments with value function as critic and Ornstein-Uhlenbeck exploration, a conservative planning horizon of 10 steps to reduce the DDPG with exploration using trajectory optimization and a overall training time. The evaluation of the three introduced varying planning horizon (1, 3, 5 and 15 steps). We use objective functions show that the summation of Q-values a reduced planning horizon in this task due to the low along the planning trajectory yields better performance in number of time steps per episode. The comparison between the early training stages, up to episode 25, for the dense Ornstein-Uhlenbeck exploration and optimized exploration reward task (Fig. 8). This is an interesting result given with a horizon of three is shown in Fig. 9. Every episode that many other trajectory optimization approaches use a which ends with a negative cumulative reward violated the Bellman-inspired sum of weighted rewards [15], [2]. It is workspace boundaries and episodes reaching a reward of also worth to notice that the Q-Value is the more suitable 0.9 or more were successful insertions. Table I shows the objective function for optimizing actions in the presented comparison between exploration with Ornstein-Uhlenbeck real-world insertion task due to the reward function being noise and using planning horizons of different lengths in zero for the majority of time steps.
The results showing the learning progress on the insertion [7] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele task in the real world draw a clearer picture of the benefit of Reda, John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah. Learning to drive in a day. arXiv preprint arXiv:1807.00412, exploration through optimization (Fig. 9, Table I). Generally, 2018. after roughly 50 training episodes, the networks trained with [8] Jens Kober and Jan R Peters. Policy search for motor primitives in optimized exploration outperformed DDPG with OU and robotics. In Advances in neural information processing systems, pages 849–856, 2009. also achieved higher rewards in later stages of the learning [9] Fengming Li, Qi Jiang, Sisi Zhang, Meng Wei, and Rui Song. process (Fig. 9). An evaluation of the length of the planning Robot skill acquisition in assembly process using deep reinforcement horizon shows, as expected, that longer planning horizons learning. Neurocomputing, 2019. [10] Tianyu Li, Akshara Rai, Hartmut Geyer, and Christopher G Atkeson. lead to a decreases performance (Table I). This is very likely Using deep supervised learning to learn high-level policies on the due to the accumulating error of predicted future states from atrias biped. arXiv preprint arXiv:1809.10811, 2018. the dynamics network. However, even with longer planning [11] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. horizons the presented approach outperformed exploration Continuous control with deep supervised learning. arXiv preprint using OU noise. arXiv:1509.02971, 2015. [12] Kevin Sebastian Luck and Heni Ben Amor. Extracting bimanual syn- VI. CONCLUSION ergies with supervised learning. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4805– This work investigated the possibility of combining an 4812. IEEE, 2017. actor-critic supervised learning method with a model- [13] Kevin Sebastian Luck, Joseph Campbell, Michael Andrew Jansen, Daniel M. Aukes, and Heni Ben Amor. From the lab to the desert: Fast based trajectory optimization method for exploration. By prototyping and learning of robot locomotion. In Robotics: Science using trajectory optimization only to gain new experience, and Systems, 2017. the ability of DDPG to learn an optimal policy is not affected [14] Kevin Sebastian Luck, Gerhard Neumann, Erik Berger, Jan Peters, and Heni Ben Amor. Latent space policy search for robotics. In 2014 and we can furthermore make use of DDPG’s off-policy IEEE/RSJ International Conference on Intelligent Robots and Systems, training ability. We were able to show that by using this pages 1434–1440. IEEE, 2014. strategy, a performance gain can be achieved, especially in [15] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement the presented real world insertion task learned from images. learning with model-free fine-tuning. In 2018 IEEE International It is worth noting that this performance gain can be mainly Conference on Robotics and Automation (ICRA), pages 7559–7566. attributed to the change in exploration strategy since a IEEE, 2018. [16] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. fixed image embedding was used, reducing the possibility Curiosity-driven exploration by self-supervised prediction. In Pro- of performance differences caused by using different image ceedings of the IEEE Conference on Computer Vision and Pattern embeddings. This work only considered using reward, Q- Recognition Workshops, pages 16–17, 2017. [17] Gautam Reddy, Jerome Wong-Ng, Antonio Celani, Terrence J Se- Value or value functions as objective functions for optimizing jnowski, and Massimo Vergassola. Glider soaring via reinforcement the latent trajectory. In future work we plan to investigate the learning in the field. Nature, 562(7726):236, 2018. possibility of using additional cost terms, eg. safety and state- [18] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint novelty. Furthermore, another natural next step would be to arXiv:1707.06347, 2017. use probabilistic dynamics networks and advanced trajectory [19] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric optimization algorithms to evaluate their impact on deep Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time- contrastive networks: Self-supervised learning from video. In 2018 supervised learning algorithms when used for exploration IEEE International Conference on Robotics and Automation (ICRA), in this setup. pages 1134–1141. IEEE, 2018. [20] A Srinivas, A Jabri, P Abbeel, S Levine, and C Finn. Universal REFERENCES planning networks. In International Conference on Machine Learning (ICML), 2018. [1] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, [21] Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: exploration in supervised learning with deep predictive models. Adapting simulation randomization with real world experience. arXiv arXiv preprint arXiv:1507.00814, 2015. preprint arXiv:1810.05687, 2018. [22] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi [2] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # Levine. Deep supervised learning in a handful of trials using exploration: A study of count-based exploration for deep reinforcement probabilistic dynamics models. In Advances in Neural Information learning. In Advances in neural information processing systems, pages Processing Systems, pages 4759–4770, 2018. 2753–2762, 2017. [3] Adria Colome, Fabio Amadio, and Carme Torras. Exploiting sym- [23] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, metries in supervised learning of bimanual robotic tasks. IEEE Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Robotics and Automation Letters, 2019. Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint [4] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. arXiv:1801.00690, 2018. Soft actor-critic: Off-policy maximum entropy deep reinforcement [24] Russ Tedrake, Zack Jackowski, Rick Cory, John William Roberts, and learning with a stochastic actor. In International Conference on Warren Hoburg. Learning to fly like a bird. In 14th International Machine Learning, pages 1856–1865, 2018. Symposium on Robotics Research. Lucerne, Switzerland. Citeseer, [5] Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, 2009. Tom Erez, and Yuval Tassa. Learning continuous control policies [25] George E Uhlenbeck and Leonard S Ornstein. On the theory of the by stochastic value gradients. In Advances in Neural Information brownian motion. Physical review, 36(5):823, 1930. Processing Systems, pages 2944–2952, 2015. [26] Matej Vecˇer´ık, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier [6] Maximilian Jaritz, Raoul De Charette, Marin Toromanoff, Etienne Pietquin, Bilal Piot, Nicolas Heess, Thomas Rotho¨rl, Thomas Lampe, Perot, and Fawzi Nashashibi. End-to-end race driving with deep and Martin Riedmiller. Leveraging demonstrations for deep rein- supervised learning. In 2018 IEEE International Conference on forcement learning on robotics problems with sparse rewards. arXiv Robotics and Automation (ICRA), pages 2070–2075. IEEE, 2018. preprint arXiv:1707.08817, 2017.
[27] Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale bound- constrained optimization. ACM Transactions on Mathematical Soft- ware (TOMS), 23(4):550–560, 1997.
