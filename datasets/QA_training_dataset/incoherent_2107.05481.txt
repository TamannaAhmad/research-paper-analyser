Prequential MDL for Causal Structure Learning with Neural Networks Jörg Bornschein Silvia Chiappa Alan Malek Rosemary Nan Ke DeepMind, London {bornschein,csilvia,alanmalek,nke}@deepmind.com Abstract Learning the structure of Bayesian networks and causal relationships from obser- vations is a common goal in several areas of science and technology. We show that the prequential minimum description length principle (MDL) can be used to derive a practical scoring function for Bayesian networks when flexible and overparametrized neural networks are used to model the conditional probability dis- tributions between observed variables. MDL represents an embodiment of Occam’s Razor and we obtain plausible and parsimonious graph structures without relying on sparsity inducing priors or other regularizers which must be tuned. Empirically we demonstrate competitive results on synthetic and real-world data. The score often recovers the correct structure even in the presence of strongly nonlinear rela- tionships between variables; a scenario were prior approaches struggle and usually fail. Furthermore we discuss how the the prequential score relates to recent work that infers causal structure from the speed of adaptation when the observations come from a source undergoing distributional shift. 1 Introduction Bayesian networks are a powerful probabilistic framework based on a graphical representation of statistical relationships between random variables. Inferring the Bayesian network structure that best represents a dataset not only allows to use the network to perform probabilistic, and possibly causal, reasoning but can also provide substantial illumination about the domain under consideration. This paper considers the problem of structure learning in settings in which modern, possibly overparametrized, neural networks are used to model the Bayesian network conditional distributions. Recent effort on structure learning with modern neural networks has focused on improving scalability w.r.t. the number of variables by relaxing the discrete search problem over structures to a continuous optimization problem [Zheng et al., 2018, Yu et al., 2019, Zheng et al., 2020]. Whilst enabling the use of large structures, the regularized maximum-likelihood score used to rank structures makes these methods prone to overfitting random fluctuations and sensitive to the regularizer. We propose an approach to ranking structures based on the minimum description length (MDL) principle. Motivated by fundamental ideas in data-compression, information theory, as well as philosophical notions like Occam’s razor, the MDL principle posits that models which lead to compact and parsimonious descriptions of the data are more plausible. In the context of structure learning, this criterion induces a preference for more compact and simpler structures as more plausible explanations of the data generation mechanism. Many traditional scores, such as AIC [Akaike, 1973], BIC [Schwarz, 1978], marginal likelihood [Heckerman et al., 1995], and more recent scores [Silander et al., 2018], can be seen as implementations of the MDL principle. However, some of these scores are approximations that, especially when applied to overparametrized neural networks, can lead to poor empirical performance [Silander et al., 2008]. In addition, many such scores can only be 1202 luJ 2 ]GL.sc[ 1v18450.7012:viXra
applied to simple model families, and therefore might not be suitable to modelling complex nonlinear relationships in the data. We propose using the prequential plug-in score, which evaluates conditional distributions by their sequential predictive performance, and gives an approach to ranking structures that balances fit to the data with overfitting without the need for an explicit sparsity inducing priors or regularizer. We provide a specific method for implementing the score with modern neural networks. We demonstrate on both artificial and real-world data that our method often retrieves the data-generating structure and is robust to neural network hyperparameter selection. 2 Structure Learning with Prequential MDL Our approach to learning the structure of a Bayesian network is to rank structures by measuring the complexity of the associated conditional probability distributions (CPDs) through the minimum description length (MDL) principle [Grünwald, 2004, Grünwald, 2007]. In particular, we propose the use of the prequential plug-in score and an implementation of this score for the case in which modern neural networks are used to model the CPDs. Before describing our method in detail, we give an introduction into Bayesian networks and structure learning with MDL. Bayesian Networks (BNs). A Bayesian network [Pearl, 1988, X1 (G, p) X2 X3 2000, Cowell et al., 2007, Koller and Friedman, 2009] is a directed acyclic graph (DAG) G whose nodes X1, . . . , XD represent ran- p(X1) p(X2|X1, X3) p(X3) dom variables and links express statistical dependencies among them. Node Xd is associated with CPD p(Xd | pa(Xd)), where pa(Xd) denote the parents of Xd, namely the nodes with a link into Xd. The joint distribution of all nodes is given by the product of all CPDs, i.e. p(X1, . . . , XD | G) = (cid:81)D p(Xd | pa(Xd)). We make the common assumption that d=1 each CPD is parametrized by separate parameters. The set of BNs that encode the same set of conditional independence assumptions forms a Markov equivalence class. A BN can be given a causal semantic by interpreting a link between two nodes as expressing causal rather than statistical dependence. Score-based Structure Learning with MDL. Let p∗ be a joint distribution over D random vari- ables with joint domain X , and let D = {x := (x1, . . . xD)}n be a dataset of n i.i.d samples from i i i i=1 p∗. The goal of structure learning is to infer the DAG G, referred to as structure, or the Markov equivalence class that best represents D. We focus on score-based approaches that rank structures w.r.t. some scoring metric [Heckerman, 1999, Drton and Maathuis, 2017]. A naïve score is the maximum log-likelihood max log p(D | θ, G), θ∈ΘG which ignores model complexity and results in a preference for dense and complex structures that do not generalize well. A simple approach to account for model complexity is to add to the log-likelihood a regularization term that can depend on the dimension of the parameters dim(θ) and on the size of the dataset n—the two most common penalty terms are − dim(θ) (AIC) and −0.5 log(n) dim(θ) (BIC). A more sophisticated approach is to instead integrate out θ, which gives the log-marginal (cid:82) likelihood log p(D | θ, G)p(θ | G)dθ. Both these approaches can be described within the unifying ΘG framework of MDL. The MDL framework is based on the principle that the model that yields the shortest description of the data is also the most plausible. In the context of structure learning, the MDL principle prescribes that we pick the model class M = {p(·|θ, G) : θ ∈ Θ } from the set {M : G ∈ G} which leads G G G to the most compact representation of the dataset D with a code derived from M . Considering the G close relationship between code-lengths and probability distributions this means selecting the model class under which the data has the highest likelihood. From this perspective maximum-likelihood log p(D|θˆMLE(D), G), where θˆMLE(D) := arg max log p(D|θ, G) alone cannot be the basis for a θ code because it does not normalize to one, it is thus not a distribution over D which precludes the existence of a code with the corresponding code-length. Maximum likelihood can only be used as a basis for a code if θMLE is known a-priori. 2
Instead, the MDL literature suggests to use the log-likelihood of the universal distribution p(·|G) of M on X n, defined as G (cid:16) (cid:17) p(·|G) := arg max min log q(Z) − log p(Z|θˆMLE(Z), G) (1) q Z∈X n to quantify the minimal code-length under a model class M . The MDL structure selection rule is G G (D) := arg max log p(D | G) (2) MDL G∈G because the model class with the most compact representation corresponds to the model class with highest p(D|G) (see Grünwald and Roos [2019]). The universal distribution is a probability distribution that, in some sense, summarizes how well M fits data: it places large probability on D G only if there is a distribution p(·|θ, G) ∈ M that places large probability on D. The requirement G that p(·|G) must normalize to one naturally induces complexity regularization. For a model class that is very expressive (e.g. G is fully connected) log p(Z|θˆMLE(Z), G) is large for many values of Z, and therefore the universal distribution must spread its mass across much of X n. This implies that log p(D|G) for the observed dataset D cannot be high. On the other hand, for a model class that is not as expressive (e.g, G includes only few links), log p(Z|θˆMLE(Z), G) is large only for data that are compatible with its graph structure and the universal distribution can have much higher log-likelihood on such data. Using the universal distribution for structure selection leads to favoring structures that have expressiveness for data similar to D but do not waste expressiveness on dissimilar data. 2.1 Prequential Plug-in Score Equation 1 provides a prescriptive definition of the universal distribution required by Eq. (2) to compute the score log p(D | G). Several constructive definitions have been proposed to closely ap- proximate Eq. (1). These are, following the MDL literature, also referred to as universal distributions. We propose approximating log p(D | G) with the prequential plug-in score from the prequential plug-in universal distribution, defined as n log p (D | G) := log (cid:89) p(x | θˆ(x ), G), preq i <i i=1 where θˆ(x ) ∈ Θ indicates a consistent parameters estimate given x := (x , . . . , x ). <i G <i 1 i−1 The prequential plug-in score is based on the idea of evaluating a model by its sequential predictive performance and therefore by its generalization capabilities [Dawid and Vovk, 1999]. The prequential approach in the context of MDL has been proposed by Grünwald [2004], Poland and Hutter [2005], Grünwald [2007]. There are advantages in using the prequential plug-in score w.r.t. other scores derived from popular and well-studied universal distributions, such as the log-marginal likelihood (cid:82) (also called Bayesian score) log p (D | G) := log p(D | θ, G)p(θ | G)dθ, the log-normalized Bayes ΘG maximum likelihood log p (D | G) := log p(D | θˆMLE(D), G) − log (cid:82) p(Z | θˆMLE(Z), G)dZ NML Z∈X n [Rissanen, 1996], or other approximations. The prequential plug-in score is better suited to neural networks than the Bayesian score, as it does not require integration over the parameters. Whilst it might appear that these two scores imply different preferences for model selection, they are equivalent for several, and often natural, choices of p(θ) and θˆ(x ) (see Sect. 3.1). The log- <i normalized maximum likelihood is widely used in theoretical treatments of MDL. However, the normalization term over all possible observed data makes this score often intractable or not defined. The well-known AIC/BIC scores can also be cast as approximations to log p(D|G) [Lam and Bacchus, 1994], but both are known to have poor empirical performance [Silander et al., 2008] as they can be quite loose. Decomposability over CPDs. The assumption that each CPD is modelled by a separate set of parameters enables us to write the prequential plug-in score as D n log p (D | G) = (cid:88) (cid:88) log p(xd | pa(xd), θˆd(x )), (3) preq i i <i d=1 i=1 where pa(xd) indicates the observed values of pa(Xd) for observation x and θˆd(x ) the parameters i i <i learned using {(xd, pa(xd))}i−1 . This decomposition allows a computationally more efficient j j j=1 ranking of structures—for example there are 29,280 DAGs with 5 nodes but only 80 underlying CPDs. 3
1.6 1.4 1.2 1.0 0 200 400 600 800 1000 i stan / ssol-gol p(A | ...) next step log-loss 30 model p(A) 25 p(A|B) 20 p(A|B,C) p(A|C) 15 10 5 0 0 200 400 600 800 1000 i stan / ssol-gol .qerp Excess preq. plugin log-loss relative to best model p(A) p(B|A) p(C|B) p(A|B) p(B) p(C|B) p(A|B) p(B|C) p(C) p(A) p(B|A) p(C|A,B) p(A|B) p(B) p(C|A,B) p(A) p(B|A,C) p(C) p(A) p(B|A,C) p(C|A) Figure 1: Prequential scoring with tabular CPDs on synthetic data generated using G∗ = A → B → C. (a): Next-step log-loss for variable A given all possible combinations of the other variables as parents. (b): Excess prequential log-loss for all possible DAGs relative to G∗. Uncertainty bands show standard deviation over 1,000 permutations of the data. 2.2 Implementation of the Prequential Plug-In Score with Neural Networks The computation of the prequential plug-in score (3) requires evaluating log p(xd|pa(xd), θˆd(x )) i i <i ∀i = 1, . . . , n. When the CPDs are modelled by neural networks, we must train the networks to convergence on many subsets of x using a stochastic gradient-based optimizer. Modern, usually <i overparametrized, neural networks 1) may overfit severely for small i, and 2) training them from scratch for each i can be computationally infeasible, while at the same time it is difficult to use them in online settings where the training set constantly grows (a topic of active research). For example, using the model parameters from training on x as a starting point for learning model parameters <i from x often leads to significantly reduced generalization [Ash and Adams, 2020]. <i+j To overcome the second obstacle, we propose to use the approach described by Blier and Ol- livier [2018], Bornschein et al. [2020], specifically to choose a set of increasing split points {s }K , with s ∈ [2, . . . , n], s =n+1 and compute the score of the data between two split k k=1 k K points xd , . . . , xd with a neural network trained from scratch on x to convergence, which sk sk+1−1 <sk corresponds to the approximation (cid:88)n log p(xd|pa(xd), θˆ(x )) ≈ K (cid:88)−1 sk(cid:88)+1−1 log p(xd|pa(xd), θˆd(x )). i i <i j j <sk i=1 k=1 j=sk In the experiments, we chose the split points to be exponentially spaced and performed K−1 independent training and evaluation runs, usually in parallel. To overcome the first obstacle, we propose to use a simple confidence calibration approach introduced by Guo et al. [2017] to independently calibrate every CPD on every training run. First, consider a network with a softmax output layer for categorical prediction. Conceptually, we could perform post- calibration by first training the network to convergence and then, with all parameters frozen, replacing the output layer softmax(h) with the calibrated output layer softmax(β · h), where β is a scalar parameter chosen to minimize the loss on validation data. In practice, we optimize β by gradient descent in parallel with the other model parameters. We alternate computing ten gradient steps for θ, calculated from the training set and using the uncalibrated network (with final layer softmax(h)), with a single gradient step on β, calculated from the validation set using the calibrated network (with final layer softmax(β · h)). This simple calibration procedure has proven to be surprisingly effective at avoiding overfitting when training large neural networks on small datasets [Bornschein et al., 2020]. To the best of our knowledge, an analogous method to calibrate continuous-valued neural network outputs does not exist. Thus, we approximate networks for a continuous random variable by networks for a categorical random variable on the quantized values. 3 Experiments We demonstrate the effectiveness of our approach, which we refer to as prequential scoring, on a variety of synthetic datasets and on a real-world dataset. 4
3.1 Prequential Scoring with Tabular CPDs Whilst our motivation for introducing the prequential plug-in score is its suitability when neural networks are used to model the CPDs, we first build intuition by considering the case of categorical data with conditional probability tables, i.e. with p(Xd = k | pa(Xd) = l, θd) = θd , which does k,l not require approximating the score. We generated synthetic data D = {x }n by using the DAG G∗ = A → B → C with each i i=1 variable taking five possible values, and by drawing the parameters for each CPD and value of parents from a Dirichlet distribution with α∗ = 1. We then computed the next-step log-loss − log p(xd | pa(xd), θˆd(x )), ∀i = 1, . . . , n and ∀d = 1, . . . , D for all possible parents sets pa(xd), i i <i i using the α = 0.5-regularized MLE estimator θˆ kd ,l(x <i) = N kd ,l + α/(cid:80) m(N md ,l + α), where N kd ,l denotes the number of times that, in x , Xd and pa(Xd) take values k and l respectively. <i Fig. 1(a) displays the next-step log-loss ∀i = 1, . . . , n for variable A given all possible parents sets, averaged over 1, 000 different permutations of the datapoints to make the plot less noisy. This average can be seen as an approximation to the generalization log-loss − E log p(x˜d | pa(x˜d), θˆd(x )) x˜∼p∗ <i (i.e. the negative log-likelihood on held-out data). The plot shows that conditioning A on B generally gives the best result. Additionally conditioning on C reaches the same performance when sufficient training data is available, but results in worse performance in the small-data regime. In other words, if we were to train on e.g. 1, 000 datapoints and use the generalization log-loss to select a model, we would not be able to reliably select p(A | B) over p(A | B, C). The generalization log-loss does not account for model complexity and might lead us to select models that are more complex than necessary. However, with only 100 training examples this loss does give a clear signal that we should prefer p(A | B), because the over conditioned p(A | B, C) has significantly worse performance in the small-data regime. These observations suggest that we should select models in the small-data regime, but finding the right regime could be difficult. Fig. 1(a) indicates that the regime is between ≈ 50 and ≈ 200 for p(A|·), but that is not known a-priori. Additionally, the optimal regime to perform model selection for e.g. p(B|·) might be different. By summing the next-step log-losses up to i, the prequential log-loss − log p (x | G) accumulates and persists the differences from the small-data preq ≤i regimes. Fig. 1(b) shows the prequential log-loss ∀i = 1, . . . , n for all DAGs relative to the best one. The ground-truth DAG A → B → C is identified as most plausible, followed by A ← B → C and A ←B ← C; all three are in the same Markov equivalence class. The fully connected DAGs, which reach about same next-step log-loss after being trained on ≈1, 000 datapoints, accumulate more than 50 nats additional loss compared to A → B → C. Notice that with our choice for the parameters estimator the prequential log-loss becomes equivalent to the log-marginal likelihood with θ1, . . . , θD independent random variables with Dirichlet distributions (see Appendix F). 3.2 Prequential Scoring with Neural Networks We evaluate prequential scoring with neural networks on several synthetic datasets and on the Sachs real-world dataset [Sachs et al., 2005]. We primarily compare with the DAG-GNN method introduced by Yu et al. [2019], which represents one of the more competitive modern methods to structure learning with neural networks. Additionally we compare with the PC algorithm, a constraint-based method using linear-regression based Pearson correlation as independence test [Spirtes et al., 2000]1. Architecture and Hyperparameters. In all experiments, we modelled the CPDs with neural networks consisting of 3 fully connected layers of 512 hidden units, ReLU activation functions, and dropout with probability of 0.5 on all the hidden units. We applied a random Fourier transformation to the data obtained by sampling 512 random frequencies from a Gaussian distribution N (0, 102), as this has been shown to improve the performance in neural networks with low-dimensional inputs [Tancik et al., 2020]. To use the softmax confidence calibration described in Sect. 2.2, we mapped the predicted values into the interval [−1, 1] with tanh and then discretized them according to a uniform 128-values grid. For optimization, we used Adam [Kingma and Ba, 2015] with a batch size of 128; for each point s , we independently choose the learning rate to be either 1·10−4 or 3·10−4 depending k on which one resulted in a lower calibrated log-loss. We performed 25,000 gradient steps but used early-stopping if the calibrated log-loss increased, which led to considerable compute savings as many 1Constraint-based approaches use independence tests to infer the existence of links between pairs of variables and require faithfulness—see Appendix B for a discussion on this assumption. 5
4.5 4.0 3.5 3.0 0 10000 20000 30000 40000 50000 i stan / ssol-gol p(C | ...) next step log-loss p(C) 3000 p(C|A) 2500 p(C|A,B) p(C|B) 2000 1500 1000 500 0 0 10000 20000 30000 40000 50000 i stan / ssol-gol .qerp Excess preq. plugin log-loss relative to best model p(A) p(B|A) p(C|B) p(A|B) p(B|C) p(C) p(A|B) p(B) p(C|B) p(A|B,C) p(B|C) p(C) p(A|B,C) p(B) p(C|B) p(A) p(B|A) p(C|A,B) p(A|B) p(B) p(C|A,B) Figure 2: Prequential scoring with neural networks on synthetic data generated from G∗ = A → B → C. (a): Next-step log-loss for variable C given all possible combinations of the other variables as parents. (b): Excess prequential log-loss for all possible DAGs relative to G∗. Uncertainty bands show standard deviation over 5 random seeds. 10000 8000 6000 4000 2000 0 20000 40000 60000 80000 100000 i stan / ssol-gol .qerp Excess preq. plugin log-loss relative to best model p(A) p(B) p(C|A,B) p(D|C) p(E|C) p(A) p(B) p(C|A,B) p(D|C,E) p(E|C) p(A) p(B) p(C|A,B) p(D|C) p(E|C,D) p(A) p(B|A) p(C|A,B) p(D|C) p(E|C) p(A|B) p(B) p(C|A,B) p(D|C) p(E|C) p(A) p(B|A) p(C|A,B) p(D|C,E) p(E|C) p(A) p(B|A) p(C|A,B) p(D|C) p(E|C,D) p(A|B) p(B) p(C|A,B) p(D|C,E) p(E|C) p(A|B) p(B) p(C|A,B) p(D|C) p(E|C,D) p(A) p(B) p(C|A,B) p(D|A,C) p(E|C) p(A) p(B) p(C|A,B) p(D|C) p(E|B,C) p(A) p(B) p(C|A,B) p(D|B,C) p(E|C) p(A) p(B) p(C|A,B) p(D|C) p(E|A,C) p(A) p(B) p(C|A,B) p(D|A,C) p(E|C,D) Figure 3: Prequential scoring with neural networks on synthetic data generated from G∗ = A → C ← B, C → D, C → E. Excess prequential log-loss for the 14 most likely DAGs relative to the the most likely one. Uncertainty bands show standard deviation over 3 random seeds for neural network initialization and mini-batching. training runs on small subsets of the data converge after only a few hundred or thousand gradient steps. All experiments were carried out on CPUs without accelerators as the networks were relatively small, and a typical run to convergence took between 5 and 15 minutes. We performed such training runs for each potential CPD and for K ≈ 6 log-equidistantly spaced split-points s . For example, k with 5 observed variables we ran 960 independent training runs corresponding to all combinations of the 80 CPDs, 6 split-points, and 2 learning-rates. We collected and accumulated the results and performed an exhaustive search over all DAGs to find the most likely one given the data. Changing the depth and width of the networks did not impact the rankings of the structures, provided that the models had sufficient capacity. Similarly, changing the optimizer to RMSprop [Graves, 2014] or Momentum SGD [Qian, 1999] had minimal effect. This robustness, together with the fact that prequential scoring has no hyperparameter for regulating the sparsity of the inferred graphs, allowed us to use the same hyperparameter settings throughout all the experiments. Case Studies for 3 and 5-Node DAGs. We first evaluated prequential scoring on data generated from hand-crafted generation mechanisms. Below, we describe the results from two mechanisms (a third mechanism is reported in Appendix C). Fig. 2 show the results obtained on data generated from the DAG G∗ = A → B → C with A ∼ N (0, 1), B = sin(A + (cid:15) ), C = sin(B + (cid:15) ), and B C (cid:15) , (cid:15) ∼ N (0, 0.12). We observe the same scaling behaviour of Sect. 3.1 for both the next-step B C log-loss and the prequential log-loss, and the retrieval of G∗ with an almost 500 nat margin. Fig. 3 shows the results obtained on data generated from G∗ = A → C ← B, C → D, C → E with A ∼ N (0, 1), B ∼ N (0, 1), C = sin(2AB + (cid:15) ), D = sin(C) + (cid:15) , E = sin(3C + (cid:15) ), and C D E (cid:15) , (cid:15) , (cid:15) ∼ N (0, 0.12). As above, we observe that the next-step log-loss accumulated on small and C D E medium-sized subsets of the dataset (smaller than roughly 20,000) is crucial to getting a discerning signal for DAG selection. Once again, prequential scoring identifies G∗ by a significant margin of 2,000 nats. Data from Yu et al. [2019]. To test prequential scoring on a larger gamut of distributions, we turned to the data generating mechanism introduced by Yu et al. [2019] for validating DAG-GNN. Specifically, we generated datasets by the fixed points X of the equations a) X = A(cid:62) cos(X +1)+Z 6
E H Inferred DAG (DAG-GNN) Ground-Truth DAG Inferred DAG (ours) H E (cid:55) 2 0 (cid:51) (cid:55) 4 0 (cid:51) (cid:51) 0 0 (cid:51) (cid:55) 5 1 (cid:55) (cid:51) 0 0 (cid:51) (cid:51) 0 1 (cid:51) (cid:51) 0 1 (cid:51) (cid:55) 2 1 (cid:51) (cid:51) 0 3 (cid:55) ; (cid:51) 0 0 (cid:51) Table 1: Results on nonlinearities from Yu et al. [2019]. We list the ground-truth DAGs and the DAGs inferred by DAG-GNN and by prequential scoring. We also report whether the inferred DAGs are in the Markov equivalence class E of the ground-truth DAGs and the structural Hamming distance H. and b) X = 2 sin(A(cid:62)(X + 0.5 · 1)) + A(cid:62)(X + 0.5 · 1) + Z, where 1 denotes the all-ones vector and Z a standard normal variable. Due to the limited scalability of the exhaustive DAG search step required in prequential scoring, we restricted ourselves to 5×5 adjacency matrices A. We generated 5 datasets using a) and 5 datasets using b). The structures inferred by prequential scoring and DAG-GNN are given in Table 1. Prequential scoring tends to recover DAGs with lower structural Hamming distance H to the ground-truth DAG (0.7 vs 1.3 on average) and that are more frequently in the same Markov equivalence class E (8 of 10 vs 6 of 10). The PC algorithm also performs reasonably well by inferring DAGs within the correct Markov equivalence class in 5 cases (details are given in Appendix D). Inspection of the data revealed that the vast majority of relationships between the observed variables can be well approximated with simple linear functions; as such, it is not surprising that the PC algorithm performs well even though it uses linear-regression based Pearson correlation as independence test. Ground-Truth DAG Inferred DAG H E Ground-Truth DAG Inferred DAG H E 0 (cid:51) 1 (cid:51) 1 (cid:51) 5 (cid:55) 1 (cid:51) 0 (cid:51) 1 (cid:55) 1 (cid:51) 2 (cid:51) 4 (cid:55) 2 (cid:55) 2 (cid:51) 1 (cid:51) 3 (cid:55) 3 (cid:55) 0 (cid:51) 2 (cid:51) 2 (cid:51) 6 (cid:55) 2 (cid:55) Table 2: Results obtained with prequential scoring on 20 randomly generated five-node graphs, with CPDs corresponding to compositions of polynomials and trigonometric functions (see Table 3 in Appendix E). We list the ground-truth DAGs, the inferred DAGs, the structural Hamming distance H, and whether the inferred DAGs are in the Markov equivalence class E of the ground-truth DAGs. Complex Nonlinearities from Polynomials and Trigonometric Functions. To investigate the performance of prequential scoring for setting in which the CPDs need to model highly nonlinear relationships, we created a potpourri of synthetic data by first sampling random 5-node DAGs using 7
the GNP algorithm [Batagelj and Brandes, 2005] with link probability 0.25 and then annotating the links with random compound functions of polynomials, trigonometric functions, reciprocals and random noise. We created a set of 20 such data generation mechanisms which are listed in Table 3 of Appendix E. For example, a typical generation mechanism is A = sin(30 (cid:15) ), B = sin(2A) + (cid:15) , A B C = sin(B3 −A+(cid:15) ), D = (C +(cid:15) )3, E = sgn(A)(|A|+0.1)−1 +(cid:15) , with (cid:15) , (cid:15) , (cid:15) , (cid:15) , (cid:15) ∼ C D E A B C D E N (0, 0.12). In Table 2 we list the ground-truth DAGs, the DAGs inferred by prequential scoring, the structural Hamming distance H, and whether the inferred DAGs are in the Markov equivalence class E of the ground-truth DAGs. We observe that prequential scoring infers DAGs with an average H of 1.9 and recovers a member of E in 12 of the 20 cases. We applied DAG-GNN and PC to the same 20 data datasets and were not able to obtain reliable and reproducible results. For DAG-GNN with default hyperparameters, the inferred structure varied significantly for different initial seeds and different sizes of the dataset. We also noticed a high sensitivity to the sparsity regularization hyperparameter. For the constraint-based method the results appear random. We did however not expect reliable results because the independence test is not designed to work on strongly nonlinear data. pkc Protein Signaling Network. As real-world dataset we pip2 considered the Sachs dataset for the discovery of a pro- pka tein signaling network [Sachs et al., 2005], a benchmark plcg dataset for structure learning with experimental annota- tions accepted by the biology research community. The raf data contains continuous measurements of expression lev- els of multiple phosphorylated proteins and phospholipid components in human immune system cells, and the net- work provides the ordering of the connections between pip3 pathway components. Based on n = 7, 466 samples of mek D = 11 cell types, Sachs et al. [2005] estimated 20 links in the graph. In addition to observational samples, the jnk dataset contains interventional samples obtained by acti- erk vating or inhibiting expression levels at particular nodes. p38 We handled interventional samples following the princi- akt ples behind intervention in causal Bayesian networks [Pearl, 2000, Pearl et al., 2016]: if sample xd j was marked as the result of an intervention, we did not consider it for the learning and evaluation of the d-th CPD (see Appendix G for a description and a visualization of the effect that interventional data can have on prequential scoring). We computed the prequential plug-in score from three runs with different random seeds for neural network initialization and mini-batching. For scalability reasons, we were only able to consider CPDs with at most 4 parents, and we used a heuristic hill-climbing method [Heckerman et al., 1995] to search the space of DAGs instead of an exhaustive search. Above, we show the DAG G∗ that has been ac- H # links cepted by the biology research community as the NOTEARS 22 16 best known solution overlayed with the DAG in- DAG-GNN 19 18 ferred by prequential scoring. Shared links are solid prequential scoring 16 15 in black; otherwise, links discovered by prequential prequential scoring (PWA) 18.4 16.8 scoring are dotted blue and the DAG links only in G∗ are dashed in red. The table on the left reports the structural Hamming distance H and the number of links found by prequential scoring, DAG-GNN and NOTEARS [Zheng et al., 2018]. Prequential scoring performs favourably and finds a graph with lower structural Hamming distance H to the ground-truth compared to DAG-GNN and NOTEARS. However, prequential scoring identified a number of DAGs with good prequential plug-in scores and partially overlapping uncertainty bands. Using Bayes rule we can approximate the posterior p(G|D) and compute the posterior weighted average (PWA) Hamming distance (cid:80) p(G|D)H(G, G∗) and the G posterior weighted number of links. To approximate the posterior we considered all graphs G visited by the hill climbing algorithm. The results are reported in the table. The posterior was dominated by a few hundred graphs G with good scores. 8
4 Discussion This paper considered the problem of learning the structure of a Bayesian network in settings in which modern, possibly overparametrized, neural networks might be used to model its conditional distributions. We proposed the use of the prequential plug-in score as a MDL-based model selection criterion that does not require any explicit sparsity regularization, and provided a specific implemen- tation using neural networks that shows good performance, leads to sparse, parsimonious structural inferences, and often recovers the structure underlying the data generation mechanism. Previous literature on MDL-based approaches to structure learning has focused on analytically tractable model families, such as tabular distributions or distributions with conjugate priors [Grünwald, 2007]. For categorical random variables, Silander et al. [2008] derived a factorized approximation to the normalized maximum likelihood by exploiting the conditional distribution structure. This work was further extended [Silander et al., 2018] to focus on model selection procedures that assign the same score to every model in a Markov equivalence class. However, it is not clear how to generalize these techniques to continuous random variables or complex models. Outside the context of structure learning, MDL-inspired model selection for neural networks has primarily focused on using variational approximations or approximations based on AIC or BIC [Hinton and Van Camp, 1993, MacKay, 2003]. Lehtokangas et al. [1993] were perhaps the first to use a prequential plug-in approach, though the structure learned was the capacity of the neural network. Blier and Ollivier [2018] pioneered using the prequential plug-in distribution for modern scale neural networks architectures and essentially argued that prequential coding leads to much shorter description lengths than state-of-the-art variational approximations. They used the block-wise estimates described in Sect. 2.2 but without the confidence calibration; as a result, they had to switch between different model classes to avoid overfitting. They thus calculated prequential MDL estimates for a particular switching pattern, not for a model class. Bornschein et al. [2020] extended the block-wise estimate with calibration and obtained MDL estimates for modern overparametrized neural networks without limiting their capacity. Recent efforts on structure learning with modern neural networks has focused on improving scalability by framing structure search as a continuous optimization problem with regularized maximum- likelihood as a scoring metric (see Zheng et al. [2018], Yu et al. [2019], Zheng et al. [2020], Pamfil et al. [2020], and Vowels et al. [2021] for a review). Scalability is an important aspect that we did not consider. As a consequence, our experiments were only feasible with a small number of variables. While this might seem like a step backwards compared to recent work, we believe that it is important to investigate new scoring metrics without the confounding effect of approximating the search procedure. Proposals for scaling prequential scoring to a higher number of variables include classical approximation methods developed for Bayesian scores [Heckerman, 1999], techniques like dynamic programming [Malone et al., 2011], branch and bound [de Campos and Ji, 2011], mathematical programming [Jaakkola et al., 2010, Cussens, 2011], and continuous optimization approaches [Zheng et al., 2018, Yu et al., 2019, Zheng et al., 2020, Pamfil et al., 2020]. Our approach effectively uses the generalization performance when trained on limited data as a model selection criterion. As such it is related to recent work that uses adaptation speed, i.e. how quickly models adapt to changes in the data generating process, to infer causal structures [Ke et al., 2019, Bengio et al., 2020] (see Le Priol et al. [2020] for a theoretical justification of this principle). The prequential MDL perspective offers an alternative and potentially simpler argument based on sample-efficiency instead of gradient-step efficiency to justify such an approach. And, as we have shown, this perspective is not only theoretically well-motivated but also applicable to i.i.d. data. References Hirotogu Akaike. Information Theory and an Extension of the Maximum Likelihood Principle, pages 199–213. Springer New York, 1973. Ankur Ankan and Abinash Panda. pgmpy: Probabilistic graphical models using Python. In Python in Science Conference, 2015. Jordan T. Ash and Ryan P. Adams. On warm-starting neural network training. In Advances in Neural Information Processing Systems, pages 3884–3894, 2020. 9
Vladimir Batagelj and Ulrik Brandes. Efficient generation of large random networks. Physical Review E, 71(3):036113, 2005. Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sebastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal mechanisms. In International Conference on Learning Representations, 2020. Léonard Blier and Yann Ollivier. The description length of deep learning models. In Advances in Neural Information Processing Systems, pages 2216–2226, 2018. Jörg Bornschein, Francesco Visin, and Simon Osindero. Small data, big decisions: Model selection in the small-data regime. In International Conference on Machine Learning, pages 1035–1044, 2020. Thomas M. Cover. Elements of Information Theory. John Wiley & Sons, 1999. Robert G. Cowell, A. Philip Dawid, Steffen Lauritzen, and David J. Spiegelhalter. Probabilistic Networks and Expert Systems, Exact Computational Methods for Bayesian Networks. Springer- Verlag, 2007. James Cussens. Bayesian network learning with cutting planes. In Uncertainty in Artificial Intelli- gence, page 153–160, 2011. A. Philip Dawid and Vladimir G. Vovk. Prequential probability: principles and properties. Bernoulli, 5(1):125–162, 1999. Cassio P. de Campos and Qiang Ji. Efficient structure learning of Bayesian networks using constraints. Journal of Machine Learning Research, 12:663–689, 2011. Mathias Drton and Marloes H. Maathuis. Structure learning in graphical modeling. Annual Review of Statistics and Its Application, 4(1):365–393, 2017. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2014. Peter Grünwald. A tutorial introduction to the minimum description length principle. arXiv preprint arXiv:0406077, 2004. Peter Grünwald. The Minimum Description Length Principle. MIT Press, 2007. Peter Grünwald and Teemu Roos. Minimum description length revisited. International journal of mathematics for industry, 11(01):1930001, 2019. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321–1330, 2017. David Heckerman. A tutorial on learning with Bayesian networks. In Michael I. Jordan, editor, Learning in Graphical Models. MIT Press, 1999. David Heckerman, Dan Geiger, and David M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20:197–243, 1995. Geoffrey E. Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Conference on Computational Learning Theory, pages 5–13, 1993. Tommi Jaakkola, David Sontag, Amir Globerson, and Marina Meila. Learning Bayesian network structure using LP relaxations. In International Conference on Artificial Intelligence and Statistics, pages 358–365, 2010. Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard Schölkopf, Michael C Mozer, Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions. arXiv preprint arXiv:1910.01075, 2019. 10
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press, 2009. Timo Koski and John Noble. A review of Bayesian networks and structure learning. Mathematica Applicanda, 40, 2012. Way Lam and Fahiem Bacchus. Learning Bayesian belief networks an approach based on the MDL principle. Computational Intelligence, 10, 1994. Rémi Le Priol, Reza Babanezhad Harikandeh, Yoshua Bengio, and Simon Lacoste-Julien. An analysis of the adaptation speed of causal models. arXiv preprint arXiv:2005.09136, 2020. Mikko Lehtokangas, Jukka Saarinen, Pentti Huuhtanen, and Kimmo Kaski. Neural network opti- mization tool based on predictive MDL principle for time series prediction. In IEEE Conference on Tools with AI, pages 338–342, 1993. Ahmed Mabrouk, Christophe Gonzales, Karine Jabet-Chevalier, and Eric Chojnacki. An efficient Bayesian network structure learning algorithm in the presence of deterministic relations. Frontiers in Artificial Intelligence and Applications, 263:567–572, 2014. David J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University Press, 2003. Brandon Malone, Changhe Yuan, and Eric A. Hansen. Memory-efficient dynamic programming for learning optimal Bayesian networks. In AAAI Conference on Artificial Intelligence, page 1057–1062, 2011. Roxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Konstantinos Geor- gatzis, Paul Beaumont, and Bryon Aragam. DYNOTEARS: Structure learning from time-series data. In International Conference on Artificial Intelligence and Statistics, pages 1595–1605, 2020. Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc., 1988. Judea Pearl. Causality: Models, Reasoning and Inference. Springer, 2000. Judea Pearl, Madelyn Glymour, and Nicholas P. Jewell. Causal Inference in Statistics: A Primer. John Wiley & Sons, 2016. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of Causal Inference: Foundations and Learning Algorithms. MIT Press, 2017. Jan Poland and Marcus Hutter. Asymptotics of discrete MDL for online prediction. IEEE Transactions on Information Theory, 51(11):3780–3795, 2005. Ning Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12(1): 145–151, 1999. Jorma J. Rissanen. Fisher information and stochastic complexity. IEEE Transactions on Information Theory, 42(1):40–47, 1996. Karen Sachs, Omar Perez, Dana Pe’er, Douglas A. Lauffenburger, and Garry P. Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. Science, 308(5721): 523–529, 2005. Gideon Schwarz. Estimating the dimension of a model. Annals of Statistics, 6(2):461–464, 1978. Tomi Silander, Teemu Roos, Petri Kontkanen, and Petri Myllymäki. Factorized normalized max- imum likelihood criterion for learning Bayesian network structures. In European Workshop on Probabilistic Graphical Models, pages 257–264, 2008. 11
Tomi Silander, Janne Leppä-aho, Elias Jääsaari, and Teemu Roos. Quotient normalized maximum likelihood criterion for learning Bayesian network structures. In International Conference on Artificial Intelligence and Statistics, pages 948–957, 2018. Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search. MIT Press, 2000. Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems, 2020. Erdogan Taskesen. bnlearn. https://github.com/erdogant/bnlearn, 2019. Matthew J. Vowels, Necati Cihan Camgoz, and Richard Bowden. D’ya like DAGs? A survey on structure learning and causal discovery. arXiv preprint arXiv:2103.02582, 2021. Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural networks. In International Conference on Machine Learning, pages 7154–7163, 2019. Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. DAGs with NO TEARS: Continuous optimization for structure learning. In Advances in Neural Information Processing Systems, pages 9472–9483, 2018. Xun Zheng, Chen Dan, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Learning sparse nonparametric DAGs. In International Conference on Artificial Intelligence and Statistics, pages 3414–3425, 2020. 12
A Notes on MDL Given a sufficiently regular k-dimensional parametric model class M (e.g. a curved exponential G family), an essential result from the MDL literature is that log p (Z | G) and log p (Z | G) Bayes plug-in closely track the maximum log-likelihood of M for all sufficiently regular data Z ∈ X n (e.g. the G MLE lies in the interior of the parameter space); see Grünwald and Roos [2019][Section 2.2] for precise statements and pointers to the literature. Essentially, this result shows that p and p Bayes plug-in are universal distributions and satisfy Eq. (1), up to a small error. Theorem 1. Under sufficient regularity conditions on the model class M and for any sufficiently G regular data Z ∈ X n, log p(Z | θˆMLE(Z), G) ≤ log p (Z | G) + O(log(n)) + O(1), preq k log p(Z | θˆMLE(Z), G) ≤ log p (Z | G) + log(n) + O(1), Bayes 2 where the constants depend on the mismatch between M and the data distribution. G Connection to Data Compression. The use of universal distributions connects to data compression through the Kraft–McMillan inequality [Cover, 1999], which states that a probabilistic model’s log- loss on D is equal to the shortest achievable code-length for encoding D using a code derived from that model. However, we cannot use the optimal choice θˆMLE(D) for compression since it is not known a priori. Instead, we want a code that can losslessly encode almost as well as p(·|θˆMLE, G) by having a code length near min − log p(Z|θ, G) for all possible data Z. A reader may recognize this θ criterion as Eq. (1), so we can alternatively define a universal distribution as one that achieves a code length as closely to that of p(·|θˆMLE, G) as possible for all data Z. B Faithfulness Example Let us denote with I(G) = {X ⊥⊥ Y | Z} the set of statistical independence relationships implied G by the structure of a BN (G, p), also called global Markov independencies, and let us denote with I(p) the set of statistical independence relationships satisfied by the distribution p. We must have I(G) ⊆ I(p), i.e. any independence encoded in G is satisfied by p—we denote this as X ⊥⊥ Y | Z ⇒ X ⊥⊥ Y |Z. If the converse relation holds, namely if any independence satisfied G p by p is encoded in G (i.e. X ⊥⊥ Y | Z ⇒ X ⊥⊥ Y | Z) giving I(G) = I(p), then we say that p p G is faithful to G. Unfaithfulness can arise for different reasons, for example when dependence along different pathways cancels out [Peters et al., 2017] or when links express deterministic relationships, which can occur in many real-world scenarios (see Koski and Noble [2012], Mabrouk et al. [2014]). Constraint-based approaches to structure learning assume faithfulness of the distribution p to the structure G underlying the data generation mechanism, and infer a member of the Markov equivalence class w.r.t. which p is faithful. In the example below we shows that, when p is not faithful, recovering the Markov equivalence class w.r.t. which p is faithful might lead to a preference for more complex CPDs. This issue is not shared by prequential scoring which does not assume faithfulness and infers a structure by taking the complexity of the CPDs into account. Let us consider the data generation mechanism • A = random prime number with 2 ≤ A ≤ M , G • B = random prime number with A ≤ B ≤ M , • C = A · B. A C B The DAG G corresponding to this mechanism is shown in the figure above and has I(G) = ∅. If we analyze the conditional independence relationships satisfied by the joint distribution p, we realize that any number c with p(C = c) > 0 can be uniquely factorized into the numbers A and B that generated it. This is a direct consequence of the fundamental theorem of arithmetic and the fact that the generative process ensures A ≤ B. Because A does not provide additional information about B given C and vice versa, the set of statistical independence relationships between the variables is given by I(p) = {A ⊥⊥ B | C}. Therefore p is not faithful to G. 13
The Markov equivalence class E of DAGs w.r.t p is faithful is E = {A ← C → B, A → C → B, A ← C ← B}. The structures in E can be considered simpler than G as they contain fewer links. However, their joint distributions must contain at least one conditional distribution which requires an integer factorization. For example, p(A | C) reads: "perform a factorization of C into prime numbers and set A to the smaller one". Factorization is considered a hard problem and it is commonly used as a one-way function in cryptography because no known algorithms can perform it efficiently. Applying structure learning methods that infer the Markov equivalence class w.r.t which the distribu- tion is faithful would return E and, along the way, the methods would have to discover (the existence of) a factorization algorithm (or table) for the natural numbers up to M 2. Therefore, whilst returning simpler structures, these methods would imply significantly more complex conditional distributions (for some intuitive definition of complexity). Prequential scoring instead takes the complexity of the conditional distributions into account, and therefore it is free to prefer a graph which is outside of E if it finds modeling a multiplication "easier" than factorization. C Case Study on 5 Nodes Chains In the sections above we argued that prequential scoring not only takes the overall DAG sparsity into account, but also considers the complexity of the CPDs as measured by their sample-efficiency. As a result, prequential scoring often has a preference for the direction of links even when the resulting DAGs contain the same number of edges and belong to the same Markov equivalence class. In this section, we demonstrate this property by generating data from two chains of 5 variables X1, · · · , X5. In the first chain, each variable is an almost linear function of the previous variable in the chain plus some noise. In the second chain, the functional relationship between successive nodes is more non-linear (periodic) and requires a multi-modal CPD when modeling the relationship in the inverse direction. Note that our models are capable of modeling multi-modal regression variables because we use categorical prediction for the discretized target variable as described in Sect. 2.2. For each of the two chains, we show the top-5 inferred DAGs and their excess scores relative to the best DAG. We observe that for both chains prequential scoring correctly identifies the ground-truth DAG as most likely. However, prequential scoring consistently avoids inverting the direction of the links in the second chain, which is not a property shared by the contenders. X1 ∼ N (0, 1) ; Xd ∼ N (sin(Xd−1), 0.12) X1 ∼ N (0, 1) ; Xd ∼ N (sin(4 · Xd−1), 0.12) Inferred DAG ∆ log p(D|G) Inferred DAG ∆ log p(D|G) 0 0 1117 2361 1178 2455 1319 4569 1348 4678 D PC Algorithm on Data from Yu et al. [2019] This section describes the behaviour of the PC algorithm on data generated according to the mecha- nism introduced by Yu et al. [2019] for validating DAG-GNN described in the main text. We used the implementation in PGMPy [Ankan and Panda, 2015] with χ2 (k2) and linear-regression based Pearson correlation (pearsonr) as conditional independence test. The PC algorithm returns a PDAG, which uses undirected edges to represent the Markov equivalence class, so the Hamming distance to ground-truth is not comparable to the one of prequential scoring. 14
E H Inferred DAG (PC) Ground-Truth DAG Inferred DAG (ours) H E (cid:55) 1 0 (cid:51) (cid:55) 3 0 (cid:51) (cid:51) 0 0 (cid:51) (cid:55) 5 1 (cid:55) (cid:51) 0 0 (cid:51) (cid:51) 0 1 (cid:51) (cid:51) 0 1 (cid:51) (cid:55) 5 1 (cid:51) (cid:55) 1 3 (cid:55) (cid:51) 0 0 (cid:51) E 5-Node DAGs We applied prequential scoring to 20 randomly generated DAGs with nonlinear relationships between variables. Each instance was created by 1) generating a random 5-node DAG with the GNP algorithm [Batagelj and Brandes, 2005] with link probability of 0.25, 2) removing links to make the adjacency matrix lower-triangular, 3) selecting the non-linear functions for each (topologically sorted) variable uniformly from a set of candidate-functions with a the corresponding number of input variables (see Table 3), 4) generating i.i.d. data (sampling a independent (cid:15) ∼ N (0, 0.12) per variable), Table 3 shows the 20 generative processes that were created this way. The results when applying prequential scoring to this data can be found in Table 2. DAG-GNN [Yu et al., 2019] with the recommended parameter settings was unstable with respect to random seed and dataset size. We were not able to find a parameter setting that worked reliably , so we are not comfortable presenting the performance of DAG-GNN on this data. A B C D E sin(30(cid:15)) sin(2A) + (cid:15) sin(B3 − B + (cid:15)) (C + (cid:15))3 sgn(A)1/|A| + 0.1 + (cid:15) 10(cid:15) (A + (cid:15))3 sin(2A) + (cid:15) sin(1/|C| + 0.1 + (cid:15)) sin(2A) + (cid:15) 10(cid:15) 10(cid:15) sgn(B)1/|B| + 0.1 + (cid:15) sin(2C3 − B2) + (cid:15) sgn(D) sin(4DA + (cid:15)) 10(cid:15) sgn(A)1/|A| + 0.1 + (cid:15) 10(cid:15) sin(2C3 − A2) + (cid:15) sgn(D) sin(4DA + (cid:15)) sin(30(cid:15)) (A + (cid:15))3 sin(30(cid:15)) (C + (cid:15))3 sin(C3 − C + (cid:15)) 10(cid:15) sin(30(cid:15)) sin(2B3 − A2) + (cid:15) sgn(C) sin(4CA + (cid:15)) sin(1/|D| + 0.1 + (cid:15)) sin(30(cid:15)) sin(A3 − A + (cid:15)) sin(2B) sin(1/|A| + 0.1) + (cid:15) sin(4CBA + (cid:15)) sin(2D3 − C2) + (cid:15) sin(30(cid:15)) sin(A3 − A + (cid:15)) sin(2B) sin(1/|A| + 0.1) + (cid:15) sgn(A)1/|A| + 0.1 + (cid:15) sin(2D) sin(1/|A| + 0.1) + (cid:15) 10(cid:15) 10(cid:15) sin(4BA + (cid:15)) sin(30(cid:15)) sin(4DCA + (cid:15)) sin(30(cid:15)) sin(A3 − A + (cid:15)) sin(2B3 − A2) + (cid:15) 10(cid:15) sin(2B) sin(4BA + (cid:15)) sin(30(cid:15)) sin(30(cid:15)) sgn(B) sin(2BA + (cid:15)) sin(30(cid:15)) sin(2D) sin(4DA + (cid:15)) sin(30(cid:15)) sin(2A) + (cid:15) sin(4BA + (cid:15)) sin(2C) sin(1/|B| + 0.1) + (cid:15) sgn(C)1/|C| + 0.1 + (cid:15) 10(cid:15) (A + (cid:15))3 sin(4BA + (cid:15)) sgn(C) sin(2CA + (cid:15)) sin(4DA + (cid:15)) sin(30(cid:15)) (A + (cid:15))3 10(cid:15) sin(4CA + (cid:15)) sgn(D)1/|D| + 0.1 + (cid:15) 10(cid:15) sin(A3 − A + (cid:15)) (B + (cid:15))3 sin(C3 − C + (cid:15)) sin(2A) + (cid:15) sin(30(cid:15)) sgn(A)1/|A| + 0.1 + (cid:15) sgn(B)1/|B| + 0.1 + (cid:15) sin(2C) + (cid:15) sgn(D) sin(4DA + (cid:15)) sin(30(cid:15)) sgn(A)1/|A| + 0.1 + (cid:15) sin(2B3 − A2) + (cid:15) 10(cid:15) sgn(D)1/|D| + 0.1 + (cid:15) 10(cid:15) sin(30(cid:15)) sgn(B) sin(2BA + (cid:15)) sin(2C) sin(1/|B| + 0.1) + (cid:15) sin(4DA + (cid:15)) sin(30(cid:15)) sin(1/|A| + 0.1 + (cid:15)) (B + (cid:15))3 sin(2B3 − A2) + (cid:15) sin(B3 − B + (cid:15)) sin(30(cid:15)) sin(1/|A| + 0.1 + (cid:15)) sin(B3 − B + (cid:15)) sin(1/|B| + 0.1 + (cid:15)) sin(30(cid:15)) Table 3: Generating equations used for the experiments presented in Table 2 ((cid:15) ∼ N (0, 0.12) for each table cell independently). 15
F Relation between log p (D | G) and log p (D | G) Bayes preq θ1 θ2 θ3 In this section we demonstrate that, if p(θ) = (cid:81)D d=1 p(θd), log p (D | G) can be written as (omitting dependence on G) Bayes D n (cid:90) (a) log p (D) = (cid:88) log (cid:89) p(xd | pa(xd), θd)p(θd | x ) dθd, Bayes i i <i X1 X2 X3 d=1 i=1 Θd θ1 θ2 θ3 (4) highlighting the connection with the factorization of p (D) over plug-in CPDs (Eq. (3)) (b) X 11 X 12 X 13 log p (D) = (cid:88)D log (cid:89)n p(xd | pa(xd), θˆd(x )). preq i i <i X1 X2 X3 d=1 i=1 2 2 2 To simplify the understanding but without loss of generality, we show the validity of Eq. (4) for the Bayesian network (a). Let D = {x := (x1, x2, x3)}2 be a dataset formed by two samples from (cid:82) p(X1, X2, X3, θ)dθ. Wi e can viei w Di ai s fi o= r1 med by one sample from (cid:82) Θ p(X , X , θ)dθ, where p(X , X , θ) = p(X | θ)p(X | θ)p(θ) with p(X | θ) = p(X | θ), and Θ 1 2 1 2 1 2 1 2 p(X := (X1, X2, X3)|θ) = p(X3 | X2, θ3)p(X2 | X1, θ2)p(X1 | θ1), for i = 1, 2 (Bayesian net- i i i i i i i i i work (b)). Using the prequential formulation log p(x , x ) = log p(x |x )p(x ), log p (D) := 1 2 2 1 1 Bayes (cid:82) log p(x , x , θ)dθ can be written as Θ 1 2 (cid:110)(cid:90) (cid:111)(cid:110)(cid:90) (cid:111) log p (D) = log p(x |x )p(x ) = log p(x , θ | x )dθ p(x , θ)dθ Bayes 2 1 1 2 1 1 Θ Θ (cid:110)(cid:90) (cid:111)(cid:110)(cid:90) (cid:111) = log p(x | θ,(cid:8)x(cid:8))p(θ | x )dθ p(x | θ)p(θ)dθ 2 1 1 1 Θ Θ (cid:110)(cid:90) (cid:111) = log p(x3 | x2, θ3)p(x2 | x1, θ2)p(x1 | θ1)p(θ1 | x )p(θ2 | x )p(θ3 | x )dθ 2 2 2 2 2 1 1 1 Θ (cid:110)(cid:90) (cid:111) × p(x3 | x2, θ3)p(x2 | x1, θ2)p(x1 | θ1)p(θ1)p(θ2)p(θ3)dθ 1 1 1 1 1 Θ D n (cid:90) (cid:88) (cid:89) = log p(xd | pa(xd), θd)p(θd | x ) dθd, (5) i i <i d=1 i=1 Θd where we have used the fact that p(x | θ, x ) = p(x | θ) and that the parameters posterior factorizes 2 1 2 over d, i.e. p(θ | x ) = p(θ1 | x )p(θ2 | x )p(θ3 | x ). 1 1 1 1 In the case in which the distribution of the parameters for each CPD and value of parents is Dirichlet with parameter α, a well-known result is (cid:90) N + α p(xd = k | pa(xd) = l, θd)p(θd | x ) dθd = k,l . i i <i (cid:80) (N + α) Θd m m,l G Prequential Scoring with Interventional Data In Fig. 4 we show the influence of interventional data on prequential scoring. We generated obser- vations from the cancer Bayesian network from the bnlearn repository [Taskesen, 2019] with DAG G∗ = P → C ← S, C → X, C → D. For i ∈ {5, 000, . . . , 6000}, we generated interventional data by randomly selecting a node from the graph and replacing it with a random value. The excess prequential loss in nats is plotted for the 14 best ranking DAGs during the first half of the sequence and the 14 best after learning from all observations. The interventional samples have a strong influence on the ranking and are necessary for the correct identification of G∗. 16
8 6 4 2 0 2 4 0 2000 4000 6000 8000 10000 n stan / )) x( pgol n< LDM Excess MDL relative to best model p(C|P,S) p(D|C) p(P) p(S) p(X|C) p(C|P,S) p(D|C) p(P) p(S|P) p(X|C) p(C|P,S) p(D|C) p(P|S) p(S) p(X|C) p(C|P,S) p(D|C) p(P) p(S) p(X|C,D) p(C|P,S) p(D|C,X) p(P) p(S) p(X|C) p(C|P,S) p(D|C,P) p(P) p(S) p(X|C) p(C|P,S) p(D|C) p(P) p(S) p(X|C,P) p(C|S) p(D|C) p(P|C) p(S) p(X|C) p(C|P) p(D|C) p(P) p(S|C) p(X|C) p(C) p(D|C) p(P|C) p(S|C) p(X|C) p(C) p(D|C,P) p(P|C) p(S|C) p(X|C) p(C|D,P) p(D) p(P) p(S|C) p(X|C) p(C|D) p(D) p(P|C) p(S|C) p(X|C) p(C|X) p(D|C) p(P|C) p(S|C) p(X) Figure 4: Excess log-loss in nats for i = 1, . . . , 10, 000 for the 14 DAGs with lowest excess. Uncertainty bands show standard deviation from 5 different different permutations of the data. Between 5, 000 ≤ i < 6, 000 (shaded area) interventional samples were supplied: we replaced the value of a random node in the graph with an random value (p = 1⁄ 2). Only due to these interventional samples could the ground-truth DAG G∗ be correctly identified as most likely. Nevertheless, there is no strong evidence in favour of a structure as many of them differ by only a few nats. 17
