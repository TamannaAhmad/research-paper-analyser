LARGE-SCALE GRAPH REPRESENTATION LEARNING WITH VERY DEEP GNNS AND SELF-SUPERVISION OPEN GRAPH BENCHMARK LARGE-SCALE CHALLENGE ENTRY (MAG240M AND PCQM4M) Ravichandra Addanki∗ Peter W. Battaglia∗ David Budden∗ DeepMind DeepMind DeepMind ravichandraa@deepmind.com peterbattaglia@deepmind.com budden@deepmind.com Andreea Deac∗ Jonathan Godwin∗ Thomas Keck∗MAG DeepMind, Mila, Université de Montréal DeepMind DeepMind adeac@deepmind.com jonathangodwin@deepmind.com thomaskeck@deepmind.com Wai Lok Sibon Li∗PCQ Alvaro Sanchez-Gonzalez∗ Jacklynn Stott∗ DeepMind DeepMind DeepMind sibon@deepmind.com alvarosg@deepmind.com jacklynnstott@deepmind.com Shantanu Thakoor∗ Petar Velicˇkovic´∗ DeepMind DeepMind thakoor@deepmind.com petarv@deepmind.com ABSTRACT Effectively and efficiently deploying graph neural networks (GNNs) at scale remains one of the most challenging aspects of graph representation learning. Many powerful solutions have only ever been validated on comparatively small datasets, often with counter-intuitive outcomes—a barrier which has been broken by the Open Graph Benchmark Large-Scale Challenge (OGB-LSC). We entered the OGB-LSC with two large-scale GNNs: a deep transductive node classifier powered by bootstrapping, and a very deep (up to 50-layer) inductive graph regressor regularised by denoising objectives. Our models achieved an award-level (top-3) performance on both the MAG240M and PCQM4M benchmarks. In doing so, we demonstrate evidence of scalable self-supervised graph representation learning, and utility of very deep GNNs—both very important open issues. Our code is publicly available at: https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc. Keywords OGB-LSC · MPNNs · Graph Networks · BGRL · Noisy Nodes 1 Introduction Effective high-dimensional representation learning necessitates properly exploiting the geometry of data [Bronstein et al., 2021]—otherwise, it is a cursed estimation problem. Indeed, early success stories of deep learning relied on imposing strong geometric assumptions, primarily that the data lives on a grid domain; either spatial or temporal. In these two respective settings, convolutional neural networks (CNNs) [LeCun et al., 1998] and recurrent neural networks (RNNs) [Hochreiter and Schmidhuber, 1997] have traditionally dominated. While both CNNs and RNNs are demonstrably powerful models, with many applications of high interest, it can be recognised that most data coming from nature cannot be natively represented on a grid. Recent years are marked with a gradual shift of attention towards models that admit a more generic class of geometric structures [Masci et al., 2015, Velicˇkovic´ et al., 2017, Cohen et al., 2018, Battaglia et al., 2018, de Haan et al., 2020, Satorras et al., 2021]. ∗All authors contributed equally. MAGTK worked solely on MAG240M. PCQWLSL worked solely on PCQM4M. 1202 luJ 02 ]GL.sc[ 1v22490.7012:viXra
Large-scale graph representation learning with very deep GNNs and self-supervision In many ways, the most generic and versatile of these models are graph neural networks (GNNs). This is due to the fact that most discrete-domain inputs can be observed as instances of a graph structure. The corresponding area of graph representaton learning [Hamilton, 2020] has already seen immense success across industrial and scientific disciplines. GNNs have successfully been applied for drug screening [Stokes et al., 2020], modelling the dynamics of glass [Bapst et al., 2020], web-scale social network recommendations [Ying et al., 2018] and chip design [Mirhoseini et al., 2020]. While the above results are certainly impressive, they likely only scratch the surface of what is possible with well-tuned GNN models. Many problems of real-world interest require graph representation learning at scale: either in terms of the amount of graphs to process, or their sizes (in terms of numbers of nodes and edges). Perhaps the clearest motivation for this comes from the Transformer family of models [Vaswani et al., 2017]. Transformers operate a self-attention mechanism over a complete graph, and can hence be observed as a specific instance of GNNs [Joshi, 2020]. At very large scales of natural language data, Transformers have demonstrated significant returns with the increase in capacity, as exemplified by models such as GPT-3 [Brown et al., 2020]. Transformers enjoy favourable scalability properties at the expense of their functional complexity: each node’s features are updated with weighted sums of neighbouring node features. In contrast, GNNs that rely on message passing [Gilmer et al., 2017]—passing vector signals across edges that are conditioned on both the sender and receiver nodes—are an empirically stronger class of models, especially on tasks requiring complex reasoning [Velicˇkovic´ et al., 2019] or simulations [Sanchez-Gonzalez et al., 2020, Pfaff et al., 2020]. One reason why generic message-passing GNNs have not been scaled up as widely as Transformers is the lack of appropriate datasets. Only recently has the field advanced from simple transductive benchmarks of only few thousands of nodes [Sen et al., 2008, Shchur et al., 2018, Morris et al., 2020] towards larger-scale real-world and synthetic benchmarks [Dwivedi et al., 2020, Hu et al., 2020], but important issues still remain. For example, on many of these tasks, randomly-initialised GNNs [Velicˇkovic´ et al., 2018], shallow GNNs [Wu et al., 2019] or simple label propagation-inspired GNNs [Huang et al., 2020] can perform near the state-of-the-art level at only a fraction of the parameters. When most bleeding-edge expressive methods are unable to improve on the above, this can often lead to controversial discussion in the community. One common example is: do we even need deep, expressive GNNs? Breakthroughs in deep learning research have typically been spearheaded by impactful large-scale competitions. For image recognition, the most famous example is the MNIST Large Scale Visual Recognition Challenge (ILSVRC) [Russakovsky et al., 2015]. In fact, the very “deep learning revolution” has partly been kickstarted by the success of the AlexNet CNN model of Krizhevsky et al. [2012] at the ILSVRC 2012, firmly establishing deep CNNs as the workhorse of image recognition for the forthcoming decade. Accordingly, we have entered the recently proposed Open Graph Benchmark Large-Scale Challenge (OGB-LSC) [Hu et al., 2021]. OGB-LSC provides graph representation learning tasks at a previously unprecedented scale—millions of nodes, billions of edges, and/or millions of graphs. Further, the tasks have been designed with immediate practical relevance in mind, and it has been verified that expressive GNNs are likely to be necessary for strong performance. Here we detail our two submitted models (for the MAG240M and PCQM4M tasks, respectively), and our empirical observations while developing them. Namely, we find that the datasets’ immense scale provides a great platform for demonstrating clear outperformance of very deep GNNs [Godwin et al., 2021], as well as self-supervised GNN setups such as bootstrapping [Thakoor et al., 2021]. In doing so, we have provided meaningful evidence towards a positive resolution to the above discussion: deep and expressive GNNs are, indeed, necessary at the right level of task scale and/or complexity. Our final models have achieved award-level (top-3) ranking on both MAG240M and PCQM4M. 2 Dataset description The MAG240M-LSC dataset is a transductive node classification dataset, based on the Microsoft Academic Graph (MAG) [Wang et al., 2020a]. It is a heterogeneous graph containing paper, author and institution nodes, with edges representing the relations between them: paper-cites-paper, author-writes-paper, author-affiliated-with-institution. All paper nodes are endowed with 768-dimensional input features, corresponding to the RoBERTa sentence embedding [Liu et al., 2019, Reimers and Gurevych, 2019] of their title and abstract. MAG240M is currently the largest-scale publicly available node classification dataset by a wide margin, at ∼240 million nodes and ∼1.8 billion edges. The aim is to classify the ∼1.4 million arXiv papers into their corresponding topics, according to a temporal split: papers published up to 2018 are used for training, with validation and test sets including papers from 2019 and 2020, respectively. The PCQM4M-LSC dataset is an inductive graph regression dataset based on the PubChemQC project [Nakata and Shimazaki, 2017]. It consists of ∼4 million small molecules (described by their SMILES strings). The aim is to accelerate quantum-chemical computations: especially, to predict the HOMO-LUMO gap of each molecule. The HOMO-LUMO gap is one of the most important quantum-chemical properties, since it is related to the molecules’ reactivity, photoexcitation, and charge transport. The ground-truth labels for every molecule were obtained through expensive DFT (density functional theory) calculations, which may take several hours per molecule. It is believed that 2
Large-scale graph representation learning with very deep GNNs and self-supervision machine learning models, such as GNNs over the molecular graph, may obtain useful approximations to the DFT at only a fraction of the computational cost, if provided with sufficient training data [Gilmer et al., 2017]. The molecules are split with a 80:10:10 ratio into training, validation and test sets, based on their PubChem ID. 3 GNN Architectures For both of the tasks above, we rely on a common encode-process-decode blueprint [Hamrick et al., 2018]. This implies that our input features are encoded into a latent space using node-, edge- and graph-wise encoder functions, and latent features are decoded to node-, edge- and graph- level predictions using appropriate decoder functions. The bulk of the computational processing is powered by a processor network, which performs multiple graph neural network layers over the encoded latents. To formalise this, assume that our input graph, G = (V, E), has node features x ∈ Rn, edge features x ∈ Rm u uv and graph-level features x ∈ Rl, for nodes u, v ∈ V and edges (u, v) ∈ E. Our encoder functions f : Rn → Rk, G n f : Rm → Rk and f : Rl → Rk then transform these inputs into the latent space: e g h(0) = f (x ) h(0) = f (x ) h(0) = f (x ) (1) u n u uv e uv G g G Our processor network then transforms these latents over several rounds of message passing: H(t+1) = P (H(t)) (2) t+1 (cid:18)(cid:110) (cid:111) (cid:110) (cid:111) (cid:19) where H(t) = h(t) , h(t) , h(t) contains all of the latents at a particular processing step t ≥ 0. u uv G u∈V (u,v)∈E The processor network P is iterated for T steps, recovering final latents H(T ). These can then be decoded into node-, edge-, and graph-level predictions (as required), using analogous decoder functions g , g and g : n e g y = g (h(T )) y = g (h(T )) y = g (h(T )) (3) u n u uv e uv G g G We will detail the specific design of f , P and g in the following sections. Generally, f and g are simple MLPs, whereas we use highly expressive GNNs for P in order to maximise the advantage of the large-scale datasets. Specifically, we use message passing neural networks (MPNNs) [Gilmer et al., 2017] and graph networks (GNs) [Battaglia et al., 2018]. All of our models have been implemented using the jraph library [Godwin et al., 2020]. 4 MAG240M-LSC Subsampling Running graph neural networks over datasets that are even a fraction of MAG240M’s scale is already prone to multiple scalability issues, which necessitated either aggressive subsampling [Hamilton et al., 2017, Chen et al., 2018, Zeng et al., 2019, Zou et al., 2019], graph partitioning [Liao et al., 2018, Chiang et al., 2019] or less expressive GNN architectures [Rossi et al., 2020, Bojchevski et al., 2020, Yu et al., 2020]. As we would like to leverage expressive GNNs, and be able to pass messages across any partitions, we opted for the subsampling approach. Accordingly, we subsample moderately-sized patches around the nodes we wish to compute latents for, execute our GNN model over them, and use the latents in the central nodes to train or evaluate the model. We adapt the standard GraphSAGE subsampling algorithm of Hamilton et al. [2017], but make several modifications to it in order to optimise it for the specific features of MAG240M. Namely: • We perform separate subsampling procedures across edge types. For example, an author node will separately sample a pre-specified number of papers written by that author and a pre-specified number of institutions that author is affiliated with. • GraphSAGE mandates sampling an exact number of neighbours for every node, and uses sampling with replacement to achieve this even when the neighbourhood size is variable. We find this to be wasteful for smaller neighbourhoods, and hence use our pre-specified neighbour counts only as an upper bound. Denoting this upper bound as K, and node u’s original neighbourhood as N , we proceed2 as follows: u – For nodes that have fewer neighbours of a particular type than the upper bound (|N | ≤ K), we simply u take the entire neighbourhood, without any subsampling; 2Note that, according to the previous bullet point, K and N are defined on a per-edge-type basis. u 3
Large-scale graph representation learning with very deep GNNs and self-supervision – For nodes that have a moderate amount of neighbours (K < |N | ≤ 5K) we subsample K neighbours u without replacement, hence we do not wastefully duplicate nodes when the memory costs are reasonable. – For all other nodes (|N | > 5K), we resort to the usual GraphSAGE strategy, and sample K neighbours u with replacement, which doesn’t require an additional row-copy of the adjacency matrix. • GraphSAGE directed the edges in the patch from the subsampled neighbours to the node which sampled them, and run their GNN for the exact same number of steps as the sampling depth. We instead modify the message passing update rule to scalably make the edges bidirectional, which naturally allows us to run deeper GNNs over the patch. The exact way in which we performed this will be detailed in the model architecture. Taking all of the above into account, our model’s subsampling strategy proceeds, starting from paper nodes as central nodes, up to a depth of two (sufficient for institution nodes to become included). We did not observe significant benefits from sampling deeper patches. Instead, we sample significantly larger patches than the original GraphSAGE paper, to exploit the wide context available for many nodes: Depth-0 Contains the chosen central paper node. Depth-1 We sample up to K = 40 citing papers, K = 40 cited papers, and up to K = 20 authors for this paper. Depth-2 We sample according to the following strategy, for all paper and author nodes sampled at depth-1: Papers Identical strategy as for depth-1 papers: up to K = 40 cited, K = 40 citing, K = 20 authors. Authors We sample up to K = 40 written papers, and up to K = 10 affiliations for this author. Overall, this inflates our maximal patch size to nearly 10, 000 nodes, which makes our patches of a comparable size to traditional full-graph datasets [Sen et al., 2008, Shchur et al., 2018]. Coupled with the fact that MAG240M has hundreds of millions of papers to sample these patches from, our setting enables transductive node classification at previously unexplored scale. We have found that such large patches were indeed necessary for our model’s performance. One final important remark for MAG240M subsampling concerns the existence of duplicated paper nodes—i.e. nodes with exactly the same RoBERTa embeddings. This likely corresponds to identical papers submitted to different venues (e.g. conference, journal, arXiv). For the purposes of enriching our subsampled patches, we have combined the adjacency matrix rows and columns to “fuse” all versions of duplicated papers together. Input preprocessing As just described, we seek to support execution of expressive GNNs on large quantities of large-scale subsampled patches. This places further stress on the model from a computational and storage perspective. Accordingly, we found it very useful to further compress the input nodes’ RoBERTa features. Our qualitative analysis demostrated that their 129-dimensional PCA projections already account for 90% of their variance. Hence we leverage these PCA vectors as the actual input paper node features. Further, only the paper nodes are actually provided with any features. We adopt the identical strategy from the baseline LSC scripts provided by Hu et al. [2021] to featurise the authors and institutions. Namely, for authors, we use the average PCA features across all papers they wrote. For institutions, we use the average features across all the authors affiliated with them. We found this to be a simple and effective strategy that performed empirically better than using structural features. This is contrary to the findings of Yu et al. [2020], probably because we use a more expressive GNN. Besides the PCA-based features, our input node features x also contain the one-hot representation of the node’s type u (paper/author/institution), the node’s depth in the sampled patch (0/1/2), and a bitwise representation of the papers’ publication year (zeroed out for other nodes). Lastly, and according to an increasing body of research that argues for the utility of labels in transductive node classification tasks [Zhu and Ghahramani, 2002, Stretcu et al., 2019, Huang et al., 2020], we use the arXiv paper labels as features [Wang et al., 2021] (zeroed out for other nodes). We make sure that the validation labels are not observed at training time, and that the central node’s own label is not provided. It is possible to sample the central node at depth 2, and we make sure to mask out its label if this happens. We also endow the patches’ edges with a simple edge type feature, x . It is a 7-bit binary feature, where the first uv three bits indicate the one-hot type of the sampling node (paper, author or institution) and the next four bits indicate the one-hot type of the sampled neighbour (cited paper, citing paper, author or institution). We found running a standard GNN over these edge-type features more performant than running a heterogeneous GNN—once again contrary to existing baseline results [Hu et al., 2021], and likely because of the expressivity of our processor GNN. Model architecture For the GNN architecture we have used on MAG240M, our encoders and decoders are both two-layer MLPs, with a hidden size of 512 features. The node and edge encoders’ output layers compute 256 features, and we retain this dimensionality for h(t) and h(t) across all steps t. u uv 4
Large-scale graph representation learning with very deep GNNs and self-supervision Our processor network is a deep message-passing neural network (MPNN) [Gilmer et al., 2017]. It computes message vectors, m(t), to be sent across the edge (u, v), and then aggregates them in the receiver nodes as follows: uv (cid:16) (cid:17) m(t+1) = ψ h(t), h(t), h(0) (4) uv t+1 u v uv (cid:32) (cid:33) (cid:88) (cid:88) h(t+1) = φ h(t), m(t+1), m(t+1) (5) u t+1 u vu uv u∈Nv v∈Nu Taken together, Equations 4–5 fully specify the operations of the P network in Equation 2. The message function t+1 ψ and the update function φ are both two-layer MLPs, with identical hidden and output sizes to the encoder t+1 t+1 network. We note two specific aspects of the chosen MPNN: • We did not find it useful to use global latents or update edge latents (Equation 4 uses h(0) at all times and does uv not include h ). This is likely due to the fact that the prediction is strongly centred at the central node, and G that the edge features and types do not encode additional information. • Note the third input in Equation 5, which is not usually included in MPNN formulations. In addition to pooling all incoming messages, we also pool all outgoing messages a node sends, and concatenate that onto the input to the sender node’s update function. This allowed us to simulate bidirectional edges without introducing additional scalability issues, allowing us to prototype MPNNs whose depth exceeded the subsampling depth. The process is repeated for T = 4 message passing layers, after which h(4) for the central node is sent to the decoder u network for predictions. Bootstrapping objective The non-arXiv papers within MAG240M are unlabelled and hence, under a standard node classification training regime, would contribute only implicitly to the learning algorithm (as neighbours of labelled papers). Early work on self-supervised graph representation learning [Velicˇkovic´ et al., 2018] had already shown this could be a wasteful approach, even on small-scale transductive benchmarks. Appropriately using the unlabelled nodes can provide the model with a wealth of information about the feature and network structure, which cannot be easily recovered from supervision alone. On a dataset like MAG240M—which contains ∼120× more unlabelled papers than labelled ones—we have been able to observe significant gains from deploying such methods. Especially, we leverage bootstrapped graph latents (BGRL) [Thakoor et al., 2021], a recently-proposed scalable method for self-supervised learning on graphs. Rather than contrasting several node representations across multiple views, BGRL bootstraps the GNN to make a node’s embeddings be predictive of its embeddings from another view, under a target GNN. The target network’s parameters are always set to an exponential moving average (EMA) of the GNN parameters. Formally, let f − and P − be the target versions of the encoder and processor networks (periodically updated to the EMA of f and P ’s parameters), and X and X(cid:101) be two views of an input patch (in terms of features, adjacency structure or both). Then, BGRL performs the following computations: H(T ) = P (f (X)) H(cid:101) (T ) = P −(f −(X(cid:101) )) (6) where P (f (X)) is short-hand for applying Equation 1, followed by repeatedly applying Equations 4–5 for T steps. The BGRL loss is then optimised to make the central node embedding h( uT ) predictive of its counterpart, h(cid:101)u(T ). This is done by projecting h(T ) to another representation using a projector network, p, as follows: u z = p(h(T )) (7) u u where p is a two-layer MLP with identical hidden and output size as our encoder MLPs. We then optimise the cosine similarity between the projector output and h(cid:101)( uT ): L = −2 z(cid:62) u h(cid:101)( uT ) (8) BGRL (cid:107)z u(cid:107)(cid:107)h(cid:101)( uT )(cid:107) using stochastic gradient ascent. Once training is completed, the projector network p is discarded. This approach, inspired by BYOL [Grill et al., 2020], eliminates the need for crafting negative samples, reduces the storage requirements of the model, and its pointwise loss aligns nicely with our patch-wise learning setting, as we can focus on performing the bootstrapping objective on each central node separately. All of this made BGRL a natural choice in our setting, and we have found that we can easily apply it at scale. 5
Large-scale graph representation learning with very deep GNNs and self-supervision Previously, BGRL has been applied on moderately-sized graphs with less expressive GNNs, showing modest returns. Conversely, we find the benefits of BGRL were truly demonstrated with stronger GNNs on the large-scale setting of MAG240M. Not only does BGRL monotonically improve when increasing proportions of unlabelled-to-labelled nodes during training, it consistently outperformed relevant self-supervised GNNs such as GRACE [Zhu et al., 2020]. Ultimately, our submitted model is trained with an auxiliary BGRL objective, with each batch containing a 10 : 1 ratio of unlabelled to labelled node patches. Just as in the BGRL paper, we obtain the two input patch views by applying dropout [Srivastava et al., 2014] on the input features (with p = 0.4) and DropEdge [Rong et al., 2019] (with p = 0.2), independently on each view. The target network (f −, P −) parameters are updated with EMA decay rate (cid:15) = 0.999. Training and regularisation We train our GNN to minimise the cross-entropy for predicting the correct topic over the labelled central nodes in the training patches, added together with the BGRL objective for the unlabelled central nodes. We use the AdamW SGD optimiser [Loshchilov and Hutter, 2017] with hyperparameters β = 0.9, β = 0.999 1 2 and weight decay rate of λ = 10−5. We use a cosine learning rate schedule with base learning rate η = 0.01 and 50, 000 warm-up steps, decayed over 500, 000 training iterations. Optimisation is performed over dynamically-batched data: we fill up each training minibatch with sampled patches until any of the following limits are exceeded: ∼ 84, 000 nodes, ∼ 185, 000 edges, or 256 patches. To regularise our model, we perform early stopping on the F1-score over the validation dataset, and apply feature dropout (with p = 0.3) and DropEdge [Rong et al., 2019] (with p = 0.25) at every message passing layer of the GNN. We further apply layer normalisation [Ba et al., 2016] to intermediate outputs of all of our MLP modules. Evaluation At evaluation time, we make advantage of the transductive and subsampled learning setup to enhance our predictions even further: first, we make sure that the model has access to all validation labels as inputs at test time, as this knowledge may be highly indicative. Further, we make sure that any “fused” copies of duplicated nodes also provide that same label as input. As our predictions are potentially conditioned on the specific topology of the subsampled patch, for each test node we average our predictions over 50 subsampled patches—an ensembling trick which consistently improved our validation performance. Lastly, given that we already use EMA as part of BGRL’s target network, for our evaluation predictions we use the EMA parameters, as they are typically slightly more stable. 5 PCQM4M-LSC Input preprocessing For featurising our molecules within PCQM4M, we initially follow the baseline scripts provided by Hu et al. [2021] to convert SMILES strings into molecular graphs. Therein, every node is represented by a 9- dimensional feature vector, x , including properties such as atomic number and chirality. Further, every edge is u endowed with 3-dimensional features, x , including bond types and stereochemistry. Mirroring prior work with GNNs uv for quantum-chemical computations [Gilmer et al., 2017], we found it beneficial to maintain graph-level features (in the form of a “master node”), which we initialise at x = 0. G As will soon become apparent, our experiments on the PCQM4M benchmark leveraged GNNs that are substantially deeper than most previously studied GNNs for quantum-chemical tasks, or otherwise. While there is implicit expectation to compute useful “cheap” chemical features from the SMILES string, such as molecular fingerprints, partial charges, etc., our experiments clearly demonstrated that most of them do not meaningfully impact performance of our GNNs. This indicates that very deep GNNs are likely implicitly able to compute such features without additional guidance. The exception to this have been conformer features, corresponding to approximated three-dimensional coordinates of every atom. These are very expensive to obtain accurately. However, using RDKit [Landrum, 2013], we have been able to obtain conformer estimates that allowed us to attain slightly improved performance with a (slightly) shallower GNN. Specifically, we use the experimental torsion knowledge distance geometry (ETKDGv3) algorithm [Wang et al., 2020b] to recover conformers that satisfy essential geometric constraints, without violating our time limits. Once conformers are obtained, we do not use their raw coordinates as features—these have many equivalent formulations that depend on the algorithm’s initialisation. Instead, we encode their displacements (a 3-dimensional vector recording distances along each axis) and their distances (scalar norm of the displacement) as additional edge features concatenated with x . Note that RDKit’s algorithm is not powerful enough to extract conformers for every molecule within uv PCQM4M; for about 0.1% of the dataset, the returned conformers will be NaN. Lastly, we also attempted to use more computationally intensive forms of conformer generation—including energy optimisation using the universal force field (UFF) [Rappé et al., 1992] and the Merck molecular force field (MMFF) [Halgren, 1996]. In both cases, we did not observe significant returns compared to using rudimentary conformers. 6
Large-scale graph representation learning with very deep GNNs and self-supervision Model architecture For the GNN architecture we have used on PCQM4M, our encoders and decoders are both three-layer MLPs, computing 512 features in every hidden layer. The node, edge and graph-level encoders’ output layers compute 512 features, and we retain this dimensionality for h(t), h(t) and h(t) across all steps t. u uv G For our processor network, we use a very deep Graph Network (GN) [Battaglia et al., 2018]. Each GN block computes updated node, edge and graph latents, performing aggregations across them whenever appropriate. Fully expanded out, the computations of one GN block can be represented as follows: (cid:16) (cid:17) h(t+1) = ψ h(t), h(t), h(t), h(t) (9) uv t+1 u v uv G (cid:32) (cid:33) h(t+1) = φ h(t), (cid:88) h(t+1), h(t) (10) u t+1 u vu G u∈Nv   h( Gt+1) = ρ t+1 (cid:88) h( ut+1), (cid:88) h( ut v+1), h( Gt)  (11) u∈V (u,v)∈E Taken together, Equations 9–11 fully specify the operations of the P network in Equation 2. The edge update t+1 function ψ , node update function φ and graph update function ρ are all three-layer MLPs, with identical t+1 t+1 t+1 hidden and output sizes to the encoder network. The process is repeated for T = 32 message passing layers, after which the computed latents h(32), h(32) and h(32) u uv G are sent to the decoder network for relevant predictions. Specifically, the global latent vector h(32) is used to predict G the molecule’s HOMO-LUMO gap. Our work thus constitutes a successful application of very deep GNNs, providing evidence towards ascertaining positive utility of such models. We note that, while most prior works on GNN modelling seldom use more than eight steps of message passing [Brockschmidt, 2020], we observe monotonic improvements of deeper GNNs on this task, all the way to 32 layers when the validation performance plateaus. Non-conformer model Recalling our prior discussion about conformer features occasionally not being trivially computable, we also trained a GN which does not exploit conformer-based features. While we observe largely the same trends, we find that they tend to allow for even deeper and wider GNNs before plateauing. Namely, our optimised non-conformer GNN computes 1,024-dimensional hidden features in every MLP, and iterates Equations 9–11 for T = 50 message passing steps. Such a model performed marginally worse than the conformer GNN overall, while significantly improving the mean absolute error (MAE) on the 0.1% of validation molecules without conformers. Denoising objective Our very deep GNNs have, in the first instance, been enabled by careful regularisation. By far, the most impactful method for our GNN regressor on PCQM4M has been Noisy Nodes [Godwin et al., 2021], and our results largely echo the findings therein. The main observation of Noisy Nodes is that very deep GNNs can be strongly regularised by appropriate denoising objectives. Noisy Nodes perturbs the input node or edge features in a pre-specified way, then requires the decoder to reconstruct the un-perturbed information from the GNN’s latent representations. In the case of the flat input features, we have deployed a Noisy Nodes objective on both atom types and bond types: randomly replacing each atom and each bond type with a uniformly sampled one, with probability p = 0.05. The model then performs node/edge classification based on the final latents (e.g., h(32), h(32) for the conformer GNN), u uv to reconstruct the initial types. Requiring the model to correctly infer and rectify such noise is implicitly imbuing it with knowledge of chemical constraints, such as valence, and is a strong empirical regulariser. Note that, in this discrete-feature setting, Noisy Nodes can be seen as a more general case of the BERT-like objectives from Hu et al. [2019]. The main difference is that Noisy Nodes takes a more active role in requiring denoising—as opposed to unmasking, where it is known upfront which nodes have been noised, and the effects of noising are always predictable. When conformers or displacements are available, a richer class of denoising objectives may be imposed on the GNN. Namely, it is possible to perturb the individual nodes’ coordinates slightly, and then require the network to reconstruct the original displacement and/or distances—this time using edge regression on the output latents of the processor GNN. The Noisy Nodes manuscript had shown that, under such perturbations, it is possible to achieve state-of-the-art results on quantum chemical calculations without requiring an explicitly equivariant architecture—only a very deep traditional GNN. Our preliminary results indicate a similar trend on the PCQM4M dataset. Training and regularisation We train our GNN to minimise the mean absolute error (MAE) for predicting the DFT-simulated HOMO-LUMO gap based on the decoded global latent vectors. This objective is combined with any 7
Large-scale graph representation learning with very deep GNNs and self-supervision auxiliary tasks imposed by noisy nodes (e.g. cross-entropy on reconstructing atom and bond types, MAE on regressing denoised displacements). We use the Adam SGD optimiser [Kingma and Ba, 2014] with hyperparameters β = 0.9, 1 β = 0.95. We use a cosine learning rate schedule with initial learning rate η = 10−5 and 50, 000 warm-up steps, 2 peaking at η = 10−4, and decaying over 500, 000 training iterations. We optimise over dynamically-batched data: we fill each training minibatch until exceeding any of the following limits: 1, 024 atoms, 2, 560 bonds, or 64 molecules. To regularise our model, we perform early stopping on the validation MAE, and apply feature dropout [Srivastava et al., 2014] (with p = 0.1) and DropEdge [Rong et al., 2019] (with p = 0.1) at every message passing layer. Evaluation At evaluation time, we exploit several known facts about the HOMO-LUMO gap, and our conformer generation procedure, to achieve “free” reductions in MAE. Firstly, it is known that the HOMO-LUMO gap cannot be negative, and that it is possible for our model to make (very rare) vastly inflated predictions on validation data if it encounters an out-of-distribution molecule. We ameliorate both of these issues by clipping the network’s predictions in the [0, 20] eV range. Secondly, as discussed, for a very small fraction (0.1% of molecules), RDKit was unable to compute conformers. We found that it was useful to fall back to the 50-layer non-conformer GNN in these cases, rather than assuming a default value. The observed reductions in MAE were significant across those specific validation molecules only. Finally, we consistently track the exponential moving average (EMA) of our model’s parameters (with decay rate (cid:15) = 0.9999), and use it for evaluation. EMA parameters are generally known to be more stable than their online counterparts, an observation that held in our case as well. 6 Ensembling and training on validation Once we established the top single-model architectures for both our MAG240M and PCQM4M entries, we found it very important to perform two post-processing steps: (a) re-train on the validation set, (b) ensemble various models together. Re-training on validation data offers a great additional wealth of learning signal, even just by the sheer volume of data available in the OGB-LSC. But aside from this, the way in which the data was split offers even further motivation. On MAG240M, for example, the temporal split implies that validation papers (from 2019) are most relevant to classifying test papers (from 2020)—simply put, because they both correspond to the latest trends in scholarship. However, training on the full validation set comes with a potentially harmful drawback: no held-out dataset would remain to early-stop on. In a setting where overfitting can easily occur, we found the risk to vastly outweigh the rewards. Instead, we decided to randomly partition the validation data into k = 10 equally-sized folds, and perform a cross-validation-style setup: we train k different models, each one observing the training set and k − 1 validation folds as its training data, validating and early stopping on the held-out fold. Each model holds out a different fold, allowing us to get an overall validation estimate over the entire dataset by combining their respective predictions. While this approach may not correspond to the intended dataset splits, we have verified that the scores on individual held-out folds match the patterns observed on models that did not observe any validation data. This gave us further reassurance that no unintended strong overfitting had happened as a result. Another useful outcome of our k-fold approach is that it allowed us a very natural way to perform ensembling as well: simply aggregating all of the k models’ predictions would give us a certain mixture of experts, as each of the k models had been trained on a slightly modified training set. Our final ensembled models employ exactly this strategy, with the inclusion of two seeds per fold. This brings our overall number of ensembled models to 20, and these ensembles correspond to our final submissions on both MAG240M and PCQM4M. 7 Experimental evaluation In this section we provide experimental evidence to substantiate the various claims we have made about the key modifications in our model, hoping to advise future research on large scale graph representation learning. To eliminate any possible confounding effects of ensembling, all results reported in this section will be on a single model, evaluated on the provided validation data. We report average performance and standard deviation over three seeds. MAG240M-LSC We will follow the plots in Figures 1–2, which seek to uncover various contributing factors to our model’s ultimate performance. We proceed one claim at a time. 8
Large-scale graph representation learning with very deep GNNs and self-supervision 0.735 0.730 0.725 0.720 0.715 0.710 0 10000 20000 30000 40000 50000 Training Step ycaruccA Message passing steps 4 steps 2 steps 0 10000 20000 30000 40000 50000 Training Step ycaruccA Eval neighborhood samples ensembled 1 sample 5 samples 0 10000 20000 30000 40000 50000 Training Step ycaruccA Using training labels as features With labels Without labels 0 10000 20000 30000 40000 50000 Training Step ycaruccA Model and node feature ablations Size of neighborhoods sampled Bigger neighborhoods Smaller neighborhoods Figure 1: Ablation studies on our final MAG240M-LSC model, covering aspects of the model depth (left), validation- time sample ensembling (middle-left), using labels as features (middle-right), and subsampling strategy (right). 0.734 0.732 0.730 0.728 0.726 0.724 0.722 0.720 0 10000 20000 30000 40000 50000 Training Step ycaruccA Without mixing unlabeled data 0.7400 0.7375 0.7350 0.7325 0.7300 0.7275 BGRL 0.7250 GRACE 0.7225 Fully supervised 0.7200 0 100000 200000 300000 400000 500000 Training Step ycaruccA Mixing varying fraction unlabeled data 0.7400 0.7375 0.7350 0.7325 0.7300 0.7275 0x unlabeled 1x unlabeled 0.7250 5x unlabeled 0.7225 10x unlabeled 0.7200 0 100000 200000 300000 400000 500000 Training Step ycaruccA Unsupervised learning ablations Mixing unlabeled data and running longer BGRL with 10x unlabeled data GRACE with 10x unlabeled data Fully supervised with 0x unlabeled data Figure 2: Ablation studies on self-supervised learning within our MAG240M-LSC entry, showing the influence of various self-supervised objectives (left), using unlabelled nodes as targets (middle) and running for longer (right). Please note, the left-most plot covers training over 50,000 steps, while the other two cover training over 500,000 steps. Making networks deeper than the patch diameter can help. We find that making the edges in every subsampled patch bidirectional allowed for doubling the message passing steps (to four) with a significant validation F1-score improvement, in spite of the fact that the MPNN was now deeper than the patch diameter. See Figure 1 (left). Ensembling over multiple subsamples helps. We find that averaging our network’s prediction over several randomly subsampled patches at evaluation time consistently improved performance. See Figure 1 (middle-left). Using training labels as features helps. On transductive tasks, we confirm that using the training node label as an additional feature provides a substantial boost to validation performance, if done carefully. See Figure 1 (middle-right). Larger patches help. Providing the model with a larger context (by subsampling more neighbours) proved significantly helpful to our downstream performance. See Figure 1 (right). Self-supervised objectives help—especially BGRL. We first validate that combining a traditional cross-entropy loss with a self-supervised loss is beneficial to final performance observed. Further, we show that BGRL [Thakoor et al., 2021] can significantly outperform GRACE [Zhu et al., 2020] in the large-scale regime. See Figure 2 (left). Self-supervised learning on unlabelled nodes helps. One of the major promises of self-supervised learning is allowing access to a vast quantity of unlabelled nodes, which now can be used as targets. We recover consistent, monotonic gains from incorporating increasing amounts of unlabelled nodes within our training routine. See Figure 2 (middle). Self-supervised learning allows for more robust models. Finally, the regularising effect of self-supervised learning means that we can train our models for 10× longer without suffering any overfitting effects. See Figure 2 (right). PCQM4M-LSC We follow Figure 3, which investigates key design aspects in our PCQM4M-LSC models. Using conformer-based features helps. Utilising features based on RDKit conformers, in the manner described before, proved beneficial to final performance. Note that the gains over our 50-layer non-conformer model are irrelevant, given 9
Large-scale graph representation learning with very deep GNNs and self-supervision 0.150 0.145 0.140 0.135 0.130 0.125 0.120 0 2 4 Global Step 1e5 EAM dilaV Ablations from Conformer model defaults: Use Conformers (True) Message Passing Steps (32) Use Noisy Nodes (True) Latent/Hidden Size (256/1024) False 8 False 512/512 True * 16 True * 256/1024 * 32 * 40 50 0 2 4 0 2 4 0 2 4 Global Step 1e5 Global Step 1e5 Global Step 1e5 0.150 0.145 0.140 0.135 0.130 0.125 0.120 0 2 4 Global Step 1e5 EAM dilaV Ablations from Non-Conformer model defaults: Use Conformers (False) Message Passing Steps (50) Use Noisy Nodes (True) Latent/Hidden Size (512/512) False * 8 False 512/512 * True 16 True * 256/1024 32 40 50 * 0 2 4 0 2 4 0 2 4 Global Step 1e5 Global Step 1e5 Global Step 1e5 Figure 3: Ablation studies on our final PCQM4M-LSC models, covering aspects of conformer usage (left), message pass- ing steps (middle-left), Noisy Nodes regularisation (middle-right) and latent/hidden dimensions (right). Results shown both for our conformer (top) and non-conformer model (bottom). * indicates our final model’s chosen hyperparameter. that the non-conformer model is only applied over molecules where conformers cannot be computed. See Figure 3 (top-left and bottom-left). Deeper models help. We demonstrate consistent, monotonic gains for larger numbers of message passing steps, at least up to 32 layers—and in the case of the non-conformer model, up to 50 layers. See Figure 3 (top-middle-left and bottom-middle-left). Noisy Nodes help. Lastly, we show that the regulariser proposed in Noisy Nodes [Godwin et al., 2021] proved very effective for this quantum-chemical task as well. It was the key behind the monotonic improvements of our models with depth. Note, for example, that removing Noisy Nodes from our best performing model makes its performance comparable with models that are at least twice as shallow. See Figure 3 (top-middle-right and bottom-middle-right). Wider message functions help. Towards the end of the contest, we noted that performance gains are possible when favouring wider message functions (in terms of hidden size of their MLP layers) opposed to the latent size of the GNN. We subsequently noticed that such a regime (256 latent dimensions, 1,024-dimensional hidden layers) consistently improved our non-conformer model as well. See Figure 3 (top-right and bottom-right). 8 Results and Discussion Our final ensembled models achieved a validation F1-score of 77.10% on MAG240M, and validation MAE of 0.110 on PCQM4M. Translated on the LSC test sets, we recover 75.19% test F1-score on MAG240M and 0.1205 test MAE on PCQM4M. We incur a minimal amount of distribution shift, which is a testament to our principled ensembling and post-processing strategies, in spite of using labels as inputs for MAG240M or training on validation for both tasks. Our entries have been designated as awardees (ranked in the top-3) on both MAG240M and PCQM4M, solidifying the impact that very deep expressive graph neural networks can have on large scale datasets of industrial and scientific relevance. Further, we demonstrate how several recently proposed auxiliary objectives for GNN training, such as BGRL 10
Large-scale graph representation learning with very deep GNNs and self-supervision [Thakoor et al., 2021] and Noisy Nodes [Godwin et al., 2021] can both be highly impactful at the right dataset scales. We hope that our work serves towards resolving several open disputes in the community, such as the utility of very deep GNNs, and the influence of self-supervision in this setting. In many ways, the OGB has been to graph representation learning what MNIST has been to computer vision. We hope that OGB-LSC is only the first in a series of events designed to drive research on GNN architectures forward, and sincerely thank the OGB team for all their hard work and effort in making a contest of this scale possible and accessible. References Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Victor Bapst, Thomas Keck, A Grabska-Barwin´ska, Craig Donner, Ekin Dogus Cubuk, Samuel S Schoenholz, Annette Obika, Alexander WR Nelson, Trevor Back, Demis Hassabis, et al. Unveiling the predictive power of static structure in glassy systems. Nature Physics, 16(4):448–454, 2020. Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor, Martin Blais, Benedek Rózemberczki, Michal Lukasik, and Stephan Günnemann. Scaling graph neural networks with approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2464–2473, 2020. Marc Brockschmidt. Gnn-film: Graph neural networks with feature-wise linear modulation. In International Conference on Machine Learning, pages 1144–1152. PMLR, 2020. Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velicˇkovic´. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. Taco S Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical cnns. arXiv preprint arXiv:1801.10130, 2018. Pim de Haan, Maurice Weiler, Taco Cohen, and Max Welling. Gauge equivariant mesh cnns: Anisotropic convolutions on geometric graphs. arXiv preprint arXiv:2003.05425, 2020. Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263–1272. PMLR, 2017. Jonathan Godwin, Thomas Keck, Peter Battaglia, Victor Bapst, Thomas Kipf, Yujia Li, Kimberly Stachenfeld, Petar Velicˇkovic´, and Alvaro Sanchez-Gonzalez. Jraph: A library for graph neural networks in jax., 2020. URL http://github.com/deepmind/jraph. Jonathan Godwin, Michael Schaarschmidt, Alexander Gaunt, Alvaro Sanchez-Gonzalez, Yulia Rubanova, Petar Velicˇkovic´, James Kirkpatrick, and Peter Battaglia. Very deep graph neural networks via noise regularisation. arXiv preprint arXiv:2106.07971, 2021. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020. Thomas A Halgren. Merck molecular force field. i. basis, form, scope, parameterization, and performance of mmff94. Journal of computational chemistry, 17(5-6):490–519, 1996. William L Hamilton. Graph representation learning. Synthesis Lectures on Artifical Intelligence and Machine Learning, 14(3):1–159, 2020. 11
Large-scale graph representation learning with very deep GNNs and self-supervision William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. arXiv preprint arXiv:1706.02216, 2017. Jessica B Hamrick, Kelsey R Allen, Victor Bapst, Tina Zhu, Kevin R McKee, Joshua B Tenenbaum, and Pe- ter W Battaglia. Relational inductive bias for physical construction in humans and machines. arXiv preprint arXiv:1806.01203, 2018. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021. Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R Benson. Combining label propagation and simple models out-performs graph neural networks. arXiv preprint arXiv:2010.13993, 2020. Chaitanya Joshi. Transformers are graph neural networks. The Gradient, 2020. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. MNIST classification with deep convolutional neural networks. Advances in neural information processing systems, 25:1097–1105, 2012. Greg Landrum. Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling, 2013. Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Renjie Liao, Marc Brockschmidt, Daniel Tarlow, Alexander L Gaunt, Raquel Urtasun, and Richard Zemel. Graph partition neural networks for semi-supervised classification. arXiv preprint arXiv:1803.06272, 2018. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Jonathan Masci, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. Geodesic convolutional neural networks on riemannian manifolds. In Proceedings of the IEEE international conference on computer vision workshops, pages 37–45, 2015. Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Sungmin Bae, et al. Chip placement with deep supervised learning. arXiv preprint arXiv:2004.10746, 2020. Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663, 2020. Maho Nakata and Tomomi Shimazaki. Pubchemqc project: a large-scale first-principles electronic structure database for data-driven chemistry. Journal of chemical information and modeling, 57(6):1300–1308, 2017. Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning mesh-based simulation with graph networks. arXiv preprint arXiv:2010.03409, 2020. Anthony K Rappé, Carla J Casewit, KS Colwell, William A Goddard III, and W Mason Skiff. Uff, a full periodic table force field for molecular mechanics and molecular dynamics simulations. Journal of the American chemical society, 114(25):10024–10035, 1992. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. arXiv preprint arXiv:1907.10903, 2019. Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael Bronstein, and Federico Monti. Sign: Scalable inception graph neural networks. arXiv preprint arXiv:2004.11198, 2020. 12
Large-scale graph representation learning with very deep GNNs and self-supervision Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. MNIST large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015. Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, pages 8459–8468. PMLR, 2020. Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. arXiv preprint arXiv:2102.09844, 2021. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93–93, 2008. Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014. Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al. A deep learning approach to antibiotic discovery. Cell, 180(4):688–702, 2020. Otilia Stretcu, Krishnamurthy Viswanathan, Dana Movshovitz-Attias, Anthony Platanios, Sujith Ravi, and Andrew Tomkins. Graph agreement models for semi-supervised learning. 2019. Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Rémi Munos, Petar Velicˇkovic´, and Michal Valko. Bootstrapped representation learning on graphs. arXiv preprint arXiv:2102.06514, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. Petar Velicˇkovic´, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018. Petar Velicˇkovic´, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution of graph algorithms. arXiv preprint arXiv:1910.10593, 2019. Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396–413, 2020a. Shuzhe Wang, Jagna Witek, Gregory A Landrum, and Sereina Riniker. Improving conformer generation for small rings and macrocycles based on distance geometry and experimental torsional-angle preferences. Journal of chemical information and modeling, 60(4):2044–2058, 2020b. Yangkun Wang, Jiarui Jin, Weinan Zhang, Yong Yu, Zheng Zhang, and David Wipf. Bag of tricks for node classification with graph neural networks. arXiv preprint arXiv:2103.13355, 2021. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International conference on machine learning, pages 6861–6871. PMLR, 2019. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolu- tional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 974–983, 2018. Lingfan Yu, Jiajun Shen, Jinyang Li, and Adam Lerer. Scalable graph neural networks for heterogeneous graphs. arXiv preprint arXiv:2011.09679, 2020. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. 2002. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive representation learning. arXiv preprint arXiv:2006.04131, 2020. Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. arXiv preprint arXiv:1911.07323, 2019. 13
