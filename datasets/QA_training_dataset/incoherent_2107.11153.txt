Constellation: Learning relational abstractions over objects for compositional imagination James C.R. Whittington 1 2 Rishabh Kabra 3 Loic Matthey 3 Christopher P. Burgess 4 2 Alexander Lerchner 3 Abstract Relational latents Learning structured representations of visual scenes is currently a major bottleneck to bridging GNN Relational Sequential perception with reasoning. While there has been encoder LSTM relational decoder exciting progress with slot-based models, which learn to segment scenes into sets of objects, learn- ing configurational properties of entire groups of Abstract entities objects is still under-explored. To address this problem, we introduce Constellation, a network Abstraction mask that learns relational abstractions of static visual Instantiation scenes, and generalises these abstractions over sensory particularities, thus offering a potential Entities basis for abstract relational reasoning. We further show that this basis, along with language associa- tion, provides a means to imagine sensory content eM nO coN de et r dM eO coN de et r in new ways. This work is a first step in the ex- plicit representation of visual relationships and Image using them for complex cognitive procedures. Figure 1. Constellation model overview. A pre-trained MONet 1. Introduction encoder is used to decompose scenes of objects into slots with a common feature representation. Each slot either contains an Humans and other animals understand the world at an ab- “object” representation or an “empty” representation. Features ir- stracted level. From a line of ducks or a queue of people, relevant to relationships are abstracted away by applying a learned we understand the common abstraction of “lineness”. This mask, m. Relational structure, r, is inferred from abstracted ob- abstract lineness, a concept divorced from sensory particu- jects, a, via a permutation invariant encoder. Finally, Constellation lars (ducks or humans), is what we reason over when saying decodes the abstracted objects using an LSTM. Abstracted away “that line is long” or “Alice is two ahead of me in the queue”. features can be ’filled in’ using encoded objects o and passed Aside from offering a basis to reason over, knowledge of through the MONet decoder for visual analysis. abstract concepts allows understanding and imagination of never-seen-before sensory configurations - you can under- novel experiences, and thus afford out-of-distribution gen- stand and imagine a house built from monkeys, because you eralisation and transfer (Dittadi et al., 2020; van Steenkiste separately understand the two components; monkeys and et al., 2019; Higgins et al., 2017). As in the line example, how houses are configured. Recent approaches to learning sensory scenes often have underlying relational structure. abstractions suggest different parts of sensory experience Factorising structure from sensory content, similarly pro- be represented separately i.e. in a factorised fashion (Gr- vides a means to generalise relational knowledge (Behrens eff et al., 2020; Higgins et al., 2017; 2018a). Factorised et al., 2018; Whittington et al., 2018; 2020). sensory representations are easily re-combined to represent Using such relational abstraction, we aim to learn and un- 1University of Oxford 2Work done at DeepMind derstand concepts of relationships between objects, and 3DeepMind 4Wayve. Correspondence to: JCRW <jcrwhit- show this understanding affords imagination of objects in tington@gmail.com>, AL <lerchner@google.com>. never-seen-before configurations. For relational abstraction Workshop on Self-Supervised Learning for Reasoning and Percep- to work, sensory scenes must contain relational regularities tion at ICML 2021, Copyright 2021 by the author(s). i.e. there exists subsets of object properties (e.g. x, y loca- 1202 luJ 32 ]GL.sc[ 1v35111.7012:viXra
Constellation: Learning relational abstractions over objects for compositional imagination sual scene? To address this, we use a “filling in” procedure that takes the partially-specified generated objects, com- pares them to the full objects in the current scene and fills in the remaining properties accordingly (details discussed later). To obtain object representations in the first place, we use a scene decomposition network, in particular, MONet (Burgess et al., 2019)). MONet takes as input a visual scene (x), containing variable numbers of objects, and encodes the objects into entities (o ) with a common disentangled i representational format (Figure 1 bottom). We will consis- tently use subscript i to index different object vectors, and subscript j to index an object feature representation. Slots Figure 2. Dataset with relational structure between objects. can either consist of an object representation or an “empty” Dataset of images containing multiple objects in “super-structures”. representation. To abstract over object features (Figure 1 Each “super-structure” is defined by a set of generative factors: middle), we use a learned mask, m, where m must learn position, number of objects, orientation, curviness. The visual to ignore irrelevant features (colour, size etc), and keep the features of each object are randomly chosen. relevant ones (x,y). The mask is applied to each object (slot) element-wise, to obtain an abstract entity a = o (cid:12) m, i i tion) described by generative relational factors. For lines (cid:80) where m > 0 and m = 1. j j j of ducks (or arbitrary objects), these underlying factors de- scribing lines might consist of number of objects, length of Generative model. While the procedure from relational line, curviness etc. The problem of relational abstraction generative factors, r, to visual scene (x) can be framed can thus be approached as an inverse problem; inferring as a hierarchical generative model, here we use a pre- these generative relational factors from data. We demon- trained MONet module to provide a slotted scene repre- strate this approach of relational abstraction is sufficient sentation, o i. Thus, in this work, we need only recon- to build Constellation; a network that learns and under- struct a, rather than x, i.e. we generate only a subset stands relational concepts. We then show that “language” of each object’s feature representations (subset a i of ob- symbols (e.g. ’circle’) can be bound to abstract relational jects o i) and thus want to maximise the log-probability, (cid:80) concepts, which allows for compositional (re-)imagination ln P (a) = i ln P (a i). We assume a generative model of never-seen-before object configurations (e.g. a line of P (a, r) = P (a | r)P (r), where r is the relational genera- hats (re-)imagined in a circle). tive factors. P (r) is Gaussian, and P (a i | r) ∼ N (µ i, σ i). The mean vectors, µ , for each object (slot) are produced i sequentially via an LSTM (Hochreiter & Schmidhuber, 2. Constellation - a network for relational 1997); µ = f (LST M (g(r), LST M )), where f (· · · ) i i i−1 abstraction and imagination and g(· · · ) are neural networks (Figure 1 Right). To learn about relational knowledge and infer relational gen- Inference network. For the recognition model for rela- erative factors, we use generative modelling, and in particu- tional structure, we choose a diagonal Gaussian latent pos- lar, the variational autoencoder framework (VAEs; (Kingma terior Q(r | a) ∼ N (µ , σ ). Since the slots containing a q q i & Welling, 2013; Rezende et al., 2014)). A key point in our may be randomly ordered, we use a permutation invariant learning of abstractions is that we do not try to predict all encoder (e.g. a graph neural network; GNN; (Battaglia et al., factors of the objects (see dataset examples in Figure 2). In- 2018)). We use the “globals” of the GNN to parameterise stead we only generate a subset of each object’s properties: µ and σ of the variational posterior. q q i.e. all the x, y positions of the objects and nothing else. Filling in abstracted visual features. Generating ab- This subset, an abstraction over all object features sufficient stracted object features is simple in ancestral sampling (from to capture 2D arrangements, is learned by our model in an r) as they can sampled from MONet’s prior. However, when unsupervised manner. Abstracting over object features, and encoding an image we need to fill back in the correct object only reconstructing these features, allows the network to features. To do this, we use a non-parametric matching learn the generative relational factors alone. This induces an algorithm (Hungarian algorithm; (Kuhn, 1955)). After pair- implicit factorisation; relational information of the scene is ing up encoded object o to decoded “abstract object” ˆa , separated from the sensory content. It is this separation that i i the masked out features are replaced using o (Figure 1, supports generalisation and imagination. This separation, i “instantiation” arrows). however, provides a quandary - pure relational knowledge lacks sensory knowledge, so how can we imagine a full vi-
Constellation: Learning relational abstractions over objects for compositional imagination Training. We use an augmented VAE loss. In particular: Permutation invariant reconstruction error: Our main goal is to accurately reconstruct abstracted relational features ˆa (as we use a pre-trained MONet, we do not reconstruct i x directly). However, the encoded and generated objects, a and ˆa , may be ordered differently, preventing simple i i slot pairing. Instead we use the Hungarian matching algo- rithm (Kuhn, 1955) to pair slots for the reconstruction error. L = 1 (cid:80) || a − ˆa ||2. rec 2 (i,j)−pairs i j Disentangling pressure: To encourage disentangling (to fa- cilitate subsequent language association and compositional Figure 3. Constellation learns disentangled relational knowl- imagination), we use a β-VAE (Higgins et al., 2017) KL edge that generalises over sensory properties. Latent traversals regularizer: L = βD (Q(r | x) || P (r)). of two different input images, 4 traversals points are shown for reg KL each latent dimension. Each latent dimension captures a single Mask entropy: We use a mask entropy loss to prevent the relational factor, that smoothly changes across the latent sweep. abstraction mask collapsing onto a trivial single feature These relational factors are the same no matter the sensory particu- (cid:80) dimension; L entropy = − j m j ln m j. lars of the objects i.e. they generalise. All other relational latents are unused and have no effects when traversed. Conditioning term: Since gradients cannot flow through masked-out features, it is difficult to “unlearn” an abstrac- tion mask. This is problematic if the incorrect features are abstracted initially. To remedy this, and allow gradi- ent information to flow through masked object features, (cid:80) we introduce an additional loss L = (1 − condition j m ) | L∗ − γ |. γ is learnable and predicts the re- j recj j j construction loss for that feature; L∗ . L∗ is obtained recj recj using the same pairings as L , but using unmasked recj representation so that ˆa and o have the same scaling i.e. (cid:16) (cid:17)2 L∗ = 1 (cid:80) o − ˆaj . L is decayed recj 2 (i,j)−pairs i mj condition to 0 throughout training. Re-ordering loss: Parsimonious structured representations would generate nearby objects in sequence i.e. for a line, starting at one end and finishing at the other. To encourage this, we add a penalty to the difference between successively generated objects. L = (cid:80) || ˆa − ˆa ||2. reorder i=1 i i−1 We optimise L = L + L + L + L + reg rec entropy re−order L with Adam (Kingma & Ba, 2014) and GECO condition (Rezende & Viola, 2018) - a constrained optimisation method. We use a learning rate of 1e − 3 annealed to 1e − 4. 2.1. Learning disentangled relational knowledge We visualise the latent traversals of Constellation by encod- ing an image and sweeping over single relational latent di- mensions (Figure 3). Constellation learns the five relational factors in separate latents, successfully disentangling them and leaving the rest of its features unused (see Appendix for a MIG analysis). Constellation learns parsimonious re- lational knowledge representations that generate objects in an order obeying the structural properties of the line (Figure 4). Note the MONet representations are unordered. This re- ordering is encouraged with L , though it is observed reorder (to a lesser extent) without this loss. Finally, we note the tupnI dedocnE detareneG Figure 4. Objects re-ordered according to prior learned rela- tional knowledge. Top: Input image. Middle: MONet latent dimensions corresponding to x,y. Size of point corresponds to order in the sequence. Bottom: Constellation decoded x,y dimen- sions, where objects are re-ordered according to the learned line structure (see size of points compared to the panels above). relational factors generalise over different sensory objects; relational knowledge is factorised from sensory particulars. 3. Imagination from language Imagination is crucial to intelligent behaviour. When in- structed to build a tower out of blocks scattered on the floor, first imagining how those blocks would look like in a tower is useful as it acts as a conceptual goal state. This, how- ever, relies on knowing what ’tower’ means - i.e. one needs to understand the concept of a vertical line and that it is labelled a ’tower’. Labelling structural forms is necessary for systems to be instructed to imagine X out of Y e.g. a house out of monkeys. We adapt SCAN (Higgins et al., 2018b), a network architecture that binds language symbols, l, to abstractions over disentangled representations, r (see Appendix for details). Constraining relational knowledge with language. This association allows “ancestral” generation of visual scenes
Constellation: Learning relational abstractions over objects for compositional imagination A B Straight Straight Straight Arch Arch Input Left Left-centre Centre Right-centre Right Input + flat + 45° + tower + top + bottom Horseshoe Horseshoe Horseshoe Horseshoe Horseshoe Input Top Top-middle Middle Bottom-middle Bottom Input + top + middle + bottom + 4 + 5 Circle Circle Circle Circle Circle + top + top-centre + centre + right-centre + right Input Straight (fail!) Bend Arch Horseshoe Circle Input + left + left-middle + middle + bottom-middle + bottom Figure 5. Imagining novel objects in different configurations. A After encoding the relationships of a visual scene, new structural constraints can be imposed (above each image). B Similarly, several constraints can be combined and enforced. based on language input (Higgins et al., 2018b). More inter- the impetus is on exploiting view-point invariances of intrin- estingly though, we can re-interpret scenes of objects under sic geometric relationships of objects. Here we are instead different relational constraints e.g. from a relational encod- interested in explicitly representing the relational structure ing of 5 squares in a straight line, we can impose ’circle’ of a scene of objects and using this representation to enable from language and re-imagine those 5 squares in a circle. complex cognitive operations like imagination. Performing To impose structural constraints on relational embeddings, inference on novel objects as recombination of sub-objects we encode a to get r, sample the predicted ˜r from l, then through a generative model has also been explored in (Lake overwrite r with the elements of ˜r that have a high relevance et al., 2015), and in (Kemp & Tenenbaum, 2008) the infer- for that relational feature (i.e. low σ ; see Appendix for ence over particular structural forms are considered. s details). The elements with low σ are the relational factors s described by language, thus this provides a mechanism for 5. Conclusion imposing “imagined” structure on existing inferred structure (i.e. turns a straight line of objects into a circle or objects). We introduced Constellation, a network that learns structural Examples of imagination are shown in Figure 5A. Visual regularities among sets of objects and represents these rela- scenes of randomly placed objects are re-imagined in lines tionships, in a disentangled manner, divorced from sensory of various forms. particularities. We have shown how these latent embeddings can be associated to language, which allows re-imagination Compositional constraints. Our model can perform ”com- of the current set of objects in new ways. positional imagination” by providing language input corre- sponding to combinations of properties. For example, ’left While we believe this work provides an exciting step in circle’ should re-imagine the objects into a circle on the learning relational abstractions and enacting imaginations, left. Figure 5B shows this for a variety of different language there is still much work to be done. For example, we use symbols, and for compositions of 2 or 3 language symbols. synthetically generated datasets that lack the visual com- plexity or noise of real life scenes. Nevertheless, our work 4. Related work provides a platform for further research; explicitly represent- ing temporal relationships of stereotyped interacting bodies Using permutation invariant encoders within the VAE frame- (e.g. courting or avoidance behaviours), imagination-based work for relational understanding of scenes has been ex- conceptual goal states, analogical reasoning on structures, plored with capsule networks (Kosiorek et al., 2019). There and hierarchical scene decomposition, to name just a few.
Constellation: Learning relational abstractions over objects for compositional imagination References Kemp, C. and Tenenbaum, J. B. The discovery of structural form. Proceedings of the National Academy of Sciences, Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez- 105(31):10687–10692, 2008. ISSN 0027-8424. doi: Gonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti, 10.1073/pnas.0802631105. URL http://www.pnas. A., Raposo, D., Santoro, A., Faulkner, R., Gulcehre, C., org/cgi/doi/10.1073/pnas.0802631105. Song, F., Ballard, A., Gilmer, J., Dahl, G., Vaswani, A., Allen, K., Nash, C., Langston, V., Dyer, C., Heess, N., Kingma, D. P. and Ba, J. L. Adam: A Method for Stochastic Wierstra, D., Kohli, P., Botvinick, M., Vinyals, O., Li, Y., Optimization. arXiv preprint arxiv:1412.6980, 0, 2014. and Pascanu, R. Relational inductive biases, deep learn- ISSN 09252312. doi: http://doi.acm.org.ezproxy.lib.ucf. ing, and graph networks. arXiv, pp. 1–38, 2018. URL edu/10.1145/1830483.1830503. URL http://arxiv. http://arxiv.org/abs/1806.01261. org/abs/1412.6980. Behrens, T. E. J., Muller, T. H., Whittington, J. C. R., Kingma, D. P. and Welling, M. Auto-Encoding Variational Mark, S., Baram, A. B., Stachenfeld, K. L., and Kurth- Bayes. arXiv preprint arXiv:1312.6114, 0(Ml):1–14, nelson, Z. What Is a Cognitive Map? Organizing Knowl- 2013. ISSN 1312.6114v10. doi: 10.1051/0004-6361/ edge for Flexible Behavior. Neuron, 100(2):490–509, 201527329. URL http://arxiv.org/abs/1312. 2018. ISSN 0896-6273. doi: 10.1016/j.neuron.2018. 6114. 10.002. URL https://www.cell.com/neuron/ fulltext/S0896-6273(18)30856-0. Kosiorek, A. R., Sabour, S., Teh, Y. W., and Hinton, G. E. Stacked Capsule Autoencoders. arXiv, 2019. URL http: Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins, //arxiv.org/abs/1906.06818. I., Botvinick, M., and Lerchner, A. MONet: Unsuper- vised Scene Decomposition and Representation. arXiv, Kuhn, H. W. The Hungarian method for the assignment pp. 1–22, 2019. URL http://arxiv.org/abs/ problem. Naval Research Logistics Quarterly, 2(1-2): 1901.11390. 83–97, 3 1955. ISSN 00281441. doi: 10.1002/nav. 3800020109. URL http://doi.wiley.com/10. Dittadi, A., Tra¨uble, F., Locatello, F., Wu¨thrich, M., 1002/nav.3800020109. Agrawal, V., Winther, O., Bauer, S., and Scho¨lkopf, B. On the transfer of disentangled representations in Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. realistic settings. CoRR, abs/2010.14407, 2020. URL Human-level concept learning through probabilistic pro- https://arxiv.org/abs/2010.14407. gram induction. Science, 350(6266):1332–1338, 12 2015. ISSN 10959203. doi: 10.1126/science.aab3050. Greff, K., van Steenkiste, S., and Schmidhuber, J. On the binding problem in artificial neural networks. CoRR, Rezende, D. J. and Viola, F. Taming VAEs. arXiv, 2018. abs/2012.05208, 2020. URL https://arxiv.org/ URL http://arxiv.org/abs/1810.00597. abs/2012.05208. Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., meta-optimization synthesis and Approximate Inference in Deep Botvinick, M., Mohamed, S., and Lerchner, A. β-VAE: Generative Models. arXiv preprint arXiv:1401.4082, Learning basic visual concepts with a constrained varia- 0, 1 2014. ISSN 10495258. doi: 10.1051/0004-6361/ tional framework. International Conference on Learning 201527329. URL http://arxiv.org/abs/1401. Representations, 0, 7 2017. 4082. Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey, L., van Steenkiste, S., Locatello, F., Schmidhuber, J., and Rezende, D., and Lerchner, A. Towards a Definition of Bachem, O. Are disentangled representations helpful for Disentangled Representations. arXiv, pp. 1–29, 12 2018a. abstract visual reasoning? CoRR, abs/1905.12506, 2019. URL http://arxiv.org/abs/1812.02230. URL http://arxiv.org/abs/1905.12506. Higgins, I., Sonnerat, N., Matthey, L., Pal, A., Burgess, C. P., Watters, N., Matthey, L., Borgeaud, S., Kabra, R., Bosnjak, M., Shanahan, M., Botvinick, M., Hassabis, D., and Lerchner, A. Spriteworld: A flexible, con- and Lerchner, A. SCAN: Learning Hierarchical Compo- figurable supervised learning environment. sitional Visual Concepts. arXiv, pp. 1–24, 2018b. ISSN https://github.com/deepmind/spriteworld/, 2019. 1346-9843. doi: 10.1186/s12884-017-1520-4. URL URL https://github.com/deepmind/ http://arxiv.org/abs/1707.03389. spriteworld/. Hochreiter, S. and Schmidhuber, J. Long Short-term Mem- Whittington, J. C. R., Muller, T. H., Mark, S., Barry, C., and ory. Neural Computation, 9(8):17351780, 1997. Behrens, T. E. J. Generalisation of structural knowledge
Constellation: Learning relational abstractions over objects for compositional imagination in the hippocampal-entorhinal system. Advances in Neu- ral Information Processing Systems 31, 31:8493–8504, 2018. Whittington, J. C. R., Muller, T. H., Mark, S., Barry, C., Burgess, N., Behrens, T. E. E., Chen, G., Barry, C., Burgess, N., and Behrens, T. E. E. The Tolman- Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation. Cell, 183(5):1249–1263, 11 2020. ISSN 00928674. doi: 10.1016/j.cell.2020.10.024.
Constellation: Learning relational abstractions over objects for compositional imagination A. Dataset We designed a dataset, using Spriteworld (Watters et al., 2019), which consists of visual scenes of objects with reg- ular structural properties. In particular, we chose the re- lational structure to be between object positions, i.e. the arrangements of the objects are all variants of lines, where the line has varying properties: number of objects, curviness of line, position, and orientation. Each object within the ‘super-structure’ is generated from an additional set of fac- tors; colour, shape, size, position, orientation. These factors are randomly sampled, except for the position factors, which are set according to the relational factors detailed above. See Figure 2 for example samples of this dataset. This dataset thus conforms to our desiderata - consistent relational struc- ture across different sensory objects. Though we chose the Figure 6. Constellation learns disentangled relational factors. relational structure to be embedded within object positions, Mutual information between Constellation latents and ground truth it could have been any of the underlying factors. The only factors. requirement is that, for each super-structure, there is a pro- jection that describes a stereotyped manifold in object factor ˆ l l space. <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""BBBBLLLLPPPPccccqqqqXXXXGGGGggggrrrrUUUUyyyy1111TTTTUUUUXXXXHHHHNNNNYYYYhhhhzzzzFFFFFFFFddddrrrrccccXXXXssss===="""">>>>AAAAAAAAAAAABBBB////XXXXiiiiccccZZZZZZZZAAAA9999SSSSwwwwNNNNBBBBEEEEIIIIbbbbnnnn4444lllleeeeMMMMXXXX1111FFFFLLLLmmmm8888MMMMggggWWWWIIIIRRRRwwwwJJJJ4444KKKKWWWWQQQQRRRRvvvvLLLLCCCCOOOOYYYYDDDDkkkkxxxxDDDD2222NNNNnnnnvvvvJJJJkkkktttt22229999YYYY3333ddddOOOOCCCCEEEEffffwwwwTTTT9999hhhhqqqqZZZZyyyyeeee2222////hhhhYYYYbbbbffff4444tttt7777yyyyRRRRWWWWaaaaDDDDCCCCzzzz77778888MMMM4444MMMMMMMM////MMMMGGGGsssseeeeAAAAGGGGPPPPeeee////bbbbKKKKaaaayyyyttttbbbb2222xxxxuuuuFFFFbbbbddddLLLLOOOO7777tttt7777++++wwwwffffllllwwww6666OOOOWWWWiiiiRRRRJJJJNNNNWWWWZZZZNNNNGGGGIIIIttttKKKKddddggggBBBBggggmmmmuuuuGGGGJJJJNNNN5555CCCChhhhYYYYJJJJ9999aaaaMMMMyyyyEEEECCCCwwwwddddjjjjCCCC5555zzzzffffLLLLttttJJJJ6666YYYYNNNNjjjj9999QQQQDDDDTTTTmmmmPPPPWWWWllll2222SSSSkkkkeeeeMMMMggggppppQQQQSSSSssss99999999iiiiTTTTBBBBccccRRRRCCCCmmmmYYYYjjjjYYYYooooVVVV7777yyyyaaaaNNNNwwww99993333FFFFffffwwwwccccKKKKppppBBBBHHHHYYYY1111DDDD++++6666QQQQ0000jjjjmmmmkkkkiiiimmmmkkkkAAAAppppiiiiTTTTNNNNffff3333YYYYuuuuyyyynnnnRRRRCCCCOOOOnnnnggggssss1111KKKKvvvvccccSSSSwwwwmmmmNNNNAAAAJJJJGGGGbbbbGGGGuuuuRRRRUUUUUUUUkkkkMMMM////11110000vvvvvvvvHHHHMMMMPPPPbbbbPPPPKKKK0000AAAA0000jjjjbbbbZZZZ9999CCCCdddd66667777++++7777UUUUiiiiJJJJNNNNGGGGYYYYqqqqAAAA1111uuuuZZZZbbbbWWWWiiiiWWWWcccc5555llllYYYYttttXXXX////WWWWbbbbTTTTLLLLAAAAssssaaaawwwwGGGGccccmmmmkkkkqqqqhhhhttttffff9999llllKKKKssss4444QQQQaaaabbbbooooYYYYmmmmiiiiYYYYCCCCBBBBccccjjjjNNNN7777PPPPCCCCHHHHXXXXLLLLNNNNKKKKIIIIqqqqppppBBBBUUUUIIII1111tttt3333uuuu7777ddddEEEEwwww0000ooooWWWWggggNNNNKKKK1111llllDDDD////OOOOXXXXzzzzVVVV6666FFFF1111UUUUffffMMMMtttt333311119999WWWW6666jjjjeeee5555NNNNUUUUUUUU4444ggggVVVVMMMM4444BBBBxxxx++++uuuuooooAAAA555533330000IIIIAAAAmmmmUUUUFFFFDDDDwwwwAAAAqqqq////wwww5555jjjjwwww7777777788886666HHHH88887777kkkkooooLLLLTTTThhhh5555zzzzzzzzHHHH8888CCCC++++ffffrrrrFFFF6666VVVVYYYYlllljjjj4444====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""22227777PPPP8888cccczzzznnnnxxxxMMMMTTTTLLLL7777VVVVnnnn1111qqqqXXXXMMMM111166662222cccc111144449999yyyyAAAA===="""">>>>AAAAAAAAAAAACCCCBBBBXXXXiiiiccccZZZZZZZZBBBBNNNNSSSS8888NNNNAAAAEEEEIIIIYYYY33339999aaaavvvvWWWWrrrr6666hhhhHHHHLLLL8888EEEEiiiieeeeCCCCggggllllEEEEUUUUGGGGPPPPRRRRSSSS8888eeeeKKKK9999ggggPPPPaaaaEEEELLLLZZZZbbbbDDDDfffftttt0000tttt1111NNNN2222JJJJ0000UUUUSSSSssssjjjjZZZZPPPP++++FFFFVVVVbbbb99997777EEEEqqqq7777////DDDDiiii7777////FFFFTTTTZZZZuuuuDDDDttttggggPPPPLLLLPPPPrrrrwwwwzzzzwwww8888yyyy8888YYYYccccKKKKZZZZBBBBttttffff9999ttttiiiioooobbbbmmmm1111vvvvbbbbOOOO9999XXXXdddd2222tttt7777++++wwwweeeeGGGGRRRRffffXXXXzzzzSSSS1111XXXXGGGGqqqqCCCCOOOO2222QQQQmmmmMMMMeeeeqqqqHHHH2222JJJJNNNNOOOOZZZZOOOO0000AAAAwwwwwwww44447777SSSSeeeeKKKKYYYYhhhhFFFFyyyy2222gggguuuunnnn99990000WWWW++++NNNN6666NNNNKKKKssss1111gggg++++wwwwTTTTyyyyhhhhggggccccBBBBjjjjyyyySSSSJJJJGGGGMMMMBBBBhhhhppppaaaaNNNNuuuu++++wwwwDDDDAAAAJJJJoooo8888yyyyffffYYYYMMMMhhhh4444nnnngggg////ttttuuuutttttttt0000FFFF++++GGGGssssgggg1111ddddCCCCHHHHZZZZXXXXRRRRHHHHttttoooo////////iiiiggggmmmmqqqqaaaaAAAASSSSCCCCMMMMddddaaaaDDDDzzzzwwww3333ggggSSSSDDDDDDDDCCCChhhhjjjjhhhhNNNNKKKK////5555qqqqaaaaYYYYJJJJJJJJllllMMMM8888ppppggggOOOODDDDEEEEgggguuuuqqqqgggg2222yyyyxxxxeeeeeeee5555ccccGGGGGGGGXXXXkkkkRRRRLLLLEEEEyyyyTTTT4444KKKKzzzzUUUUPPPP99992222ZZZZFFFFhhhhooooPPPPRRRReeeehhhhqqqqSSSSzzzz22221111KKKKuuuu5555QQQQmmmmyyyyYYYYvvvv++++jjjjWWWWBBBBccccBBBBEEEENNNNEEEEKKKKxxxxMMMMhhhhWWWWiiii2222yyyyBBBBjjjjMMMMkkkkmmmmBBBBSSSSrrrrIIIIccccGGGGqqqqXXXXccccggggddddggggppppLLLLHHHHFFFFGGGGTTTTFFFFEEEECCCCffffGGGG4444AAAAEEEE8888XXXXMMMM3333gggg6666ZZZZYYYYIIIIUUUUJJJJGGGGOOOONNNNqqqqxxxxhhhhBBBBvvvv9999ffffxxxx11116666FFFF44441111PPPPccccOOOOPPPP1111////XXXXWWWWXXXXWWWWllllNNNNFFFFZZZZ2222hhhhcccc3333SSSSJJJJPPPPHHHHSSSSDDDDWWWWuuuuggggBBBBttttVVVVEEEEHHHHEEEETTTTRRRRDDDDLLLL++++ggggVVVVvvvvVVVVnnnnPPPP1111rrrrvvvv1111YYYYXXXX0000uuuuSSSSyyyyttttWWWW2222XXXXOOOOKKKK////ooooXXXX11119999QQQQsssseeeebbbbZZZZkkkk8888<<<<////llllaaaatttteeeexxxxiiiitttt>>>> [“circle”, …, …, “four”] [“circle”, …, …, “four”] B. Model architecture Inference network. We use a graph neural network ˜r (GNN) comprising: Language symbol latents <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""tttt2222iiiiVVVVggggwwww////66666666BBBB2222WWWWWWWW7777EEEEqqqqvvvvVVVV66662222JJJJNNNNQQQQLLLL5555llllIIII===="""">>>>AAAAAAAAAAAACCCCBBBB3333iiiiccccZZZZZZZZDDDDNNNNSSSSssssNNNNAAAAEEEEMMMMcccc3333ffffttttbbbb6666FFFFeeeevvvvRRRRSSSS7777AAAAIIIIHHHHkkkkppppJJJJRRRRNNNNBBBBjjjj0000YYYYvvvvHHHHCCCCvvvvYYYYDDDDmmmmllllAAAA22222222000022227777ddddHHHHccccTTTTddddiiiiddddiiiiCCCCXXXXkkkkAAAAXXXX8888KKKKrrrr3333rrrryyyyJJJJVVVVxxxx////DDDDiiii8888////iiiippppssss1111BBBB22224444FFFFllllffff////xxxxnnnnhhhhppppnnnn5555hhhhwwwwllllnnnnGGGGllllzzzz333322221111ppppbbbb33339999jjjjcccc2222qqqq7777ssssVVVVHHHHffff33339999gggg8888OOOO7777aaaaNNNNaaaaVVVV8888eeeeppppIIIIrrrrRRRRDDDDYYYYhhhh6666rrrrffffoooogggg11115555UUUUzzzzSSSSDDDDjjjjDDDDggggttttJJJJ8888ooooiiiikkkkXXXXIIIIaaaaSSSS++++cccc3333hhhhbbbb55553333iiiiNNNNVVVVmmmmssssXXXXyyyyAAAAWWWWYYYYJJJJDDDDQQQQQQQQeeeeSSSSxxxxYYYYxxxxggggssssFFFFIIIIQQQQ7777vvvvmmmmCCCCwwwwyyyyTTTTMMMMMMMMpppp8888YYYYHHHHxxxxEEEEMMMM5555XXXXnnnnQQQQ7777vvvvuuuuNNNNtttt11115555OOOOKKKKvvvvggggllllVVVVBBBBHHHHZZZZbbbbSSSSHHHH9999oooo8888////iiiikkkkkkkkqqqqqqqqAAAATTTTCCCCssssddddYYYYDDDDzzzz00000000ggggyyyyLLLLAAAACCCCRRRRjjjjjjjjNNNNqqqq33336666qqqqaaaaYYYYLLLLJJJJFFFFIIII////ppppwwwwKKKKDDDDEEEEgggguuuuooooggggmmmm++++++++eeeeOOOO2222ddddGGGGGGGGTTTTllllRRRRrrrrMMMMyyyyTTTT4444MMMMzzzzVVVVvvvvxxxx0000ZZZZFFFFllllrrrrPPPPRRRRGGGGggggqqqqiiii000033331111ccccqqqq4444QQQQGGGG++++YYYYvvvvuuuunnnnUUUUBBBBMMMMBBBBGGGGNNNNUUUUCCCCxxxxNNNNhhhheeeegggg6666yyyyJJJJhhhhMMMMUUUUqqqqCCCCSSSSLLLLIIIIZZZZGGGGKKKKXXXXccccggggddddggggppppTTTTnnnnBBBBFFFFTTTTllllAAAACCCCffffGGGGccccBBBBEEEEMMMMbbbbOOOO3333QQQQyyyyZZZZYYYYYYYYQQQQLLLLGGGGuuuuqqqqooooxxxxxxxxFFFFssss++++ffffxxxxWWWW6666FFFF00003333PPPP8888PPPP1111llllvvvvXXXXVVVVTTTTWWWWllllNNNNBBBBJJJJ++++ggggUUUUnnnnSSSSMMMMPPPPXXXXaaaaEEEEWWWWuuuukkkkNNNNtttt1111EEEEEEEEEEEEPPPPaaaaEEEEXXXX9999IIIIrrrreeeerrrrGGGGffffrrrr3333ffffqqqqwwwwPPPPhhhheeeellllaaaa1111bbbbZZZZcccc4444zzzz++++hhhhffffXXXX1111CCCC8888zzzzIIIImmmmiiiissss====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> • an edge MLP with [64, 64] units r Relational latents <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""OOOOhhhhgggg5555gggg8888ZZZZIIIIPPPPCCCCmmmmllll////5555ppppQQQQ2222UUUUQQQQUUUUbbbbEEEE9999OOOOVVVVsssskkkk===="""">>>>AAAAAAAAAAAABBBB////XXXXiiiiccccZZZZZZZZAAAA9999SSSSwwwwNNNNBBBBEEEEIIIIbbbbnnnn4444lllleeeeMMMMXXXX1111FFFFLLLLmmmm8888MMMMggggWWWWIIIIRRRRwwwwJJJJ4444KKKKWWWWQQQQRRRRvvvvLLLLCCCCOOOOYYYYDDDDkkkkxxxxDDDD2222NNNNnnnnvvvvJJJJkkkktttt22229999YYYY3333ddddOOOOCCCCEEEEffffwwwwTTTT9999hhhhqqqqZZZZyyyyeeee2222////hhhhYYYYbbbbffff4444tttt7777yyyyRRRRWWWWaaaaDDDDCCCCzzzz77778888MMMM4444MMMMMMMM////MMMMGGGGsssseeeeAAAAGGGGPPPPeeee////bbbbKKKKaaaayyyyttttbbbb2222xxxxuuuuFFFFbbbbddddLLLLOOOO7777tttt7777++++wwwwffffllllwwww6666OOOOWWWWiiiiRRRRJJJJNNNNWWWWZZZZNNNNGGGGIIIIttttKKKKddddggggBBBBggggmmmmuuuuGGGGJJJJNNNN5555CCCChhhhYYYYJJJJ9999aaaaMMMMyyyyEEEECCCCwwwwddddjjjjCCCC5555zzzzffffLLLLttttJJJJ6666YYYYNNNNjjjj9999QQQQDDDDTTTTmmmmPPPPWWWWllll2222SSSSkkkkeeeeMMMMggggppppQQQQSSSSssss99999999iiiiTTTTBBBBccccRRRRCCCCmmmmeeeejjjjYYYYooooVVVV7777yyyyaaaaNNNNwwww99993333FFFFffffwwwwccccKKKKppppBBBBHHHHYYYY1111DDDD++++6666QQQQ0000jjjjmmmmkkkkiiiimmmmkkkkAAAAppppiiiiTTTTNNNNffff3333YYYYuuuuyyyynnnnRRRRCCCCOOOOnnnnggggssss1111KKKKvvvvccccSSSSwwwwmmmmNNNNAAAAJJJJGGGGbbbbGGGGuuuuRRRRUUUUUUUUkkkkMMMM////11110000vvvvvvvvHHHHMMMMPPPPbbbbPPPPKKKK0000AAAA0000jjjjbbbbZZZZ9999CCCCdddd66667777++++7777UUUUiiiiJJJJNNNNGGGGYYYYqqqqAAAA1111uuuuZZZZbbbbWWWWiiiiWWWWcccc5555llllYYYYttttXXXX////WWWWbbbbTTTTLLLLAAAAssssaaaawwwwGGGGccccmmmmkkkkqqqqhhhhttttffff9999llllKKKKssss4444QQQQaaaabbbbooooYYYYmmmmiiiiYYYYCCCCBBBBccccjjjjNNNN7777PPPPCCCCHHHHXXXXLLLLNNNNKKKKIIIIqqqqppppBBBBUUUUIIII1111tttt3333uuuu7777ddddEEEEwwww0000ooooWWWWggggNNNNKKKK1111llllDDDD////OOOOXXXXzzzzVVVV6666FFFF1111UUUUffffMMMMtttt333311119999WWWW6666jjjjeeee5555NNNNUUUUUUUU4444ggggVVVVMMMM4444BBBBxxxx++++uuuuooooAAAA555533330000IIIIAAAAmmmmUUUUFFFFDDDDwwwwAAAAqqqq////wwww5555jjjjwwww7777777788886666HHHH88887777kkkkooooLLLLTTTThhhh5555zzzzzzzzHHHH8888CCCC++++ffffrrrrFFFF66666666++++llllkkkkQQQQ====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> • a node MLP with [128, 128] units Figure 7. SCAN architecture. Scan learns to encode a language • a globals MLP with [256, 256, d ] units, where d is symbol, l, to the same latent space as Constellation. This forms r r an effective association between language symbols and relational the dimensionality of r. abstractions. The GNN is applied for two message-passing iterations, following which the globals are used as the variational pos- the pressure on the network to represent empty slots explic- terior parameters µ q, σ q. itly. A more general approach would be to learn whether a slot was empty or not, but we leave this to further work. Generative model. The generative model takes a sample r ∼ Q(r | a) and passes it through an MLP with [64, 128, D. Relational latents characteristics 32] units. The output is then decoded for a fixed number of steps (corresponding to the number of slots, K) by an LSTM To characterise the disentanglement of the relational latents, with 64 units. Finally, each decoded abstract entity aˆ is ˜r, we analyse the mutual information between latent di- i passed through a shared MLP with [128, d ] units, where mensions of ˜r and ground truth relational factors (from the a d is the dimensionality of each entity. dataset generation). These results are shown in Figure 6, and a demonstrate that Constellation learns disentangled relational knowledge. C. Object representation pre-processing To ensure disentangled object representation, and to not E. SCAN bias the mask learning towards any of the object features in particular, we perform some preprocessing steps to the We now describe a method for associating language symbols MONet representations: we first set the empty slots to de- to abstractions over our relational representations. We adapt fault values, and whiten the remaining features (across the SCAN (Higgins et al., 2018b), short for Symbol-Concept whole dataset). Setting empty slots to default values eases Association Network, a network architecture that binds lan-
Constellation: Learning relational abstractions over objects for compositional imagination Figure 8. SCAN test F1-score of symbol description from a vi- sual scene’s encoded relational factors. guage symbols to abstractions over disentangled image rep- resentations (concepts). E.g. the language symbol, l, for ‘straight’ will be associated to relational representations, r, for any images with objects in a straight line. To facilitate zero-shot composition, we choose to encode the symbols in a multi-hot, disentangled format, such that after binding ‘straight’ and ‘long’ separately, the system can immediately understand what ‘straight, long’ means - a relational repre- sentation with latents, r, corresponding to any number of objects in a straight line. SCAN consists of a VAE taking a language symbol, l, as input, and embedding it in a latent space ˜r via the posterior Q (˜r | l). The posterior is a diagonal Gaussian with scan mean, µ , and standard deviation, σ , parameterised by s s neural networks. Learning proceeds via a β-VAE loss com- bined with a reverse-KL term that encourages Q (˜r | l) scan to match Q(r | a). Since only the curviness latent dimen- sion has a consistent value when paired with ‘circle’ across several input examples, SCAN learns the same value in ˜r and a high precision σ for the corresponds latent dimen- s sion, while other latent dimensions will have low precision σ . Now if ‘circle’ is provided as an input (instruction), only s the latent dimension encoding curviness should have high precision. This is equally true for compositional language instructions; if ‘circle, four’ is provided, both the curviness and number dimension should have high precision. We show an example of the SCAN architecture in the context of our relational latents in Figure 7. After this weak supervised learning by pairing symbolic instructions with superlatents, ˜r, from corresponding ex- amples of scenes, we should be able to categorise, using language symbols, the relational elements of each scene. In Figure 8, we demonstrate good classification F1-score on held out data.
