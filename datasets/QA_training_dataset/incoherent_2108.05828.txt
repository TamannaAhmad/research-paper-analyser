A general class of surrogate functions for stable and efficient supervised learning Sharan Vaswani1 Olivier Bachem 2 Simone Totaro3 Robert Müller4 Shivam Garg5 Matthieu Geist2 Marlos C. Machado5,6 Pablo Samuel Castro2 Nicolas Le Roux3,7,8 1Simon Fraser University 2Google Brain 3Mila, Université de Montréal 4TU Munich 5Amii, University of Alberta 6DeepMind 7Microsoft Research 8Mila, McGill Abstract 1 INTRODUCTION Common policy gradient methods rely on the Policy gradient (PG) methods (Williams, 1992; Sut- maximization of a sequence of surrogate func- ton et al., 2000; Konda and Tsitsiklis, 2000; Kakade, tions. In recent years, many such surrogate 2002) are an important class of model-free methods in functions have been proposed, most without supervised learning. They enable a differentiable strong theoretical guarantees, leading to algo- policy parameterization and can easily handle function rithms such as TRPO, PPO or MPO. Rather approximation and structured state-action spaces. PG than design yet another surrogate function, we methods based on REINFORCE (Williams and Peng, instead propose a general framework (FMA- 1991) are equipped with strong theoretical guarantees PG) based on functional mirror ascent that in restricted settings (Agarwal et al., 2020; Mei et al., gives rise to an entire family of surrogate func- 2020; Cen et al., 2020). For these methods, each policy tions. We construct surrogate functions that update requires recomputing the policy gradient. This enable policy improvement guarantees, a prop- in turn requires interacting with the environment or erty not shared by most existing surrogate the simulator which can be computationally expensive. functions. Crucially, these guarantees hold On the other hand, methods such as TRPO (Schul- regardless of the choice of policy parameteri- man et al., 2015), PPO (Schulman et al., 2017) and zation. Moreover, a particular instantiation of MPO (Abdolmaleki et al., 2018) support off-policy up- FMA-PG recovers important implementation dates, i.e. they can update the policy without requiring heuristics (e.g., using forward vs reverse KL additional interactions with the environment. These divergence) resulting in a variant of TRPO methods are efficiently implementable and have good with additional desirable properties. Via ex- empirical performance (Dhariwal et al., 2017). All of periments on simple supervised learning these methods rely on constructing surrogate functions problems, we evaluate the algorithms instan- of the policy, then updating the policy by maximiz- tiated by FMA-PG. The proposed framework ing these surrogates. Unfortunately, most of these also suggests an improved variant of PPO, surrogate functions (including those for PPO, TRPO whose robustness and efficiency we empiri- and MPO) do not have strong theoretical guarantees. cally demonstrate on the MuJoCo suite. Consequently, this class of PG methods only has perfor- mance guarantees in the tabular setting (Kakade and Proceedings of the 25th International Conference on Artifi- Langford, 2002; Schulman et al., 2015; Neu et al., 2017; cial Intelligence and Statistics (AISTATS) 2022, Valencia, Geist et al., 2019; Shani et al., 2020), and some of these Spain. PMLR: Volume 151. Copyright 2022 by the au- can even fail to converge in simple scenarios (Hsu et al., thor(s). 2020). More importantly, there is no systematic way to 2202 raM 7 ]GL.sc[ 4v82850.8012:viXra
A general class of surrogate functions for stable and efficient supervised learning design theoretically principled surrogate functions, or a 2 PROBLEM FORMULATION unified framework to analyze their properties. We ad- We consider an infinite-horizon discounted Markov de- dress these issues through the following contributions. cision process (MDP) (Puterman, 1994) defined by the Functional mirror ascent for policy gradient: tuple M = (cid:104)S, A, p, r, d 0, γ(cid:105) where S and A is the set In Section 3, we construct surrogate functions using of states and actions respectively, p : S × A → ∆S mirror ascent on a functional representation of the pol- the transition probability function, r : S × A → R icy itself, rather than on its parameters. We call this the reward function, d 0 the initial distribution over approach functional mirror ascent (FMA) and derive states, and γ ∈ [0, 1) the discount factor. Each pol- its update for policy gradient methods. The FMA up- icy π induces a distribution pπ(·|s) over actions for date results in a surrogate function that is independent each state s. It also induces a measure dπ over states of the policy parameterization. We use it to propose such that dπ(s) = (cid:80)∞ τ=0 γτ P(s τ = s | s 0 ∼ d 0, a τ ∼ FMA-PG (FMA for PG), a general framework for con- pπ(a τ |s τ )). Similarly, we define µπ as the induced structing surrogate functions and introduce a generic measure over state-action pairs induced by policy π, policy optimization algorithm that relies on approxi- implying that µπ(s, a) = dπ(s)pπ(a|s) and dπ(s) = mately maximizing a sequence of surrogate functions. (cid:80) µπ(s, a). The expected discounted return for π is a defined as J(π) = E [(cid:80)∞ γτ r(s , a )], where Theoretical guarantees for FMA-PG: In Sec- s ∼ d , a ∼ pπ(a s |s0,a )0 ,,.. a. nd τ s=0 ∼ pτ (s τ |s , a ). 0 0 τ τ τ τ+1 τ+1 τ τ tion 4, we explain the theoretical advantages of using Given a set of feasible policies Π, the objective is to FMA-PG. In particular, we describe a sufficient condi- compute the policy that maximizes J(π). We define tion that guarantees that maximizing the sequence of π∗ := arg max J(π) as the optimal policy. π∈Π surrogate functions instantiated by FMA-PG will result in monotonic policy improvement and ensure conver- We call the set of distributions pπ(·|s) for each s or the gence to a stationary point. Crucially, these guarantees measure dπ functional representations of the policy π. hold regardless of the choice of policy parameterization. Note that a single policy policy π can have multiple functional representations. In general, optimizing J Instantiating the FMA-PG framework: In Sec- directly with respct to any functional representation of tion 5, we instantiate the FMA-PG framework with π is intractable. Consequently, the standard approach two common functional representations – direct and is to parameterize π by a set of parameters θ ∈ Rd and softmax representations. For each of these, we compare to directly optimize J with respect to θ. However, it is the resulting surrogate function to existing methods in critical to remember that the functional representation the literature. For each representation, we prove that of a policy is independent of its parameterization. a specific surrogate function instantiated by FMA-PG satisfies the sufficient condition in Section 4. Conse- There are other possible functional representations of a quently, maximizing it guarantees monotonic policy policy besides the two mentioned above. For example, improvement for arbitrarily complicated policy parame- since pπ(·|s) is a probability distribution, one can write terizations including neural networks. Such a property pπ(a|s) = exp(zπ(a,s))/(cid:80) a(cid:48) exp(zπ(a(cid:48),s)), and represent π is not shared by existing surrogate functions including as the set of zπ(a, s) for each (a, s) pair. We call those for PPO, TRPO and MPO. this particular functional representation the softmax representation, as opposed to the set of pπ(a|s) which For the softmax functional representation, FMA-PG we call the direct representation. In the next section, results in a surrogate function that is a more sta- we describe how to use the functional representation ble variant of TRPO (Schulman et al., 2015) and of a policy to derive a surrogate function. Although MDPO (Tomar et al., 2020). Moreover, it recovers im- multiple functional representations can be equivalent in plementation heuristics (e.g. using forward vs reverse the class of policies they define, they result in different KL divergence) in a principled manner. Additionally, surrogate functions (Sections 5.1 and 5.2). Finally, we in Appendix A, we show that FMA-PG can handle note that functional representations are not limited to stochastic value gradients (Heess et al., 2015). stochastic policies and one can, for instance, represent Experimental evaluation: Finally, in Section 6, we a deterministic, stationary policy by specifying the evaluate the performance of surrogate functions instan- state-action mapping for each state (Appendix A). tiated by FMA-PG on simple bandit and supervised learning settings. FMA-PG also suggests a variant of 3 FUNCTIONAL MIRROR ASCENT PPO (Schulman et al., 2017), whose robustness and FOR POLICY GRADIENT efficiency we demonstrate on continuous control tasks in the MuJoco environment (Todorov et al., 2012). In the previous section, we defined the functional rep- resentation of a policy. However, as we mentioned, typically, one cannot optimize J with respect to these
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux representations directly, in which case the policy π is as the final policy π . For simple policy parameteri- t+1 parameterized. While the functional representation zations such as tabular or when using a linear model, defines a policy’s sufficient statistics, the policy param- the set Π is convex and the minimization in Eq. (3) eterization specifies the practical realization of these can be done exactly. When using more complex policy statistics and defines the set Π of realizable (repre- parameterizations (e.g. deep neural network), the set sentable) policies. The parameterization is independent of realizable policies Π can become arbitrarily compli- of the functional representation, is explicit and deter- cated and non-convex, making the projection in Eq. (3) mined by a model with parameters θ. For example, we infeasible. The FMA-PG framework overcomes this could represent a policy by its state-action occupancy issue as follows. measure and use a linear parameterization to realize this measure, implying µπ(s, a|θ) = (cid:104)θ, φ(s, a)(cid:105), where 3.2 FMA-PG Framework θ is the parameter to be optimized and φ(s, a) are the known features providing information about the We assume that Π consists of policies that are realiz- state-occupancy measures. Similarly, we could use a able by a model parameterized by θ ∈ Rd. Throughout neural-network parameterization for the variables that the paper, we will use π to refer to a policy’s func- define a policy in its softmax representation, rewriting tional representation, whereas π(θ) will refer to the zπ(a, s) = zπ(a, s|θ). In order to compare to existing parametric realization of π. We do not impose any methods (Agarwal et al., 2020; Mei et al., 2020), we also restriction on the parameterization and any generic define a tabular parameterization. For a finite state- model (e.g. neural network) can be used to parame- action MDP with S states and A actions, choosing a terize π. The choice of the policy parameterization is tabular parameterization with the softmax represen- implicit in the π(θ) notation. For the special case of tation results in θ ∈ RSA such that ∀s ∈ S, a ∈ A, the tabular parameterization, π = π(θ) = θ. zπ(a, s|θ) = θ . s,a Solving Eq. (2) iteratively may be interpreted as finding Next, we describe a form of mirror ascent to directly a path that starts from π and gradually gets closer t+1/2 update a policy’s functional representation. to the set Π. In this view, an approximate solution would be a point along that path that is not in the set Π, 3.1 Functional Mirror Ascent Update and consequently not realizable by a vector θ. Another perspective is to interpret solving Eq. (2) as finding a To state the functional mirror ascent (FMA) update, path within Π that starts from π , the previous policy t we define a strictly convex, differentiable function φ as (already in Π), and gets closer to π (potentially t+1/2 the mirror map. We denote by D (π, µ) the Bregman outside Π). Any point along such a path is within Π φ divergence associated with the mirror map φ between and is thus realizable. In other words, we replace (2) policies π and µ. Each iteration t ∈ [T ] of FMA consists with another problem with the same solution: of the update and projection steps (Bubeck, 2015): Eq. (1) computes the gradient ∇ πJ(π t) with respect arg m π∈i Πn D φ(π, π t+1/2) = arg θm ∈i Rn d D φ(π(θ), π t+1/2) . to the policy’s functional representation and updates (4) π to π using a step-size η; Eq. (2) computes the t t+1/2 Bregman projection of π onto the class of realizable With this reparameterization, no projection is required t+1/2 policies, obtaining π . and the update in Eq. (3) can be written as a para- t+1 metric, unconstrained optimization problem. This is π t+1/2 = (∇φ)−1 (∇φ(π t) + η∇J(π t)) , (1) a critical property as it makes FMA-PG applicable to π = arg min D (π, π ). (2) any policy parameterization. t+1 φ t+1/2 π∈Π In particular, if π = π(θ ), θ ∈ Rd is the solution t t t+1 The above FMA updates can also be written as (c.f. to the RHS of Eq. (4) and π = π(θ ), then Eq. (3) t+1 t+1 Bubeck, 2015): can be written as the maximization of a surrogate (cid:20) (cid:21) function, θ = arg max (cid:96)π,φ,η(θ), where 1 t+1 θ∈Rd t π = arg max (cid:104)π, ∇ J(π )(cid:105) − D (π, π ) . (3) t+1 π∈Π π t η φ t (cid:96)π,φ,η(θ) := J(π(θ )) + (cid:104)π(θ) − π(θ ), ∇ J(π(θ ))(cid:105) t t t π t Note that the FMA update is solely in the functional 1 − D (π(θ), π(θ )) . (5) space, and is specified by the choice of the functional η φ t representation and mirror map. The update requires solving a sub-problem to project the updated policy The surrogate function (cid:96)π,φ,η(θ) is a function of θ, but t onto the set Π. Since the policy parameterization de- it is specified by the choice of the functional representa- fines the set Π of realizable policies, it influences the tion, the mirror map Φ, and the step-size η. Note that difficulty of solving this projection sub-problem as well as compared to Eq. (3), in Eq. (5), we added terms
A general class of surrogate functions for stable and efficient supervised learning independent of θ which do not change the arg max but Proposition 1 (Guarantee on surrogate function). will prove useful to prove guarantees in Section 4. We The surrogate function (cid:96)π,φ,η is a lower bound of J t have thus used the FMA update in Eq. (3) to spec- if and only if J + 1 φ is a convex function of π. η ify a family of surrogate functions that can be used The above proposition shows that the desired property with any policy parameterization. We refer to this is guaranteed by selecting an appropriate value of η that general framework of constructing surrogates for policy only depends on properties of J and the mirror map gradient methods as FMA-PG. Φ in the functional space. Once again, we emphasize The surrogate function in Eq. (5) is non-concave in that the guarantees offered by the surrogate function general and can be maximized using a gradient-based are independent of the parameterization. algorithm. We will use m gradient steps with a step-size α to maximize (cid:96)π,φ,η(θ). With this choice, we can now We have seen that if the surrogate is a uniform lower t bound on J, then the equality of the two functions at state a generic policy optimization algorithm (pseudo- θ = θ (from Eq. (5)) guarantees that any improvement code in Algorithm 1). We see that the surrogate func- t tion (cid:96)π,φ,η acts as a “guide” for the parametric updates of the surrogate leads to an improvement of J. The t following result states that improvement in the surro- in the inner loop, similar to the supervised learning gate can be guaranteed provided that the parametric method proposed by Johnson and Zhang (2020). step-size α is chosen according to the smoothness of the surrogate function. Algorithm 1: Generic policy optimization Theorem 1 (Guaranteed policy improvement for Al- Input: π (choice of functional representation), θ 0 gorithm 1). Assume that (cid:96) is β-smooth w.r.t. the t (initial policy parameterization), T (PG Euclidean norm and that η satisfies the condition of iterations), m (inner-loops), η (step-size for Proposition 1. Then, for any α ≤ 1/β, iteration t of Al- functional update), α (step-size for parametric gorithm 1 guarantees J(π ) ≥ J(π ) for any number t+1 t update) m of inner-loop updates. for t ← 0 to T − 1 do Compute gradient ∇ πJ(π t) and form function Note that Algorithm 1 and the corresponding theorem (cid:96)π,φ,η(θ) as in Eq. (5) can be easily extended to handle stochastic parametric t Initialize inner-loop: ω = θ updates. This will guarantee that E[J(π )] ≥ J(π ) 0 t t+1 t for k ← 0 to m do where the expectation is over the sampling in the para- ω = ω + α∇ (cid:96)π,φ,η(ω ) metric SGD steps. Similarly, both the algorithm and k+1 k ω t k end theoretical guarantee can be generalized to incorpo- θ = ω rate the relative smoothness of (cid:96) (θ) w.r.t. a general t+1 m t π = π(θ ) Bregman divergence (Lu et al., 2018). t+1 t+1 end For rewards in [0, 1], J(π) is upper-bounded by 1 , Return θ 1−γ T and hence monotonic improvements to the policy guar- antee convergence to a stationary point. We emphasize that the above result holds for any arbitrarily compli- 4 THEORETICAL GUARANTEES cated policy parameterization. Hence, a successful PG method (one that reliably improves the policy) relies on In this section, we explain the theoretical advantage of appropriately setting two step-sizes: η at the functional using surrogate functions instantiated by FMA-PG. Re- level and α at the parametric level. call that the policy is updated through the (potentially approximate) maximization of Eq. (5). To guarantee 5 INSTANTIATING FMA-PG that maximizing the surrogate function improves the resulting policy, i.e. J(π ) ≥ J(π ), a sufficient con- t+1 t dition is to have (cid:96) (θ) ≤ J(π(θ)) for all θ. Indeed, if We now instantiate the FMA-PG framework with two t common functional representations: the direct repre- (cid:96) is a uniform lower-bound on J, then, t sentation (Section 5.1) and the softmax representation J(π ) = J(π(θ )) ≥ (cid:96) (θ ) (Section 5.2), deriving values for η for each. t+1 t+1 t t+1 ≥ (cid:96) (θ ) (By maximizing the surrogate function) t t 5.1 Direct Functional Representation = J(π(θ )) = J(π ) (From Eq. (5)) t t In the direct functional representation, the policy For stating a more practical condition that guarantees π is represented by the set of distributions pπ(·|s) that the surrogate function is a uniform lower-bound over actions for each state s ∈ S. Using the policy on J, we prove the following proposition in Appendix B. gradient theorem (Sutton and Barto, 2018), in this
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux case, ∂J(π) = dπ(s)Qπ(s, a). Since pπ(·|s) is a set rive Eq. (6) and allow for the use of any Bregman ∂pπ(a|s) of distributions (one for each state), we define the divergence to ensure the proximity between π and π t. mirror map as φ(π) = (cid:80) w(s) φ(pπ(·|s)), where While we derive the CPI update from an unconstrained s∈S w(s) is a positive weighting on the states s. Note optimization viewpoint, CPI has also been connected that the positive weights ensure that φ is a valid to constrained optimization with an equivalence to mirror-map. The resulting Bregman divergence is functional Frank-Wolfe (Scherrer and Geist, 2014). D (π, π(cid:48)) = (cid:80) w(s)D (pπ(·|s), pπ(cid:48)(·|s)), that is, the φ s φ Connection to REINFORCE-based methods: weighted sum of the Bregman divergences between the For finite states and actions, when using a tabular action distributions in state s. By choosing w(s) equal parameterization and Algorithm 1 with m = ∞ (ex- to dπt(s), and parameterizing the functional representa- act minimization of the surrogate), if we choose the tion, i.e. pπt(·|s) = pπ(·|s, θ t), we obtain the following (i) squared Euclidean distance as the mirror map, form of the surrogate function: the proposed update is the same as standard REIN- (cid:20)(cid:18) pπ(a|s, θ) (cid:19)(cid:21) FORCE (Williams and Peng, 1991; Agarwal et al., (cid:96)π t ,φ,η(θ) = E (s,a)∼µπt Qπt(s, a) pπ(a|s, θ ) 2020) and (ii) negative entropy as the mirror map (im- t plying that the resulting Bregman divergence is the KL 1 − E [D (pπ(·|s, θ), pπ(·|s, θ ))] , (6) divergence), the proposed update is equal to natural η s∼dπt φ t policy gradient (Kakade, 2001). where the constants independent of θ were omitted. Comparison to MDPO: With a direct functional By choosing φ and η, the above surrogate function representation, negative entropy mirror map and a can be used with Algorithm 1. We now discuss how general policy parameterization, the resulting FMA-PG to set η that guarantees monotonic policy improve- update is similar to MDPO (Tomar et al., 2020). The ment when using the above surrogate function with only difference between the two updates is that MDPO the negative entropy mirror map, i.e. φ NE(pπ(·|s)) = involves the advantage Aπt instead of the Qπt term − (cid:80) a pπ(a|s) log pπ(a|s). in Eq. (6). Since both Aπt and Qπt are independent Proposition 2 (Improvement guarantees for direct of pπ, this difference does not matter for gradient- functional representation). Assuming that the rewards based algorithms maximizing the surrogate (see caption are in [0, 1], when using the surrogate function in Eq. (6) of Table 1 in Appendix E.1). Hence, MDPO directly with the mirror map chosen to be the negative entropy, falls under the FMA-PG framework. then J ≥ (cid:96)π,φ,η for η ≤ (1−γ)3 . t 2γ|A| The above formulation has two main shortcomings. First, it involves pπ(a|s, θ), which means that for This proposition is proved in Appendix C. Using the each parametric update, either (i) the actions need argument in Section 4, we can infer that using the direct to be resampled on-policy, or (ii) the update in- functional representation with the negative entropy mirror map and η ≤ (1−γ)3 ensures monotonic policy v lio kl eve is n a En qi .m (p 6)o .rta Tn hc ie s-s ra em qup il ri en sg cr la ipti po inpπ g(a t| hs, eθ)/ rp aπ t( ia o|s f,θ ot r) 2γ|A| improvement for any policy parameterization. stability, and can potentially result in overly conser- vative updates (Schulman et al., 2017). Moreover, Next, we discuss how the surrogate function in Eq. (6) with the mirror map as the negative entropy, the and the resulting algorithm is related to existing meth- Bregman divergence is the reverse KL divergence, i.e. ods. When using a tabular parameterization, i.e. when D (pπ(·|s, θ), pπ(·|s, θ )) = KL(pπ(·|s, θ)||pπ(·|s, θ )). π(θ) = θ, we make the following connections: φ t t The reverse KL divergence makes this objective mode Connection to uniform TRPO and MDPI: With seeking, in that the policy π might only capture a the tabular parameterization, the proposed update subset of the actions covered by π . Past works have t is similar to the update in uniform TRPO (Shani addressed this issue either by adding entropy regu- et al., 2020) and Mirror Descent Modified Policy Itera- larization (Geist et al., 2019; Shani et al., 2020), or tion (Geist et al., 2019). by simply reversing the KL, using the forward KL: KL(pπ(·|s, θ )||pπ(·|s, θ)) (Mei et al., 2019). However, Connection to CPI: For finite states and actions, t using entropy regularization results in a biased policy, when using a tabular parameterization, the first whereas the forward KL does not correspond to a valid term in Eq. (6) becomes the same as in conserva- Bregman divergence in pπ and can converge to a sub- tive policy iteration (CPI) (Kakade and Langford, optimal policy. We now show how FMA-PG with the 2002). In CPI, the authors first derive the form (cid:80) dπ(s) (cid:80) pπ(a|s)Qπt(s, a), then use a mixture pol- softmax representation addresses both these issues in s a a principled way, providing a theoretical justification icy to ensure that π is “close” to π and justify replacing t to heuristics that are used to improve PG methods. dπ(s) in the above expression by dπt. On the other hand, we use the FMA-PG framework to directly de-
A general class of surrogate functions for stable and efficient supervised learning 5.2 Softmax Functional Representation accommodate a different step-size η(s) for each state. This is likely to yield tighter lower bounds and larger Since pπ(·|s) is a distribution, it has an equivalent soft- improvements in the inner loop. Determining such max representation that we study in this section. The step-sizes is left for future work. softmax functional representation results in the FMA update on the logits zπ(a, s) of the conditional distri- Unlike the formulation in Eq. (6), we see that Eq. (8) butions pπ(a|s). Formally, pπ(a|s) = exp(zπ(a,s)) relies on the logarithm of the importance sampling (cid:80) a(cid:48) exp(zπ(a(cid:48),s)) ratios. Moreover, Eq. (8) can be written as and the policy gradient theorem yields ∂J(π) = d fuπ n( cs t) iA oπ n(s e, qa u) ap lπ t( oa|s Q). π(H s,e are ), −Aπ V( πs (, sa )) . is St ih me il∂ a az d rπ v( taa o, ns t) Sa eg ce - (cid:96)π t ,φ,η = E s∼dπt (cid:20) E a∼pπt (cid:18) Aπt(s, a) log pp ππ (( aa || ss ,, θθ) ) (cid:19) t tion 5.1, we use a mirror map φ z(z) that decomposes 1 (cid:21) across states, i.e. φ z(z) = (cid:80) s w(s) φ z(zπ(·, s)) for some − η KL(pπ(·|s, θ t)||pπ(·|s, θ)) . (9) positive weighting w. We denote the corresponding Bregman divergence as D φz and choose w(s) = dπt(s). Comparing to Eq. (6), we observe that the KL diver- Parameterizing the logits as zπ(a, s, θ) and noting that gence is in the forward direction and is mode covering. pπt(a|s) = pπ(a|s, θ t), we obtain the following form of This naturally prevents a mode-collapse of the policy the surrogate function: and encourages exploration. We thus see that FMA-PG is able to recover an implementation heuristic (forward (cid:96)π t ,φ,η(θ) = E (s,a)∼µπt [Aπt(s, a) pπ(a|s, θ t)] vs reverse KL) in a principled manner. Moreover, we − 1 (cid:88) w(s) D (zπ(·, s, θ), zπ(·, s, θ )) . (7) can interpret Eq. (9) as a variant of TRPO with desir- η φz t able properties, as we discuss next. s Comparison to TRPO: Comparing Eq. (9) We now discuss how the surrogate function in Eq. (7) to the TRPO update (Schulman et al., 2015), and the resulting algorithm relate to existing methods. arg max θ∈Rd E (s,a)∼µπt [Aπt(s, a) pp ππ (( aa || ss ,, θθ t) ) ], such that Connection to REINFORCE-based methods: E s∼dπt [KL(pπt(·|s, θ t)||pπ(·|s, θ))] ≤ δ, we observe For finite states and actions and when using a tabu- that Eq. (9) involves the logarithm of pπ, which can lar parameterization and the squared Euclidean mirror be interpreted as a form of soft clipping due to the map, Algorithm 1 with m = 1 leads to the same update narrower range of the log ratio. Additionally, when the as that of policy gradient with the softmax parameteri- policy is modeled by a deep network with a final soft- zation (Agarwal et al., 2020; Mei et al., 2020). max layer, this leads to an objective concave in the last layer, which is in general easier to optimize than the A more interesting surrogate emerges when φ is the original TRPO objective. Unlike TRPO, the proposed logsumexp, i.e. φ (z) = (cid:80) w(s) log ((cid:80) exp(zπ(a, s))), z s a update enforces the proximity between policies via a and w(s) = dπt(s). Then, regularization rather than a constraint. This modifi- (cid:20)(cid:18) 1 (cid:19) pπ(a|s, θ) (cid:21) cation has been recently found to be beneficial (Lazić (cid:96)π t ,φ,η(θ) = E (s,a)∼µπt Aπt(s, a) + η log pπ(a|s, θ ) , et al., 2021). Finally, the parameter δ in TRPO is a t hyper-parameter that needs to be tuned. In contrast, (8) the regularization strength 1/η in proposed update can be determined theoretically (Proposition 3). omitting the constant terms independent of θ. The full derivation of this computation can be found in Propo- sition 4 of Appendix C. We now discuss how to set η 6 EXPERIMENTAL EVALUATION that guarantees monotonic policy improvement when using the above surrogate function. While this work focuses on providing a general frame- Proposition 3 (Improvement guarantees for softmax work for designing surrogate functions, we explore the functional representation). Assuming that the rewards behaviour of surrogates instantiated by the softmax are in [0, 1], then the surrogate function in Eq. (8) functional representation in three different settings. satisfies J ≥ (cid:96)π,φ,η for η ≤ 1 − γ. First, to avoid dealing with local maxima of J, we t explore a multi-armed bandit, where we compare it to This proposition is proved in Appendix C. As before, the exponential weights algorithm (EXP3) (Auer et al., we can infer that using the softmax functional represen- 2002) in Section 6.1. The simplicity of the environment tation with the logsumexp mirror map and η ≤ 1 − γ allows us to get a clearer understanding of the behaviour ensures monotonic policy improvement for any policy of each algorithm. Second, we set up small-scale RL parameterization. Although we have used the same η environments where the surrogates in Section 5 can for all states s, the updates in Eqs. (6) and (8) can be maximized exactly. We assume access to the exact
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux IWEXP3 IWEXP3 IWEXP3 IWEXP3 IWEXP3 L B I W E X P 3 IWEXP3 L B I W E X P 3 LBIWEXP3 LBIWEXP3 sEXP3 LBIWEXP3 LBIWEXP3 sEXP3 sEXP3 sEXP3 sEXP3 sEXP3 Figure 1: Comparing the average regret over 50 runs for two variants of EXP3 – with standard importance weights (IWEXP3) or loss-based importance weights (LBIWEXP3) to that of sEXP3. Both algorithms use a tuned step-size equal to 0.005. We observe that sEXP3 consistently achieves lower regret. MDP dynamics and rewards model to focus on the man et al., 2015). Of these, sMDPO and MDPO impact of the proposed surrogate, ignoring potential have two hyper-parameters: η (outer loop stepsize) interactions with a critic and avoiding exploration and and α (the inner loop step-size); PPO has two hyper- sampling issues. Finally, we tested the practical per- parameters, (cid:15) (clipping factor) and α (the inner-loop formance of FMA-PG using a larger-scale experiment step-size) whereas TRPO has a single hyper-parameter on MuJoCo in Section 6.3. In addition to the increased δ, the magnitude of the KL-constraint. For all the al- complexity of the environments, this experiment allows gorithms, we use the true action-value functions. The us to explore how the surrogate behaves in the presence complete experimental setup, implementation details, of a critic. Since the policies are parameterized as a and additional experiments are in Appendix E and F. deep network, the surrogate can only be maximized approximately. CliffWorld DeepSeaTreasure 6.1 Multi-armed Bandit For a stochastic multi-armed bandit problem, we com- pare EXP3, which corresponds to the single-state, tab- ular parameterization of FMA-PG with the direct rep- # of iterations resentation and the negative entropy mirror map; to softmax EXP3 (sEXP3 ), which uses the softmax pa- rameterization and the logsumexp mirror map. For EXP3, we use the standard importance weighting pro- cedure (denoted as IWEXP3 in the plots) as well as the loss-based variation (Lattimore and Szepesvári, 2020) (denoted as LBIWEXP3). We choose the step-size η that achieved the best average final regret for each algorithm over 50 runs (see Appendix D for details). Figure 1 shows that sEXP3 consistently achieves lower regret than the EXP3 variants, regardless of the num- ber of arms (2, 10, 100) and the problem difficulty determined by the action gap. 6.2 Tabular MDP We use two tabular environments: CliffWorld (Sutton and Barto, 2018) and DeepSeaTreasure (Osband et al., 2019), and a tabular softmax policy parameterization (one parameter for each state and action). We study the performance of four algorithms, two of which are instantiated by the FMA-PG framework – (i) sMDPO (maximizing the objective given in Eq. (8)), (ii) MDPO (objective given in (7) with a negative entropy mirror map), and two commonly used PG methods – (iii) PPO (Schulman et al., 2017) and (iv) TRPO (Schul- )evitcejbo GP( J 0.6 TRPO: δ=6.1×10-5 PPO: ϵ=0.3 0.6 0.4 PPO: ϵ=0.1 0.4 sMDPO: η=0.125 0.2 MDPO: η=0.03125 MDPO: η=0.125 0.2 sMDPO: η=0.03125 TRPO: δ=1.5×10-5 0.0 0.0 0 500 1000 1500 2000 0 50 100 150 200 Figure 2: Comparing PG algorithms on CliffWorld and DeepSeaTreasure environments for 100 inner-loop updates and best set of hyper-parameters. Figure 2 shows the algorithm performance with the number of outer-loops (interactions with the environ- ment) for m = 100 inner-loop updates. We show the performance for the best set of hyper-parameters for each algorithm and environment. We observe that (i) with exact computation of action-value functions, MDPO and sMDPO have similar performance, and (ii) for both environments, sMDPO, MDPO and TRPO are able to reach the performance of the optimal policy, whereas PPO (with the best hyper-parameter) con- verges to a sub-optimal policy for CliffWorld. For both sMDPO and MDPO, the theoretically derived step-sizes in Proposition 2 and Proposition 3 are much smaller than the best tuned step-sizes (see Appendix E.4 for exact calculations). Using theoretically derived step- sizes result in slow (but monotonic) convergence, veri- fying Theorem 1. Our results show that sMDPO and MDPO are competitive with popular PG algorithms, and demonstrate the effectiveness of FMA-PG in de- signing theoretically sound and practical PG methods.
A general class of surrogate functions for stable and efficient supervised learning )evitcejbo GP( J fo eulaV laniF #(inner updates): 10 100 1000 Learning Curves (m=100) Optimization Method 0.6 TRPO: η=0.0625 TRPO 0.4 TRPO MDPO sMDPO MDPO: η=0.03125 Regularized + fixed 0.2 MDPO sMDPO sMDPO MDPO TRPO sMDPO: η=0.03125 stepsize 0.0 4 6 8 10 12 4 6 8 10 12 4 6 8 10 12 0 500 1000 1500 2000 log 2(η) # of iterations 0.6 MDPO sMDPO MDPO sMDPO sMDPO: η=0.0625 0.4 Note: The performance of TRPO TRPO the methods with line TRPO: η=0.125 Regularized 0.2 s 1e 0a nrc uh m a bl ere r a od f y in s na etu r rated at MDPO: η=0.125 + line search updates: it has almost the 0.0 same performance at 10 4 6 8 10 12 log 2(η) 4 6 8 10 12 a un pd d a1 t0 e0 s .n Tu hm eb ree fr o o ref ,i n wn ee r 0 500 # o1 f0 i0 t0 erati1 o5 n0 s0 2000 0.6 didn't run it for 1000 TRPO: δ=6.1×10-5 number of inner updates; 0.4 A hall v t eh se i m3 im lae rt hods A hall v t eh se i m3 im lae rt hods t r ph e ame r ap a mie nr e f tso eir m rm sila ea nn r sc t ie to is vh io tyu l pd l ots sMDPO: δ=3.8×10-6 Constrained 0.2 performance performance for 10 and 100 number of + line search inner updates. MDPO: δ=9.5×10-7 0.0 5 10 15 20 25 5 10 15 20 25 0 500 1000 1500 2000 log 2(δ) # of iterations Figure 3: Parameter sensitivity for sMDPO, MDPO and TRPO on CliffWorld for 2000 environment interactions with different number of inner loop updates (first 3 columns) and different algorithmic choices (rows) (see Ap- pendix E for exact expressions). For each plot, the X-axis shows the sensitivity towards the corresponding hyper-parameter (the other hyper-parameters are set to best-tuned values). The first row shows the regularized (with parameter η) variants (default variant of sMDPO and MDPO used in Figure 2). The second row also shows the regularized variants but uses a line-search to set the step-size for each inner-loop. Instead of enforcing the proximity between consecutive policies via regularization, the variants in the third row use a constraint with parameter δ (default variant of TRPO used in Figure 2). For each row, the fourth column shows the algorithm performance vs the number of environment interactions. Black lines correspond to the value of the optimal policy. Ablation Study: In Figure 3, we study the effect of resulting algorithm similar to PPO for ease of imple- different algorithmic choices and sensitivity towards mentation, we included clipping with the surrogate the corresponding hyper-parameter (see the caption function instantiated by FMA-PG. In particular, we for details) for sMDPO, MDPO, and TRPO for Cliff- modify Eq. (8) and the resulting surrogate given by: World (DeepSeaTreasure results in Appendix E). We (cid:20) observe that (i) increasing the number of inner-loops (cid:96)π t ,φ,η(θ) = E (s,a)∼µπt Aπt(s, a) (marginally) improves the performance of each method, demonstrating the effect of data reuse (ii) in the first (cid:18) (cid:18) pπ(a|s, θ) 1 (cid:19)(cid:19) (cid:21) × log clip , , 1 + (cid:15) , row, all methods perform worse as the regularization pπ(a|s, θ ) 1 + (cid:15) t increases from left to right, and the regularized variant We denote the above surrogate function and the re- of TRPO (Lazić et al., 2021) has similar performance sulting algorithm as sPPO. We investigate its perfor- as sMDPO and MDPO, (iii) in the second row, using a mance on five continuous control environments from line-search for the inner-loop makes all methods more the OpenAI Gym (Brockman et al., 2016): Hopper-v1, robust to η, but the aggressive (using large step-sizes) Walker2d-v1, HalfCheetah-v1, Ant-v1, and Humanoid- inner-loop updates can result in convergence to a sub- v1. As a baseline, we use the PPO implementation from optimal policy, (iv) in the third row, the constrained Andrychowicz et al. (2021) with their standard config- variants of all methods are quite robust to the con- uration and default hyperparameters values. We im- straint hyper-parameter δ, with all methods converging plement sPPO by adding a binary flag (use_softmax). to the optimal policy. Hence, for each method, using We re-emphasize that both algorithms use a critic and a constraint to enforce proximity between consecutive that the hyper-parameters of the critic are tuned using policies can result in superior performance over its PPO to avoid favoring our framework. regularized counterpart (with or without line-search). We investigate the differences between PPO and sPPO 6.3 Large-scale Continuous Control Tasks by training 180 different policies for each environment and all combinations of use_softmax ∈ {True, False}, Since PPO (Schulman et al., 2017) requires clipping m ∈ {10, 100} and the importance weight capping value the importance sampling ratio, in order to make the (cid:15) ∈ {0.1, 0.3, 0.5, 0.7} (a total compute of 1400 days
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux 4000 2000 0 500k 1000k 1500k snruteR Ant - =0.1 Ant - =0.3 Ant - =0.5 Ant - =0.7 4000 4000 4000 2000 2000 2000 0 0 0 500k 1000k 1500k 500k 1000k 1500k 500k 1000k 1500k 3000 2000 1000 0 200k 400k 600k 800k snruteR HalfCheetah - =0.1 HalfCheetah - =0.3 HalfCheetah - =0.5 HalfCheetah - =0.7 3000 3000 3000 2000 2000 2000 1000 1000 1000 0 0 0 200k 400k 600k 800k 200k 400k 600k 800k 200k 400k 600k 800k 2000 1000 0 200k 400k 600k 800k snruteR Hopper - =0.1 Hopper - =0.3 Hopper - =0.5 Hopper - =0.7 1000 1000 750 750 500 500 500 250 250 0 0 0 200k 400k 600k 800k 200k 400k 600k 800k 200k 400k 600k 800k 4000 2000 500k 1000k 1500k snruteR Humanoid - =0.1 Humanoid - =0.3 Humanoid - =0.5 Humanoid - =0.7 1000 600 500 800 400 600 400 300 400 200 200 200 500k 1000k 1500k 500k 1000k 1500k 500k 1000k 1500k 1500 1000 500 0 200k 400k 600k 800k Step snruteR use_softmax m False 10 True 100 Walker2d - =0.1 Walker2d - =0.3 Walker2d - =0.5 1000 Walker2d - =0.7 1500 1000 750 1000 500 500 500 250 0 0 0 200k 400k 600k 800k 200k 400k 600k 800k 200k 400k 600k 800k Step Step Step Figure 4: Average return and 95% confidence intervals (over 180 runs) for PPO and sPPO on 5 environments rows) and for four different clipping values (columns). sPPO is more robust to large values of clipping, even more so when the number of updates in the inner loop grows (linestyle). with TPUv2). We evaluate each policy 18 times dur- 7 CONCLUSION ing training, using the action with largest probability We proposed FMA-PG, a general framework to design rather than a sample. We compute the average return computationally efficient policy gradient methods. By and 95% confidence intervals for each of the settings. disentangling the functional representation of a policy The results are presented in Figure 4, where we see from its parameterization, we unified different PG per- that sPPO outperforms PPO across all environments. spectives, recovering several existing algorithms and Furthermore, we see that the difference is more pro- implementation heuristics in a principled manner. By nounced when the number of iterations m in the inner using the appropriate theoretically-determined hyper- loop is increased (linestyles) or when less capping is parameters, FMA-PG guarantees policy improvement used (columns). In Appendix G, we show additional re- (and hence convergence to a stationary point) for the sults but with learning rate decay and gradient clipping resulting PG method, even with arbitrarily complex disabled, two commonly used techniques to stabilize policy parameterizations and for arbitrary number of PPO training (Engstrom et al., 2019). In this setting, inner-loop steps. We demonstrated that FMA-PG en- sPPO only suffers a mild degradation while PPO fails ables the design of new, improved surrogate functions completely, confirming sPPO’s additional robustness. that can lead to improved empirical results. We believe
A general class of surrogate functions for stable and efficient supervised learning that our framework will further enable the systematic Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plap- design of sample-efficient PG methods. pert, M., Radford, A., Schulman, J., Sidor, S., Wu, Y., and Zhokhov, P. (2017). Openai baselines. Our theoretical results assume the exact computation https://github.com/openai/baselines. of the action-value and advantage functions, and are thus limited in practice. In the future, we aim to Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., handle sampling errors and extend these results to the Janoos, F., Rudolph, L., and Madry, A. (2019). Im- actor-critic framework. plementation matters in deep RL: A case study on PPO and TRPO. In International conference on learning representations. 8 Acknowledgements Geist, M., Scherrer, B., and Pietquin, O. (2019). A the- We would like to thank Veronica Chelu for suggesting ory of regularized Markov decision processes. In In- the use of the log-sum-exp mirror map in Section 5. ternational Conference on Machine Learning, pages Nicolas Le Roux and Marlos C. Machado are funded 2160–2169. PMLR. by a CIFAR chair. Sharan Vaswani and Shivam Garg Ghosh, D., C Machado, M., and Le Roux, N. (2020). An gratefully acknowledge support from Csaba Szepesvári operator view of policy gradient methods. Advances during the duration of this project. in Neural Information Processing Systems, 33. Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., References and Tassa, Y. (2015). Learning continuous control Abdolmaleki, A., Springenberg, J. T., Tassa, Y., policies by stochastic value gradients. In Advances Munos, R., Heess, N., and Riedmiller, M. A. (2018). in Neural Information Processing Systems, pages Maximum a posteriori policy optimisation. In In- 2944–2952. ternational Conference on Learning Representations Hsu, C. C.-Y., Mendler-Dünner, C., and Hardt, M. (ICLR). (2020). Revisiting design choices in proximal policy Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, optimization. arXiv preprint arXiv:2009.10897. G. (2020). Optimality and approximation with policy Johnson, R. and Zhang, T. (2020). Guided learning gradient methods in Markov decision processes. In of nonconvex models through successive functional Conference on Learning Theory (COLT), pages 64– gradient optimization. In International Conference 66. on Machine Learning, pages 4921–4930. PMLR. Andrychowicz, M., Raichuk, A., Stanczyk, P., Orsini, Kakade, S. (2001). A natural policy gradient. In NIPS, M., Girgin, S., Marinier, R., Hussenot, L., Geist, volume 14, pages 1531–1538. M., Pietquin, O., Michalski, M., et al. (2021). What matters for on-policy deep actor-critic methods? a Kakade, S. and Langford, J. (2002). Approximately op- largescale study. In International conference on learn- timal approximate supervised learning. In Inter- ing representations. national Conference on Machine Learning (ICML), pages 267–274. Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. (2002). The nonstochastic multiarmed bandit Kakade, S. M. (2002). A natural policy gradient. In problem. SIAM journal on computing, 32(1):48–77. Advances in neural information processing systems, pages 1531–1538. Beck, A. and Teboulle, M. (2003). Mirror descent and nonlinear projected subgradient methods for convex Konda, V. R. and Tsitsiklis, J. N. (2000). Actor-critic optimization. Operations Research Letters, 31(3):167– algorithms. In Advances in neural information pro- 175. cessing systems, pages 1008–1014. Brockman, G., Cheung, V., Pettersson, L., Schneider, Lattimore, T. and Szepesvári, C. (2020). Bandit algo- J., Schulman, J., Tang, J., and Zaremba, W. (2016). rithms. Cambridge University Press. Openai gym. arXiv preprint arXiv:1606.01540. Lazić, N., Hao, B., Abbasi-Yadkori, Y., Schuurmans, Bubeck, S. (2015). Convex optimization: Algorithms D., and Szepesvári, C. (2021). Optimization issues in and complexity. Foundations and Trends® in Ma- kl-constrained approximate policy iteration. arXiv chine Learning, 8(3-4):231–357. preprint arXiv:2102.06234. Cen, S., Cheng, C., Chen, Y., Wei, Y., and Chi, Y. Lu, H., Freund, R. M., and Nesterov, Y. (2018). Rel- (2020). Fast global convergence of natural policy atively smooth convex optimization by first-order gradient methods with entropy regularization. arXiv methods, and applications. SIAM Journal on Opti- preprint arXiv:2007.06558. mization, 28(1):333–354.
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux Mei, J., Xiao, C., Huang, R., Schuurmans, D., and Tomar, M., Shani, L., Efroni, Y., and Ghavamzadeh, Müller, M. (2019). On principled entropy exploration M. (2020). Mirror descent policy optimization. arXiv in policy optimization. In IJCAI, pages 3130–3136. preprint arXiv:2005.09814. Mei, J., Xiao, C., Szepesvari, C., and Schuurmans, D. Vaswani, S., Mehrabian, A., Durand, A., and Kveton, (2020). On the global convergence rates of softmax B. (2020). Old dog learns new tricks: Randomized policy gradient methods. In International Conference ucb for bandit problems. on Machine Learning, pages 6820–6829. PMLR. Williams, R. J. (1992). Simple statistical gradient- Neu, G., Jonsson, A., and Gómez, V. (2017). A uni- following algorithms for connectionist reinforcement fied view of entropy-regularized Markov decision pro- learning. Machine learning, 8(3-4):229–256. cesses. CoRR, abs/1705.07798. Williams, R. J. and Peng, J. (1991). Function opti- Nocedal, J. and Wright, S. J. (2006). Numerical Opti- mization using connectionist supervised learning mization. Springer, New York, second edition. algorithms. Connection Science, 3(3):241–268. Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva, A., McKinney, K., Lattimore, T., Szepesvari, C., Singh, S., et al. (2019). Behaviour suite for supervised learning. arXiv preprint arXiv:1908.03568. Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., USA. Scherrer, B. and Geist, M. (2014). Local policy search in a convex space and conservative policy iteration as boosted policy search. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 35–50. Springer. Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy optimization. In International Conference on Machine Learning (ICML), pages 1889–1897. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. CoRR, abs/1707.06347. Shani, L., Efroni, Y., and Mannor, S. (2020). Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5668–5675. Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic policy gradient algorithms. Journal of Machine Learning Research. Sutton, R. S. and Barto, A. G. (2018). supervised learning: An Introduction. MIT Press, 2 edition. Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000). Policy gradient methods for re- inforcement learning with function approximation. In Advances in Neural Information Processing Systems (NeurIPS), pages 1057–1063. Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE.
Supplementary Material: A general class of surrogate functions for stable and efficient supervised learning Organization of the Appendix A Handling stochastic value gradients B Proofs for Section 4 C Proofs for Section 5 D Experimental details in the bandit setting E Experiments in the tabular setting F Analytical Updates and Gradient Expressions for tabular PG Algorithms G Additional experiments on MuJoCo environments A Handling stochastic value gradients Thus far we have worked with the original formulation of policy gradients where a policy is a distribution over actions given states. An alternative approach is that taken by stochastic value gradients (Heess et al., 2015), that rely on the reparametrization trick. In this case, a policy is not represented by a distribution over actions but rather by a set of actions. Formally, if ε are random variables drawn from a fixed distribution ν, then policy π is a deterministic map from S × ν → A. This corresponds to the functional representation of the policy. The action a chosen by π in state s (when fixing the random variable (cid:15) = ε) is represented as π(s, (cid:15)) and (cid:90) (cid:88) J(π) = dπ(s) ν(ε) r(s, π(s, ε)) dε (10) s ε and Silver et al. (2014) showed that ∂∂ πJ (( sπ , (cid:15)) ) = dπ(s)∇ aQπ(s, a)(cid:12) (cid:12) a=π(s,(cid:15)). If the policy π is parameterized by model f with parameters θ, then π(s, (cid:15)) = f (θ, s, (cid:15)). If f (θ , (cid:15)) and f (θ, (cid:15)) are t S-dimensional vectors, then Eq. (3) is given as (cid:34) (cid:35) θ t+1 = arg min E (cid:15)∼ν − (cid:88) dπt(s)f (θ, s, (cid:15))∇ aQπt(s, a)(cid:12) (cid:12) a=f(θt,s,(cid:15)) + η1 D φ(f (θ, (cid:15)), f (θ t, (cid:15))) . (11) s Similar to Sections 5.1 and 5.2, we will use a mirror map that decomposes across states. Specifically, we choose D φ(π, µ) = (cid:80) s∈S dπt(s) ||π(s) − µ(s)||2. With this choice, Eq. (11) can be written as: (cid:20) (cid:20) (cid:20) (cid:21)(cid:21)(cid:21) θ t+1 = arg max E s∼dπt E (cid:15)∼ν f (θ, s, (cid:15))∇ aQπt(s, a)(cid:12) (cid:12) a=f(θt,s,(cid:15)) − η1 ||f (θ, (cid:15)) − f (θ t, (cid:15))||2 (12) This formulation is similar to Eq (15) of (Silver et al., 2014), with Qπt instead of Qπ. Additionally, while the authors justified the off-policy approach with an approximation, our formulation offers guarantees provided η satisfies the condition of Proposition Proposition 1.
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux B Proofs for Section 4 Proposition 1 (Guarantee on surrogate function). The surrogate function (cid:96)π,φ,η is a lower bound t of J if and only if J + 1 φ is a convex function of π. η Proof. 1 J(π) − (cid:96)π,φ,η(π) = J(π) − J(π ) − (cid:104)π − π , ∇ J(π )(cid:105) + D (π, π ) t t t π t η φ t 1 = J(π) − J(π ) − (cid:104)π − π , ∇ J(π )(cid:105) + (φ(π) − φ(π ) − (cid:104)∇ φ(π ), π − π (cid:105)) t t π t η t π t t (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 1 1 = J + φ (π) − J + φ (π ) − (cid:104)π − π , ∇ J + φ (π )(cid:105) . η η t t π η t The last equation is positive for all π and all π if and only if J + 1 φ is convex. t η Theorem 1 (Guaranteed policy improvement for Algorithm 1). Assume that (cid:96) is β-smooth w.r.t. t the Euclidean norm and that η satisfies the condition of Proposition 1. Then, for any α ≤ 1/β, iteration t of Algorithm 1 guarantees J(π ) ≥ J(π ) for any number m of inner-loop updates. t+1 t Proof. Using the update in Algorithm 1 with α = 1 and the β-smoothness of (cid:96) (ω), for all k ∈ [m − 1], β t 1 (cid:96) (ω ) ≥ (cid:96) (ω ) + ||∇(cid:96) (ω )||2 t k+1 t k 2β t k After m steps, m−1 1 (cid:88) (cid:96) (ω ) ≥ (cid:96) (ω ) + ||∇(cid:96) (ω )||2 t m t 0 2β t k k=0 Since θ = ω and ω = θ in Algorithm 1, t+1 m 0 t m−1 1 (cid:88) =⇒ (cid:96) (θ ) ≥ (cid:96) (θ ) + ||∇(cid:96) (θ )||2 + ||∇(cid:96) (ω )||2 t t+1 t t 2β t t t k k=1 Note that J(π ) = (cid:96) (θ ) and if η satisfies Proposition 1, then J(π ) ≥ (cid:96) (θ ). Using these relations, t t t t+1 t t+1 m−1 1 (cid:88) J(π ) ≥ J(π ) + ||∇(cid:96) (θ )||2 + ||∇(cid:96) (ω )||2 =⇒ J(π ) ≥ J(π ). t+1 t 2β t t t k t+1 t k=1 (cid:124) (cid:123)(cid:122) (cid:125) +ve
A general class of surrogate functions for stable and efficient supervised learning C Proofs for Section 5 In this section, we first prove the equivalence of the formulations in terms of the logits and in terms of log π. Lemma 1. Let (cid:32) (cid:33) (cid:88) φ(z) = log exp(z(a)) (13) a exp(z(a)) pπ(a) = . (14) (cid:80) exp(z(a(cid:48))) a(cid:48) Then D (z, z(cid:48)) = KL(pπ(cid:48) ||pπ) . (15) φ where pπ and pπ(cid:48) use z and z(cid:48) respectively. Proof. (cid:32) (cid:88) (cid:33) (cid:32) (cid:88) (cid:33) (cid:80) exp(z(cid:48)(a))(z(a) − z(cid:48)(a)) D (z, z(cid:48)) = log exp(z(a)) − log exp(z(cid:48)(a)) − a φ (cid:80) exp(z(cid:48)(a)) a a a (cid:32) (cid:32) (cid:33) (cid:32) (cid:33)(cid:33) = (cid:88) pπ(cid:48) (a) z(a) − z(cid:48)(a) + log (cid:88) exp(z(a)) − log (cid:88) exp(z(cid:48)(a)) a a a = (cid:88) pπ(cid:48) (a) log pπ(a) . pπ(cid:48)(a) a Proposition 4. (cid:96)z t π,φ,η(θ) = J(π t) + E (s,a)∼µπt (cid:18) Aπt(s, a) + η1 (cid:19) log pp ππ t( (a a| |s s, ,θ θ) ) (16) Proof. Because (cid:80) pπt(a|s)Aπt(s, a) = 0, we can shift all values of z by a term that does not depend on a without a changing the sum, in particular by log ((cid:80) exp(zπ(a(cid:48), s|θ)). Thus, a(cid:48) (cid:32) (cid:32) (cid:33)(cid:33) (cid:96)z t π,φ,η(θ) = J(π t) + E (s,a)∼µπt Aπt(s, a) zπ(a, s|θ) − log (cid:88) exp(zπ(a(cid:48), s|θ)) a(cid:48) 1 (cid:88) − dπt(s)D (zπ(·, s|θ), zπ(·, s, θ )) η φz t s 1 (cid:88) = J(π t) + E (s,a)∼µπt Aπt(s, a) log pπ(a|s, θ) − η dπt(s)D φz (zπ(·, s|θ), zπ(·, s|θ t)) s = J(π t) + E (s,a)∼µπt Aπt(s, a) log pπ(a|s, θ) − η1 (cid:88) dπt(s)KL((pπ(cid:48) (·|s)||pπ(·|s)) , s where the last line is obtained using Lemma 1. Expanding the KL leads to the desired result. Proposition 2 (Improvement guarantees for direct functional representation). Assuming that the rewards are in [0, 1], when using the surrogate function in Eq. (6) with the mirror map chosen to be the negative entropy, then J ≥ (cid:96)π,φ,η for η ≤ (1−γ)3 . t 2γ|A|
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux (cid:16) (cid:17) Proof. Agarwal et al. (2020) show that, when using the direct parameterization, J is 2γ|A| -smooth w.r.t. (1−γ)3 the Euclidean distance. By using the properties of relative smoothness (Lu et al., 2018), if the mirror map φ is µ-strongly convex w.r.t. Euclidean distance, then J is L-smooth with L = (2γ|A|/(1−γ)3 µ). Using the fact that negative entropy is 1-strongly convex w.r.t. the 1-norm, we can set η = (1−γ)3/2γ|A| in Eq. (6). To prove the value of η guaranteeing improvement for the softmax parameterization, we first need to extend a lower bound result from Ghosh et al. (2020): Lemma 2. Let us assume that the rewards are lower bounded by −c for some c ∈ R. Then we have (cid:20)(cid:18) c (cid:19) pπ(a|s) (cid:21) J(π) ≥ J(π t) + E (s,a)∼µπt Qπt(s, a) + 1 − γ log pπt(a|s) . (17) Proof. Let us define the function J for a policy ν as ν J (π) = (cid:88)+∞ γh (cid:90) (r(s , a ) + c) (cid:18) 1 + log π h(τ h) (cid:19) ν (τ ) dτ − c , ν h h ν (τ ) h h h 1 − γ h=0 τh h h where τ is a trajectory of length h that is a prefix of a full trajectory τ and π is the policy restricted to h h trajectories of length h. We first show that it satisfies J (π) ≤ J(π) for any ν and any π such that the support of ν ν covers that of π. Indeed, we can rewrite (cid:90) (cid:18) c (cid:19) c J(π) = R(τ ) + π(τ ) dτ − 1 − γ 1 − γ τ (cid:32) (cid:33) = (cid:90) (cid:88) γh(r(a , s ) + c) π(τ ) dτ − c (using (cid:80) γhc = c/(1 − γ)) h h 1 − γ h τ h (cid:88) (cid:90) c = γh (r(a , s ) + c)π (τ ) dτ − , h h h h h 1 − γ h τh where the last line is obtained by marginalizing over steps h + 1, . . . , +∞ for all h and all trajectories τ . Because r(a , s ) + c is positive, as the rewards are lower bounded by −c, we have h h J(π) = (cid:88) γh (cid:90) (r(a , s ) + c) π h(τ h) ν (τ ) dτ − c h h ν (τ ) h h h 1 − γ h τh h h ≥ (cid:88) γh (cid:90) (r(a , s ) + c) (cid:18) 1 + log π h(τ h) (cid:19) ν (τ ) dτ − c (using x ≥ 1 + log x) h h ν (τ ) h h h 1 − γ h τh h h = J (π) . ν Let us denote JSA the right-hand side of Eq. (17), i.e.: ν (cid:20)(cid:18) c (cid:19) pπ(a|s) (cid:21) JSA(π) = J(ν) + E Qν(s, a) + log . ν (s,a)∼µν 1 − γ pν(a|s) We now prove that J has the same gradient as JSA: ν ν ∇ J (π) = ∇ (cid:32) (cid:88) γh (cid:90) (r(a , s ) + c) (cid:18) 1 + log π h(τ h) (cid:19) ν (τ ) dτ (cid:33) θ ν θ h h ν (τ ) h h h h τh h h (cid:32) (cid:33) (cid:90) (cid:88) = ∇ γh (r(a , s ) + c) log π (τ )ν (τ ) dτ θ h h h h h h h h τh (cid:90) (cid:88) = γh (r(a , s ) + c)∇ log π (τ )ν (τ ) dτ , h h θ h h h h h h τh
A general class of surrogate functions for stable and efficient supervised learning where all terms independent of θ were moved outside of the gradient. As the log probability of a trajectory decomposes into a sum of the probabilities of actions given states and of the transition probabilities, and as the latter are independent of θ, we get (cid:90) (cid:88) ∇ J (π) = γh (r(a , s ) + c)∇ log π (τ )ν (τ ) dτ θ ν h h θ h h h h h h τh (cid:32) (cid:33) (cid:90) (cid:88) (cid:88) = γh (r(a , s ) + c) ∇ log pπ(a |s ) ν (τ ) dτ h h θ h(cid:48) h(cid:48) h h h h τh h(cid:48) (cid:90) (cid:32) +∞ (cid:33) (cid:88) (cid:88) = ∇ log pπ(a |s ) γh(r(a , s ) + c) ν(τ ) dτ . θ h(cid:48) h(cid:48) h h τ h(cid:48) h=h(cid:48) But +∞ (cid:18) (cid:19) (cid:88) γh(r(a , s ) + c) = γh(cid:48) Qν(s, a) + c h h 1 − γ h=h(cid:48) (cid:90) ν(τ )dτ 1 1 = dh(cid:48) (s)ν(a|s) , a h(cid:48)=a s h(cid:48)=s ν τ with dh(cid:48)(s) the undiscounted probability of reaching state s at timestep h(cid:48). Hence, we have ν (cid:90) (cid:32) +∞ (cid:33) (cid:88) (cid:88) ∇ J (π) = ∇ log pπ(a |s ) γh(r(a , s ) + c) ν(τ ) dτ θ ν θ h(cid:48) h(cid:48) h h τ h(cid:48) h=h(cid:48) (cid:18) (cid:19) = (cid:88) (cid:88) (cid:88) ∇ log pπ(a|s)dh(cid:48) (s)ν(a|s)γh(cid:48) Qν(s, a) + c θ ν 1 − γ h(cid:48) s a (cid:18) (cid:19) = (cid:88) γh(cid:48) (cid:88) dh(cid:48) (s) (cid:88) ∇ log pπ(a|s)ν(a|s) Qν(s, a) + c ν θ 1 − γ h(cid:48) s a (cid:18) (cid:19) (cid:88) (cid:88) c = dν(s) Qν(s, a) + ν(a|s)∇ log pπ(a|s) 1 − γ θ s a (cid:32) (cid:18) (cid:19) (cid:33) (cid:88) (cid:88) c = ∇ dν(s) Qν(s, a) + ν(a|s) log pπ(a|s) θ 1 − γ s a (cid:18) (cid:20)(cid:18) c (cid:19) pπ(a|s) (cid:21)(cid:19) = ∇ J(ν) + E Qν(s, a) + log θ (s,a)∼µν 1 − γ pν(a|s) = ∇ JSA(π) , θ ν with dν(s) the unnormalized probability of s under the discounted stationary distribution. Because J and JSA have the same gradient, they differ by a constant, i.e. JSA = J + C for some C. But we ν ν ν ν also know that J (ν) = J(ν), which means that ν C = JSA(ν) − J (ν) ν ν = JSA(ν) − J(ν) ν (cid:20)(cid:18) c (cid:19) pν(a|s) (cid:21) = E Qν(s, a) + log (s,a)∼µν 1 − γ pν(a|s) = 0 . Hence, J = JSA and, becomes J is a lower bound of J, we have ν ν ν (cid:88) (cid:88) (cid:18) c (cid:19) pπ(a|s) J(π) ≥ J(ν) + dν(s) Qν(s, a) + pν(a|s) log . (18) 1 − γ pν(a|s) s a Setting ν = π concludes the proof. t
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux Proposition 3 (Improvement guarantees for softmax functional representation). Assuming that the rewards are in [0, 1], then the surrogate function in Eq. (8) satisfies J ≥ (cid:96)π,φ,η for η ≤ 1 − γ. t Proof. Assume 1 − γ η = . (19) r − r m l We know from Proposition 4 that (cid:96)z t π,φ,η(θ) ≤ J(π t) + E (s,a)∼µπt (cid:18) Aπt(s, a) + η1 (cid:19) log pp ππ t( (a a| |s s, ,θ θ) ) . Since the rewards are between r and r , we have l m (cid:96)z t π,φ,η(π) ≤ J(π t) + E (s,a)∼µπt (cid:20)(cid:18) Aπt(s, a) + η1 (cid:19) log pp ππ t( (a a| |s s) ) (cid:21) (cid:20)(cid:18) r − r (cid:19) pπ(a|s) (cid:21) = J(π t) + E (s,a)∼µπt Aπt(s, a) + 1m − γ l log pπt(a|s) (cid:20)(cid:18) (cid:18) r (cid:19) r (cid:19) pπ(a|s) (cid:21) = J(π t) + E (s,a)∼µπt Aπt(s, a) + V πt(s) + 1 −m γ − V πt(s) − 1 −l γ log pπt(a|s) (cid:20)(cid:18) r (cid:19) pπ(a|s) (cid:21) = J(π t) + E (s,a)∼µπt Qπt(s, a) − 1 −l γ log pπt(a|s) (cid:20)(cid:18) (cid:19) (cid:21) r − E s∼dπt 1 −m γ − V πt(s) KL(pπt(·|s)||pπ(·|s)) . The last term on the RHS of the last equation is negative. Indeed, because the rewards are less than r , the m value functions are less than r m/(1 − γ) and r m/(1 − γ) − V πt(s) is positive. As the KL divergences are positive, the product of the two is positive and the whole term is negative because of the minus term. Thus, we have (cid:96)z t π,φ,η(π) ≤ J(π t) + E (s,a)∼µπt (cid:20)(cid:18) Qπt(s, a) − 1 −r l γ (cid:19) log pp ππ t( (a a| |s s) ) (cid:21) ≤ J(π) . (by Lemma 2) Hence, choosing η = 1−γ leads to an improvement guarantee. Because our rewards are bounded between 0 and rm−rl 1, setting r = 1 and r = 0 gives η = 1 − γ. This concludes the proof. m l D Experimental details in the bandit setting In this section, we detail the experimental setup for the bandit experiments in Section 6.1. We consider different K-armed Bernoulli bandit problems. For sEXP3, we specialising the update rule in Eq. (8) to this multi-armed bandit case yielding: pπt+1(a) = pπt(a)(1 + ηAπt(a)), where η needs to be chosen such that the probabilities are always positive. However, the computing the advantage either requires knowledge of the rewards of all arms, or an estimate thereof. Since EXP3 is an adversarial bandit algorithm and does not exploit the stochasticity in the rewards, to ensure a fair comparison, we cannot use such an estimate and thus replace the advantage with the immediate reward, leading to the final sEXP3 update: pπt+1(a) = pπt(a)(1 + ηrˆ (a)) , t where rˆ (a) an estimator of the reward r (a) obtained at round t. t t
A general class of surrogate functions for stable and efficient supervised learning For sEXP3, if A is the action taken at round t, then we use the importance weighted estimator rˆ (a) = I{A = t t t 1}r (a)/π (a). For EXP3, we consider both the standard importance weighted estimator (referred to as IWEXP3 t t in the plots) and the loss based importance weighted estimator (referred to as LBIWEXP3 in the plots) for which rˆ (a) = I{A = 1}(1 − r (a))/π (a). t t t t Before describing our experimental setup, we emphasize that there are two different sources of randomness in our experiments. First, we have the environment seed that controls the mean rewards in the bandit problem. Considering different environment seeds guarantees that our results are not specific to a particular choice of the rewards. Given a specific bandit problem, since EXP3 and sEXP3 are randomized bandit algorithms, there is a stochasticity in the actions chosen. We can use different agent seeds to control the algorithm randomness. Following the evaluation protocol of (Vaswani et al., 2020), we consider two classes of bandits with different action gaps (difference in the mean rewards) – hard instances (∆ = 0.5) and easy instances (∆ = 0.1). The mean vector defining a Bernoulli bandit is then sampled entry wise (for each arm) from U(0.5 − ∆/2, 0.5 + ∆/2). To obtain the plot in Section 6.1, we run the experiment for 50 different environment seeds and one agent seed. We evaluated the three algorithms for Bernoulli bandits with K ∈ {2, 10, 100} arms and the difficulty of the problem, as determined by the action gap. For each algorithm, we set the step-size via a grid search over η ∈ {0.5, 0.05, 0.005, 0.0005, 0.00005}. The plot shows the regret corresponding to the step-size with lowest final average regret.
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux E Experiments in the tabular setting In this section1, we study the performance of four different policy gradient (PG) algorithms. Two of these can be directly obtained from the FMA-PG framework: sMDPO (FMA-PG with a softmax policy and log-sum-exp mirror map; see Eq. 8 in the main text) and MDPO (FMA-PG with direct parameterization and a negative entropy mirror map; see Eq. 7 in the main text). And the other two are the existing popular PG algorithms: TRPO and PPO. Further, to better understand the reason behind the performance of each of these methods, in addition to studying the objective functions used by these PG algorithms, we will also consider the impact of the optimization techqniques used to implement them. In particular, we will look at three different variants of sMDPO, MDPO, and TRPO based on whether they use a regularized objective with a fixed step-size (similar to the conventional sMDPO and MDPO), a regularized objective with Armijo line search, or a constrained objective with line search (similar to the conventional TRPO). E.1 Algorithmic Details We begin by specifying the different surrogate objectives used by the different algorithms and the two optimization procedures we use for maximizing these objectives. The sMDPO and MDPO algorithms motivated by the FMA-PG framework can be considered as regularized algorithms, which can be summarized by 1 max J − C , (20) θ PG-Alg η PG-Alg where the terms J and C are given in Table 1. One way of solving this objective is by gradient descent PG-Alg PG-Alg using a fixed step-size α, as specified in Algorithm 1; we call this setting as Regularized + fixed step-size. We can equivalently solve such an unconstrained optimization problem by using an Armijo-style backtracking line search, which we call as Regularized + line search. Note that, we can use this same form to obtain a regularized version of the TRPO algorithm2 as well. PG Alg. Objective (J ) Constraint (C) sMDPO (cid:80) dπt(s) (cid:80) pπt(a|s)Aπt(s, a) log pπθ (s,a) (cid:80) dπt(s) · KL(pπt(·|s)(cid:107)pπθ (·|s)) s a pπt(s,a) s TRPO (cid:80) dπt(s) (cid:80) pπt(a|s)Qπt(s, a) pπθ (s,a) (same as above) s a pπt(s,a) MDPO (cid:80) dπt(s) (cid:80) pπt(a|s)Aπt(s, a) pπθ (s,a) (cid:80) dπt(s) · KL(pπθ (·|s)(cid:107)pπt(·|s)) s a pπt(s,a) s Table 1: The objectives and the constraints corresponding to the different PG algorithms. Note that the objective J for both TRPO and MDPO is essentially equivalent to each other, since maximizing either of them would lead to the same solution. The reason for this is that the difference between the two objectives is (cid:80) dπtV πt(s), which s is independent of the policy weight θ). On the other hand, the conventional TRPO algorithm instead solves a constrained optimization problem given by the equation max J subject to C ≤ δ, (21) PG-Alg PG-Alg θ with the terms J and C again given in Table 1. The regularized program of Eq. 20 can be considered PG-Alg PG-Alg a “softer” version of the constrained program of Eq. 21. To solve the constrained optimization problem, we use the exact same process used by the TRPO paper (Schulman et al., 2015): we use line search to find the maximal step-size that increases the objective value in the direction of maximum ascent while satisfying the constraint; see Section F.3 for details. We call this setting as Constrained + line search. Further, using Eq. 21, we can also obtained constrained versions of sMDPO and MDPO. 1The code implementation for the algorithms and the environment corresponding to experiments presented in this section is available at https://github.com/svmgrg/fma-pg. 2For TRPO, this objective is almost the same as PPO with KL penalty (Eq. 8, Schulman et al. (2017)) except that PPO uses the advantage function and we used the action value function (which, as we discussed in the caption of Table 1, doesn’t really matter). It is also similar to the objective stated in the TRPO paper (Section 4, Schulman et al. (2015)) except that this has an average KL divergence instead of the max KL divergence given in the original paper.
A general class of surrogate functions for stable and efficient supervised learning The motivation behind considering these three different variants for sMDPO, MDPO, and TRPO is to figure out how much of the performance difference between these algorithms comes from their exact objectives (Table 1) and how much of it comes from the optimization techniques employed. We also summarize the gradient of these objectives in Table 2. The corresponding gradient derivations for the algorithms (including PPO) are presented in Appendix F. PG Alg. Grad. objective (∇ J ) Grad. constraint (∇ C) θ(s,a) θ(s,a) sMDPO dπt(s)pπt(a|s)Aπt(s, a) dπt(s) [pπ(a|s) − pπt(a|s)] TRPO dπt(s)pπ(a|s) [Qπt(s, a) − (cid:80) pπ(b|s)Qπt(s, b)] (same as above) b dπt(s)pπ(a|s)× MDPO dπt(s)pπ(a|s) [Aπt(s, a) − (cid:80) b pπ(b|s)Aπt(s, b)] (cid:104) log pπ(a|s) − KL(pπ(·|s)(cid:107)pπt(·|s))(cid:105) pπt(a|s) Table 2: The gradients of the objectives and constraints w.r.t. the policy parameter corresponding to the different PG algorithms. Note that the gradient of the objective for both TRPO and MDPO is exactly equal to each other. E.2 Empirical Details In all our tabular experiments, we assumed full access to the environment dynamics and used the analytically calculated expected gradient updates for all the algorithms, and therefore the results closely follow the theoretical properties of the PG methods. Doing so, essentially made this a study of the optimization properties of the four PG algorithms considered. We use a policy gradient agent with a tabular softmax policy parameterization, and evaluate the algorithms on two tabular episodic environments: CliffWorld (environment description and its properties are discussed in Figure 5) and DeepSeaTreasure (Osband et al. (2019); with n = 5, discount factor γ = 0.9, 25 different states, and two actions). 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0 200 400 600 800 1000 )evitcejbo GP( J Visualizing the learned policy at different steps optimal policy performance CliffWorld # of iterations ffilC Te r m Si tn aa tl e G oal Start Figure 5: The episodic CliffWorld environment and the learning curve for MDPO on it illustrating three different locally optimal policies. (Left) We consider a variant of the CliffWorld environment (Example 6.6, Sutton and Barto (2018)) containing 21 different states and four actions per state. The agent starts in the Start state and has four cardinal actions which deterministically move it into the corresponding next state. The objective is to reach the Goal state as quickly as possible. If the agent falls into a state marked by Cliff, any subsequent action taken by it moves it back to the start state and yields a reward of −100. Similarly, once in the goal state, any action takes the agent into the terminal state and yields a reward of +1. All the other transitions have zero reward and the discount factor is γ = 0.9. It is easy to see that the optimal policy will have a value of v∗(s ) = 0 + γ · 0 + · · · + γ5 · 0 + γ6 · 1 = 0.96 = 0.53. (Right) We show the learning curve for the analytical 0 MDPO update using η = 1. This curve shows three different locally optimal policies. We later show in our experiments, that the different PG agents often get stuck on one of these policies.
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux )evitcejbo GP( J fo eulaV laniF #(inner updates): 10 100 1000 Learning Curves (m=100) Optimization Method 0.6 All the 3 methods sMDPO: η=0.125 Regularized have similar 0.4 sMDPO performance MDPO: η=0.125 + fixed 0.2 MDPO TRPO: η=0.125 stepsize TRPO 0.0 4 6 8 10 12 4 6 log8 (η)10 12 4 6 8 10 12 0 50 100 150 200 2 00 .. 46 A hall v t eh se i m3 im lae rt hods sMDPO N t sh eo ae r t m ce he: t a hT lroh ed ae s dp w ye ir stf aho tr ulm i rn aa e tn ec de ao tf s MM DD PP OO :: ηη == 00 .. 11 22 55 Regularized 0.2 performance M TRD PP OO 1 u0 p dn au tm esb :e ir t o hf a i sn an le mr ost the TRPO: η=0.125 + line search 0.0 same performance at 10 4 6 8 10 12 log 2(η) 4 6 8 10 12 a un pd d a1 t0 e0 s .n Tu hm eb ree fr o o ref ,i n wn ee r 0 50 100 150 200 didn't run it for 1000 0.6 All the 3 methods number of inner updates; have similar the performance should TRPO: δ=1.5×10-5 0.4 performance sMDPO r pe am raa min e ts eim r sil ea nr s t ito i vity plots sMDPO: δ=6.1×10-5 Constrained 0.2 MDPO for 10 and 100 number of TRPO inner updates. MDPO: δ=6.3×10-2 + line search 0.0 5 10 15 20 25 5 10 15 20 25 0 50 100 150 200 log (δ) 2 Figure 6: The parameter sensitivity plots for the PG algorithms on the DeepSeaTreasure environment for different number of inner loop updates. The x axis shows sweep over one parameter of the corresponding PG algorithm. And for each point on the x-axis, we chose the best performing second parameter of the algorithm: the inner loop step-size α for the first row, the Armijo constant for the second row, and there is no additional parameter for the last row. The faint black line near the top of each subplot depicts the value of the optimal policy. The last column shows the learning curves for the best performing parameter configuration for each method. Hyperparameter configurations: We trained each of the method for 2000 iterations for CliffWorld (200 iterations for DeepSeaTreasure). Each iteration consisted of multiple inner loop updates (we represent this number by m); these updates are performed in an off-policy fashion that is typical of all these algorithms (also see Algorithm 1 in the main paper). We also swept over the relevant parameters of the PG algorithms. For the Regularized variants (both with and without line search) of sMDPO, MDPO, and TRPO, this was η ∈ {2−13, 2−12, . . . , 2−1}. For fixed step-size variant of sMDPO, MDPO, TRPO, and PPO, we swept over the inner loop step-size α ∈ {2−13, 2−12, . . . , 23} for CliffWorld (and α ∈ {2−13, 2−12, . . . , 2−2} for DeepSeaTreasure). For PPO, we additionally considered the clipping parameter (cid:15) ∈ {0.01, 0.1, 0.2, 0.3 . . . , 0.8, 0.9, 0.99}. For the Regularized + line search variant of sMDPO, MDPO, TRPO, we also considered different Armijo constants in the set {0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99}, used a decay factor of 0.9, initialized the maximal step-size to 10.0 and fixed the warm-start factor to 2.0. Finally, for the Constrained + line search variant of sMDPO, MDPO, and TRPO, we swept over the trust region size δ ∈ {2−24, 2−22, . . . , 2−2}, used a fixed backtracking decay parameter of 0.9, an analytically obtained maximal step-size (see Appendix F.3), and an Armijo constant of 0.0 (i.e. no Armijo line search). E.3 Experimental Results Learning Curves: We show the learning curves corresponding to the best performing hyperparameters for the four algorithms conventional sMDPO and MDPO (Regularized + fixed step-size), conventional TRPO (Constrained + line search), and PPO in Figure 2 (main paper). To select the hyperparameters for each setting, we ran sweeps over different configurations and chose the ones that resulted in the best final performance at the end of 2000 iterations for CliffWorld (and 200 iterations for DeepSeaTreasure). From Figure 2, we see that for CliffWorld, all the methods except PPO (PPO got stuck in a “safe” sub-optimal policy) were able to converge to the optimal policy, and TRPO had the fastest convergence (learned the optimal policy in less than 200 iterations). On the other hand for DeepSeaTreasure, we note that all the methods converged to the optimal policy, with PPO having the fastest convergence and TRPO the slowest. Additionally, we should also mention that the TRPO’s update was the costliest (more than two times slower than the rest of the methods) in terms of wall time, likely because of the backtracking from the line-search. Parameter Sensitivity and Ablation Study: We show the final performance for sMDPO, MDPO, and TRPO in Figure 3 (after 2000 iterations for CliffWorld; main paper) and Figure 6 (after 200 iterations for DeepSeaTreasure). The different rows correspond to the variants Regularized + fixed step-size, Regularized + line search, and Constrained + line search for each of the methods. And the different columns correspond to different
A general class of surrogate functions for stable and efficient supervised learning 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 0 500 1000 1500 2000 Epsilon value (ϵ) # of iterations 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 0 50 100 150 200 Epsilon value (ϵ) # of iterations )evitcejbo GP( J fo eulaV laniF #(inner updates): 10 100 1000 Learning Curves (m=100) Environment PPO: ϵ=0.1 CliffWorld PPO PPO PPO: ϵ=0.3 DeepSeaTreasure Figure 7: The parameter sensitivity plots for PPO on the CliffWorld and DeepSeaTreasure environments for different number of inner loop updates. The x axis shows sweep over the clipping parameter (cid:15). The curve shows the final performance of the method for the best performing inner loop stepsize α given the (cid:15) value. number of inner loop updates3. The last column in each row shows the learning curves for the best performing parameter setting. The x-axis on each subplot of the first two rows shows the regularization strength η. For the Regularization + fixed stepsize variant, we chose the best performing α for each η, and for Regularized + line search variant, we chose the best performing Armijo constant for each η. The last row (constrained variant) had only a single parameter, the trust region magnitude, δ that is shown on the x-axis. From these figures, we see that as the value of m increased, the performance of the fixed step-size algorithms improved. We also note that adding line search to regularized methods improved their parameter sensitivity to a large extent. Although, for CliffWorld, none of the Regularized + line search variant were able to achieve the optimal policy. We believe that the reason for this is that with warm-start the algorithms started using very large stepsizes (as large as 1000), which lead to an early convergence to a locally optimal policy. To verify this further, we tried running these algorithms (experiments not shown here) without warm start and a maximal stepsize of 1.0; this allowed the methods to achieve the optimal policy for a small range of η values, but also made them much more sensitive different values of η. For the constrained version, we see that all the three algorithms achieved the optimal policy and were generally insensitive to the δ values. This is likely because the constrained variant used the (near) optimal steepest ascent direction with the maximal stepsize, achieved via a backtracking line search. Finally, we note that for DeepSeaTreasure, all the methods had essentially the same performance and achieved the optimal policy in each case; we attribute this to the simplicity of the environment coupled with access to the true gradient updates. We also provide the sensitivity plot for PPO for the two environments in Figure 7. We again see that increasing the number of inner loop updates helps the performance of PPO on both the environments. We also note that for no value of the parameters we tested, did PPO achieve the optimal policy on CliffWorld. E.4 Discussion These experiments served to demonstrate three major points: 1. The optimization methods might matter as much as the policy gradient objectives being considered. We found that much of TRPO’s performance came from formalizing the optimization problem as a constrained program and solving it using the optimal descent direction and a stepsize found using line search. In particular, not only did TRPO’s performance suffer when we replaced the constraint with regularization, but the performance of both sMDPO and MDPO also improved significantly when we used TRPO style of optimization on their objectives. Additionally, we found that line search greatly improved the parameter sensitivity of all the algorithms. 2. The optimal η values chosen by the Regularized + fixed stepsize variants of sMDPO and MDPO were 3For the Constrained + line search of each method, we observed that the performance saturated after m = 10; in particular the sensitivity plots are identical for m = 10 and m = 100. Therefore, the performance at m = 1000 should be exactly equivalent to the performance given at m = 100, and consequently we skipped running that experiment.
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux much larger than the values predicted by our theoretical results. For instance, the maximal η values for CliffWorld, as studied by the FMA-PG framework, are 1 − γ 1 − 0.9 η = = = 9.9 × 10−4, sMDPO r − r 100 − (−1) m l (1 − γ)3 (1 − 0.9)3 η = = = 1.4 × 10−6. MDPO (r − r ) · 2γ|A| 101 × 2 × 0.9 × 4 m l Similarly for DeepSeaTreasure, they are 1 − 0.9 η = = 1.0 × 10−2 sMDPO 1 − (−0.01/5) (1 − 0.9)3 η = = 2.8 × 10−4. MDPO (1 − (−0.01/5)) × 2 × 0.9 × 2 Note that these values of η are extremely small, and while the FMA-PG framework still guarantees policy improvement with these values, the convergence would be much slower than that shown in our experiments. This is natural since these bounds on η are based on the smoothness of the policy objective J and from optimization literature, we know that such bounds are usually loose. Finally, also note that the optimal η for sMDPO found by the experiments (for instance, that given in Figure 2) is closer to that predicted by the theory, as compared to MDPO. 3. Each of the algorithms benefited from increasing the number of inner loop updates. These off-policy type of updates enables the PG algorithms to “maximally squeeze” out all the information present in the data they have already collected, thereby allowing them to improve their performance without any additional interaction with the environment. This demonstrates the strength of these methods over simpler algorithms, such as REINFORCE (Williams, 1992), which have only a single update per batch of sampled data. To conclude, our experiments suggest that the FMA-PG framework provides general purpose surrogate functions with policy improvement guarantees, which when combined with existing optimization techniques can yield policy gradient algorithms that are competitive to existing state-of-the-art methods.
A general class of surrogate functions for stable and efficient supervised learning F Analytical Updates and Gradient Expressions for tabular PG Algorithms In this section, we give the calculations for the closed form analytical solutions for sMDPO and MDPO, and the gradient expressions for all the four algorithms employed in our implementation for tabular PG algorithms given in Appendix E. F.1 sMDPO with Tabular Parameterization We begin by considering the conventional sMDPO algorithm with a regularized objective. F.1.1 Closed Form Update with Direct Representation Our goal is to find the closed form solution to the following optimization problem (from Eq. 8, main paper): (cid:34) (cid:88) (cid:88) (cid:18) 1 (cid:19) pπ(s, a) (cid:35) π = arg max dπt(s) pπt(a|s) Aπt(s, a) + log , (22) t+1 π∈Π η pπt(s, a) s a (cid:124) (cid:123)(cid:122) (cid:125) =:(cid:96)πt sMDPO subject to the constraints on policy pπ. We will solve this problem by assuming the policy π ≡ pπ as an |S| × |A| table satisfying the standard constraints (cid:88) pπ(a|s) = 1, ∀s ∈ S a pπ(a|s) ≥ 0, ∀s ∈ S, ∀a ∈ A. We begin by formulating this problem using Lagrange multipliers {λ } and {λ } for all states s and s s∈S s,a s,a∈S×A actions a: (cid:88) (cid:88) (cid:18) 1 (cid:19) pπ(a|s) L(pπ, λ , λ ) = dπt(s) pπt(a|s) Aπt(s, a) + log s s,a η pπt(a|s) s a (cid:18) (cid:19) (cid:88) (cid:88) (cid:88) − λ pπ(a|s) − λ pπ(a|s) − 1 , (23) s,a s s,a s a where we abused the notation, in L(pπ, λ , λ ), by using λ to represent the set {λ } and λ to represent s s,a s s s∈S s,a the set {λ } . The KKT conditions (Theorem 12.1, Nocedal and Wright (2006)) for this constrained s,a s,a∈S×A optimization problem can be written as: ∇ L(pπ, λ , λ ) = 0, ∀x ∈ S, ∀b ∈ A (C1) pπ(b|x) s s,a (cid:88) pπ(a|s) = 1, ∀s ∈ S (C2) a pπ(a|s) ≥ 0, ∀s ∈ S, ∀a ∈ A (C3) λ ≥ 0, ∀s ∈ S (C4) s (cid:18) (cid:19) (cid:88) λ pπ(a|s) − 1 = 0, ∀s ∈ S (C5) s a λ pπ(a|s) = 0, ∀s ∈ S, ∀a ∈ A. (C6) s,a We now solve this system. Simplifying Eq. C1 for an arbitrary state-action pair (x, b) gives us: (cid:18) (cid:19) 1 1 ∇ pπ(b|x)L(pπ, λ s, λ s,a) = dπt(x)pπt(b|x) Aπt(x, b) + η pπ(b|x) − λ x,b − λ x = 0 dπt(x)pπt(b|x)(1 + ηAπt(x, b)) ⇒ pπ(b|x) = . (24) η(λ + λ ) x x,b Let us set λ = 0, ∀s ∈ S, ∀a ∈ A. (25) s,a
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux Combining Eq. 24 with the second KKT condition gives us 1 (cid:88) λ = dπt(s)pπt(a|s)(1 + ηAπt(s, a)). (26) s η a Therefore, with the standard coverage assumption dπt(s) > 0, pπ(a|s) becomes pπt(a|s)(1 + ηAπt(s, a)) pπ(a|s) = . (27) (cid:80) pπt(b|s)(1 + ηAπt(s, b)) b Note that dπt(s), pπt(a|s) ≥ 0 for any state-action pair, since they are proper measures. We also need to ensure that 1 + ηAπt(s, a) ≥ 0 to satisfy the third and fourth KKT conditions. One straightforward way to achieve this is to define pπ(a|s) = 0 whenever 1 + ηAπt(s, a) < 0, and accordingly re-define λ s. This gives us the final solution to our original optimization problem (Eq. 22): pπt(a|s) max(1 + ηAπt(s, a), 0) π = pπ(s, a) = . (28) t+1 (cid:80) pπt(b|s) max(1 + ηAπt(s, b), 0) b However, it leaves us one last problem to deal with: ensuring that for any state s, there always exists at least one action a, such that 1 + ηAπt(s, a) > 0. This is not a problem since we can put a condition on η in order to fulfill this constraint. F.1.2 Gradient of the Loss Function with Softmax Policy Representation Consider the softmax policy representation eθ(x,b) pπ(b|x) = , (29) (cid:80) eθ(x,c) c where θ(x, b) for all state-action pairs (x, b) are action preferences maintained in a table (tabular parameterization). Also note that the derivative of the policy with respect to the action preferences is given by ∂ (cid:16) (cid:17) pπ(b|x) = I(x = s) I(b = a) − pπ(a|x) pπ(b|x), (30) ∂θ(s, a) where I(a = b) is the identity function when a = b and zero otherwise. We will use gradient ascent to approximately solve Eq. 22; to do that, the quantity of interest is (cid:20) (cid:21) (cid:20) (cid:21) ∂ (cid:88) (cid:88) ∂ ∂ (cid:96)πt = pπ(b|x) (cid:96)πt (using total derivative) ∂θ(s, a) sMDPO ∂θ(s, a) ∂pπ(b|x) sMDPO x∈S b∈A (cid:88) (cid:104) (cid:16) (cid:17) (cid:105) (cid:20) (cid:18) 1 (cid:19) 1 (cid:21) = I(x = s) I(b = a) − pπ(a|x) pπ(b|x) dπt(x)pπt(b|x) Aπt(x, b) + η pπ(b|x) x,b (cid:20) (cid:16) (cid:17) (cid:18) 1 (cid:19)(cid:21) = E X∼dπt,B∼pπt(·|X) I(X = s) I(B = a) − pπ(a|x) Aπt(X, B) + η (31) (cid:88) (cid:16) (cid:17) (cid:18) 1 (cid:19) = dπt(s) I(b = a) − pπ(a|s) pπt(b|s) Aπt(s, b) + η b (cid:34) (cid:18) (cid:19) (cid:18) (cid:19)(cid:35) 1 (cid:88) 1 = dπt(s) pπt(a|s) Aπt(s, a) + − pπ(a|s) pπt(b|s) Aπt(s, b) + η η b (cid:20) (cid:18) 1 (cid:19) pπ(a|s) (cid:21) = dπt(s) pπt(a|s) Aπt(s, a) + − , η η Now we can simply update the inner loop of FMA-PG (Algorithm 1, main paper) via gradient ascent: (cid:20) (cid:18) 1 (cid:19) pπ(a|s) (cid:21) θ(s, a) ← θ(s, a) + αdπt(s) pπt(a|s) Aπt(s, a) + − . (32) η η
A general class of surrogate functions for stable and efficient supervised learning F.2 Mirror Descent Policy Optimization (MDPO) In this section, we study the MDPO type FMA-PG update (Eq. 7 in main paper). We first calculate the analytical solution to that optimization problem, and then calculate its gradient which we use in the experiments. However, in the analysis that follows, we we replace the advantage function Aπt with the action-value function Qπt to make it exactly same as the original MDPO (Tomar et al., 2020) update. F.2.1 Closed Form Update with Direct Parameterization While giving the MDPO type FMA-PG equation (Eq. 7), the paper considers the direct representation along with tabular parameterization of the policy, albeit with a small change in notation as compared to the previous subsection: π(a|s) ≡ pπ(a|s, θ). However, since this notation is more cumbersome, we will stick with our the notation of the previous subsection: π(a|s) ≡ pπ(a|s). The constraints on the parameters pπ(s, a) are the same as before: (cid:80) pπ(a|s) = 1, ∀s ∈ S; and pπ(a|s) ≥ 0, ∀s ∈ S, ∀a ∈ A. Our goal, this time, is to solve the following a optimization problem (from Eq. 6, main paper) (cid:34) (cid:88) (cid:88) (cid:18) pπ(a|s) 1 (cid:19)(cid:35) π = arg max dπt(s) pπt(a|s) Qπt(s, a) − D (pπ(·|s), pπt(·|s)) , (33) t+1 π∈Π pπt(a|s) η φ s a (cid:124) (cid:123)(cid:122) (cid:125) =:(cid:96)πt MDPO with the mirror map as the negative entropy (Eq. 5.27, Beck and Teboulle (2003)). This particular choice of the mirror map simplifies the Bregman divergence as follows (cid:88) pπ(a|s) D (pπ(·|s), pπt(·|s)) = KL(pπ(·|s)(cid:107)pπt(·|s)) := pπ(a|s) log . (34) φ pπt(a|s) a The optimization problem (Eq. 33) then simplifies to (cid:34) (cid:32) (cid:33)(cid:35) (cid:88) (cid:88) pπ(a|s) 1 (cid:88) pπ(a(cid:48)|s) π = arg max dπt(s) pπt(a|s) Qπt(s, a) − pπ(a(cid:48)|s) log . (35) t+1 π∈Π pπt(a|s) η pπt(a(cid:48)|s) s a a(cid:48) Proceeding analogously to the previous subsection, we use Lagrange multipliers λ , λ for all states s and actions s s,a a to obtain the function (cid:88) (cid:88) pπ(a|s) 1 (cid:88) (cid:88) pπ(a(cid:48)|s) L(pπ, λ , λ ) = dπt(s) pπt(a|s)Qπt(s, a) − dπt(s) pπ(a(cid:48)|s) log s s,a pπt(a|s) η pπt(a(cid:48)|s) s a s a(cid:48) (cid:18) (cid:19) (cid:88) (cid:88) (cid:88) − λ pπ(a|s) − λ pπ(a|s) − 1 . (36) s,a s s,a s a The KKT conditions are exactly the same as before (Eq. C1 to Eq. C6). Again, we begin by solving the first KKT condition: Qπt(x, b) dπt(x) (cid:20) pπ(b|x) (cid:21) ∇ pπ(b|x)L(pπ, λ s, λ s,a) = dπt(x)pπt(b|x) pπt(b|x) − η log pπt(b|x) + 1 − λ x,b − λ x = dπt(x) (cid:20) ηQπt(x, b) − log pπ(b|x) − 1 − η(λ x,b + λ x) (cid:21) η pπt(b|x) dπt(x) = 0 pπ(b|x) η(λ + λ ) ⇒ log = ηQπt(x, b) − x,b x − 1 pπt(b|x) dπt(x) ⇒ pπ(b|x) = pπt(b|x) · eηQπt(x,b) · e− η(λ dx π, tb (+ xλ )x) −1, (37) where in the fourth line, we used the assumption that dπt(x) > 0 for all states x. We again set λ = 0, ∀s ∈ S, ∀a ∈ A. (38) s,a
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux And, we put Eq. 37 in the second KKT condition to get (cid:32) (cid:33)−1 e− dπη tλ (x x) −1 = (cid:88) pπt(b|x) · eηQπt(x,b) . (39) b Therefore, we obtain pπt(a|s) · eηQπt(s,a) pπ(a|s) = . (40) (cid:80) pπt(b|s) · eηQπt(s,b) b This leaves us one last problem to deal with: ensuring λ ≥ 0 for all states s. Again, we can set the step-size η to s ensure this constraint. F.2.2 Gradient of the MDPO Loss Function with Tabular Softmax Representation We again take the softmax policy representation given by Eq. 29, and compute ∇ (cid:96)πt for the MDPO loss θ(s,a) MDPO (we substitute Qπt with Aπt in this calculation): (cid:20) (cid:21) (cid:20) (cid:21) ∂ (cid:88) ∂ ∂ (cid:96)πt = pπ(b|x) (cid:96)πt (using total derivative) ∂θ(s, a) MDPO ∂θ(s, a) ∂pπ(b|x) MDPO x,b (cid:88) (cid:104) (cid:16) (cid:17) (cid:105) (cid:20) dπt(x) (cid:18) pπ(b|x) (cid:19)(cid:21) = I(x = s) I(b = a) − pπ(a|x) pπ(b|x) ηAπt(x, b) − log − 1 η pπt(b|x) x,b dπt(s) (cid:88) (cid:16) (cid:17) (cid:20) pπ(b|s) (cid:21) = I(b = a) − pπ(a|s) pπ(b|s) ηAπt(s, b) − log − 1 η pπt(b|s) b (cid:34) (cid:35) dπt(s) (cid:88) pπ(a|s) = pπ(a|s) ηAπt(s, a) − η pπ(b|s)Aπt(s, b) − log + KL(pπ(·|s)(cid:107)pπt(·|s)) , η pπt(a|s) b where in the last line, we used the fact that (cid:88) (cid:20) pπ(b|s) (cid:21) (cid:88) pπ(b|s) ηAπt(s, b) − log − 1 = η pπ(b|s)Aπt(s, b) − KL(pπ(·|s)(cid:107)pπt(·|s)) − 1. pπt(b|s) b b F.3 Trust Region Policy Optimization (TRPO) At each step of the policy update, TRPO (Eq. 14, Schulman et al. (2015)) solves the following problem: (cid:88) (cid:88) (cid:88) max dπt(s) pπθ (a|s)Qπt(s, a) subject to dπt(s) · KL(pπt(·|s)(cid:107)pπθ (·|s)) ≤ δ. (41) θ s a s (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =:JTRPO =:CTRPO Unlike the sMDPO and the MDPO updates, an analytical solution cannot be derived for this update (since it would require solving a system of non-trivial non-linear equations). Therefore, we will use gradient based methods to approximately solve this problem. From Appendix C of Schulman et al. (2015), the descent direction is given by s ≈ A−1g where the vector g is defined as g := ∂ J , and the matrix A is defined (s,a) ∂θ(s,a) TRPO as A := ∂ ∂ C . We analytically compute the expression for this direction assuming a (s,a),(s(cid:48),a(cid:48)) ∂θ(s,a) ∂θ(s(cid:48),a(cid:48)) TRPO softmax policy (Eq. 29). The vector g can be readily calculated as ∂ (cid:88) (cid:88) ∂pπθ (b|x) J = dπt(x) Qπt(x, b) ∂θ(s, a) TRPO ∂θ(s, a) x b (cid:88) (cid:88) (cid:16) (cid:17) = dπt(x) Qπt(x, b)I(x = s) I(b = a) − pπθ (a|x) pπθ (b|x) x b (cid:34) (cid:35) (cid:88) (cid:88) (cid:88) = dπt(x)I(x = s) I(b = a)pπθ (b|x)Qπt(x, b) − pπθ (a|x) pπθ (b|x)Qπt(x, b) x b b (cid:34) (cid:35) (cid:88) = dπt(s)pπθ (a|s) Qπt(s, a) − pπθ (b|s)Qπt(s, b) . (42) b
A general class of surrogate functions for stable and efficient supervised learning For calculating the matrix A, we use the law of total derivative to obtain ∂ (cid:88) (cid:20) ∂ (cid:21) (cid:34) ∂ (cid:88) (cid:88) pπt(a|s) (cid:35) ∂θ(s, a) C TRPO = ∂θ(s, a) pπθ (b|x) ∂pπθ (b|x) dπt(s) pπt(a|s) log pπθ (a|s) x,b s a (cid:88) (cid:104) (cid:16) (cid:17) (cid:105) (cid:20) pπt(b|x) (cid:21) = I(x = s) I(b = a) − pπθ (a|x) pπθ (b|x) −dπt(x) pπθ (b|x) x,b (cid:88) (cid:16) (cid:17) = −dπt(s) I(b = a) − pπθ (a|s) pπt(b|s) b (cid:34) (cid:35) (cid:88) (cid:88) = −dπt(s) I(b = a)pπt(b|s) − pπθ (a|s) pπt(b|s) b b (cid:104) (cid:105) = dπt(s) pπθ (a|s) − pπt(a|s) . (43) Finally, using the above result yields ∂ ∂ ∂ (cid:104) (cid:105) ∂θ(s, a) ∂θ(s(cid:48), a(cid:48)) C TRPO = ∂θ(s, a) dπt(s(cid:48)) pπθ (a(cid:48)|s(cid:48)) − pπt(a(cid:48)|s(cid:48)) ∂ = dπt(s(cid:48)) · pπθ (a(cid:48)|s(cid:48)) ∂θ(s, a) (cid:16) (cid:17) = I(s(cid:48) = s) · dπt(s(cid:48)) I(a(cid:48) = a) − pπθ (a|s(cid:48)) pπθ (a(cid:48)|s(cid:48)) (44) (cid:16) (cid:17) ⇒ A = dπt(s) diag(pπθ (·|s)) − pπθ (·|s)pπθ (·|s)(cid:62) , (45) (s,:),(s,:) where pπθ (·|s) ∈ R|A| is the vector defined as [pπθ (·|s)] a = pπθ (a|s) and A (s,:),(s,:) denotes the square sub-block of the matrix A corresponding to the given state s and all the actions. In our experiments, since our A matrix is small, we directly take its inverse to compute the update direction, thereby bypassing the conjugate method. Once we have the update direction, we then compute the maximal stepsize β and perform a backtracking line search similar to the TRPO paper. F.4 Proximal Policy Optimization (PPO) The Proximal Policy Optimization algorithm (Schulman et al., 2017) solves the following optimization problem at each iteration step: m θax (cid:88) dπt(s) (cid:88) pπt(a|s) · min (cid:32) clip (cid:104) pπθ (ap p |π π sθ t )(( ,aa 1|| ss −)) A (cid:15)π ,t 1(s +, a (cid:15)) (cid:105), Aπt(s, a)(cid:33) . (46) s a pπt(a|s) (cid:124) (cid:123)(cid:122) (cid:125) =:JPPO The gradient of the objective J can be shown to be equivalent to PPO (cid:88) (cid:88) (cid:16) (cid:17) ∇pπθ (a|s) ∇J = dπt(s) pπt(a|s) · I cond(s, a) Aπt(s, a), (47) PPO pπt(a|s) s a where (cid:18) (cid:94) pπθ (a|s) (cid:19) (cid:95) (cid:18) (cid:94) pπθ (a|s) (cid:19) cond(s, a) = Aπt(s, a) > 0 < 1 + (cid:15) Aπt(s, a) < 0 > 1 − (cid:15) . (48) pπt(a|s) pπt(a|s)
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux Repeating our usual drill, we assume a softmax policy to obtain: ∂ J ∂θ(s, a) PPO (cid:88) (cid:88) (cid:16) (cid:17) ∂pπθ (b|x) = dπt(x) I cond(x, b) Aπt(x, b) ∂θ(s, a) x b (cid:88) (cid:88) (cid:16) (cid:17) (cid:16) (cid:17) = dπt(x) I cond(x, b) I(x = s) I(b = a) − pπθ (a|x) pπθ (b|x)Aπt(x, b) x b (cid:34) (cid:35) (cid:88) (cid:16) (cid:17) (cid:88) (cid:16) (cid:17) = dπt(s) I(b = a)I cond(s, b) pπθ (b|s)Aπt(s, b) − pπθ (a|s) I cond(s, b) pπθ (b|s)Aπt(s, b) b b (cid:34) (cid:35) (cid:16) (cid:17) (cid:88) (cid:16) (cid:17) = dπt(s)pπθ (a|s) I cond(s, a) Aπt(s, a) − pπθ (b|s)I cond(s, b) Aπt(s, b) . (49) b The PPO gradient (Eq. 49) is exactly the same as the TRPO gradient (Eq. 42) except for the additional condition on choosing only specific state-action pairs while calculating the difference between advantage under the current policy and the approximate change in advantage under the updated policy. F.5 MDPO with Constraints In this section, we calculate the second derivative of the MDPO constraint as given in Table 1. This will allow us compute the Hessian AMDPO, which is the analog of the A matrix from TRPO implementation, and help us implement MDPO with a constrained objective and line search. Continuing from the gradient of the MDPO constraint given in Table 2, we get (cid:32) (cid:33) ∂ ∂C MDPO = ∂ dπt(s(cid:48))pπ(a(cid:48)|s(cid:48)) log pπ(a(cid:48)|s(cid:48)) − (cid:88) pπ(c|s(cid:48)) log pπ(c|s(cid:48)) ∂pπ(b|x) ∂θ(s(cid:48), a(cid:48)) ∂pπ(b|x) pπt(a(cid:48)|s(cid:48)) pπt(c|s(cid:48)) c (cid:34) (cid:32) (cid:33) pπ(a(cid:48)|s(cid:48)) (cid:88) pπ(c|s(cid:48)) = I(x = s(cid:48))dπt(s(cid:48)) I(b = a(cid:48)) log − pπ(c|s(cid:48)) log pπt(a(cid:48)|s(cid:48)) pπt(c|s(cid:48)) c I(b = a(cid:48)) (cid:18) pπ(b|s(cid:48)) (cid:19) (cid:35) + pπ(a(cid:48)|s(cid:48)) − pπ(a(cid:48)|s(cid:48)) log + 1 pπ(a(cid:48)|s(cid:48)) pπt(b|s(cid:48)) (cid:34) (cid:18) pπ(b|s(cid:48)) (cid:19) (cid:35) = I(x = s(cid:48))dπt(s(cid:48)) I(b = a(cid:48)) · T (s(cid:48), a(cid:48)) − pπ(a(cid:48)|s(cid:48)) log + 1 , (50) itd pπt(b|s(cid:48)) where we introduced an intermediate variable T itd(s(cid:48), a(cid:48)) := log pp ππ t( (a a(cid:48) (cid:48)| |s s(cid:48) (cid:48)) ) − KL(pπ(·|s(cid:48))(cid:107)pπt(·|s(cid:48))) + 1. Now, using the law of total derivative, we obtain ∂ ∂C MDPO = (cid:88) ∂pπ(b|x) × ∂ ∂C MDPO ∂θ(s, a) ∂θ(s(cid:48), a(cid:48)) ∂θ(s, a) ∂pπ(b|x) ∂θ(s(cid:48), a(cid:48)) x,b (cid:88) (cid:104) (cid:105) = I(x = s) I(b = a) − pπ(a|x) pπ(b|x) × I(x = s(cid:48))dπt(s(cid:48)) x,b (cid:34) (cid:18) pπ(b|s(cid:48)) (cid:19) (cid:35) × I(b = a(cid:48)) · T (s(cid:48), a(cid:48)) − pπ(a(cid:48)|s(cid:48)) log + 1 itd pπt(b|s(cid:48)) = I(s = s(cid:48))dπt(s) · T , (51) aux
A general class of surrogate functions for stable and efficient supervised learning where the auxillary term T is aux (cid:88) (cid:104) (cid:105) (cid:34) (cid:18) pπ(b|s) (cid:19) (cid:35) T := I(b = a) − pπ(a|s) pπ(b|s) I(b = a(cid:48)) · T (s, a(cid:48)) − pπ(a(cid:48)|s) log + 1 (52) aux itd pπt(b|s) b (cid:88) (cid:88) (cid:18) pπ(b|s) (cid:19) = T (s, a(cid:48)) I(b = a)pπ(b|s)I(b = a(cid:48)) − pπ(a(cid:48)|s) I(b = a)pπ(b|s) log + 1 itd pπt(b|s) b b (cid:88) (cid:88) (cid:18) pπ(b|s) (cid:19) − pπ(a|s)T (s, a(cid:48)) pπ(b|s)I(b = a(cid:48)) + pπ(a(cid:48)|s)pπ(a|s) pπ(b|s) log + 1 itd pπt(b|s) b b (cid:18) pπ(a|s) (cid:19) = T (s, a(cid:48))pπ(a|s)I(a = a(cid:48)) − pπ(a(cid:48)|s)pπ(a|s) log + 1 itd pπt(a|s) (cid:16) (cid:17) − pπ(a|s)T (s, a(cid:48))pπ(a(cid:48)|s) + pπ(a(cid:48)|s)pπ(a|s) KL(pπ(·|s)(cid:107)pπt(·|s)) + 1 itd (cid:104) (cid:105) = I(a = a(cid:48))pπ(a|s)T (s, a(cid:48)) − pπ(a(cid:48)|s)pπ(a|s) T (s, a(cid:48)) + T (s, a) + pπ(a(cid:48)|s)pπ(a|s). (53) itd itd itd Therefore, (cid:20) ∂ ∂C MDPO = I(s = s(cid:48))dπt(s) I(a = a(cid:48))pπ(a|s)T (s, a(cid:48)) − pπ(a|s)pπ(a(cid:48)|s)T (s, a(cid:48)) ∂θ(s, a) ∂θ(s(cid:48), a(cid:48)) itd itd (cid:21) − pπ(a(cid:48)|s)pπ(a|s)T (s, a) + pπ(a(cid:48)|s)pπ(a|s) (54) itd ⇒ AMDPO = dπt(s) · (cid:104) diag(cid:0) T (s)(cid:1) − pπ(·|s)T (s)(cid:62) (s,:),(s,:) vec vec (cid:105) − T (s)pπ(·|s)(cid:62) + pπ(·|s)pπ(·|s)(cid:62) , (55) vec where we introduced yet another intermediate term T (s), defined as vec T (s) := pπ(·|s) (cid:12) T (s, ·) (56) vec itd (cid:104) (cid:16) (cid:17) (cid:105) = pπ(·|s) (cid:12) log pπ(·|s) (cid:11) pπt(·|s) − KL(pπ(·|s)(cid:107)pπt(·|s))1 + 1 , (57) |A| |A| and (cid:11) in the above equation represents the elementwise vector division defined as [a (cid:11) b] := a /b for any two i i i vectors a and b. As a sanity check, note that the matrix AMDPO is symmetric, as any Hessian matrix should be.
Vaswani, Bachem, Totaro, Müller, Garg, Geist, Machado, Castro, Le Roux G Additional experiments on MuJoCo environments In this section, we present results on a series of MuJoCo environments where learning rate decay and gradient clipping have not been applied. Figure 8 shows that, while sPPO (in orange) still learns something, PPO is unable to make progress, regardless of the capping ((cid:15)) and the number of inner loop steps (m), further reinforcing our intuition that the softmax paramaterization leads to a more robust optimization. 4000 2000 0 500k 1000k 1500k snruteR Ant - =0.1 Ant - =0.3 Ant - =0.5 Ant - =0.7 4000 4000 4000 3000 2000 2000 2000 1000 0 0 0 500k 1000k 1500k 500k 1000k 1500k 500k 1000k 1500k 3000 2000 1000 0 200k 400k 600k 800k snruteR HalfCheetah - =0.1 HalfCheetah - =0.3 HalfCheetah - =0.5 HalfCheetah - =0.7 3000 3000 3000 2000 2000 2000 1000 1000 1000 0 0 0 200k 400k 600k 800k 200k 400k 600k 800k 200k 400k 600k 800k 2000 1000 0 200k 400k 600k 800k snruteR Hopper - =0.1 Hopper - =0.3 Hopper - =0.5 Hopper - =0.7 1000 800 1000 750 600 500 400 500 250 200 0 0 0 200k 400k 600k 800k 200k 400k 600k 800k 200k 400k 600k 800k 2000 1500 1000 500 500k 1000k 1500k snruteR Humanoid - =0.1 600 Humanoid - =0.3 Humanoid - =0.5 Humanoid - =0.7 500 500 500 400 400 400 300 300 300 200 200 200 100 500k 1000k 1500k 500k 1000k 1500k 500k 1000k 1500k 2000 1000 0 200k 400k 600k 800k Step snruteR use_softmax m False 10 True 100 Walker2d - =0.1 Walker2d - =0.3 Walker2d - =0.5 Walker2d - =0.7 800 1000 1000 600 500 500 400 200 0 0 0 200k 400k 600k 800k 200k 400k 600k 800k 200k 400k 600k 800k Step Step Step Figure 8: Average discounted return and 95% confidence interval (over 180 runs) for PPO and softmax PPO on 4 environments (env - rows) and for four different clipping strengths (epsilon - columns). We see that sPPO is more robust to large values of clipping, even more so when the number of updates in the inner loop grows (linestyle).
