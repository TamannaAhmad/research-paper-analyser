Relating Graph Neural Networks to Structural Causal Models Matej Zecˇevic´ MATEJ.ZECEVIC@CS.TU-DARMSTADT.DE Computer Science Department, TU Darmstadt Devendra Singh Dhami DEVENDRA.DHAMI@CS.TU-DARMSTADT.DE Computer Science Department, TU Darmstadt Petar Velicˇkovic´ PETARV@GOOGLE.COM DeepMind, London Kristian Kersting KERSTING@CS.TU-DARMSTADT.DE Computer Science Department, Centre for Cognitive Science, TU Darmstadt, and Hessian Center for AI (hessian.AI) Abstract Causality can be described in terms of a structural causal model (SCM) that carries information on the variables of interest and their mechanistic relations. For most processes of interest the un- derlying SCM will only be partially observable, thus causal inference tries leveraging the exposed. Graph neural networks (GNN) as universal approximators on structured input pose a viable can- didate for causal learning, suggesting a tighter integration with SCM. To this effect we present a theoretical analysis from first principles that establishes a more general view on neural-causal mod- els, revealing several novel connections between GNN and SCM. We establish a new model class for GNN-based causal inference that is necessary and sufficient for causal effect identification. Our empirical illustration on simulations and standard benchmarks validate our theoretical proofs. Keywords: Structural Causal Models, Neural Causal Models, Graph Neural Networks 1. Introduction Understanding causal interactions is central to human cognition and thereby of high value to science, engineering, business, and law (Penn and Povinelli, 2007). Developmental psychology has shown how children explore similar to the manner of scientist, all by asking ”What if?” and ”Why?” type of questions (Gopnik, 2012; Buchsbaum et al., 2012; Pearl and Mackenzie, 2018), while artificial intelligence research dreams of automating the scientist’s manner (McCarthy, 1998; McCarthy and Hayes, 1981; Steinruecken et al., 2019). Deep learning has brought optimizable universality in approximation which refers to the fact that for any function there will exist a neural network that is close in approximation to arbitrary precision (Cybenko, 1989; Hornik, 1991). This capability has been corroborated by tremendous success in various applications (Krizhevsky et al., 2012; Mnih et al., 2013; Vaswani et al., 2017). Thereby, combining causality with deep learning is of critical importance for research on the verge to a human-level intelligence. Preliminary attempts on a tight integration for so-called neural-causal models (Xia et al., 2021; Pawlowski et al., 2020) exist and show to be promising towards the dream of a system that performs causal inferences at the same scale of effectiveness as modern-day neural modules in their most impressive applications. While causality has been thoroughly formalized within the last decade (Pearl, 2009; Peters et al., 2017), deep learning on the other hand saw its success in practical applications with theoretical breakthroughs remaining in the few. Bronstein et al. (2017) pioneer the notion of geometric deep learning and an important class of neural networks that follows from the geometric viewpoint and generalize to modern architectures is the graph neural network (GNN) (Velicˇkovic´ et al., 2017; Kipf 1202 tcO 22 ]GL.sc[ 3v37140.9012:viXra
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING and Welling, 2016a; Gilmer et al., 2017). Similar to other specialized neural networks, the GNN has resulted in state-of-the-art performance in specialized applications like drug discovery (Stokes et al., 2020) and more recently on ETA prediction in google maps (Derrow-Pinion et al., 2021). These specialities, to which we refer to as inductive biases, can leverage otherwise provably impossible inferences (Gondal et al., 2019). As the name suggests, the GNN places an inductive bias on the structure of the input i.e., the input’s dimensions are related such that they form a graph structure. To link back to causality, at its core lies a Structural Causal Model (SCM) which is considered to be the model of reality responsible for data-generation. The SCM implies a graph structure over its modelled variables, and since GNN work on graphs, a closer inspection on the relation between the two models seems reasonable towards progressing research in neural-causal AI. Instead of taking inspiration from causality’s principles for improving machine learning (Mitrovic et al., 2020), we instead show how GNN can be used to perform causal computations i.e., how causality can emerge within neural models. To be more precise on the term causal inference: we refer to the modelling of Pearl’s Causal Hierarchy (PCH) (Bareinboim et al., 2020). That is, we are given partial knowledge on the SCM in the form of e.g. the (partial) causal graph and/or data from the different PCH-levels. Overall, we make a number of key contributions: (1) We derive, from first principles, a theoret- ical connection between GNN and SCM; (2) We define a more fine-grained NCM; (3) We for- malize interventions for GNN and by this establish a new neural-causal model class that makes use of auto-encoders; (4) We provide theoretical results and proofs on the feasibility, expressiv- ity, and identifiability of this new model class while relating to existing work (5) We empiri- cally examine our theoretical model for practical causal inference on identification and estimation tasks. We make our code publicly available: https://anonymous.4open.science/r/ Relating-Graph-Neural-Networks-to-Structural-Causal-Models-A8EE. 2. Background and Related Work Before presenting our main theoretical findings, we briefly review the background on variational methods for generative modelling, on graph neural networks as non-parametric function approxi- mator that leverage structural information, and conclusively on causal inference through the process of intervention/mutilation. Notation. We denote indices by lower-case letters, functions by the general form g(·), scalars or random variables interchangeably by upper-case letters, vectors, matrices and tensors with different boldface font v, V, V respectively, and probabilities of a set of random variables X as p(X). Pearl’s Causal Hierarchy (PCH) is denoted with L , i ∈ {1, 2, 3}, and an intervention via the do-operator. i Variational Inference. Similar to the notions of disentanglement and causality, latent variable models propose the existence of apriori unknown variables Z to jointly model the phenomenon of interest with observed data, p(X, Z). The Variational Inference (VI) technique makes use of optimization, as an alternative to Markov chain Monte Carlo sampling (MCMC) approaches, for overcoming the curse of dimensionality1 when estimating probability distributions (Jordan et al., 1999; Blei et al., 2017). In this Bayesian setting, the inference problem amounts to estimating the 1. Uniformly covering a unit hypercube of n dimensions with k samples scales exponentially, O(kn). 2
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS latent variable conditional p(Z | X) through the closest density of a pre-specified family Q, that is, q∗(Z) = arg min KL(q(Z) || p(Z | X)) (1) q∈Q where the distance measure is set to be the Kullback-Leibler divergence. Inspecting Bayes Rule p(X,Z) exposes that p(Z | X) = where the evidence in the denominator is an exponential term in p(X) (cid:82) Z, that is p(X) = p(X, Z) dZ, thus rendering the overall problem described in Eq.1 intractable in the average case. Originally derived using Jensen’s inequality (Jordan et al., 1999), a tractable lower bound on the evidence is revealed, log p(X) − KL(q(Z) || p(Z | X)) = (2) E [log p(X | Z)] − KL(q(Z) || p(Z)) q where the first term expresses likelihood (or reconstruction) of the data under the given parame- ters while the divergence terms counteracts such parameterization to adjust for the assumed prior. Choosing p (X | Z) and q(Z):=q (Z | X) to be parameterized as neural networks leads to the φφφ θθθ variational auto-encoder (VAE) model class (Kingma and Welling, 2019). Importance sampling (Rubinstein and Kroese, 2016) reveals a connection between variational methods (VAE) and sam- pling techniques for performing marginal inference i.e., since n 1 (cid:88) p φφφ(X | z i)p(z i) p(X) ≈ (3) n q (z | X) θθθ i i=1 where the number of samples n is being kept moderate through the likelihood ratio induced by q. Graph Neural Networks. In geometric deep learning, as portrayed by (Bronstein et al., 2021), graph neural networks (GNN) constitute a fundamental class of function approximator that place an inductive bias on the structural relations of the input. A GNN layer f (D, A ) over some data G considered to be vector-valued samples of our variables {d }n D ∈ Rd×n and an adjacency rep- i i=1 resentation A ∈ [0, 1]d×d of a graph G is generally considered to be a permutation equivariant2 G application of permutation invariant functions φ(d , D ) on each of the variables (features) d X N G i X and their respective neighborhoods within the graph N G. The most general form of a GNN layer is i specified by (cid:18) (cid:19) (cid:77) h = φ d , ψ(d , d ) , (4) i i i j j∈N G i (cid:76) where h represents the updated information of node i aggregated ( ) over its neighborhood in the i form of messages ψ. The flavour of GNN presented in Eq.4 is being referred to as message-passing (Gilmer et al., 2017) and constitutes the most general class of GNN that supersets both convolutional (Kipf and Welling, 2016a) and attentional (Velicˇkovic´ et al., 2017) flavours of GNN. In the context of representation learning on graphs, GCN were previously used within a VAE pipeline as means of parameterization to the latent variable posterior p(Z | X) (Kipf and Welling, 2016b). Causal Inference. A (Markovian) Structural Causal Model (SCM) as defined by Pearl (2009); Peters et al. (2017) is specified as C := (S, P (U)) where P (U) is a product distribution over 2. That is, for some permutation matrix P ∈ [0, 1]d×d, it holds that f(PD, PA PT) = Pf(D, A ). G G 3
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING exogenous unmodelled variables and S is defined to be a set of d structural equations V := f (pa(V ), U ), where i = 1, . . . , d (5) i i i i with pa(V ) representing the parents of variable V in graph G(C). An intervention do(W), W⊂V i i on a SCM C as defined in (5) occurs when (multiple) structural equations are being replaced through new non-parametric functions g W thus effectively creating an alternate SCM C 2 := Cdo(W=gW). Interventions are referred to as imperfect if the parental relation is kept intact, g (pa , ·), and as i i atomic if g = a for a ∈ R. An important property of interventions often referred to as ”modularity” i or ”autonomy”3 states that interventions are fundamentally of local nature, formally pC1(V | pa(V )) = pC2(V | pa(V )) , (6) i i i i where the intervention of C occurred on variable V opposed to V . This suggests that mechanisms 2 j i remain invariant to changes in other mechanisms which implies that only information about the ef- fective changes induced by the intervention need to be compensated for. An important consequence of autonomy is the truncated factorization (cid:89) p(V) = p(V | pa(V ))) (7) V ∈/W derived by Pearl (2009), which suggests that an intervention do(W) introduces an independence of a set of intervened nodes W to its causal parents. Another important assumption in causality is that causal mechanisms do not change through intervention suggesting a notion of invariance to the cause-effect relations of variables which further implies an invariance to the origin of the mecha- nism i.e., whether it occurs naturally or through means of intervention (Pearl et al., 2016). A SCM C is capable of emitting various mathematical objects such as graph structure, statistical and causal quantities placing it at the heart of causal inference, rendering it applicable to machine learning applications in marketing (Hair Jr and Sarstedt, 2021)), healthcare (Bica et al., 2020)) and educa- tion (Hoiles and Schaar, 2016). A SCM induces a causal graph G, an observational/associational distribution pC, can be intervened upon using the do-operator and thus generate interventional distri- butions pC;do(...) and given some observations v can also be queried for interventions within a system with fixed noise terms amounting to counterfactual distributions pC|V=v;do(...). As suggested by the Causal Hierarchy Theorem (CHT) (Bareinboim et al., 2020), these properties of an SCM almost always form the Pearl Causal Hierarchy (PCH) consisting of different levels of distributions be- ing L associational, L interventional and L counterfactual. This hierarchy suggests that causal 1 2 3 quantities (L , i ∈ {2, 3}) are in fact richer in information than statistical quantities (L ), and the i 1 necessity of causal information (e.g. structural knowledge) for inference based on lower rungs e.g. L (cid:54)→ L . Finally, to query for samples of a given SCM, the structural equations are being simu- 1 2 lated sequentially following the underlying causal structure starting from independent, exogenous variables U and then moving along the causal hierarchy of endogenous variables V. To conclude, i consider the formal definition of valuations for the first two layers being (cid:88) pC(y | do(x)) = p(u) (8) {u|Yx(u)=y} for instantiations x, y of the node sets X, Y ⊆ V where Y : U (cid:55)→ Y denotes the value of Y x under intervention x. 3. See Section 6.6 in (Peters et al., 2017). 4
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS 3. The GNN-SCM-NCM Connection To expand further on the boundaries of the integration between causality and machine learning, we perform a theoretical investigation on the relation between graph neural networks (GNN) and structural causal models (SCM), thereby transitively also to neural causal models (NCM). While all the established results on causal identification have proven that intervention/manipulation is not necessary for performing causal inference, the concept of intervention/manipulation still lies at the core of causality as suggested by the long-standing motto of Peter Holland and Don Rubin ’No causation without manipulation’ (Holland, 1986). The centrality of interventions is why we choose to consider them as a starting point of our theoretical investigation. To this effect, we first define a process of intervention within the GNN computation layer that will subsequently reveal sensible properties of the process akin to those of intervention on SCM. Definition 1 (Interventions within GNN.) An intervention x on the corresponding set of variables X ⊆ V within a GNN layer f (D, A ), denoted by f (D, A | do(X = x)), is defined as a modified G G layer computation, (cid:18) (cid:19) (cid:77) h = φ d , ψ(d , d ) , (9) i i i j j∈MG i where the intervened local neighborhood is given by MG = {j | j ∈ N G, j (cid:54)∈ pa ⇐⇒ i ∈ X} (10) i i i where N G denotes the regular graph neighborhood. Such GNN-layers are said to be interventional. An intervention, just like in an SCM, is of local nature i.e., the new neighborhood of a given node is a subset of the original neighborhood at any time, M ⊆ N . The notion of intervention belongs to the causal layers of the PCH i.e., layers 2 (interventional) and 3 (counterfactual). Fig.1 presents an intuitive illustration each for both the underlying SCM with its various properties and the in- tervention process within the GNN layer. The motivational origin of this work lies in the tighter integration of causality with today’s machine learning methodologies, more specifically neural net- work variants. We envision a fully-differentiable system that combines the benefits of both worlds. As a step towards this goal, we introduce the concept of intervention for GNNs (Def.1). The reader might wonder why counterfactuals (L ) are not being covered in this work. The reason for this 3 lies in the fact that a conversion between GNN and SCM will necessarily have to cope with trans- forming a shared, global function ψ into the collection of all local partial mechanisms f of any ij structural equation. Thereby, optimization becomes tremendously difficult. More formally, we state the following theorem on the model conversion. Theorem 1 (GNN-SCM Conversion.) Consider the most general formulation of a message- passing GNN node computation h :F (cid:55)→ F (cid:48) as in Eq.4. For any SCM C=(S, P (U)) there ex- i ists always a choice of feature spaces F , F (cid:48) and shared functions φ, ψ, such that for all structural equations f ∈S it holds that h = f . i i (cid:80) Proof Compact (details in Appendix). Let f (pa(i), U ) = f (U , A ) + f (V ) be a i i i i i j∈pa(i) ij j structural equation (A i ∈ 2| pa i |, f i ∈ S, C=(S, P (U))) and its scalar-decomposition following Thm.1 in (Kuo et al., 2010). The following mapping: (cid:88) F = V ∪ U = F (cid:48), φ(i, . . . ) = f (U , A ) + . . . ψ(i, j) = f (11) UX X i ij 5
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING Graph SCM (unobserved Nature) read-off Data PCH Figure 1: Graph Neural Networks and Interventions akin to SCM. A schematic overview. (a) shows the unobserved SCM C that generates data D through instantiations of the exoge- nous variables U. The SCM implies a graph structure G and the PCH (L ) (b) shows the i intuition behind interventions using the do-operation (c) presents the mathematical for- malism (Def.1) highlighting the intervention (red) and the regular neighborhood (blue). (Best viewed in color.) where (. . . ) is the remainder of the GNN-computation (Eq.4), defines a general construction scheme: (cid:18) (cid:19) (cid:76) (cid:80) h = φ d , ψ(d , d ) = f (U , A ) + f (V ) = f . (12) i i j∈N iG i j Ui i i j∈pa(i) ij j i The common ground between SCM and GNN lies within the assumed graph structure and thus is deemed suitable as a starting point for a reparameterization from SCM to GNN as Thm.1 suggests. However, while Thm.1 is powerful in the sense that any GNN can be seen as a neural SCM variant, the theorem does not give away any information on optimization. It follows naturally that ψ is a shared function amongst all nodes of the graph while an SCM considers a specific mechanism for each of the nodes in the graph, and thus optimization becomes difficult. In a nutshell, the messages ψ(i, j) need to model each of the dependency terms f within a structural equation, such that the ij messages themselves become a descriptor of the causal relation for (i ← j). Nonetheless, the theo- retical connection’s existence suggests tighter integration for NCM with an important consequence being the connection to the base-NCM definition (see Xia et al. (2021)). Corollary 1 (NCM-Type 2.) Allowing for the violation of sharedness of ψ as depicted in Thm.1 and choosing F = F (cid:48) = U ∪ V to be the union over endo- and exogenous variables, φ(i, . . . ) = f Ui(U i, A i) + (cid:80) (. . . ) to be a sum-aggregator with noise term selection with A i∈2| pa i |, and ψ = {f ij} to be the dependency terms of the structural equations f modelled as feedforward neural θθθ i |V | networks. Then the computation layer {h } is a special case of the NCM as in (Xia et al., 2021). i i Because of space restrictions we provide the proof to Cor.1 and all subsequent mathematical results within the supplementary section. To be more precise, the NCM-Type 2 portrayed in Cor.1 is more fine-grained than the the definition of NCM in (Xia et al., 2021) since their formulation models structural equations using feedforward nets (|V |) while the NCM-Type 2 additionally models the 6
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS Figure 2: NCM-Type 2 and the GNN-SCM Conversion. A schematic overview of the results established in Thm.1 and Cor.1 (a) shows an example SCM as the underlying reality with its structural equations and their decompositions into univariate parent dependency-terms f ij alongside a potentially non-empty (A i (cid:54)= ∅, A i∈2| pa i |) joining function f Ui(U i, A i) (b) shows the NCM as defined by (Xia et al., 2021) which models on node-level opposed to the edge-level as for NCM-Type 2 (Cor.1) (c) shows an example of an infeasible ψ function that has to model all relevant dependency terms. (Best viewed in color.) dependency terms within each of the structural equations (|E2|). Fig.2 provides a schematic illus- tration of the discussed concepts, that is, both for the GNN to SCM conversion from Thm.1 and the NCM-Type 2 comparison to regular NCM from Cor.1. Again, the decomposition (or fine-grained view) in Fig.2(a) follows from (Kuo et al., 2010). To illustrate, consider the following example: C = ({f (Z, U ) := Z ∧ U , f (U ) := U }, P (U , U )), (13) X X X Z Z Z X Z U = 0 U = 1 X X (cid:18) (cid:19) f (Z) + f (U , Z) f = 0 1 Z = 1 ⇐⇒ XZ UX X (14) X 0 0 Z = 0 [Z] + [U − (Z ∨ U )]. X X The decomposition in (14) simply takes the identity of the parent (Z) while considering the negated logical OR for the remainder term f . Its sum then results in the original logical AND function UX f of C. While the decomposition for this specific example does not seem to reveal any sensible X advantage, it does for other examples more easily, e.g. when f is a linear function of its arguments X then the term f will only depend on the unmodelled term U and not the other variables (i.e., UX X the argument list is empty A = ∅). The advantage becomes evident in that we have a more fine- grained view onto NCM. On a side note, it is worth noting that the computation layer in Cor.1, as soon as sharedness is being violated, generally is not considered to be a GNN layer anymore since sharedness just like adjacency information and permutation-equivariance belong to key properties of the GNN definition. Nonetheless, Cor.1 predicts a new type of NCM that should be investigated in more detail in future research as it should provide the same theoretical guarantees as regular NCM, while being more flexible and interpretable because of the fine-grained modelling scope (Fig.2(b)). 7
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING 4. GNN-based Causal Inference Upon observing the difficulty of optimization for a general causal model based on GNN as suggested by both the variant in Thm.1 (Fig.2(c)) and the NCM-variant in Cor.1 (Fig.2(b)), we decide to focus our theoretical investigation on what is reasonably attainable within the GNN framework since the inductive bias on graphs still seems appealing. The graph-intersection of these two model classes can in fact be leveraged to partially represent the PCH as we show next. I.e., up to rung 2 of interventional queries (L ). Instead of turning a GNN into an SCM (as in Thm.1), we will consider 2 the reverse direction. Thereby, we define a GNN construction based on an SCM. Definition 2 (G-GNN construction.) Let G be the graph induced by SCM C. A GNN layer f (D, A ) for which G = G is said to be G-constructed. G We believe that G-GNN (Def.2) make an important step towards connecting the graph structure induced by an SCM with the structured computation within a GNN layer. Developing further on this notion in future research might allow for neural-graph models that are inherently causal4. Proposition 1 (Graph Mutilation Equivalence.) Let C be an SCM with graph G and let f be a G-GNN layer. An intervention do(X), X ⊆ V, on both C and f produces the same mutilated graph. It is worth noting that an intervention within a GNN layer (Def.1) is dual to the notion of intervention within an SCM i.e., like observing within the mutilated graph is akin to interventions, computing a regular GNN layer on the mutilated graph is akin to intervening on the original GNN layer, that is, f (D, A | do(X)) = f (D, A ) where G(cid:48) is the graph upon intervention do(X). In the spirit of G G(cid:48) (Xia et al., 2021), we define what it means for an SCM to be consistent with another causal model in terms of the PCH layers L . i Definition 3 (Partial L -Consistency.) Consider a model M capable of partially emitting PCH, i that is L for i∈{1, 2}, and an SCM C. M is said to be partially L -consistent w.r.t. C if L (M) ⊂ i i i L (C) with |L (M)| > 0. i i In the following we will usually omit the word partial since Def.3 is the only consistency employed. A causal model most generally defined can be considered to be something that can carry out causal inferences and in Def.3 we consider such models that are capable of emitting the L -distributions. i The definition proposes that an SCM can match with such a model, if it agrees on a subset of all conceivable distributions for a level i, which for |L | = 1 but |L | → ∞. In their work on 1 2 variational graph auto-encoders (VGAE) (Kipf and Welling, 2016b) considered the application of the GNN model class to the posterior parameterization, q(Z | D, A) = f (D, A). However, their main incentive posed to be a generative model on graphs themselves i.e., p(A | Z). In the following, we define VGAE on the original data as traditionally done within vanilla VAE models that don’t consider (or ignore) structured input. Definition 4 (Variational Graph Auto-Encoder.) We define a VGAE V=(q(Z|D), p(D|Z)), as a data-generative model, with inference and generator model respectively such that q:=f (D, A) is θθθ a GNN layer and p:=g (D, Z) some parameterized data D dependent model, where θθθ, φφφ are the φφφ variational parameters. 4. Consider (Xia et al., 2021) for a treatise on Neural Causal Model (NCM) using sets of feed-forward neural networks. 8
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS Since SCM induce a graph structure dictated by the set of structural equations that implicitly govern the structural relations between the modelled variables, it is important to define a density estimator which acts on the variable space opposed to the space of plausible graph structures (Def.4). Using a GNN layer for modelling the original data does not compromise on the expressiveness of the model class as the following theorem suggests. Theorem 2 (Universal Density Approximation.) There exists a latent variable family q and a corresponding data-generative VGAE V=(q, p) such that the modelled density can approximate any smooth density at any nonzero error. This theorem suggests that the defined VGAE class, for a suitable choice of latent variable model, is a universal approximator of densities (UDA). A suitable choice for q in Thm.2 is the mixture of Gaussians family (Goodfellow et al., 2015; Plataniotis and Hatzinakos, 2017). Since causal es- timates are determined by their theoretical identifiability from the partially available information on the true underlying SCM C∗ (e.g. structural constraints), UDA properties of the estimator are beneficial. Since the data-generative VGAE is a probabilistic model capable of approximating dis- tributions to arbitrary precision, we are finally ready to define a causal model based on GNN layers by introducing interventional GNN layers (Def.1) into said VGAE class. Definition 5 (Interventional VGAE.) An interventional VGAE is a data-generative VGAE V=(q, p) where both q, p are set to be interventional GNN layers f (D , A | do(X)) with D ∈{D, Z}. i i G i While the reader might expect the data-generative VAE model to make use of the interventions within GNN layers from Def.1, what might come as surprise is the usage of a second (interventional) GNN layer as decoder because the second layer parameterization considers the reverse module p(·|Z) which takes the latent variable Z which only transitively via D acquires its connection to the adjacency A . Interventional capability within both models is beneficial since the knowledge G on the specific mutilation becomes available throughout the complete model. Modified decoder that leverage pre-defined inductive biases have been deployed in the literature by for instance (Kipf et al., 2018). Fig.3 illustrates this intuition and parallels it with the computation within an SCM. The interventional VGAE (iVGAE) is a causal model, like an SCM, capable of emitting L -distributions i from the PCH. With this, we are able to provide statements on the causal capabilities of this extended model class. More precisely, following the mathematical ideas of (Xia et al., 2021) we show that iVGAE overlap with the space of all SCMs Ω. Theorem 3 (Expressivity.) For any SCM C there exists an iVGAE V(θθθ, φφφ) for which V is L - 2 consistent w.r.t C. Similar to the peculiar specifications of NCM (Xia et al., 2021), an iVGAE does not loose in expres- sivity to SCM regarding the first two rungs of the PCH. Thus it can also act as a proxy for causal inferences. An important consequence of the CHT (Bareinboim et al., 2020), which also applies to iVGAE is that the PCH is kept in tact across scenarios. Corollary 2 (iVGAE Partial Causal Hierarchy Theorem.) Consider the sets of all SCM and iVGAE, Ω, Υ, respectively. If for all V ∈ Υ it holds that V is Lp(V)=Lp(C) =⇒ Lq(V)=Lq(C) 1 1 2 2 with C ∈ Ω, where Lp ⊂ L is a selection p, q over the set of all level 1,2 distributions respectively, i i then we say that layer 2 of iVGAE collapses relative to C. On the Lebesgue measure over SCMs, the subset in which layer 2 of iVGAE collapses to layer 1 has measure zero. 9
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING Figure 3: Neural-Causal Model based on Interventional GNN Layers. A schematic overview of the inference process within the neural-causal iVGAE V model and its similarities to the same process within an SCM C. While C can be directly queried for the causal effect p(D| do(Y)) through valuations U, the iVGAE makes use of corresponding data D. (Best viewed in color.) As previously pointed out by (Xia et al., 2021; Bareinboim et al., 2020), the CHT does not impose a negative result by claiming impossibility on low-to-high-layer inferences, however, it suggests that even sufficient expressivity of a model does not allow for the model to overcome the boundaries of the layers (unless causal information e.g. in the form of structural constraints is available). Another noteworthy consequence of both Thms.3 and 1 is the incapability of handling counterfactuals. Corollary 3 (iVGAE Limitation.) For any SCM C there exists no iVGAE V such that V is L - 3 consistent w.r.t. C. While the formulation of an iVGAE restricts itself from the full expressivity of the PCH (like SCM or NCM), there are no restrictions on the lower causal layers in addition to a more compact overall model. In Def.2 we pointed to the relation between SCM and GNN via the deployed graph. The following theorem states that if G-GNN layers are being deployed within a corresponding VGAE, then we can have consistency to any SCM of choice. Corollary 4 (L Representation.) For any SCM C that induces a graph G, there exists a corre- 2 sponding iVGAE V=(q, p) where q, p are G-GNN layers such that V is L -consistent with C. 2 The main claim of this theorem is two-fold, first, that the expressivity established in Thm.3 is being preserved for G-GNN layers, and second, that the shared graph G allows for tighter integration. 5. Identifiability and Estimation In line with (Xia et al., 2021), we finally set stage for identifiability within the neural-causal iVGAE model by first extending their notion of neural identifiability, since iVGAE is a neural model that makes trades the set of feedforward networks for two interventional graph neural networks. Definition 6 (Neural Identifiability.) Again, let Ω, Υ be the sets of SCMs and corresponding G- GNN based iVGAE. For any pair (C, V) ∈ Ω × Υ, a causal effect p(V | do(V )) is called neurally i j identifiable iff the pair agrees on both the causal effect [pC(V | do(V )) = pV (V | do(V ))] and i j i j the observational distributions [pC(V) = pV (V)]. This definition enforces a matching on all the relevant levels such that a causal effect becomes identifiable. With this, a final central claim on the relation of general identifiable effects and neurally identifiable effects (i.e., using the iVGAE as the model of choice) is being established. 10
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS Algorithm 1 Causal Inference with GNN Input: iVGAE modules V , Intervention v , Variable v i j i Parameter: Number of Samples n Output: log(pˆ(v | do(V = v ))) i j j 1: Let pˆ = 0 2: for i = 1..n do 3: Encoding z, log(p(z do(v j))) ← V 1(v i, v j) 4: Decoding log(p(v i|, z do(v j))) ← V 2(z, v i, v j) 5: Aggregate pˆ ← exp(log(p(v i|, z, do(v j))) − log(p(z| do(v j)))) 6: end for 7: return log(pˆ) − log(n) Theorem 4 (Dual Identification.) Consider the causal quantity of interest Q = pC(V | do(V )), i j G the true causal graph underlying SCM C ∈ Ω and p(V) the observational distribution. Q is neurally identifiable from iVGAE V∈Υ with G-GNN modules iff Q is identifiable from G and p(V). This theorem is analogues to the statement on neural identifiability in (Xia et al., 2021) thus follow- ing the same implications in that theoretically lower-to-higher-layer inference is possible within our neural setting, avoiding the usage of do-calculus altogether. The hope lies within the differentiabil- ity of the considered (neural) models to allow for leveraging causal inferences automatically from large-scale data. From a causal perspective, it is important to note that Def.6 alongside the theoreti- cal result Thm.4 consider identifiability and not actual identification, the former refers to the power of cross-layer inferences while the latter refers to the process of using e.g. do-calculus for SCM or mutilation in the case of NCM to obtain the estimands5. For the iVGAE identification thus refers to cross-layer modelling which is a special case of multi-layer modelling, to which we will simply refer to as estimation. The estimation is performed using a modified version of the variational objective in Eq.2 to respect the causal quantities, E [log p(V|Z, do(W))] − KL(q(Z|do(W))||p(Z)), where q W⊂V are intervened variables and p, q the G-GNNs of the iVGAE model. After optimizing the iVGAE model with this causal ELBO-variant, we can consider any quantity of interest dependent on the modelled levels Q(L ). One interesting choice for Q is the average treatment effect (ATE), i ATE(X, Y ) := E[Y | do(X=1)] − E[Y | do(X=0)], where the binary6 variable X is being referred to as treatment. In Alg.1 we show how marginal inference under intervention is being performed. 6. Empirical Illustration To assist our theoretical results we provide an extensive set of empirical illustrations. Due to space constraints, we only highlight relevant key results while pointing to the extensive Appendix. Density Estimation. We investigate properties of the causal density estimation when using iVGAE (Def.5). We inspect various aspects like for instance how interventional influence propagates along the topological sequence or how precisely the model is able to fit. Fig.4(a) shows an iVGAE model on the real-world inspired ASIA data set (Lauritzen and Spiegelhalter, 1988) that estimates the 5. These estimands can then be estimated using data. 6. Note that we can extend the ATE to be categorical/continuous. 11
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING 0 0.1 0.2 asia smoke tub lung either bronc xray dysp Figure 4: GNN Capture Causal Influence. Empirical illustration (a) shows causal density estima- tion (top L1, bottom L with do(tub = B( 1 ))) on ASIA where the property ’tub’ turns 2 2 into a fair coin flip (red box) that also causally influences its descendants (olive boxes) (b) shows estimation of ATE of X on Y for different SCM-families (Best viewed in color.) observational and interventional distributions from data sufficiently well in terms of both quality and recall. Inference is performed using Alg.1. An extensive study is provided in the Appendix. Causal Effect Estimation. We investigate the estimation of the ATE of X on Y using iVGAE on different families of SCMs, that is, different structures with different parameterizations. Estimation considers the predictive quality amongst different parameterizations. In Fig.4(b) chain (M ), con- 1 founder (M ), and backdoor (M ) structures are shown. The iVGAE model adequately captures 2 3 the un- and confounded causal effects. An extensive elaboration is provided in the Appendix. 7. Conclusions and Future Work We derive the first theoretical connection between GNNs and SCMs (Thm.1) from first principles and in the process extend the family of neural causal models with two new classes: NCM-Type 2 (Cor.1) and iVGAE (Def.5). We provide several theoretical results on the feasibility, expressivity and identifiability (Thms.3,4; Cors.3,4). To support the theoretical results empirically, we system- atically investigate practical causal inference on several benchmarks. Following, an extension that supports the complete PCH without feasibility trade-off would be desirable. Differently encoding the intervention within GNN (Def.1) or concepts like PMP (Strathmann et al., 2021) might allow for this direction. Further, testing on larger causal systems is a pragmatic, natural next step. 12
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS Acknowledgments This work was supported by the ICT-48 Network of AI Research Excellence Center “TAILOR” (EU Horizon 2020, GA No 952215) and by the Federal Ministry of Education and Research (BMBF; project “PlexPlain”, FKZ 01IS19081). It benefited from the Hessian research priority programme LOEWE within the project WhiteBox, the HMWK cluster project “The Third Wave of AI.” and the Collaboration Lab “AI in Construction” (AICO). References Elias Bareinboim, Juan D Correa, Duligur Ibeling, and Thomas Icard. On pearl’s hierarchy and. 2020. Ioana Bica, Ahmed Alaa, and Mihaela Van Der Schaar. Time series deconfounder: Estimating treatment effects over time in the presence of hidden confounders. In ICML, 2020. David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti- cians. Journal of the American statistical Association, 2017. Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet- ric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 2017. Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velicˇkovic´. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021. Daphna Buchsbaum, Sophie Bridgers, Deena Skolnick Weisberg, and Alison Gopnik. The power of possibility: Causal learning, counterfactual reasoning, and pretend play. Philosophical Trans- actions of the Royal Society B: Biological Sciences, 2012. George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of con- trol, signals and systems, 1989. Austin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange, Todd Hester, Luis Perez, Marc Nunkesser, Seongjae Lee, Xueying Guo, Brett Wiltshire, et al. Eta prediction with graph neural networks in google maps. CIKM, 2021. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In ICML, 2017. Muhammad Waleed Gondal, Manuel Wu¨thrich, Dorde Miladinovic´, Francesco Locatello, Martin Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Scho¨lkopf, and Stefan Bauer. On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset. arXiv preprint arXiv:1906.03292, 2019. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. ICLR, 2015. 13
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING Alison Gopnik. Scientific thinking in young children: Theoretical advances, empirical research, and policy implications. Science, 2012. Joseph F Hair Jr and Marko Sarstedt. Data, measurement, and causal inferences in machine learning: opportunities and challenges for marketing. Journal of Marketing Theory and Practice, 2021. Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 2012. William Hoiles and Mihaela Schaar. Bounded off-policy evaluation with missing data for course recommendation and curriculum design. In ICML, 2016. Paul W Holland. Statistics and causal inference. Journal of the American statistical Association, 1986. Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 1991. Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni- versal approximators. Neural networks, 1989. Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 1999. Diederik P Kingma and Max Welling. An introduction to variational autoencoders. arXiv preprint arXiv:1906.02691, 2019. Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In ICML, 2018. Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net- works. arXiv preprint arXiv:1609.02907, 2016a. Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016b. Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009. Kevin B Korb and Ann E Nicholson. Bayesian artificial intelligence. CRC press, 2010. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. MNIST classification with deep convo- lutional neural networks. NeurIPS, 2012. F Kuo, I Sloan, Grzegorz Wasilkowski, and Henryk Woz´niakowski. On decompositions of multi- variate functions. Mathematics of computation, 2010. Steffen L Lauritzen and David J Spiegelhalter. Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society: Series B (Methodological), 1988. 14
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS John McCarthy. What is artificial intelligence? 1998. John McCarthy and Patrick J Hayes. Some philosophical problems from the standpoint of artificial intelligence. In Readings in artificial intelligence. 1981. Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, and Charles Blundell. Represen- tation learning via invariant causal mechanisms. arXiv preprint arXiv:2010.07922, 2020. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier- stra, and Martin Riedmiller. Playing atari with deep supervised learning. arXiv preprint arXiv:1312.5602, 2013. Nick Pawlowski, Daniel C Castro, and Ben Glocker. Deep structural causal models for tractable counterfactual inference. arXiv preprint arXiv:2006.06485, 2020. Judea Pearl. Causality. Cambridge university press, 2009. Judea Pearl and Dana Mackenzie. The book of why: the new science of cause and effect. Basic books, 2018. Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. Causal inference in statistics: A primer. John Wiley & Sons, 2016. Derek C Penn and Daniel J Povinelli. Causal cognition in human and nonhuman animals: A com- parative, critical review. Annu. Rev. Psychol., 2007. Jonas Peters, Dominik Janzing, and Bernhard Scho¨lkopf. Elements of causal inference. The MIT Press, 2017. Kostantinos N Plataniotis and Dimitris Hatzinakos. Gaussian mixtures and their applications to signal processing. In Advanced signal processing handbook. 2017. Reuven Y Rubinstein and Dirk P Kroese. Simulation and the Monte Carlo method. John Wiley & Sons, 2016. Christian Steinruecken, Emma Smith, David Janz, James Lloyd, and Zoubin Ghahramani. The automatic statistician. In Automated Machine Learning. 2019. Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al. A deep learning approach to antibiotic discovery. Cell, 2020. Heiko Strathmann, Mohammadamin Barekatain, Charles Blundell, and Petar Velicˇkovic´. Persistent message passing. arXiv preprint arXiv:2103.01043, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. 15
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING Norbert Wiener. Tauberian theorems. Annals of mathematics, 1932. Kevin Xia, Kai-Zhan Lee, Yoshua Bengio, and Elias Bareinboim. The causal-neural connection: Expressiveness, learnability, and inference. 2021. 16
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS Appendix for Relating Graph Neural Networks to Structural Causal Models. We make use of this appendix following the main paper to provide the proofs to the main theorems, propositions and corollaries in addition to systematic investigations regarding practical causal inference in terms of causal effect estimation and general density estimation. Appendix A. Proofs This section provides all the proofs for the mathematical results established in the main paper. A.1. Proofs for Theorem 1 and Corollary 1 While Thm.1 does not talk about optimization and feasibility, still it suggests that indeed GNN can be converted into SCM by suggesting that there always exists at least one construction of such. In the following we prove the theorem by giving a general construction scheme. Theorem 1 (GNN-SCM Conversion.) Consider the most general formulation of a message- passing GNN node computation h :F (cid:55)→ F (cid:48) as in Eq.4. For any SCM C=(S, P (U)) there ex- i ists always a choice of feature spaces F , F (cid:48) and shared functions φ, ψ, such that for all structural equations f ∈S it holds that h = f . i i Proof It is sufficient to provide a general construction scheme on SCMs. Therefore, let C=(S, P (U)) (cid:80) be any SCM. Let f (pa(i), U ) = f (U , A ) + f (V ) be a structural equation (f ∈ S) i i i i i j∈pa(i) ij j i and its scalar-decomposition following Thm.1 in (Kuo et al., 2010) where A∈2| pa i | is a poten- tially empty argument list. We choose the following mapping for the respective GNN computation components: F = V ∪ U = F (cid:48) (15) (cid:88) φ(i, . . . ) = f (U , A ) + . . . (16) i i i ψ(i, j) = f (17) ij where (· · · ) represents the remainder of the GNN-computation. Finally, it holds that (cid:18) (cid:19) (cid:77) h = φ d , ψ(d , d ) i i i j j∈N G i (18) (cid:88) = f (U , A ) + f (V ) = f . i i i ij j i j∈pa(i) A simple deduction of Thm.1 in which we allow for the violation of sharedness, which lies at the core of the GNN formulation, leads to the formulation of a more fine-grained NCM model than what has been defined by Xia et al. (2021). It is more fine-grained in that this NCM-Type 2 operates on the edge-level opposed to the node-level as for regular NCM. Corollary 1 (NCM-Type 2.) Allowing for the violation of sharedness of ψ as dicated in Thm.1 and choosing F = F (cid:48) = U ∪ V to be the union over endo- and exogenous variables, φ(i, . . . ) = 17
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING f i(U i, A i) + (cid:80) (. . . ) to be a sum-aggregator with noise term selection with A i∈2| pa i |, and ψ = {f ij} to be the dependency terms of the structural equations f modelled as feedforward neural θθθ i |V | networks. Then the computation layer {h } is a special case of the NCM as in (Xia et al., 2021). i i Proof The proposed computation layer (NCM-Type 2) is a special case in the way it specifies the function approximators and in that it covers non-Markovianity i.e., no hidden confounding or relations on the noise terms. A NCM N is specified the same way as an SCM C with the difference being that the the noise terms being uniformly distributed over the intervall [0, 1], that is U ∼ Unif(0, 1), and the structural equations being parameterized by feedforward neural networks f := i MLPθθθ with learnable parameters θθθ. (Kuo et al., 2010) suggests we know that f = f (U , A ) + i i i i i (cid:80) f (V ). Furthermore, we know that MLPθθθ = (cid:80) MLPθθθ k (Hornik et al., 1989). Thereby, j∈pa(i) ij j i k k we have that h = f (U , A ) + (cid:88) f (V ) = (cid:88) MLPθθθj = MLPθθθ = f i i i i ij j j i i (19) j j where i ∈ {1 . . . |V |}, j ∈ pa(i) and A i ∈ 2| pa i |. A.2. Proof for Proposition 1 Prop.1 reassures that the established connection between SCM and GNN based on the graph that is being induced by the former is natural. More specifically, an intervention within such a G-GNN will produce the same mutilated graph as an intervention within the SCM. Proposition 1 (Graph Mutilation Equivalence.) Let C be an SCM with graph G and let f be a G-GNN layer. An intervention do(X), X ⊆ V, on both C and f produces the same mutilated graph. Proof Following Def.3 in (Bareinboim et al., 2020), an interventional SCM C is a submodel of the x original SCM C where the structural equations for variables X are being replaced by the assignment x. Through this operation, denoted by do(X = x), the dependency between the causal parents of any node V ∈ X is being lifted (as long as the assignment x is not dependent on the parents). i Therefore the mutilated graph is given by G = (V, GE \ {(j, i) | V ∈ pa , V ∈ X} where GE x j i i denotes the edge list of the original graph. Intervening on a G-GNN layer implicitly considers a modified neighborhood MG = {j | j ∈ N G, j (cid:54)∈ pa ⇐⇒ i ∈ X} which removes exactly the i i i relatons to the parents. Since G = G, the mutilated graphs are the same. A.3. Proof for Theorem 2 Thm.2 suggests a long standing result on universal approximators for densities (Goodfellow et al., 2016; Plataniotis and Hatzinakos, 2017) but applied to the data-driven VGAE. Thus the proof fol- lows analogously. Theorem 2 (Universal Density Approximation.) There exists a latent variable family q and a corresponding data-generative VGAE V=(q, p) such that the modelled density can approximate any smooth density at any nonzero error. 18
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS Proof It is sufficient for the proof to show one example of a suitable family, that is encodable by a data-driven VGAE (Def.4) and that can act as a universal approximator of densities. Let’s inspect the Gaussian Mixture Model family of latent variable models. We will use Wiener’s approximation theorem (Wiener, 1932) and knowledge delta functions of positive type. A delta family of positive type is defined under following conditions (cid:82) a 1. δ (x) dx → λ as λ → λ for some a. −a λ 0 2. For every constant γ > 0, δ tends to zero uniformly for γ ≤ |x| ≤ ∞ as λ → λ . λ 0 3. δ (x) ≥ 0 for all x and λ. λ (cid:82) ∞ and if it additionally satisfies δ (x) dx = 1, then such a function definies a probability density −∞ λ for all λ. A Gaussian density is delta in the zero limit of its variance. The sequence p (x) formed λ by the convolution of the delta function δ and an arbitrary density function p can be expressed in λ terms of Gaussian. Thus, (cid:90) ∞ p (x) = δ (x − u)p(u) du λ λ −∞ (20) (cid:90) ∞ = N (x − u)p(u) du, λ −∞ forms the basis for the sum of Gaussians. It follows that p (x) can in fact be approximated with λ a Riemann sum p (x) = 1 (cid:80)n N (x − x )[ξ − ξ ] on some interval (a, b) with interval- λ k i=1 λ i i i−1 boundary points ξ , ξ = a, ξ = b. It follows that we can express p (x) as a convex combination i 0 n λ,n of different variance Gaussians. Finally, the sought density can be expressed as k (cid:88) p(x) = w N (x; µ , Σ) (21) i i i=1 with (cid:80) w = 1 and w ≥ 0, ∀i. Note that x = x ∈ Rd holds for all the previously established results. i Thus a GMM can approximate any density. To conclude, a GMM can be modelled by data-driven VGAE by designing the output of the encoder f (D, A) = Z such that z := w , µ , Σ . θθθ i i i i A.4. Proofs for Theorem 3 and Corollaries 2 and 3 The iVGAE is a feasible model opposed to the GNN-reparameterization from Thm.1 for the price of expressivity. Nonetheless, the following Thm.3 reassures that the iVGAE can model causal quantites up to the second level of the PCH, namely that of interventions (L ). 2 Theorem 3 (Expressivity.) For any SCM C there exists an iVGAE V(θθθ, φφφ) for which V is L - 2 consistent w.r.t C. |V| Proof Let V(θθθ, φφφ) be an iVGAE and D = {d } a data set on the variables V of an arbitrary i i=1 SCM C for multiple interventions of said SCM, that is, d ∼ p ∈ L (C) for j ∈ {1, 2} and i k j k > 1. Note that the observational case (L ) is considered to be an intervention on the empty set, 1 p(V| do(∅)) = p(V). Through Thm.2 we know that there always exists a parameterization θ, φ for 19
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING V such that any distribution p can be modelled to an arbitrary precision, thus pV = p . Since k > 1 k we have that V models the PCH up to level partially L (V). Finally, the distributions are modelled 2 relative to C and L (V) ⊂ L (C). 2 2 It is important to note that the L -consistency defined in Def.3 is a weakened notion of consistency i across causal models since any model might only agree on a single distribution of a given level p ∈ L . Due to the nature of iVGAE being a compact model class opposed to a neural copy of an i SCM, this is the only consistency achievable. However, it is not a negative impossibility result, on the contrary, iVGAE can perform any causal inference within the first two rungs of the PCH when provided with the corresponding data and sufficient model capacity i.e., the trade-off in expressivity solely comes from the fact that a compression on the model description is being performed. An important corollary to Thm.3 is that the CHT still applies across settings. That is, causal inferences within iVGAE as choice of parameterization remain sensible since the layer boundaries are strict. Corollary 2 (iVGAE Partial Causal Hierarchy Theorem.) Consider the sets of all SCM and iVGAE, Ω, Υ, respectively. If for all V ∈ Υ it holds that V is Lp(V)=Lp(C) =⇒ Lq(V)=Lq(C) 1 1 2 2 with C ∈ Ω, where Lp ⊂ L is a selection p, q over the set of all level 1,2 distributions respectively, i i then we say that layer 2 of iVGAE collapses relative to C. On the Lebesgue measure over SCMs, the subset in which layer 2 of iVGAE collapses to layer 1 has measure zero. Proof The proof is analogue to (Xia et al., 2021; Bareinboim et al., 2020) and assumes Fact 1 (or Thm.1 for the latter) to define an SCM-collapse relative to some SCM C(cid:48). If layer 2 SCM- collapses to layer 1 relative to C(cid:48) then any SCM C will follow the properties Lp(C) = Lp(C(cid:48)) and 1 1 Lk(C) = Lk(C(cid:48)) for some set selections p, k. By Thm.3 we know that there will always exist a 2 2 corresponding iVGAE V that is L -consistent with C but since it is consistent with C and not C(cid:48) it 2 follows the equivalent properties Lp(V) = Lp(C(cid:48)) and Lk(V) = Lk(C(cid:48)), which means that the layer 1 1 2 2 2 also collapses for the iVGAE model. The analogue argument holds in reverse when layer 2 does not SCM-collapse to layer 1 relative to C(cid:48). Since both directions together suggest an equivalence on the way collapse occurs for both SCM and iVGAE, Fact 1 from (Xia et al., 2021) (or Thm.1 from (Bareinboim et al., 2020)) holds. The iVGAE model is capable of causal inference but it cannot act as a complete reparamterization of an SCM since it lacks the same amount of model description, that is, an SCM models structural equations for each of the variables while iVGAE expresses a causal probabilistic model paramter- ized by two functions. A consequence of Thm.3 is thus that the general layer of counterfactuals (L ) 3 cannot be inferred using the description in Def.5. While it is easy to see why this is the case, since iVGAE deploys a single model, it is important to consider Def.2 from (Bareinboim et al., 2020) to proceed with the proof. Definition 7 (L Valuations.) Let C be an SCM, then for instantiations x, y of the node sets 3 X, Y, Z, W · · · ⊆ V where Y : U (cid:55)→ Y denotes the value of Y under intervention x, a counter- x factual distribution is given by (cid:88) pC(y , z , . . . ) = p(u) (22) x w u∈U with U = {u | Y (u) = y, Z (u) = z, . . . } being all noise instantiations consistent with x w Y , Z , . . . representing different ”worlds”. x w 20
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS With Def.7 the simplicity of the proof for Cor.3 becomes evident, since a single iVGAE is not ca- pable of representing the different (counter-factual) ”worlds” implied by the different interventions p(V | do(V )) = V . i j i,vj Corollary 3 (iVGAE Limitation.) For any SCM C there exists no iVGAE V such that V is L - 3 consistent w.r.t. C. Proof For an iVGAE model V to be L -consistent with an SCM requires V to have the capability of 3 inducing distributions from that layer pV = p ∼ L . Def.7 suggests that a counterfactual involves 3 the instantiations of multiple worlds for any SCM C, p(V | do(V )) for different tuples (i, j). While i j V is capable of modelling each of the ω = p(V | do(V )) jointly, it is not capable of accumulating ij i j the probability masses p(u) for which u is consistent with ∪ω since it is a single model that ij directly estimates the l.h.s. of Eq.8 opposed to an algorithmic procedure of checking over all possible instances u for consistency. A.5. Proof for Corollary 4 While Thm.3 points out that iVGAE are causally expressive and that their existence is theoretically guaranteed, the following corollary is an important consequence that reduces the search space for such an iVGAE in practice significantly. That is, the induced graph G of any SCM C can be inducted into the GNN layers of the iVGAE such that L -consistency is still warranted. 2 Corollary 4 (L Representation.) For any SCM C that induces a graph G, there exists a corre- 2 sponding iVGAE V=(q, p) where q, p are G-GNN layers such that V is L -consistent with C. 2 Proof We repeat the arguments of the proof in Thm.3 with the slight modification that V := VG, indicating the G-GNN layers p, q, where G is the SCM C induced graph. A.6. Proof for Theorem 4 While expressivity is an important property of any model for inferential processes in science, iden- tifiability stands at the core of causal inference. That is, using partial information on higher levels of the PCH (or of the SCM) to perform inferences starting from the lower levels. E.g., inferring the causal effect of X on Y denoted by p(Y | do(X)) using only observational data from p(X, Y ) re- quires the identification of an estimand in the purely causal setting. If the graph is given by X → Y , then the identification is given by the conditional p(Y | X) = p(Y | do(X)). However, if the struc- ture is more involved, imagine a (hidden) confounder Z such that X ← Z → Y , then an adjustment regarding Z’s influence would be interesting. Pearl’s do-calculus provides an graphical-algebraic tool for performing identification (Pearl, 2009). Thm.4 establishes that these causal inferences are equivalent between the domains. In a nutshell, if one can perform an identification of inference for an SCM, one can also do it for an iVGAE model. Theorem 4 (Dual Identification.) Consider the causal quantity of interest Q = pC(V | do(V )), i j G the true causal graph underlying SCM C ∈ Ω and p(V) the observational distribution. Q is neurally identifiable from iVGAE V∈Υ with G-GNN modules iff Q is identifiable from G and p(V). 21
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING Proof The proof uses the same trick as the proof for Thm.4 in (Xia et al., 2021) to establish the duality in identification for SCM and iVGAE. Let Ω, Υ be the sets of all SCMs and iVGAEs respectively. If Q is not identifiable from a graph G and the observational distribution p(V) with full support ∀v : p(v) > 0, then there exists a pair of SCMs C, C(cid:48) such that they agree on L 1 and G but not on L i.e., pC(V | do(V )) (cid:54)= pC(cid:48)(V | do(V )). By Cor.4 we know that there will 2 i j i j always be an iVGAE model VG based on G-GNN layers that is L -consistent with C. This is all, 2 since, pVG(V | do(V )) = pC(V | do(V )) (cid:54)= pC(cid:48)(V | do(V )) which suggests that if Q is not i j i j i j identifiable by G and p(V), then it is also not neurally identifiable through VG. Again, as in the proof for Cor.2, the reverse direction in which we assume Q to be identifiable analogously holds true for the same reason in this scenario. Appendix B. Further Experimental Insights and Details The following subsections provide an extensive empirical analysis of causal effect estimation (B.1) and properties of the density estimation (B.2). Code. We make our code publically available at https://anonymous.4open.science/r/ Relating-Graph-Neural-Networks-to-Structural-Causal-Models-A8EE. B.1. Causal Effect Estimation Our experiments in the following investigate causal inference, namely causal effect estimation. We are interested in the average causal (or treatment) effect defined as ATE(X, Y ) := E[Y | do(X=1)]− E[Y | do(X=0)] (Pearl, 2009; Peters et al., 2017), where the binary variable X is being referred to as treatment (and Y is simply the effect e.g. recovery). Note that we can extend the ATE to be categorical/continuous, however, we focus on binary structures in the following, thereby the men- tioned formulation is sufficient. Also, note that ATE is a special case of density estimation in which the same intervention location X is being queried for the different intervention parameterizations do(X = x), for binary variables this amounts to do(X = i), i ∈ {0, 1}. General properties of the density estimation for the iVGAE are being systematically investigated in later sections of the Appendix (B.2, Figs.5,6). Nonetheless, it is an important sub-category since usually one is inter- ested in causal inference rather than minuscule approximation of all derivable probability densities. In the following we do not consider identification, since the iVGAE is a data-dependent model (Def.4,Thm.4) it can not act as a proper SCM like NCM can (which are simply a neural net based reparameterization of the SCM formulation, and thus by construction capable of identification). In the following, Fig.4(b) from the main paper will act as guiding reference to the subsequent analysis. Considered SCM Structures. We consider three different structural causal models structures (1: chain, 2: confounder, 3: backdoor) that are of significantly different nature in terms of information flow as dictated by the d-separation criterion (Koller and Friedman, 2009; Pearl, 2009). Fig.4(b) bottom portraits the different structure (note the re-ordering of the variables, the graphs are being drawn in a planar manner). In the following we provide the exact parametric form for each of the 22
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS SCMs M , i ∈ {1..3}, the chain is given by i  X ← f (U ) =U  X X X    Y ← f (X, U ) =X ∧ U Y Y Y M = (23) 1 Z ← f (Y, U ) =Y ∧ U   Z Z Z   W ← f (Z, U ) =Z ∧ U , W W W the confounded structure is given by  X ← f (Z, U ) =Z ⊕ U  X X X    Y ← f (X, Z, U ) =(X ∧ U ) ⊕ (Z ∧ U ) Y Y Y Y M = (24) 2 Z ← f (U ) =U   Z Z Z   W ← f (X, U ) =X ∧ U , W W W and finally the backdoor structure is given by  X ← f (Z, U ) =Z ⊕ U  X X X    Y ← f (W, X, U ) =X ∧ (W ∧ U ) Y Y Y M = (25) 3 Z ← f (U ) =U   Z Z Z   W ← f (Z, U ) =Z ∧ U , W W W where ⊕ denotes the logical XOR operation. We refer to logical operations to assert that the vari- ables remain within {0, 1}. Note that we consider Markovian SCM, thus the exogenous/noise terms are independent. We choose Bernoulli B(p), p ∈ [0, 1] distributions for the noise terms. We choose the p for each of the terms U uniformly at random to generate 5 different parameterizations of the i i same structure. For each intervention we create a data set of size 10000 and train a model consisting of two iVGAE modules. We consider three random seeds for each of the three parameterizations for each of the three structures, 33 = 27 distinct optimizations. In the following we always consider the ATE of X on Y , that is Q = ATE(X, Y ), which can be positive/negative Q (cid:54)= 0 or neutral Q = 0 if their is neither a direct nor indirect influence from X to Y . All ATE estimates we observed were ap- proaching zero thus reasonable (more specifically, bounded in [0, 0.2] whereas the maximum error is |ATE∗ − ATˆE| = |1 − (−1)| = 2 i.e., the worst-case single approximation was off by 10%). Interpretation for ATE Estimation on the chain structure M . The causal effect of X on Y 1 is both direct and unconfounded. It is arguably the easiest structure to optimize, and the iVGAE correctly performs (see top/green row in Fig.4(b)). The variance is further reduced in comparison, arguably due the relatively low variance in ground truth ATEs (since M ATE are mostly positive). 1 Interpretation for ATE Estimation on the confounder structure M . The causal effect of X 2 on Y is direct yet confounded via Z. The ATE can thus obtain, given parameterizations, positive, negative and the zero value. The iVGAE reacts accordingly and is able to adequately estimate the causal effect. This is the key observation to causal inference since a correlation-based model would fail to return the correct answer i.e., it would simply return p(Y |X) instead of p(Y | do(X)). Interpretation for ATE Estimation on the backdoor structure M . The most difficult case 3 since the causal effect of X on Y is confounded through a backdoor path X ← Z . . . Y . Nonethe- less, the iVGAE adequately models the causal effect. The variance, like for M , is increased which 2 is arguably due to the spectrum of available ATE instantiations dependent on the concrete parame- terization of the SCM. 23
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING Numerical Report. In Tab.1 we show numerical statistics on the trained models applied to the different SCM structures M for one of the three parameterizations averaged across seven random i seeds. We show the two interventions on X for computing the ATE, and report mean, best and worst ELBO (Def.2) and likelihood log p(x) performances (the higher the better). As expected, ELBO lower bounds the marginal log-likelihood and the validation performance, as desired, corresponds with the test performance. B.2. Systematic Investigation on Density Estimation We perform multiple experiments to answer various interesting questions. The following list enu- merates all the key questions to be highlighted and discussed in this section: (a) What aspects of an interventional change through do(·) does the method capture? (b) How does variance in ELBO 2 during variational optimization affect the method? (c) When and how does the method fail to capture interventional distributions? (d) At what degree does the performance of the method vary for different training durations? (e) How does the method scale w.r.t. interventions while keeping capacity constant? (f) How important is parameter tuning? For all the subsequent experiments we considered the same architecture. That is, an iVGAE (Def.5) model consisting of two interventional GNNs (Def.1) for the encoder and decoder respectively where each GNN consists of 2 Sum-Pool Layers as introduced by (Kipf and Welling, 2016a). The decoder has 2B2 parameters, whereas the encoder has 3B2 parameters, where B is the batch size, to allow for modelling the variance of the latent distribution. We consider datasets of size 10000 per intervention. The interventions are collected by modifying the data generating process of the data sets. For simplicity, we mostly consider uniform interventions. Optimization is done with RMSProp (Hinton et al., 2012) and the learning rate is set to 0.001 throughout. We perform a mean-field variational approximation using a Gaussian latent distribution and a Bernoulli distribu- tion on the output. All data sets we have considered are binary but extensions to categorical or continous domains follow naturally. In the following, we focus on ASIA introduced in (Lauritzen and Spiegelhalter, 1988), and Earthquake/Cancer covered within (Korb and Nicholson, 2010) re- spectively. We employ a training, validation and test set and use the validation set to optimize performance subsequently evaluated on the test set. We use a 80/10/10 split. We use 50 samples per importance sampling procedure to account for reproducibility in the estimated probabilities. Training is performed in 6000 base steps where each step considers batches of size B that are being scaled multiplicatively with the number of interventional distributions to be learned. The adjacency provided to the GNNs is a directed acyclic graph (DAG) summed together with the identity matrix to allow for self-reference during the computation. The densities are acquired using an adjusted application of Alg.1. All subsequent experiments are being performed on a MacBook Pro (13-inch, 2020, Four Thunderbolt 3 ports) laptop running a 2,3 GHz Quad-Core Intel Core i7 CPU with a 16 GB 3733 MHz LPDDR4X RAM on time scales ranging from a few minutes up to approximately an hour with increasing size of the experiments. In the following, Figs.5 and 6 act as reference for the subsequent subsections’ elaborations on the questions (a)-(f). Numerical statistics are provided 24
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS in Tab.2. For reproducibility and when reporting aggregated values (e.g. mean/median) we con- sider 10 random seeds. Our code is available at https://anonymous.4open.science/r/ Relating-Graph-Neural-Networks-to-Structural-Causal-Models-A8EE. Q-(a) What aspects of an interventional change through do(·) does the method capture? Con- sider Fig.5(a) in the following. It shows an iVGAE model trained on the observational (L ) and one 1 interventional (L , intervention do(tub = B( 1 ))) distributions, where the former is shown on the 2 2 left and the latter on the right. We can observe that both the change within the intervention location (tub) but also in the subsequent change propagations along the causal sequence (either, xray, dysp) are being captured. In fact, they are being not only detected but also adequately modelled for this specific instance. If the optimization is successful in fitting the available data with the available model capacity, then this is the general observation we make across all the other settings we have evaluated i.e., the model can pick-up on the interventional change without restrictions. Q-(b) How does variance in ELBO 2 during variational optimization affect the method? Consider Fig.5(b) in the following. Two different random seeds (that is, different initializations and thus optimization trajectories) for the same iVGAE under same settings (data, training time, etc.) are being shown. Clearly, the optimization for the seed illustrated on the left was successful in that the quantities of interest are being adequately estimated. However, the random seed shown on the right overestimates several variables (tub, either, xray) and simply does not fit as well. We argue that this is a general property of the variational method and ELBO (Eq.2) i.e., the optimiza- tion objective is non-convex and only a local optimum is guaranteed. Put differently, the variance in performance amongst random seeds (as measured by ELBO) is high. Q-(c) When and how does the method fail to capture interventional distributions? Consider Fig.5(c) in the following. The predicted marginals of a single iVGAE model on the Earthquake dataset (Korb and Nicholson, 2010) are being presented for the observational density (right) and the interventional do(Earthquake = B( 1 )) (left). The underlying graph in this real-world inspired data 2 set is given by G =({B, E, A, M, J }, (26) {(B → A), (E → A), (A → {M, J })}) where B, E, A, M, J are Burglary, Earthquake, Alarm, MaryCalls and JohnCalls respectively. From G we can deduce that the mutilated graph G that is generated by the aforementioned Bernoulli- I intervention I = do(E = B( 1 )) will in fact be identical G = G . Put differently, conditioning 2 I and intervening are identical in this setting. The formulation for performing interventions in GNN (Def.1) only provides structural information i.e., information about the intervention location (and thus occurence) but not about the content of the intervention. While this generality is beneficial in terms of assumptions placed onto the model, it also restricts the model in this special case where associational and internvetional distributions coincide. In a nutshell, computationally, the two posed queries I = I and I = do(∅) are identical in this specific setting (I = I ) and this is also being 1 2 1 2 confirmed by the empirical result in Fig.5(c) i.e., the predictions are the same across all settings as follows naturally from the formulation in Def.1 which in this case is a drawback. Generally, this insight needs to be considered a drawback of formulation Def.1 opposed to being a failure mode since the formulation indeed behaves as expected. In all our experiments, actual failure in capturing the densities seems to occur only in low model-capacity regimes, with early-stoppage or due to numerical instability. 25
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING Q-(d) At what degree does the performance of the method vary for different training dura- tions? Consider Fig.6(d) in the following. It shows the same model being probed for its predic- tions of the observational distributions at different time points, left early and right later (at con- vergence). Following intuition and expectation, training time does increase the performance of the model fit. Consider nodes tub and lung which were both underestimating in the earlier iterations while being perfectly fit upon convergence. Q-(e) How does the method’s scaling towards many interventions behave when keeping capac- ity constant? Consider Fig.6(e) in the following. We show the same iVGAE model configurations being trained on either 2 interventional (top row) or 4 interventional distributions from the Earth- quake dataset (Korb and Nicholson, 2010). I.e., we keep the model capacity and the experimental settings consistent while increasing the difficulty of the learning/optimization problem by provid- ing double the amount of distributions. As expected, we clearly see a degeneration in the quality of density estimation. The iVGAE model trained on 2 distributions adequately estimates the Bernoulli- interventional distribution do(A = B( 1 )) (top right) while the model trained on more distributions 2 (lower right) fails. Q-(f) How important is parameter tuning? Consider Fig.6(f) in the following. It shows an iV- GAE model before (left) and (after) parameter tuning on the Bernoulli-intervention do(tub = B( 1 )) 2 on the ASIA dataset (Lauritzen and Spiegelhalter, 1988) where the tuned parameters involve aspects like pooling (sum, mean), layer numbers, learning rate, batch size etc. We clearly see a an improve- ment towards a perfect fit for certain nodes (smoke, tub, either). As expected, parameter tuning, as for any other machine learning model, is essential for improving the predictive performance. Especially, for universal density approximation, as discussed in Thm.2, this is crucial - since in principle any density can be approximated with sufficient capacity and thus the dependence relies on the model itself but also on the optimization for optimality in practice. Numerical Report. In Tab.2 we show numerical statistics on the trained models applied to the different data sets for answering the investigated questions (a)-(f). For reproducibility and stability, we performed 10 random seed variants per run. NaN values might occur for a single seed due to numerical instability in training, thus invalidating the whole run. We show the performance on different, increasing interventional data sets at various training iterations. We report mean, best and worst ELBO (Def.2) and likelihood log p(x) performances (the higher the better). Question (b) regarding the variance of ELBO becomes evident when considering the best-worst gaps. As expected, ELBO lower bounds the marginal log-likelihood. Also, by providing more distributions to learn, thus increasing difficulty, the quality of the fits in terms of ELBO/likelihood degenerates which is inline with what we observed regarding question (e). Finally, we might also note that the validation performance, as desired, corresponds with the test performance. Full-width Figures and Tables. Following this page are the figures and tables (Fig.5, 6, Tabs.1, 2) that were referenced in the corresponding sections of the appendix. 26
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS (a) (b) (c) Figure 5: Systematic Investigation: Questions (a), (b), and (c). The questions being answered by the presented illustrations in the respective blue box are (a) What aspects of an in- terventional change through do(·) does the method capture? (b) How does variance in ELBO 2 during variational optimization affect the method? and (c) When and how does the method fail to capture interventional distributions? Please consider the elaboration within the corresponding text segment. (Best viewed in color.) 27
ZECˇ EVIC´ DHAMI VELICˇ KOVIC´ KERSTING (d) (e) (f) Figure 6: Systematic Investigation: Questions (d), (e), and (f). The questions being answered by the presented illustrations in the respective blue box are (d) At what degree does the per- formance of the method vary for different training durations? (e) How does the method’s scaling towards many interventions behave when keeping capacity constant? and (f) How important is parameter tuning? Please consider the elaboration within the corresponding text segment. (Best viewed in color.) 28
RELATING GRAPH NEURAL NETWORKS TO STRUCTURAL CAUSAL MODELS Mean Mean Mean Mean Mean Best Worst C do(X = x) Steps i Train ELBO Valid ELBO Test ELBO Valid log p(x) Test log p(x) Test ELBO Test ELBO 1 1 2.5k -1.48 -1.60 -1.59 -1.43 -1.43 -1.50 -1.86 1 0 2.5k -1.86 -2.12 -2.07 -1.85 -1.84 -1.70 -2.47 2 1 2.5k -1.48 -1.60 -1.59 -1.43 -1.43 -1.37 -1.86 2 0 2.5k -1.86 -2.12 -2.07 -1.85 -1.84 -1.91 -2.47 3 1 2.5k -1.48 -1.60 -1.59 -1.43 -1.43 -1.37 -1.86 3 0 2.5k -1.86 -2.12 -2.07 -1.85 -1.84 -1.91 -2.47 Table 1: Causal Effect Estimation: Key Statistics. The aggregations cover three random seeds per model. Mean Mean Mean Mean Mean Best Worst Dataset |L | Steps 2 Train ELBO Valid ELBO Test ELBO Valid log p(x) Test log p(x) Test ELBO Test ELBO ASIA 2 16k -3.43 -6.02 -4.60 -4.15 -4.11 -4.10 -5.37 ASIA 4 16k NaN NaN -4.61 NaN -4.05 -3.79 -5.59 Cancer 2 12k -2.26 -4.76 -3.17 -3.66 -2.76 -2.35 -4.49 Cancer 4 12k NaN NaN -3.26 NaN -2.88 -2.43 -4.53 Earthquake 2 12k -1.21 -3.02 -2.43 -1.92 -1.93 -1.49 -3.50 Earthquake 4 12k -0.78 -4.67 -2.77 -2.31 -2.27 -1.75 -3.46 Table 2: Density Estimation: Key Statistics. The aggregations cover 10 random seeds for each of the models respectively. 29
