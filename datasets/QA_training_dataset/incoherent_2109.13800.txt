Faster Improvement Rate Population Based Training Valentin Dalibard Max Jaderberg DeepMind, London, {vdalibard, jaderberg}@deepmind.com Abstract The successful training of neural networks typically involves careful and time consuming data augmentation. Population Based Training (PBT) has recently been proposed to automate this process. PBT trains a population of neural networks concurrently, frequently mutating their hyperparameters throughout their training. However, the decision mechanisms of PBT are greedy and favour short-term im- provements which can, in some cases, lead to poor long-term performance. This paper presents Faster Improvement Rate PBT (FIRE PBT) which addresses this problem. Our method is guided by an assumption: given two neural networks with similar performance and training with similar hyperparameters, the network show- ing the faster rate of improvement will lead to a better final performance. Using this, we derive a novel fitness metric and use it to make some of the population members focus on long-term performance. Our experiments show that FIRE PBT is able to outperform PBT on the MNIST benchmark and match the performance of networks that were trained with a hand-tuned learning rate schedule. We apply FIRE PBT to supervised learning tasks and show that it leads to faster learning and higher final performance than both PBT and random hyperparameter search. 1 Introduction In order to successfully train neural networks, it is crucial that their hyperparameters are carefully tuned. This is a time consuming task and in recent years, the field of AutoML [11] has emerged to design methods automating this process [19, 10, 1, 15]. Population based training (PBT) is one such method. In PBT, a population of workers concurrently train their respective neural networks. The workers regularly explore the hyperparameter space by mutating their hyperparameters while training. With the same amount of resources, PBT can outperform random hyperparameter search on many important problems [13, 6, 12]. However, PBT’s decisions favour immediate improvements in performance and can sometimes lead to unsatisfactory solutions later in the training process. For example, this is often the case with the learning rate hyperparameter. The best learning rate schedules will often keep the learning rate high for a large fraction of training before reducing it. However, decaying the learning rate during training often results in an immediate boost in performance. When PBT is made to optimise the learning rate, it will often greedily reduce it early on in training (see Figure 1) resulting in a poor final performance. This paper presents Fastest Improvement Rate (FIRE) PBT which addresses this problem. In FIRE PBT, disjoints sub-populations , . . . each train with their own dedicated fitness 1 2 n P P P function. The fitness functions are designed so that each sub-population converges on hyperparameters corresponding to a different section of a good hyperparameter schedule. They are based on a novel criterion which measures the improvement rate of training curves. Our experiments show that FIRE PBT outperforms PBT and matches the performance of networks that were trained with a hand-tuned learning rate schedule on the MNIST benchmark [17]. We also Preprint. Under review. 1202 peS 82 ]EN.sc[ 1v00831.9012:viXra
Figure 1: Hand-tuned learning rate schedule for MNIST training [8] compared with the best schedules found by PBT and FIRE PBT after similar number of steps. As shown in Section 4, the average test F1-score scored by executing a schedule found by PBT is 71.7, while FIRE PBT and the hand-tuned schedule score 76.5 and 76.6 respectively. apply FIRE PBT to a V-MPO agent [20] on a selection of OpenAI Gym tasks [3] and show that it leads to faster learning and higher performance than PBT or random hyperparameter search. We review the mechanisms of PBT in Section 2. Section 3 introduces FIRE PBT. We compare the performance of Random Search (RS), PBT and FIRE PBT in Section 4. Finally, we review the related work in Section 5 and conclude in Section 6. 2 Population based training This section reviews the core mechanisms of Population based training as described by Jaderberg et al. [13]. PBT is an optimisation method for the training of neural networks. The goal is to maximize an objective function , such as classification F1-score on a validation set. Q In PBT, a population of workers (also called population members) concurrently train their respective neural networks, each with their own hyperparameters. At regular intervals, each population member compares its evaluation of , called fitness, with the rest of the population. If a population member ρ Q has a low fitness compared with its peers, it undergoes an exploit-and-explore step. In the exploit step, ρ discards its own state and copies the neural network weights and hyperparameters of a better performing population member. In the explore step, ρ mutates the copied hyperparameters. Compared with sequential hyperparameter optimisation methods such as Bayesian optimisation [2], PBT uses parallel training to complete within the wall clock time of a single learning process. The hyperparameters will be optimised concurrently to the training of the neural network. Com- pared with parallel optimisation such as random hyperparameter search, PBT tends to yield better performance [13]. However, PBT uses a greedy process which can lead to low performance later in training. For example, this can occur when PBT is made to optimise the learning rate using the current validation F1-score as fitness. Because decaying the learning rate results in an immediate improvement in performance, PBT will greedily decay the learning rate from the start of the experiment, as shown in Figure 1. However, good hand-tuned learning rate schedules will often keep the learning rate high for a significant fraction of training before decaying it. As a result, the performance of networks trained with PBT will be lower than the one obtained by a good hand-tuned schedule. 3 FIRE PBT We now present the mechanisms of FIRE PBT, which are summarised in Figure 2. Like in PBT, a FIRE PBT experiment consists of many workers training neural networks concurrently. Each worker ρ holds a pair (θ , h ) where θ are neural network weights and h are training hyperparameters. ρ ρ ρ ρ We divide the workers into two groups. = ρ P , which we call the population members, and P { i }i=1 = η E , which we refer to as the evaluators. The population is further divided into multiple H { i }i=1 P disjoint sub-populations , . . . . 1 2 n P P P 2
Evaluators Regular PBT within each sub-population. The members execute the exploit-and-explore step and copy and mutate each other’s weights and P3 hyperparameters. Evaluator copying the neural network weights of the sub-population member it is evaluating. P2 Evaluator copying the hyperparameters of the highest fitness sub-population member. Population member copying the neural network weights held by the evaluator after evaluator success. P1 Figure 2: The mechanisms of FIRE PBT. Each circle represents a population member. The arrows indicate the copying of neural network weights and/or hyperparameters between members. Within each sub-population , the population members internally run regular PBT. Members have i P an associated fitness and the ones with low fitness perform the exploit-and-explore step with other members of . What distinguishes sub-populations from one another is the fitness function they use. i P Sub-population is greedy and uses as its fitness, similar to regular PBT. For example, could 1 P Q Q be the classification F1-score on a validation set. All other sub-populations with i > 1 are parent i P sub-populations and behave differently. Their fitness encourages them to produce neural network weights which reach high values of when trained with the hyperparameters used in their child Q sub-population . Ultimately, our method will produce neural networks which have trained with i−1 P the hyperparameters of each sub-population in turn, starting with and ending with . The fitness n 1 P P functions are designed to encourage this sequence of hyperparameter values to form a good schedule. Evaluating fitness in parent sub-populations is the role of the evaluator workers. Throughout the experiment, an evaluator η repeats the following procedure: 1. Find the population member ρ (cid:83)P and its sub-population which has least parent ∈ j=2 Pj Pi recently been evaluated and copy its weights: θ θ . We say that η is then assigned η ← ρparent to ρ . parent 2. Find the highest fitness member ρ in and copy its hyperparameters: h h . child Pi−1 η ← ρchild We say ρ is the target of the evaluator. child 3. Start training from the newly assigned evaluator weights θ with the evaluator hyperparam- η eters h and regularly measure the objective function . This sequence of evaluations of η Q will generate a training curve which will be used to evaluate the fitness of ρ . The parent Q larger the rate of improvement of the curve, the higher the fitness (detailed in Section 3.1). 4. Continue training until either the evaluator’s stopping or success criteria are reached (detailed in Section 3.2). If the evaluator’s success criteria are reached, make the evaluator’s target ρ copy the evaluator’s current weights: θ θ . child ρchild ← η Evaluators have two purposes: First, they evaluate the fitness of population members of parent sub-populations. Section 3.1 describes how the training curve of an evaluator assigned to ρ is parent used to compute the fitness of ρ . Second, evaluators make the link between sub-populations parent by propagating the trained weights of a parent sub-population into its child sub-population. Section 3.2 describes the conditions for evaluator success where the evaluator’s performance is considered greater than the one of its target ρ . When that occurs, ρ copies the weights of the evaluator. child child This mechanism is how weights are propagated between sub-populations. 3.1 Computing the fitness of parent sub-population members As in regular PBT, evolution in parent sub-populations is decided based on a fitness assigned to its members. This section discusses how the training curve of an evaluator that is assigned to a population member ρ is used to compute the fitness of ρ . The intuition behind our method parent parent 3
(cid:611) (cid:608) (cid:608) (cid:609) (cid:610) (cid:609) (cid:610) Figure 3: Example comparison of two curves. Although the left curve has a higher average im- provement rate, our procedure find the right curve to be comparatively better. The circled numbers indicate the step in the best_score_diff procedure: 1) Smooth the curves, 2) Find the starts of the overlapping sections, 3) Find the length of the overlapping sections, and 4) Compare the best score within each overlapping section. is that we can use the improvement rate of the training curve as a proxy for the quality of the original weights θρ . Stated explicitly, the assumption that guides our method is as follows. parent Assumption 1 Given two neural networks with similar performance and training with the same or similar hyperparameters, the network showing the fastest rate of improvement will lead to a better final performance. Section 3.1.1 proposes a comparative method which exploits this assumption. Given two curves, it determines whether they are comparable and, if so, which of the two is improving faster. Section 3.1.2 will describe how this relative metric is turned into a global fitness. 3.1.1 Comparing two curves Each evaluator training curve consists of a series of points [(x , y ), (x , y ), . . . , (x , y )] where 1 1 2 2 n n the xs are regularly spaced training steps and ys are the evaluated value of the objective function at Q that step. x is the step at which the evaluator copied the weights of its assigned member ρ 1 parent and x is its current step. For example, Figure 3 shows a comparison between two curves as they n both reach step 2000. The evaluator that produced the left curve copied the weights of its assigned member at step 1000, while the evaluator that produced the right curve did so at step 1500. Given two evaluators η and κ and their respective training curves (xη, yη) and (xκ, yκ), we describe below a function best_score_diff(η, κ) which is positive if η is considered to be improving faster than κ and negative if κ is improving faster than η. It works by finding an overlapping section where the two curves have similar performance and their rates of improvement can be compared. The function best_score_diff executes the following procedure: 1. Smooth the curves. Fit a Gaussian Process (GP) [16] through each curve to smooth them. This is useful in contexts where fluctuates rapidly throughout training. We use a Matérn 5/2 kernel and use empirical BayeQ s to fit its hyperparameters. Let yˆη and yˆκ be the smoothed version of yη and yκ respectively. 2. Find the starts of the overlapping sections. Find the curve whose GP fit has the highest starting performance by comparing yˆη and yˆκ. Then, find the first point when the other 1 1 curve’s GP fit reaches that performance for the first time. That is, say yˆη < yˆκ, we find the 1 1 smallest r such that yˆη yˆκ. The overlapping section of (xη, yη) will start at (xη, yη), and the one of (xκ, yκ) ar t (≥ xκ,1 yκ). For example, in Figure 3, the right curve starts hr ighr er and 1 1 so its overlapping section starts from its beginning. The left curve first reaches a similar performance at around 1150 steps, which is where its overlapping section starts. 3. Find the length of the overlapping sections. Find the maximum number of steps for which both curves have evaluations of data past their starting point. That is, say the Q 4
overlapping sections start at (xη, yη) and (xκ, yκ), then the length of the overlapping r r s s sections is n = max(len(xη) r, len(xκ) s). In Figure 3, the right curve has fewer − − steps past its starting point and therefore constrains the length of the overlapping section. 4. Compare the best score within each overlapping section. Find the highest recorded evaluation of within each curve’s overlapping section and compute their difference: best_score_dQ iff(η, κ) = max yη max yκ . r:r+n − s:s+n Note that the smoothed curves obtained in Step 1 are only used in the second step. In general, we prefer relying on the data of the original training curve for simplicity and robustness. However, we find smoothing the curves is necessary to obtain reasonable overlapping sections. A special case occurs when two evaluators do not have an overlapping section because one curve is strictly higher than the other. This is an important case to tackle: it frequently occurs when poor hyperparameter values are explored. These will often lead to degenerate neural network weights and flat training curves with a minimal score. In order to respond to these contexts appropriately we make a second assumption. Assumption 2 When training with a fixed set of hyperparameters, the training curves of neural networks are concave. We use this assumption in the following way. Say the training curve of η is strictly higher than the one of κ: min(yη) > max(yκ). Our method tries to penalise the training curve of η by a constant amount to see if, despite being higher, it is also improving faster. That is, we say penalised (η) δ has the training curve (x, y δ) and check if best_score_diff(penalised (η), κ) > 0. We try multiple values for δ in the r− ange [min(yη) max(yκ), min(yη) min(yκ)].δ If some values of δ − − lead to a positive score, we make best_score_diff(η, κ) return the highest found score. Otherwise, we set best_score_diff(η, κ) = 0. 3.1.2 Computing the fitness of members of parent sub-population We now show how to compute the fitness of the members of a parent sub-population . First, we i P find the set Φ of members in which currently have an assigned evaluator. Note that members never i P have more than a single evaluator assigned to them. Members that have not yet been assigned an evaluator do not have a fitness and are exempt from evolution. Second, for each member in this set, we compare its evaluator curve to the evaluator curve of each other member in the set. We then sum the computed best_score_diff over all comparisons. Call η the evaluator of ρ, we set: ρ (cid:88) fitness(ρ ) = best_score_diff(η , η ). j ρj ρk ρk∈Φ 3.2 Evaluator stopping and success criteria Evaluators have two purposes. The first one is producing the training curves to evaluate the members of parent sub-populations as described in the previous section. The second is providing the link between sub-populations which we present here. Our method keeps evaluators training as long as it is plausible that they will reach a higher performance than the one of their target ρ . If it looks like this will not happen, we stop the evaluator, making child it available to evaluate other population members. In order to make robust decisions, our method uses a notion of statistical significance which we present below. Testing for statistical significance. To see if an evaluator η is likely to outperform its target ρ , child we compare their training curves. Our goal is to establish whether the training curve of η is improving statistically significantly faster than ρ . According to Assumption 1, this would imply the neural child network weights held by η will result in better long-term performance. As before, the training curve of η starts from when it copied the weights of its assigned member ρ . The training curve of parent ρ starts from when it last copied the weights of another member via an exploit-and-explore step. child Given the training curves (xη, yη) of η and (xρchild, yρchild) of ρ child, we start by extracting over- lapping regions (xη , yη ) and (xρchild , yρchild) as described in Section 3.1.1. We then do a r:r+n r:r+n s:s+n s:s+n 5
pairwise comparison of each of the points in yη and yρchild: yη > yρchild, . . . , yη > yρchild. r:r+n s:s+n r+1 s+1 r+n s+n Let k be the number of times yη was higher than yρchild. We run a binomial test with k successes and n trials and compute the p-value. We call this quantity binom_test(η, ρ ). child Stopping criteria. At regular intervals, an evaluator will compare its training curve with the one of its target ρ using the procedure described above. An evaluator that has been training for T steps child will stop if either of the following criteria is met: • The evaluator’s curve does not overlap with ρ ’s curve and T is greater than a hyperpa- child rameter max_eval_steps, or • The evaluator’s curve does overlap with ρ ’s curve but binom_test(η, ρ ) is greater child child than p + max(0, 1 T /max_eval_steps) where p is a hyperparameter of our stat stat − method. We use p = 0.01 throughout our experiments. stat Success criteria. If η succeeds, meaning it is deemed better than ρ , ρ will discard its current child child neural network weights and copy the one of the evaluator: θ θ . The conditions for evaluator ρchild ← η success are: 1) best_score_diff(η, ρ ) > 0, and 2) binom_test(η, ρ ) < p . child child stat Finally, an evaluator will stop training and look for a new task if either the member it is assigned to ρ or its target ρ loses an evolution event and executes an exploit-and-explore step. When parent child choosing a new member to evaluate, the evaluator will pick the one which has been training for the longest since either it was evaluated or lost an evolution event. In some cases, we find it beneficial to use a hyperparameter min_steps_before_eval per parent i sub-population which determines the minimum number of steps before its members can be i P evaluated. This allows us to make sure that its child population has converged on a set of i−1 P hyperparameters. This way, the evaluators generating training curves for the members of will all i P be using similar hyperparameters. We tune min_steps_before_eval by inspecting the number of i steps needed for to converge on a narrow set of hyperparameter values. i−1 P 4 Experiments Our experiments focus on hyperparameter optimisation methods that are able to complete within the wall clock time of a single learning process. We compare FIRE PBT to PBT and random hyperparameter search (RS) on an image classification task and a supervised learning (RL) task. Selecting FIRE PBT hyperparameters. The main hyperparameters to choose in FIRE PBT are the number of sub-populations and the distribution of workers into the different sub-populations and evaluator set. Within an experiment, we always make all sub-populations be the same size. That size is a function of the difficulty of the optimisation problem. We find 8 to be a good size for our image classification experiments where learning is very stable and use 18 in our supervised learning experiments where training is more stochastic. We set the number of evaluators to be three quarters of the total size of the parent sub-populations. This is because, at any one time, roughly one quarter of members will have just performed an explore-and-exploit step and so should train for some time before being evaluated. 4.1 Image classification We compare FIRE PBT with PBT and RS on the classification task of training a ResNet-50 [9] on the MNIST dataset [17] with a batch size of 1024. We optimise the learning rate hyperparameter. The MNIST dataset consists of ~1.28 million training images and 50,000 validation images. To avoid overfitting the validation set, we extract 10,000 samples at random from the training set and use them for measuring the objective function . For clarity, we refer to these extracted samples as Q the validation set, and the original 50,000 validation images as the test set. For each method, we run five experiments each with 50 workers training on the truncated training set. To show the impact of population size, we also run FIRE PBT experiments with 22 workers and 36 workers. For each of these experiments, we find the highest recorded validation score throughout training. We report this score as well as the test score that was achieved by the same neural network at that same point in training. Finally, we retrace the hyperparameter schedule that lead to this score, 6
Top validation acc. Matching test acc. Schedule replay test acc. RS Hand-tuned schedule 80.49 0.13 76.04 0.16 76.60 0.16 ± ± ± PBT 75.78 0.78 70.91 0.78 71.69 0.71 ± ± ± FIRE PBT (22) 80.07 0.31 76.17 0.08 76.04 0.08 ± ± ± FIRE PBT (36) 80.36 0.35 76.43 0.24 76.36 0.29 ± ± ± FIRE PBT (50) 80.59 0.31 76.06 0.51 76.51 0.34 ± ± ± Table 1: MNIST scores. Shows mean and standard deviation of the top validation F1-score scored throughout the experiment, the test F1-score scored at the same point in training, and the test F1-score obtained by replaying the hyperparameter schedule found and training on the full training set. including the number of steps trained with each hyperparameter. We then train a new neural network, from scratch, following this hyperparameter schedule and training on the entire training set. We record the final test performance. This final step is so that our scores can be compared with previously published results produced in this regime. 4.1.1 Experimental details In our random search experiments, we follow the already tuned hyperparameter schedule proposed by Goyal et al. [8]. Their procedure uses a reference learning rate λ batch_size with λ = 1e 1. During 256 − the first 5 epochs of training, the schedule linearly increases the learning rate from 0 to the reference learning rate. It then keeps it constant, reducing it by 1/10 at the 30-th, 60-th and 80-th epoch. In our experiments, we keep this relative schedule and set each of the 50 workers to randomly sample λ log-uniformly between 1e 2 and 3e 1 at the start of the experiment. − − We use the same hyperparameters for both regular PBT and FIRE PBT’s within sub-population PBT: Hyperparameters We optimise the hyperparameter λ which determines the learning rate of λ batch_size . As with our random search experiments, we sample the initial value of λ log-uniformly 256 between 1e 2 and 3e 1. − − Objective function We evaluate a model by computing its top-1 F1-score on the validation set. In order to generate detailed training curves for FIRE PBT, we evaluate this every 200 training steps. Ready A member of the population is deemed ready to exploit and explore every 2400 steps. Similarly, in FIRE PBT, evaluators check their stopping and success criteria every 2400 steps. Exploit We use a truncation selector: If a population member has a fitness in the bottom 25% of the population, it copies the neural network weights and hyperparameters of a random member in the top 25% of the population. Explore We multiply λ by a value sampled at uniform random from [0.5, 0.8, 1.25, 2.0]. In our PBT experiments, the 50 workers form a single population. In FIRE PBT, our experiments with 22, 36 and 50 workers use two, three and four sub-populations respectively. We set each sub-population to be of size 8 and use the remaining workers as evaluators. We set max_eval_steps to 7200 steps. We find that using the min_steps_before_eval hyperparameter is unnecessary as the sub-populations quickly converge to reasonable hyperparameter values and set it to 0 for all sub-populations. Both PBT and FIRE PBT use a synchronous implementation which will wait for all workers to have reached the next evolution step before making any decision. 4.1.2 Results Table 1 shows for each method: the mean and standard deviation of the highest validation score, the test score that was achieved at the same point, and the test score that was achieved by replaying the same hyperparameter schedule and training on the full training set. FIRE PBT significantly outperforms PBT and achieves results similar to those of Random Search with a hand-tuned schedule. Figure 4a shows the hyperparameter distribution of the four sub-populations for one of our FIRE PBT experiments as well as the hyperparameter schedule that lead to the best validation score. The black circles indicate the points at which evaluators copied the neural network weights of their assigned member, leading to evaluator success. All other hyperparameter mutations that are shown are the result of an exploit-and-explore step. 7
(a) Distribution of the λ parameter (learning rate up to (b) Distribution across experiments of the population’s a linear scale) of the four sub-populations throughout a best validation F1-score scored by a worker at each FIRE PBT experiment. step. Figure 4: Evolution of the hyperparameters (left) and validation scores (right) throughout MNIST training. One of the advantages of FIRE PBT is that at any time throughout the experiment, the population members of provide a current best effort performance. Figure 4b shows the best validation score 1 P achieved at any point in our experiments. We can see that FIRE PBT rapidly reaches high F1-score without compromising the long-term performance. Compute costs. The cost of a single end-to-end MNIST training in our setup including the frequent evaluations is approximately 4 TPUv3-core-days. This brings the cost of an experiment with 50 workers to 200 TPUv3-core-days. 4.2 supervised learning We compare FIRE PBT with RS and PBT on the task of optimising the hyperparameters of a supervised learning agent training with a V-MPO loss [20]. We replicate the setting of the original V-MPO experiments on the OpenAI Gym tasks of Ant-v1 and Humanoid-v1 [3], including all hyperparameters. These experiments sweep over two hyperparameters showing robustness to their values: (cid:15) and (cid:15) , which bound the rate of change of the policy, are sampled log-uniformly αµ αΣ in the ranges [5e 3, 1e 2] and [5e 6, 5e 5] respectively. As their values have little impact on − − − − performance, we fix them to 7.5e 3 and 1e 5 respectively. − − For each method, we run three experiments each with 50 workers. All three methods optimise the same hyperparameters and use the same initial sampling distributions. We also use the same hyperparameters for both PBT and FIRE PBT’s within sub-population PBT. In FIRE PBT, we split the workers into two sub-populations of size 18 and use the remaining 14 workers as evaluators. Sub-population uses min_steps_before_eval of 1e9 actor steps and max_eval_steps of 6e8 2 P actor steps. We let agents train for 6e9 actor steps. Hyperparameters We optimise the learning rate and the (cid:15) hyperparameter which sets the size of η a trust region. We initially sample the learning rate log-uniformly between 1e 5 and 1e 3 and (cid:15) η − − log-uniformly between 5e 3 and 2e 2. − − Objective function We evaluate the current model by computing the average episode return. Ready A member of the population is deemed ready to exploit and explore every 6e7 actor steps. Similarly, in FIRE PBT, evaluators check their stopping and success criteria every 6e7 steps. Exploit We use a truncation selector: If a population member has a fitness in the bottom 25% of the population it copies the neural network weights and hyperparameters of a random member in the top 25% of the population. Explore We perturb each of the hyperparameters independently. The learning rate is multiplied by a random value selected from [0.5, 0.8, 1.25, 2.0]. Likewise, (cid:15) is multiplied by a random value η selected from [0.8, 0.95, 1.05, 1.2]. 8
Figure 5: Distribution across experiments of the top average episode return scored by a worker at each step on Ant-v1 (left) and Humanoid-v1 (right). Figure 5 shows the distribution across experiments of the top average episode return achieved by one of the population’s workers throughout training. FIRE PBT shows faster learning and higher performance than PBT and RS. Compute costs. The cost of training a single OpenAI Gym agent in our setup is approximately 3.2 TPUv2-core-days and 3.2k CPUs-days. This brings the cost of an experiment with 50 workers to 160 TPUv2-core-days and 160k CPUs-days. 5 Related work The field of AutoML [11] has grown rapidly in recent years due to the demand for machine learning methods that work out of the box. Most of these methods focus on sequential optimisation, where the result of a training run is used to inform the next iteration of hyperparameter search. In this domain, Bayesian optimisation has seen a large amount of attention [19, 10, 1]. Some approaches use parallelism and evaluate multiple hyperparameters at once to reduce the number of iterations [18, 7, 22]. Compared with these approaches, PBT and FIRE PBT have the advantage of completing within the wall clock time of a single learning process. Some work in this area models the training curves of models to predict how they will do in the future [21, 4, 14]. In concept, this is similar to the faster improvement rate approach presented in this paper. However, these works typically use this approach to compare the training curves of networks training with different hyperparameters, whereas we restrict our method to comparing networks that are training with similar hyperparameters. Hyperband [15] is a purely parallel approach which models hyperparameter selection as an infinite- armed bandit. However, the initial amount of parallelism required by the method is large and hence a practical implementation will start by sequentially evaluating independent models. An important fraction of the work in AutoML focuses on the hyperparameters that guide the architec- ture of the neural network [5]. It is an open research question how such hyperparameters should be included in the PBT or FIRE PBT framework as it would involve altering a network’s architecture during training. 6 Conclusion We have presented Faster Improvement Rate PBT, an extension of regular PBT, which tackles the cases in which greedy hyperparameter mutations lead to poor long-term performance. We have shown the effectiveness of FIRE PBT on the distinct domains of image classification and supervised learning. Our experiments showed that FIRE PBT was able to find sensible hyperparameter schedules which matched the performance of hand-tuned ones and outperformed static schedules. We hope that the use of FIRE PBT will become a powerful platform for researchers and practitioners to explore new methods. 9
References [1] J. S. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl. Algorithms for hyper-parameter optimiza- tion. In Advances in Neural Information Processing Systems, pages 2546–2554, 2011. [2] E. Brochu, V. M. Cora, and N. de Freitas. A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical supervised learning. CoRR, abs/1012.2599, 2010. [3] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016. [4] T. Domhan, J. T. Springenberg, and F. Hutter. Speeding up automatic hyperparameter optimiza- tion of deep neural networks by extrapolation of learning curves. In Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI’15, page 3460–3468. AAAI Press, 2015. [5] T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture search: A survey. Journal of Machine Learning Research, 20(55):1–21, 2019. [6] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1407–1416. PMLR, 10–15 Jul 2018. [7] J. González, Z. Dai, P. Hennig, and N. Lawrence. Batch Bayesian optimization via local penalization. In Artificial Intelligence and Statistics, pages 648–657, 2016. [8] P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch sgd: Training MNIST in 1 hour. arXiv preprint arXiv:1706.02677, 2017. [9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. [10] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm configuration. LION, 5:507–523, 2011. [11] F. Hutter, L. Kotthoff, and J. Vanschoren, editors. Automatic machine learning: methods, systems, challenges. Challenges in Machine Learning. Springer, Germany, 2019. [12] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G. Castañeda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman, N. Sonnerat, T. Green, L. Deason, J. Z. Leibo, D. Silver, D. Hassabis, K. Kavukcuoglu, and T. Graepel. Human-level performance in 3d multiplayer games with population-based supervised learning. Science, 364(6443):859–865, 2019. [13] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue, A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fernando, and K. Kavukcuoglu. Population based training of neural networks. CoRR, abs/1711.09846, 2017. [14] A. Klein, S. Falkner, J. T. Springenberg, and F. Hutter. Learning curve prediction with bayesian neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [15] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. arXiv preprint arXiv:1603.06560, 2016. [16] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. Adaptive Computa- tion and Machine Learning. MIT Press, Cambridge, MA, USA, Jan. 2006. [17] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. MNIST Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. 10
[18] A. Shah and Z. Ghahramani. Parallel predictive entropy search for batch global optimization of expensive objective functions. In Advances in Neural Information Processing Systems, pages 3330–3338, 2015. [19] J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959, 2012. [20] H. F. Song, A. Abdolmaleki, J. T. Springenberg, A. Clark, H. Soyer, J. W. Rae, S. Noury, A. Ahuja, S. Liu, D. Tirumala, N. Heess, D. Belov, M. A. Riedmiller, and M. M. Botvinick. V-MPO: on-policy maximum a posteriori policy optimization for discrete and continuous control. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [21] K. Swersky, J. Snoek, and R. P. Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [22] J. Wu and P. Frazier. The parallel knowledge gradient method for batch Bayesian optimization. In Advances in Neural Information Processing Systems, pages 3126–3134, 2016. 11
