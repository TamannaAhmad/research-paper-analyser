Deep Kernel Shaping Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping James Martens jamesmartens@deepmind.com Andy Ballard aybd@deepmind.com Guillaume Desjardins gdesjardins@deepmind.com Grzegorz Swirszcz swirszcz@deepmind.com Valentin Dalibard vdalibard@deepmind.com DeepMind, London, UK Jascha Sohl-Dickstein jaschasd@google.com Samuel S. Schoenholz schsam@google.com Google, Mountainview, USA Abstract Using an extended and formalized version of the Q/C map analysis of Poole et al. (2016), along with Neural Tangent Kernel theory, we identify the main pathologies present in deep networks that prevent them from training fast and generalizing to unseen data, and show how these can be avoided by carefully controlling the “shape” of the network’s initialization- time kernel function. We then develop a method called Deep Kernel Shaping (DKS), which accomplishes this using a combination of precise parameter initialization, activation func- tion transformations, and small architectural tweaks, all of which preserve the model class. In our experiments we show that DKS enables SGD training of residual networks without normalization layers on MNIST and CIFAR-10 classification tasks at speeds comparable to standard ResNetV2 and Wide-ResNet models, with only a small decrease in generaliza- tion performance. And when using K-FAC as the optimizer, we achieve similar results for networks without skip connections. Our results apply for a large variety of activation func- tions, including those which traditionally perform very badly, such as the logistic sigmoid. In addition to DKS, we contribute a detailed analysis of skip connections, normalization layers, special activation functions like RELU and SELU, and various initialization schemes, explaining their effectiveness as alternative (and ultimately incomplete) ways of “shaping” the network’s initialization-time kernel. 1 1202 tcO 5 ]GL.sc[ 1v56710.0112:viXra
Martens et al. 1. Introduction The current standard approach to deep learning relies on a combination of architectural elements including skip connections, normalization layers, and carefully chosen activation functions (such as RELU) to overcome the well-documented optimization difficulties present in traditional deep neural networks (Ioffe and Szegedy, 2015; He et al., 2016a; Szegedy et al., 2017). While this approach has proven very successful, enabling many applications in diverse fields such as vision (e.g. He et al., 2016a; Tan and Le, 2019), language (e.g. Vaswani et al., 2017; Brown et al., 2020), protein folding (Jumper et al., 2021) and supervised learning (e.g. Espeholt et al., 2018; Silver et al., 2018), it is not entirely satisfying for at least several reasons. First, the precise mechanism of action of these elements, as well as their interaction, is still not well understood, despite some recent progress in this area. This lack of under- standing makes it difficult to design new network architectures, as architectural choices not only affect the network’s expressivity, but also its trainability, and in ways that are hard to predict. Second, without competitive alternatives to compare to, it’s not clear whether the current standard approach enables deep networks to reach their full potential, or whether it has unseen drawbacks and limitations. For example, while the use of skip connections helps very deep networks to train much faster, this might only be because it makes them behave like an ensemble of shallower networks (Veit et al., 2016). Finally, the extra complexity introduced by these architectural elements, and their non-trivial interactions, makes the- oretical analyses much more difficult, potentially holding us back from developing a more fundamental understanding of deep learning. And while existing theoretical analyses can (and often do) drop these elements, they do so at the risk of missing an essential piece of the picture. In an ideal world, modelling and trainability would be decoupled, so that architectures could be designed with only modelling considerations in mind, and rapid training would be guaranteed as long as they conformed to a well-defined set of rules. One might also hope that the components of such a framework would each have a clear purpose, be theoretically well-understood, and interact with each other in simple and predictable ways. In the present work we take an important step towards this “ideal world”, while si- multaneously providing a competitive alternative to the current standard approach to deep learning. We do so by developing a theoretically well-founded method for constructing deep networks which allows them to be rapidly trained without the use of skip connec- tions, normalization layers, or standard activation functions. Our approach, which we call Deep Kernel Shaping (DKS), requires only minor model class-preserving modifications to the architecture and activation functions, and is fully compatible with existing analysis frameworks such as Neural Tangent Kernel (NTK) theory (Jacot et al., 2018). As we show in experiments, DKS enables very deep residual networks without normaliza- tion layers to be trained using SGD on MNIST and CIFAR-10 classification tasks at sim- ilar speeds to standard ResNetV2 (He et al., 2016b) and Wide-ResNet models (Zagoruyko and Komodakis, 2016). It also achieves the same for networks without skip connections or normalization layers when combined with stronger optimizers like K-FAC (Martens and Grosse, 2015) or Shampoo (Gupta et al., 2018). Moreover, it works well with a large variety of activation functions, including those that traditionally perform very poorly (such as the 2
Deep Kernel Shaping logistic sigmoid). As a caveat, we observe a small decrease in generalization performance compared to standard ResNets, which we believe can be addressed in future work. While there have been some recently proposed methods for training very deep networks without skip connections and normalization layers (e.g Schoenholz et al., 2017; Balduzzi et al., 2017; Xiao et al., 2018), to the best of our knowledge, DKS is the first to achieve training speeds competitive with standard ResNet models on a challenging dataset like MNIST. And while our use of K-FAC plays an important role in these results, our experiments show that K-FAC alone is not enough, even when used in combination with the aforementioned methods. The starting point for our development of DKS is the work of Poole et al. (2016), who described the approximate initialization-time behavior of fully-connected combined layers (which we define here as an affine layer followed by an element-wise nonlinear layer) using special one-dimensional maps known as “Q and C maps”, and then used the fixed-point behavior of these maps to describe the depth-limiting behavior of a network composed of many such layers in sequence. We also take inspiration from Schoenholz et al. (2017), who applied this analysis framework to design an initialization method which modulated the fixed point behavior of each layer’s C map to slow the loss of “geometric information” with depth, and demonstrated encouraging results training very deep networks without skip networks or normalization layers. While originally derived within the semi-rigorous framework of “mean field analysis”, it turns out that Q/C maps also describe the approximate behavior of a combined layer’s kernel function in wide networks, and as we will show, can be applied to convolutional layers if one uses a Delta initialization for the filter banks. These maps can be further extended to describe entire networks with arbitrary topologies, where they provide useful information outside of the depth-limiting case considered by Poole et al. (2016). In the case of a fully connected network f , its Q map approximates the mapping from (cid:107)x(cid:107)2/ dim(x) to (cid:107)f (x)(cid:107)2/ dim(f (x)), and its C map approximate the mapping from x(cid:62)x(cid:48)/((cid:107)x(cid:107)(cid:107)x(cid:48)(cid:107)) to f (x)(cid:62)f (x(cid:48))/((cid:107)f (x)(cid:107)(cid:107)f (x(cid:48))(cid:107)). In deeper networks, the C map can easily become “degenerate”, mapping most of its input domain [−1, 1] to a small point-like subset of its codomain, [−1, 1]. The implication of this is that the distance between any pair of output vectors from the network is effectively independent of the distance between the corresponding pair of input vectors. As we argue, both heuristically and using NTK theory, this behavior inevitably leads to very slow training and/or poor generalization under gradient descent. We thus design DKS to prevent this problem, while also guarding against certain sec- ondary pathologies such as a badly behaved Q map, high approximation error in the Q/C maps themselves, and network behavior that is “too linear” (which limits network expres- sivity under gradient descent). To do this, we relate the overall “shape” of the network’s C map and its tendency to become degenerate, to its value and derivative at a couple of points, which we in turn relate to the values of derivatives of the C maps for the network’s individual layers. We then control these properties (and a couple of additional ones to ad- dress the aforementioned secondary pathologies), by transforming each activation function using a model class preserving scale and shift operation its input and output. This trans- formation is the same for each nonlinear layer for a given activation function, but depends 3
Martens et al. on the global structure of the network. In theory, we also require that sum operations in the network are “normalized” in a certain way, that a special kind of data preprocessing is used, and that pooling layers are replaced with certain roughly equivalent alternatives, although we find the latter two of these to be non-essential in practice. In addition to developing DKS, we also use Q/C maps to help explain the effectiveness of standard deep learning techniques such as normalization layers, skip connections, initial- ization methods, and common activation functions such as RELU and SELU (Klambauer et al., 2017), in terms of their effect on the network’s initialization-time kernel. This is facil- itated in part by the connections we establish between Q/C maps and alternative analysis frameworks such as “variance propagation” and “signal propagation” which underlie many of the said techniques. 2. Outline This manuscript is organized into five parts. Part I gives our assumptions and establishes the theoretical concepts used in subsequent parts. We begin in Sections 3 and 4 by defining our notation and stating our initial assump- tions on network architecture and initialization. In Section 5 we discuss kernel functions for networks conforming to these assumptions, and how they can be approximated with much simpler functions at initialization time. In Sections 6 and 7 we show how these kernel approximations can be further broken down in terms of a generalized version of the Q/C maps originally proposed in Poole et al. (2016). Derivative computations for Q/C maps are given in Section 8, and Section 9 discusses how to handle sum operations when computing Q/C maps. In Section 10, we show how C maps can be simplified down to one dimensional functions (from three dimensions) using a special type of data preprocessing which is de- signed to make two of their three inputs constant. And in Section 11 we discuss additional consequences of this, including that C maps become “positive definite functions”. With the theoretical groundwork established, Part II focuses on identifying desirable Q/C map properties and ways to achieve these. In Section 12 we discuss C map behavior in deep networks and how it can – and usually does – become “degenerate”, leading to slow training and/or poor generalization. We then set out to analyze C maps with the hope of controlling their properties so as to prevent this. To that end, in Section 13 we use the positive definiteness of a C map to show how its deviation from the identity function (which is large in degenerate maps) can be predicted from its derivative at 1 and value at 0, implying that we can prevent degeneration by enforcing certain conditions on these quantities. In Section 14 we identify another way that a network can fail to be trainable: that its parameters must move very far from their initial values before the network can exhibit any significantly nonlinear behavior. We then show this failure mode can be avoided by enforcing a condition on the C map of each nonlinear layer. In Section 15 we identify the breakdown of our kernel approximations as a third problem that we must avoid, and propose several solutions to this, including a condition to enforce on the network’s Q map. Having identified three distinct ways that a network can fail to be trainable, and condi- tions to enforce on the Q and C maps to prevent or mitigate these failures, we proceed with the specification and derivation of DKS in Part III. In Section 16 we list the four conditions 4
Deep Kernel Shaping on the Q and C maps of the network (or more precisely its “subnetworks”) which we will enforce. Then in Section 17 we show how these conditions can be reduced to ones on the Q/C maps of the individual layers of the network via a special translation mechanism called the “maximal slope function” which encodes structural information about the network (in- cluding its depth). In Section 18 we describe our main mechanism of enforcement for these per-layer conditions: scaling and shifting operations applied to the input and output of each nonlinear layer’s activation function (which preserve the model class). Finally, in in Sections 19 and 20 we discuss how to deal with normalization and pooling layers in DKS. With DKS fully derived, we give a step-wise summary of it in Section 21.1, and provide details for the more difficult aspects of its implementation in Section 22. In Section 23 we demonstrate the application of DKS on the various modified ResNet and Wide-ResNet models which we use in our experiments, including ones with skip connections and/or normalization layers removed. Before proceeding to experiments, in Part IV we delve deeper into the theory underlying DKS, and analyze various related approaches from the perspective of kernel approximations and Q/C maps. In Section 24 we review Neural Tangent Kernel (NTK) theory, and give an elegant expression for the NTK using (extended) C maps. We then show how NTK theory predicts slow training and poor generalization for networks with degenerate C maps, and characterize the form of the NTK for networks constructed using DKS. In Section 25 we review certain previously published methods for understanding the behavior of neural networks at initialization time (such as variance/signal propagation), show how they give rise to what are essentially Q and C maps (but different interpretations for what they actually compute), and advocate for the use of approximate kernel analysis as a more flexible and mathematically rigorous alternative. Exploiting these connections, we then review and analyze some prior methods for initializing and constructing neural networks in Section 26, including standard techniques such as normalization layers and residual networks, as well as methods aimed at replacing them. In each case we argue that the method can interpreted as enforcing some set of conditions on the network’s Q/C map, which is often a strict subset of those enforced by DKS. Finally, in Part V we discuss experiments and conclude. This begins in Sections 27 and 28, where we describe the setup of our experiments, and discuss their results. Our exper- iments include comparisons of DKS to standard ResNets, the methods reviewed/analyzed in Section 26, and various “ablated”/modified versions of DKS. We then summarize our conclusions in Section 29, and in Section 30 discuss the limitations of DKS and possible ways to address them in future work. 5
Martens et al. Table of contents 1 Introduction 2 2 Outline 4 I Theoretical preliminaries 9 3 Neural network terminology and architectural assumptions 9 4 Parameter distributions 10 5 Kernel function approximations for neural networks 11 6 Q and C maps for combined layers 18 7 Extended Q and C maps 20 8 Q and C map derivative computations 22 9 Handling weighted sum operations 24 10 Uniform q values 26 11 Additional consequences of uniform q values for C maps 28 II Desirable Q/C map behavior and how to achieve it 30 12 C map behavior in deep networks and necessary requirements for train- ability 31 13 Mathematical analysis of C maps 37 14 C map behavior in linear networks and the problem of being “too linear” 41 15 Mitigating kernel approximation error 43 III Specification and derivation of Deep Kernel Shaping 45 16 Conditions on Q/C maps that we will enforce 45 6
Deep Kernel Shaping 17 From global map conditions to local ones 48 18 Activation function transformations 51 19 Addressing normalization layers 56 20 Addressing pooling layers 58 21 Summary of our method 62 22 Some implementation details 65 23 Application to various modified ResNets 66 IV Additional analysis of DKS and related methods 70 24 Neural Tangent Kernel analysis 71 25 Variance propagation, signal propagation, and their relationship to ap- proximate kernel analysis 79 26 Review and analysis of related approaches for constructing and initializing deep neural networks 86 V Experiments and conclusions 95 27 Experimental setup 95 28 Experimental results 100 29 Conclusions 121 30 Limitations and future directions 121 VI Appendix 124 A Approximating average unit values 124 B Mathematical details for Section 8.1.2 125 C A detailed analysis of C map convergence in deep networks 125 D Mathematical details for Section 13 132 7
Martens et al. E Mathematical details for Section 20.1.2 137 F Mathematical details for Section 18.5 139 G Mathematical details for Section 20.2 139 H Mathematical details for Section 24 140 I Path-weight analysis and its relationship to approximate kernel analysis 143 J Analyzing (nearly) standard ResNets using Q/C maps 145 K Empirical evidence for the relationship between Q map derivatives and kernel approximation error 148 L Example learning rate schedules from FIRE PBT 150 M Meta-parameter studies 151 N Experiments with ablations and modifications of DKS 156 8
Deep Kernel Shaping Part I Theoretical preliminaries 3. Neural network terminology and architectural assumptions 3.1 Basic neural network terminology Throughout this work we will assume that the reader is already familiar with convolu- tional neural networks (Fukushima and Miyake, 1982; LeCun et al., 1998a) for which many overviews and tutorials are available (e.g. Goodfellow et al., 2016, Chapter 9). The pur- pose of this subsection won’t be to define convolutional network concepts from scratch, but rather to lay out the specific terminology we will use when referring to them. In this work will consider neural networks consisting of affine layers of the standard fully-connected and convolutional types, and nonlinear layers that compute element-wise activation functions (that are typically nonlinear). We define a combined layer to be an affine layer, immediately followed by a nonlinear layer. (Note that a combined layer is what was traditionally referred to as a “layer” in the neural network literature, before the modern trend of referring to the individual affine and nonlinear parts as their own separate “layers”.) The input and output of convolutional layers (or networks) are called feature maps, and consist of an array of locations vectors, with the entries of these vectors being called channels. The parameters of an affine layer are its weights (sometimes called a filter bank in the convolutional case), and its bias vector. So for example, in the fully-connected case, a combined layer would compute φ(W z + b), where z is its input vector, W its matrix of weights, b its bias vector, and φ its activation function (which is defined from R to R, and applied element-wise for higher dimensional inputs). In this work, the discussion will center around a single neural network which we will refer to simply as the network or sometimes the entire network. We will define a subnetwork as a neural network formed from a subset of the entire network’s layers which preserves all dependency relationships and has a well-defined and singular input and output (unlike the entire network, which can have multiple inputs and outputs in general). Subnetworks can be thought of as performing part of the computation of the network. So for example, if the network consists of a sequence of five layers, then layers 2, 3 and 4 form a subnetwork whose input is the input to layer 2, and whose output is the output of layer 4. But layers 2, 4, and 5 do not form a subnetwork since the dependency of layer 4 on layer 2 is not preserved. 3.2 Initial architectural assumptions We observe that a fully-connected layer is equivalent to an convolutional layer with a 1x1 feature map and 1x1 filter size, where the input/output data dimensions are just the in- put/output channel dimensions. Thus, going forward, we will restrict our analysis to the convolution case, which implicitly handles the fully-connected case via this reduction. We will also assume, for now, that the network can be entirely built out of three compo- nents: combined layers (as define above), non-zero constant scalar multiplications operations 9
Martens et al. applied to individual feature maps, and concatenation operations, which concatenate two feature maps of compatible sizes along their channel dimensions. We will permit a given feature map to act as the input to multiple operations/layers in the network, thus allowing “branching structures” and multiple “output heads”. The restriction to combined layers isn’t as severe as it might seem, as an isolated affine layer is equivalent to a combined layer with an identity activation function. And while sum operations are not explicitly included among the allowed operations, under certain conditions they can be simulated via a simple construction whose details we will defer to Section 9. This means that our analysis can apply to networks containing actual sum operations, under said conditions. Two or more consecutive nonlinear layers are also not allowed by our assumptions, however one can simply fuse two such layers into a single one by composing their activation functions. For now we will assume that the network does not contain any pooling layers. We will (partially) relax this assumption later in Section 20. 4. Parameter distributions 4.1 Assumptions on the form of the parameter distributions In order to obtain a sufficiently simple characterization of the function computed by a neural network at initialization time, we will make certain assumptions about the distribution of its parameters at initialization. Our first one will be that the bias vector is initialized to zero. While not strictly necessary to the derivation and viability of DKS, this assumption will simplify our presentation. Our second will be that if the input of one layer depends on the output of another, either directly or indirectly, then the parameters of these layers must be initialized independently from each other. This rules out recurrent neural networks, for example, since parameters are shared across time-steps. Finally, except where stated otherwise, we will assume the use of a “Delta initializa- tion” (Balduzzi et al., 2017; Xiao et al., 2018), which requires that filter bank tensors are initialized to zero everywhere except for their central location/offset (and have odd-sized filter dimensions to make this possible). As an example, if we have a 5 × 5 filter, then only the weights corresponding to entry (3, 3) would be non-zero. Note that for fully-connected layers there is only one location, so that a Delta initialization becomes equivalent to a standard one. The non-zero weights of a Delta-initialized filter bank form a m × k matrix, where k is the input channel dimension and m is the output channel dimension. To initialize this matrix we have two options. First, we can use an entry-wise iid Gaussian distribution with mean 0 and variance 1/k, which gives rise to the Gaussian Delta initialization. While it might seem restrictive to assume a variance of 1/k (instead of σ2/k for general σ > 0), this will simplify our presentation going forward, and other choices can be simulated by rescaling the network’s activation functions (which will be part of DKS). The second option is to use a scaled-corrected uniform orthogonal (SUO) dis- tribution , which is a special distribution of rescaled orthogonal matrices. When m (cid:54) k, samples from this distribution can be generated as (XX(cid:62))−1/2X, where X is a m×k matrix 10
Deep Kernel Shaping with entries sampled iid from N (0, 1). When m > k, we may apply the same procedure but with k and m reversed, and then transpose the result. The resulting distribution is given by the well-known Haar measure on orthogonal matrices (e.g. Meckes, 2019), and is also sometimes called the uniform distribution. To be consistent with the scaling characteristics (cid:16)(cid:112) (cid:17) of the Gaussian initialization, we further multiply by the scaling factor max m/k, 1 , which will have an effect only when m > k. We will call Delta initializations that use the SUO distribution Orthogonal Delta initializations. 4.2 A brief discussion about random orthogonal matrices and the SUO distribution The scaled-corrected uniform orthogonal distribution, as we have defined it, has the property that it is invariant to pre or post-multiplication of the matrix by a constant square orthonor- mal matrix (Eaton, 1989, Chapter 7). This implies that left-multiplying an input vector by an unobserved matrix sampled from this distribution erases all information about the vector’s direction. The input vector’s dimension-normalized squared norm (i.e. 1 (cid:107)x(cid:107)2) dim(x) can meanwhile be exactly recovered when k ≤ m, and is equal to the output vector’s dimension-normalized squared norm. For the computations in the next section to be valid for a given orthogonal weight distribution, we require that the distribution satisfies these properties. However, many ran- domized procedures used in practice for sampling orthogonal matrices lack the directional invariance property. And even procedures whose distributions do possess it often don’t (cid:16)(cid:112) (cid:17) include the max m/k, 1 scale correction factor, which is required for the dimension- normalized squared norm to be preserved. Thus, we strongly recommend that anyone im- plementing DKS use the sampling procedure for orthogonal matrices that we have outlined, unless they are confident that their own procedure gives precisely the same distribution. Note that Saxe et al. (2014) and Xiao et al. (2018) have used distributions over orthogonal matrices to initialize neural networks. It turns out that the formulas they derive also require SUO-distributed weights to be correct, even though they did not state this explicitly. Finally, note that the entry-wise iid N (0, 1/k) distribution for m × k matrices behaves very similarly to the SUO distribution with respect to multiplication by an input vector, and gives a distribution on the output vector which is identical up to a multiplication by a random scalar (which is distributed according to the chi distribution with m degrees of freedom). The output vector’s dimension-normalized squared norm is thus a random multiplicative perturbation of the input vector’s (instead of being equal to it), where the perturbation’s mean and variance are 1 and 2/m respectively. From these observations we can see that Gaussian initializations, like SUO ones, give rise to directional invariance, but only approximately preserve the dimension-normalized squared norm (and in a way that gets more precise as m grows). 5. Kernel function approximations for neural networks The starting point for our analysis of the initialization-time behavior of neural networks will be kernel functions, and the approximations of these that hold at initialization-time when 11
Martens et al. the channel dimensions are large. This type of analysis was originally pioneered by Neal (1996), and developed further in various subsequent works (e.g. Williams, 1997; Rahimi and Recht, 2008; Cho and Saul, 2009; Mairal et al., 2014; Anselmi et al., 2015; Hazan and Jaakkola, 2015; Daniely et al., 2016; Matthews et al., 2018; Lee et al., 2018; Garriga-Alonso et al., 2018; Novak et al., 2018; Arora et al., 2019). In this section we will review these concepts and establish our notation and terminology for the key quantities. We will depart from the index-heavy tensor notation of some previous works (such as Novak et al., 2018) in favor of a more compact one based on matrices. 5.1 Simplified version for the fully-connected case Before we launch into our full treatment of kernel function approximations for convolutional neural networks, in this subsection we will quickly give a simplified version for the fully- connected case, with the goal of building intuition. Note that the notation defined here is only a special case of the more general notation we will develop in subsequent subsections. For a vector-valued function f : Rk → Rm, we define its kernel function κ by f 1 κ (z, z(cid:48)) = f (z)(cid:62)f (z(cid:48)). f m It turns out (e.g. Daniely et al., 2016) that when f is a sufficiently wide fully-connected combined layer with iid N (0, 1/k) weights and activation function φ, κ (z, z(cid:48)) is closely f approximated with high probability by κ (Σ ), where (cid:102)f z,z(cid:48) κ (cid:102)f (Σ z,z(cid:48)) = E (cid:20) u u1 2 (cid:21) ∼ N (0, Σ z,z(cid:48))[φ(u 1)φ(u 2)], (1) where 1 (cid:20) (cid:107)z(cid:107)2 z(cid:62)z(cid:48) (cid:21) Σ = ∈ R2×2. z,z(cid:48) k z(cid:62)z(cid:48) (cid:107)z(cid:48)(cid:107)2 This can be derived by observing that any two units in f ’s nonlinear layer are Gaussian distributed (when conditioned on z and z(cid:48)) with mean zero and covariance matrix Σ . z,z(cid:48) And so if we consider enough of these units, their average statistics (given κ (z, z(cid:48))) converge f in probability to the expectation. Using the notable fact that κ (z, z(cid:48)) only depends on Σ (and not the full details of (cid:102)f z,z(cid:48) z and z(cid:48)), we can then compose these layer-wise kernel approximations to form ones for networks consisting of many such layers. 5.2 Notation for feature maps and subnetworks Throughout this work we will represent feature maps as matrices in R#channels × #locations, where #channels is the number of channels in the feature map and #locations is the number of locations. Note that for fully-connected layers these matrices are just column vectors. We will represent subnetworks (of which single layers are a special case) by symbols such as “f ” or “g”. Implicit in these representations is a dependence on all the structural details of the subnetwork, including its parameters, its activation functions, and anything 12
Deep Kernel Shaping else we need in order to construct our various approximations. At the same time, we will use standard functional notation such as f (Z) when we want to treat f as a function from its input to its output. 5.3 Inner product matrices (IPMs) and Pair-location kernel functions (PKFs) Suppose X, Y ∈ Rk×(cid:96) are feature maps with channel dimension k and number of locations (cid:96). We will define the inner product matrix (or IPM) of X and Y , denoted as Σ , by X,Y 1 (cid:20) X(cid:62)X X(cid:62)Y (cid:21) Σ ≡ ∈ R2(cid:96)×2(cid:96). X,Y k Y (cid:62)X Y (cid:62)Y The entries of an IPM are the (dimension-normalized) inner products between all pairs of column vectors from X and Y , or in other words, the average (across channels) of the entry-wise products between pairs of location vectors from the feature maps X and Y . Now suppose f is a subnetwork whose output feature map is in Rm×(cid:96). We define the paired-location kernel function (or PKF) of f , denoted by κ , as f 1 (cid:20) f (Z)(cid:62)f (Z) f (Z)(cid:62)f (Z(cid:48)) (cid:21) κ (Z, Z(cid:48)) ≡ Σ = . f f(Z),f(Z(cid:48)) m f (Z(cid:48))(cid:62)f (Z) f (Z(cid:48))(cid:62)f (Z(cid:48)) If f is a fully-connected combined layer, then κ (Z, Z(cid:48)) is just a 2x2 matrix, while for general f convolutional combined layers it has a 2 × 2 block structure, with blocks of size (cid:96) × (cid:96). Note that PKFs are analogous to Novak et al.’s (2018) “activation covariance matrices”. f ’s PKF κ gives us a “geometric view” of f ’s input-output behavior. In particular, f because κ determines the inner-products between all pairs of output vectors (across the f different locations and both inputs), it determines the distances between all such vectors (cid:112) via the formula (cid:107)x − y(cid:48)(cid:107) = x(cid:62)x + y(cid:62)y(cid:48) − 2x(cid:62)y(cid:48). 5.4 Initialization-time approximations to the PKF for combined layers In this subsection we will assume that f is a combined layer with element-wise activation function φ. We will also assume Gaussian-distributed weights, as part of either a Delta or non-Delta initialization scheme. (An extension to SUO-distributed weights given in Subsection 5.9). We are interested in extracting a simple mathematical approximation of κ that is valid f at initialization time, which we can use in order to construct approximations of the PKF of larger subnetworks. To begin with, we will assume that the convolutional part of f uses padding and has a stride of 1, which means that input and output locations will be in one to one correspondence. (This assumption will be relaxed in the next subsection.) In general, computing κ for combined layers f boils down to direct evaluation of the f defining formula, with no simplifications possible. But when f ’s initial parameters are distributed as per Section 4, there exists a much simpler function κ that approximates (cid:102)f κ at initialization time with high probability, which we call the approximate paired- f location kernel function (or APKF) of f . κ is obtained from κ by taking the limit (cid:102)f f as the output channel dimension go to infinity, and is a good approximation when the actual (finite) output channel dimension is sufficiently large. 13
Martens et al. As shown by Garriga-Alonso et al. (2018) and Novak et al. (2018), the APKF for con- volutional combined layers initialized with a standard Gaussian fan-in initialization 1 (LeCun et al., 1998b) is given by κ (Σ ) = E [φ(u)φ(u)(cid:62)], (2) (cid:102)f Z,Z(cid:48) u∼N (0, A(Σ Z,Z(cid:48))) where A is the operator which maps Σ to Σ , with P (Z) denoting the matrix Z,Z(cid:48) P (Z),P (Z(cid:48)) of patch vectors2 generated from Z. A key property of κ is that it only depends on Z and (cid:102)f Z(cid:48) via the associated IPM Σ . Z,Z(cid:48) As discussed in Section 4, we are assuming the use of a Delta initialization scheme in this work. Intuitively, a Delta initialization makes a convolutional layer behave like a set of fully-connected layers that operate independently over locations in the feature map (and share parameters). This results in a simplified form for κ which is a directly analogous to (cid:102)f the kernel approximation for fully-connected combined layers (i.e. Equation 1). It is given by3 κ (Σ ) = E [φ(u)φ(u)(cid:62)]. (cid:102)f Z,Z(cid:48) u∼N (0,Σ Z,Z(cid:48)) A minor technical point is that Σ may be singular, in which case N (0, Σ ) will Z,Z(cid:48) Z,Z(cid:48) be “degenerate”, and its density function technically undefined. The easiest way this can happen is if Z = Z(cid:48). However, one can still meaningfully define a distribution and sample from it using (Σ )1/2v for v ∼ N (0, I), which is equivalent to adding (cid:15)I to Σ and Z,Z(cid:48) Z,Z(cid:48) then letting (cid:15) → 0. With this extended definition of N (0, Σ ) our formulas remain valid. Z,Z(cid:48) 5.5 Padding, strides, and dropped locations If the stride of f ’s convolution is not 1, or if it doesn’t use padding and has a filter size larger than 1 × 1, then the locations in the input and output feature maps won’t be in one to one correspondence. Instead, they will be related to each other via a projection function s(u), which maps input locations (given by the entries of u) to their corresponding output locations (given by the entries of s(u)). This results in the following generalized formula for κ : f κ (Σ ) = E [φ(s(u))φ(s(u))(cid:62)]. (3) (cid:102)f Z,Z(cid:48) u∼N (0,Σ Z,Z(cid:48)) When the input and output locations are in one to one correspondence, s is just the identity function. Otherwise, s essentially “drops” the input locations that are never visited by the center of the filter (i.e. s(u) will be independent of the entries of u that are “dropped”). We will refer to these as dropped locations, and most of our discussions going forward will assume that the location under consideration has not been dropped at the layer in question. When a location is dropped at some layer, both the exact and approximate PKFs of that layer (and all subsequent layers) will be effectively zero for that location. 1. This initialization uses an entry-wise iid Gaussian distribution with mean 0 and variance 1/d, where d is the filter size times the input channel dimension. 2. A “patch vector” is one formed by concatenating together the subset of columns of Z corresponding to a particular location visited by the convolutional filter. They have dimension kb2 for b × b convolutions. 3. This formula can be obtained from Equation 2 by observing that a Delta-initialized filter bank behaves like a 1x1 filter, and that A is the identity operator in the case of a 1x1 filter (since P (Z) = Z). 14
Deep Kernel Shaping 5.6 Deriving APKFs given Gaussian distributed weights At a high level, the APKF formulas given above for a combined layer f can be derived by observing that each pair of outputs from the affine part of f are linear combinations of Gaussian random variables (i.e. the filter weights) when conditioned on the two inputs Z and Z(cid:48), and are thus are jointly Gaussian distributed with mean zero. A straightforward computation then shows that the covariance matrix C of this distribution is Σ ⊗ I Z,Z(cid:48) m×m or A(Σ ) ⊗ I , where ⊗ denotes the Kronecker product. Because units in different Z,Z(cid:48) m×m channels have zero covariance they are independent, and so κ (Z, Z(cid:48)) is equal to an aver- f age over output channels of iid random variables, and thus converges in probability to its expectation as the number of output channels goes to infinity. We set κ (Σ ) equal to (cid:102)f Z,Z(cid:48) this expectation, whose formula then follows from the one for C. Probabilistic bounds on the approximation error can then be obtained using concentration inequalities. 5.7 The APKF Condition and network-level PKF approximations The main approximation which we will use going forward is that the PKF of each combined layer is equal to its associated APKF at initialization time. Or in other words, that Σ = κ (Z, Z(cid:48)) ≈ κ (Σ ) f(Z),f(Z(cid:48)) f (cid:102)f Z,Z(cid:48) for each combined layer f of the network. We will refer to this as the APKF Condition. Observe that a combined layer’s APKF depends on Z and Z(cid:48) only through the associated IPM Σ . Thus, under the APKF Condition, we can compose APKFs for each combined Z,Z(cid:48) layer to form an initialization-time approximations of the PKFs for arbitrary subnetworks, which we will call network-level PKF approximations. Extending our notation from the combined layer case, we will denote these approximations by κ for arbitrary subnetworks (cid:102)f f . (Note that we rely on the property that subnetworks have a single input and output feature maps for this definition and notation to make sense.) An additional complication that we must deal with when constructing network-level PKF approximations is the presence of concatenation operations, where Z is the concate- nation of two feature maps X and Y along their channel dimensions. In this cases, we observe that k Σ + k Σ 1 X,X(cid:48) 2 Y,Y (cid:48) Σ = , (4) Z,Z(cid:48) k + k 1 1 where k and k are the number of channels in X and Y respectively. 1 2 As we will see in the following sections, network-level PKF approximations are amenable to detailed analysis, and expose several key properties which end up being crucial determi- nants of network trainability (and which can be controlled through careful interventions). 5.8 How accurate are these approximations? As discussed above, the APKF for a combined layer f is derived by observing that the entries of κ (Z, Z(cid:48)) are empirical averages of iid variables that converge in probability to f their expectations as the output channel dimension goes to infinity. Applying concentration inequalities then leads to statements of the form: “for any (cid:15) > 0 and δ > 0 there exists 15
Martens et al. an integer m ((cid:15), δ) such that if the output channel dimension satisfies m (cid:62) m ((cid:15), δ) then 0 0 (cid:107)κ (Z, Z(cid:48))−κ (Σ )(cid:107) < (cid:15) with probability 1−δ.”. The precise dependency of m ((cid:15), δ) and f (cid:102)f Z,Z(cid:48) 0 on (cid:15) and δ is of practical interest, as the output channel dimension of real neural network layers is finite, and may not even be particularly large in some cases. Ultimately, we are interested in bounding the kernel approximation error not just for single combined layers but for entire networks. In general, such bounds will be worse than anything provable for single layers, as approximation error will compound with depth (since the output of one approximation is fed as input into the next). The only work we are aware of that gives such bounds is that of Daniely et al. (2016). In that work, the authors analyze what are essentially networks of fully-connected combined layers arranged in arbitrary topologies, with certain technical conditions imposed on their input data and activation functions. Translating their main result into the language and assumptions of this work yields the following theorem: Theorem 1 (Adapted from Theorem 2 of Daniely et al. (2016)) Suppose that f is a network containing only fully-connected combined layers and concatenation operations, the former of which are initialized independently of each other with a standard Gaussian fan-in initialization, and use the same activation function φ. Suppose further that φ is twice continuously differentiable and satisfies E [φ(x)2] = 1 and (cid:107)φ(cid:107) , (cid:107)φ(cid:48)(cid:107) , (cid:107)φ(cid:48)(cid:48)(cid:107) (cid:54) C x∼N (0,1) ∞ ∞ ∞ for some C (with (cid:107) · (cid:107) denoting the supremal value), and that each layer has output ∞ dimension (aka “width”) greater than or equal to (4C4)D log(8L/δ) , (cid:15)2 where D is maximum number of nonlinear layers in any input-output path through the network (i.e. its “depth”), L is its number of combined layers, and δ, (cid:15) > 0. Then at initialization time, for all input vectors z and z(cid:48) to f satisfying (cid:107)z(cid:107)2 = (cid:107)z(cid:48)(cid:107)2 = dim(z), we have that |[κ (z, z(cid:48))] − [κ (Σ )] | (cid:54) (cid:15) f 1,2 (cid:102)f z,z(cid:48) 1,2 with probability at least 1 − δ. Remark 2 Note that in our notation, both κ (z, z(cid:48)) and κ (Σ ) are 2 × 2 matrices, and f (cid:102)f z,z(cid:48) [·] extracts the (1, 2)-th entry, or in other words, the value of 1 f (z)(cid:62)f (z(cid:48)) and its 1,2 dim(f(z)) approximation. One can estimate the error for diagonal entries simply by setting z = z(cid:48). (cid:113) (cid:113) Remark 3 Because 1 = E [φ(u)2] (cid:54) E [(cid:107)φ(cid:107)2 ] = (cid:107)φ(cid:107) , it thus follows u∼N (0,1) u∼N (0,1) ∞ ∞ that C (cid:62) 1 in the above theorem. And while the theorem assumes the use of the Gaus- sian fan-in initialization, we note that for fully-connected networks this is equivalent to the Gaussian Delta initialization. Remark 4 This theorem statement differs from the one in Daniely et al. (2016) by explic- itly assuming that the activation function φ satisfies E [φ(x)2] = 1, or is in other x∼N (0,1) words “normalized”. As far as we can tell, this assumption is implicit in the definitions made by Daniely et al. (2016). 16
Deep Kernel Shaping Remark 5 The condition that E [φ(x)2] = 1 can be achieved by normalizing the x∼N (0,1) output of the activation functions by an appropriate constant. And the condition that (cid:107)z(cid:107)2 = (cid:107)z(cid:48)(cid:107)2 = dim(z) can be achieved through data pre-processing (as discussed in Section 10.2). Both of these conditions will be enforced as part of DKS (although motivated differently). The bound in Theorem 1 predicts an exponential dependence of the minimum required width and depth D, and a 1/(cid:15)2 dependence on the error tolerance (cid:15). The exponential dependence on D means that this bound could never realistically be applied to a moderately deep network running on actual hardware, as the required width would be prohibitive. While it could easily be the case that some choices of φ give an exponential dependence as the bound predicts, we conjecture that with more carefully designed assumptions on the properties of φ, a bound with better dependence could be proven. Indeed, Daniely et al. (2016) themselves give a more specialized bound for networks with rescaled RELU activations (which technically violate the hypotheses of Theorem 1 since they are unbounded and not differentiable everywhere), where the required width is only quadratic in D. The main limitation of Theorem 1 is that it applies only to networks of fully-connected combined layers that don’t share weights. We conjecture that a similar result may also hold for networks with convolutional layers and a restricted type of inter-layer weight sharing. 5.9 The orthogonal initialization case (assuming SUO-distributed weights) The kernel formulas and theory given so far in this section have all assumed the use of Gaussian Delta initializations. However, our assumptions also permit the use of Orthogo- nal Delta initializations, which as discussed in Section 4, use the SUO distribution instead of an iid Gaussian one to initialize the non-zero weights of the filter. While some previous works (e.g. Xiao et al., 2018) have used these kinds of kernel approximation formulas in the orthogonal case, and have appealed to the vague notion that random orthogonal matrices “look like” Gaussian-distributed ones in high dimensions, there hasn’t been any mathemat- ically rigorous justification of this practice until the recent work of Martens (2021). The following theorem, which is adapted from Martens (2021), establishes convergence in probability of the APKF to the associated PKF for a fully-connected combined layer with SUO-distributed weight matrix. Like Theorem 1, it provides an explicit and fairly reasonable convergence rate. An extension of this result to multi-layer networks would likely proceed along similar lines to the argument given in Daniely et al. (2016) for the Gaussian case. Theorem 6 (Adapted from Theorem 2 of Martens (2021)) Let f be a fully-connected combined layer with an SUO distributed m × k weight matrix W , a bias vector equal to 0, and an activation function φ satisfying (cid:107)φ(cid:107) , (cid:107)φ(cid:48)(cid:107) (cid:54) C for some C (with (cid:107) · (cid:107) denoting ∞ ∞ ∞ the supremal value). Denote n = max(k, m), and suppose that for δ, (cid:15) (cid:62) 0 we have √ m5/2 n − 1 8 2C2 (cid:62) log(2/δ) and (cid:62) . (n + 1)2 m3/4 (cid:15) Then, at initialization time, for all pairs of vectors z, z(cid:48) ∈ Rk satisfying (cid:107)z(cid:107)2 = (cid:107)z(cid:48)(cid:107)2 = k, we have that |[κ (z, z(cid:48))] − [κ (Σ )] | (cid:54) (cid:15) f 1,2 (cid:102)f z,z(cid:48) 1,2 17
Martens et al. with probability at least 1 − δ. Remark 7 The conditions on k, m, and n ≡ max(k, m) in the theorem statement will be satisfied as long as n is sufficiently large and k is not too much larger than m. In the case where m (cid:62) k, the LHS’s of these bounds simplifies to approximately m1/2 and m1/4, respectively. It thus follows that the APKF converges in probability to the PKF as the output dimension m goes to ∞. Remark 8 In the case where m (cid:62) k, the conditions imply that 128C4 log(2/δ)2 m (cid:38) , (cid:15)2 which is similar to the width bound from Theorem 1 for D = 1. Remark 9 Note that while this theorem is stated only for fully-connected combined layers, it also applies to convolutional combined layers that use Orthogonal Delta initializations by taking z and z(cid:48) to be any pair of vectors from the union of the columns Z and Z(cid:48). 6. Q and C maps for combined layers Q maps and C maps are mathematical constructs introduced by Saxe et al. (2014) and Poole et al. (2016) that describe the initialization time behavior of deep fully-connected networks. While original derived within the semi-rigorous “signal propagation” framework (which is discussed in Section 25.6), they can also be applied under certain conditions within the more rigorous context of kernel function approximations. In that context, they provide a compact alternative representation of approximate kernel functions that is easier to work with. As will be discussed later in Part II, the Q/C maps of a network tell us a lot about its trainability. Indeed, they have appeared either implicitly or explicitly, often in simplified forms, in much of the previous work on network design and initialization (as will be made clear in Sections 25 and 26). They are also central to the derivation of DKS, and over the next few sections we will develop the generalized version of them that we will use in this work. In this section we will formally introduce Q/C maps maps and their associated notation, and give formulas to compute them for combined layers under our stated hypotheses. Note that while the connection between Q/C maps and approximate kernel functions has been previously observed (e.g. Lee et al., 2018), it hasn’t before been carefully worked out, nor has it been generalized to convolutional layers (as we will do here). In the section that follows we will show how Q/C maps can be naturally extended beyond single combined layers to describe the behavior of network-level PKF approximations for arbitrary subnetworks with complex topologies. 6.1 Q maps for combined layers Consider a combined layer f with φ as its element-wise activation function, and Z and Z(cid:48) as its two inputs. By Equation 3 and basic properties of Gaussian expectations, any given 18
Deep Kernel Shaping diagonal entry q of κ (Σ ) depends only on the corresponding diagonal entry q of out (cid:102)f Z,Z(cid:48) in Σ , and can be computed as Z,Z(cid:48) (cid:104) √ (cid:105) q = Q (q ) = E [φ(u)2] = E φ ( q x)2 , (5) out f in u∼N (0,qin) x∼N (0,1) in where Q is defined as the Q map of f . We will call such diagonal entries q values, and note f that they are equal to the dimension-normalized squared norms of their associated location vectors under the APKF Condition. Notably, the form of the Q map is the same for each location, and so we may associate them with combined layers in a location-independent way. 6.2 C maps for combined layers An off-diagonal entry m of κ (Σ ) has a slightly more complex dependence on Σ out (cid:102)f Z,Z(cid:48) Z,Z(cid:48) in Equation 3, as it depends on both the corresponding entry m of Σ , as well as the in Z,Z(cid:48) two associated diagonal entries (q and q ) that share a row or column. It is given by 1 2 m out = E (cid:20) u1 (cid:21) ∼ N (cid:18) 0, (cid:20) q1 min (cid:21)(cid:19)[φ(u 1)φ(u 2)]. (6) u2 min q2 We call such off-diagonal entries m values, and note that they are equal to the dimension- normalized inner product of their two associated location vectors under the APKF Condi- tion. Following Poole et al. (2016), we focus on “length-normalized” versions of the m values called c values. A c value can be obtained from an m value by dividing it by the square √ root of the product of its two associated q values. (e.g. c = m / q q in the context in in 1 2 of Equation 6.) Under the APKF Condition, c values are equal to the cosine similarity between their two associated location vectors. c values are computed using C maps, which for a combined layer f are given by c = C (c , q , q ) out f in 1 2 1 ≡ (cid:112) Q f (q 1)Q f (q 2) E (cid:20) u u1 2 (cid:21) ∼ N (cid:18) 0, (cid:20) mq1 in m q2in (cid:21)(cid:19)[φ(u 1)φ(u 2)] 1 √ √ = (cid:112) Q f (q 1)Q f (q 2) E (cid:20) v v1 2 (cid:21) ∼ N (cid:18) 0, (cid:20) c1 in c 1in (cid:21)(cid:19) [φ ( q 1v 1) φ ( q 2v 2)] 1 (cid:20) √ (cid:18) √ (cid:18) (cid:113) (cid:19)(cid:19)(cid:21) = E φ ( q x) φ q c x + 1 − c2 y , (7) (cid:112) Q (q )Q (q ) x,y∼N (0,1) 1 2 in in f 1 f 2 √ √ (cid:16) (cid:113) (cid:17) where we have used the fact that q x and q c x + 1 − c2 y are mean-zero Gaussian 1 2 in in (cid:20) √ (cid:21) (cid:20) (cid:21) q q q c q m distributed with covariance matrix √ 1 1 2 in = 1 in . Like the Q q q c q m q 1 2 in 2 in 2 map, the C map is the same for each location, and so we may associate a single C map to each combined layer. We note that C , when considered as a function of c, maps from [−1, 1] to [−1, 1]. f This is immediate from the interpretation of c values as cosine similarities if we assume 19
Martens et al. the APKF Condition, but is true more generally. Intuitively, it must be the case, since the APKF Condition becomes exact in the limit as the channel dimension grows, and thus c values are precisely equal to cosine similarities in infinite-dimensional spaces. To be more rigorous, one may apply H¨older’s inequality within the Hilbert space of functions defined by the inner product (cid:104)g, h(cid:105) = E [g(x, y)h(x, y)], taking g(x, y) = φ (cid:0)√ q x(cid:1) and x,y∼N (0,1) 1 (cid:16)√ (cid:16) (cid:113) (cid:17)(cid:17) h(x, y) = φ q c x + 1 − c2 y . 2 in in 6.3 Q/C maps for more general combined layers? Note that the existence of Q/C maps, as we have defined them, depends on our stated hy- potheses for combined layers. In particular, that they are convolutional (or fully-connected), and use a Delta initialization scheme. While APKFs do exist for certain other layer types and initialization schemes, they may not always give rise to low dimensional maps that fully describe their behavior. For example, if we use a conventional fan-in initialization instead of a Delta initialization for the filter weights, then the resulting APKF (given in Equation 2) implies a more complex dependence of the entries of the output IPM on the input IPM, where output q values will depend on (many) input c values. 7. Extended Q and C maps In Poole et al. (2016) and Schoenholz et al. (2017), the neural networks analyzed were as- sumed to be sequences of D fully-connected combined layers, each with the same activation function. Thus, the network’s initialization-time behavior could be approximated using a single per-layer Q/C map composed with itself D times, and a dynamical systems analysis of this map could thus be performed. This analysis looked for the map’s stable points and attractors, and characterized its asymptotic behavior as the number of self-compositions D (i.e. the network’s depth) went to infinity. In this work we consider architectures with a more general structure, and with layers that can be convolutional and employ a variety of activation functions. We are also interested in the given architecture’s finite structure, instead of its depth-limiting behavior, as this will allow us to more carefully tailor our manipulations to the given network. To facilitate this, in this section we will extend the notion of Q maps and C maps to arbitrary subnetworks (consisting of potentially many layers) in the natural way. Going forward, we will refer to Q maps and C maps defined specifically for combined layers as local Q/C maps, and maps defined specifically for larger subnetworks, via the extension procedure defined in the next subsection, as extended Q/C maps. Unqualified, “Q/C maps” will be a general term referring to both. 7.1 Definition of extended Q/C maps The definition for extended Q/C maps is the natural generalization of the definition for local Q/C maps, where we replace APKF approximations for combined layers with network-level PKF approximations for subnetworks. In particular, given a subnetwork f , an extended Q map maps input q values, corresponding to the diagonal entries of the input IPM Σ , Z,Z(cid:48) to the associated output q values, corresponding to the diagonal entries of the associated 20
Deep Kernel Shaping output IPM κ (Σ ), as computed by the network-level PKF approximation κ . The (cid:102)f Z,Z(cid:48) (cid:102)f definition for extended C maps is similar. That these definitions can be made in a location-independent way (as with the definitions of local Q/C maps), follows from the fact that extended Q/C maps can be constructed from local ones via composition and weighted averaging (as will be detailed below), which are both operations that preserve the location-independence property. 7.2 Computing extended maps Because Q maps compose with each other, and C maps compose with the combination of both, we can take the per-combined-layer maps and compose them in a way that mirrors the composition of the subnetwork’s combined layers, analogously to how we assembled network-level PKF approximations from APKF approximations of each combined layer. For example, if we have two consecutive combined layers f and g, and wish to compute the Q and C map for the subnetwork h consisting of their composition, this is simply Q (q) = Q (Q (q)) and C (c, q , q ) = C (C (c, q , q ), Q (q ), Q (q )). h g f h 1 2 g f 1 2 f 1 f 2 The only complication is that we need to describe how q and c values can be computed when feature maps are concatenated along their channel dimensions, or when they are multiplied by a non-zero scalar constant. To handle the former situation, we recall from Equation 4 that concatenation leads to a weighted averaging of the feature maps’ associated IPMs, with weights given by their respective number of channels. Thus, the q values, which are the diagonal entries of these matrices, average in the same way under concatenation. So given the channel dimensions k and k , and the q values q and q , we have that the 1 2 1 2 associated q value of the concatenation is simply k q + k q 1 1 2 2 . (8) k + k 1 2 c values are slightly more complicated to deal with, but still relatively straightforward. We note that m values, the unnormalized counterparts of c values, are the off-diagonal entries of the IPMs, and thus exhibit the same kind of averaging as q values. We can thus obtain the c values by first converting them to m values, performing the required weighted average, and then converting back to c values. This gives us the analogous formula √ √ k q q c + k q q c 1 1,1 1,2 1 2 2,1 2,2 2 √ √ , (9) k q q + k q q 1 1,1 1,2 2 2,1 2,2 where q refers to the j-th q value associated with the c value from the i-th feature map i,j being concatenated. (Recall that each c value is associated to two q values.) Note that the property that local C maps send [−1, 1] to [−1, 1] carries over to extended C maps, as this clearly preserved under composition and weighted averages. To handle multiplication of a feature map by a constant α (cid:54)= 0, we note that the IPM of αZ and αZ(cid:48) is equal to α2 times the IPM of Z and Z(cid:48), or in other words: Σ = α2Σ . αZ,αZ(cid:48) Z,Z(cid:48) We thus have that an output q value (or m value) for such an operation is simply α2 times the corresponding input q value (or m value). And an output c value is just equal to the corresponding input c value, since the constant α2 will cancel out when we divide by the geometric mean of the q values. 21
Martens et al. 7.3 Generalization to subnetworks with isolated affine and nonlinear layers Because it will simplify Q/C map computations for certain architectures (such as residual networks), we will also generalize Q/C maps to subnetworks that may contain affine or nonlinear layers in isolation (i.e. separated from their parent combined layer). To do this, we will define local Q/C maps for isolated affine and nonlinear layers in a way that is consistent with our previous definitions (with one small proviso), and then use the previous composition argument to extend Q/C maps to larger subnetworks containing such layers. The isolated affine layer case is trivial, as an affine layer is equivalent to a combined layer with an identity activation function, and so is covered under the previous discussion. It follows that affine layers have local Q and C maps that are the identity function (which can easily be verified by setting φ(u) = u in Equations 5 and 7), and can thus be essentially ignored in the extended map computations. The case of nonlinear layers is more subtle. APKFs, from which local Q and C maps are defined, don’t actually exist for nonlinear layers in isolation. In particular, for arbitrary input vectors it is not the case that one can closely approximate the norm of the output vector given only the norm of the input vector (with high probability). However, when the layer is part of a larger network in which its input vector is always the output of some affine layer (with a suitable parameter distribution), such a prediction can be made, and is given by the APKF for the corresponding combined layer. (To see this, note affine layers have identity Q and C maps and thus the input to the nonlinear layer has the same q and c values as the input to the corresponding combined layer.) Thus, we can define the local Q and C map for an isolated nonlinear layer to be equal to the local Q and C maps for its associated combined layer, with the proviso that it describes the layer’s kernel behavior only for “typical” input vectors (i.e. those that are produced with high probability by the previous layers’ computation) and not arbitrary input vectors. Note that these definitions are consistent with our definitions for combined layers, as the composition of the local Q/C map for an affine and nonlinear layer (as we have defined them here) does indeed recover the local Q/C map of the associated combined layer. Also, it should be emphasized that these arguments rely crucially on the fact that nonlinear layers may be “isolated” only from the point of the view of a given subnetwork. From the perspective of the entire network, it is still required that they are always part of a combined layer, or in other words, are always directly preceded by an affine layer. 8. Q and C map derivative computations Central to our analysis of Q and C maps are their derivatives, which encode many of the properties that we will care about. In this section we show how to compute them, first for local maps, and then for extended maps of arbitrary subnetworks. 8.1 Local map case A conceivable approach to computing the derivatives of local maps would be to derive a closed form expression for the required integrals, and then apply standard differentiation 22
Deep Kernel Shaping techniques. Unfortunately, closed form expressions for these integrals are not generally available for most the activation functions. Instead, following Poole et al. (2016), we will give integral expressions for the derivatives which are similar to the original maps themselves, and which can be efficiently approximated using numerical integration (as discussed in Section 22.2). 8.1.1 Local Q map derivative for combined layers (or isolated nonlinear layers) Let f be a combined layer (or an isolated nonlinear layer) with element-wise activation function φ. The derivative for Q (q) with respect to q, which we denote by Q(cid:48) (q), can be computed f f straightforwardly from Equation 5, and is equal to Q(cid:48) (q) = √1 E (cid:2) φ (√ qx) φ(cid:48) (√ qx) x(cid:3) , f q x∼N (0,1) where φ(cid:48) is the derivative of φ. Note that because φ is continuous, we are still able to compute this expectation, and similar ones to follow, when φ(cid:48) is undefined on a finite set of inputs (which is permitted under our global assumptions). 8.1.2 Local C map derivatives The derivative of local C maps with respect to their c value argument has an especially nice form which we make use of later in Section 11. We begin by defining the following notation: (cid:104) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17)(cid:105) Γ (c, q , q ) ≡ E φ ( q x) φ q cx + 1 − c2y . (10) φ 1 2 x,y∼N (0,1) 1 2 This function is closely related to the local C map of f (given by Equation 7) in the sense that C (c, q , q ) = √ 1 Γ (c, q , q ). The derivative of Γ (c, q , q ) with respect to f 1 2 φ 1 2 φ 1 2 Q f (q1)Q f (q2) c, which we denote as Γ(cid:48) (c, q , q ), is given by φ 1 2 √ Γ(cid:48) (c, q , q ) = q q Γ (c, q , q ), φ 1 2 1 2 φ(cid:48) 1 2 This elegant formula was stated in Poole et al. (2016), although no explicit derivation of it was given. For completeness we provide one in Appendix B. An immediate consequence of this result is that the i-th derivative of Γ (c, q , q ) with φ 1 2 (i) respect to c, which we denote by Γ (c, q , q ), is equal to φ 1 2 Γ(i) (c, q , q ) = (q q )i/2Γ (c, q , q ), φ 1 2 1 2 φ(i) 1 2 where φ(i) denotes the i-th derivative of φ. From this it follows that the i-th derivative of C (c, q , q ) w.r.t. c can be written as f 1 2 (q q )i/2 (i) 1 2 C (c, q , q ) = Γ (c, q , q ). (11) f 1 2 (cid:112) Q (q )Q (q ) φ(i) 1 2 f 1 f 2 23
Martens et al. This formula is valid even when i = 0, where the 0-th derivative is defined as the function itself (i.e. φ(0) = φ), as is standard convention. When φ(i)(u) isn’t defined on a measure zero set of points, the formula may still be valid, provided that φ(i−1) is continuous. For example, if φ is the RELU function, φ(u) is continuous everywhere and has a deriva- tive everywhere except at u = 0, so the formula is valid for i = 1. However, φ(1)(u) is not (2) continuous at u = 0, and one can use Equation 15 to show that C (c, 1, 1) → ∞ as c → 1, f while the formula would wrongly predict a value of 0. 8.2 Derivatives of extended maps Because extended maps can be expressed as compositions and weighted averages of local maps, their derivative computations can be performed straightforwardly using automatic differentiation. The resulting formulae will still depend on the derivatives of local maps, but these can be computed (or numerically approximated) as per the previous subsection. In such a scheme, composition corresponds to multiplication, and weighted averages correspond to weighted averages (since differentiation is linear). Notably, because q values don’t depend on c values, the derivative of extended C maps with respect to their input c values can be computed as if all the q values in the network are constant (although they still depend on the network’s input in general). So for example, the C map derivative for a composition of many combined layers is just the product of the local C map derivatives for each layer, evaluated at the appropriate values of c as per the forward evaluation. 9. Handling weighted sum operations An operation commonly performed in neural network models is the (weighted) sum of two or more feature maps. For example, in the ResNet-V2 architecture (which is described in detail in Section 23.1), the input to a “residual block” is added to its output, using what is known as a “residual connection”. Since sum operations are not among those listed as allowed in Section 3.2, it would seem that our assumptions rule out such architectures. However, for the purposes of our analysis, there is no requirement that a network be formally constructed the same way it would implemented in code or drawn in a diagram; it only matters that it can be constructed in a way that conforms to the assumptions outlined in Section 3.2. With this in mind, we will now describe a way that a certain restricted class of weighted sum operations can be simulated using only directly supported operations. The consequence of this is that our analysis will in fact apply to architectures that contain such sum operations. Typically, the feature maps that are summed in neural networks are the outputs of a set affine layers f , f , . . . , f that don’t share parameters. (This is true in ResNet-V2 1 2 n architectures, for example.) In such situations, we can replace the sum (cid:80)n f (Z ) with i=1 i i (cid:16) (cid:17) a single affine layer h (cid:2) Z(cid:62) Z(cid:62) · · · Z(cid:62) (cid:3)(cid:62) , which is obtained by concatenating the 1 2 n filter banks, the bias vectors, and the input feature maps (i.e. the Z ’s) together along i their respective channel dimensions. (If (cid:80)n f (Z ) is followed by a nonlinear layer in the i=1 i i network, then one simply forms a new combined layer consisting of this and h.) 24
Deep Kernel Shaping While almost good enough, the issue with this construction is that the implied initial distribution of h’s filter bank parameters is not one of the ones described in Section 4, and in particular, the variance/scale is not correct. To account for this, we must renormalize by the new number of channels (after the concatenation), the effect of which is that h will instead compute a weighted sum of the form √ (cid:80)n k f (Z ) i=1 i i i , (cid:112)(cid:80)n k i=1 i where k is the input channel dimension for f . i i Fortunately, we can extend this construction to support a more general class of weighted √ sums (with weights w ) by multiplying each Z by a scalar α = w (cid:112)(cid:80)n k / k before i i i i i=1 i i concatenating them. Doing so gives √ √ (cid:80)n i=1 k if i(α iZ i) = (cid:80)n i=1 k iα if i(Z i) = (cid:88)n w f (Z ), (cid:112)(cid:80)n k (cid:112)(cid:80)n k i i i i=1 i i=1 i i=1 where we have used the fact that the affine f ’s are in fact linear (given that the biases are i initialized to 0). If the layer f is still in the network after this replacement is performed for some i i (e.g. because its output is used in more than one place), this creates parameter sharing between h and f . However, as long as the network with sum operations that we are trying i to simulate doesn’t violate our parameter independence assumptions from Section 4, neither will our simulating network. The existence of this construction thus implies that weighted sums between the outputs of two or more affine layers (and directly followed by an optional nonlinear layer) are supported within our framework, provided that said affine layers don’t share parameters. Note that the weighed sum operation can be performed directly in the model code, and the concatenation-based construction only needs to be referenced in the theoretical analysis. To deal with sum operations in Q map computations, we observe that the q value of (cid:2) α Z(cid:62) α Z(cid:62) · · · α Z(cid:62) (cid:3)(cid:62) is, according to Equation 8, equal to 1 1 2 2 n n (cid:80)n i=1 k iα i2q i = (cid:80)n i=1 k iw i2 ((cid:80)n i=1 k i) /k iq i = (cid:88)n w2q , (12) (cid:80)n k (cid:80)n k i i i=1 i i=1 i i=1 where q is the q value associated with Z , and we have used the fact that the q value for i i α Z is α2q . From this it follows that the output q value from the sum is also (cid:80)n w2q , i i i i i=1 i i since Q is just the identity function. h Given uniform q values, a similar derivation based on Equation 9 lets us compute the corresponding c value as (cid:80)n w2q c i=1 i i i , (13) (cid:80)n w2q i=1 i i where c is the c value associated with Z . Note that unlike the formula for the q value, i i this is always a weighted average of the c ’s, regardless of the values of the w ’s. And in the i i case where all input q values are equal, it simplifies to (cid:0)(cid:80)n w2c (cid:1) / (cid:80)n w2. i=1 i i i=1 i 25
Martens et al. 10. Uniform q values In general, C maps are three dimensional functions that depend on an input c value and two associated q values. While simpler objects than a network’s PKF (or even the network-level PKF approximation), they are not yet simple enough for our purposes. In particular, the behavior of C maps depends strongly on the two input q values, and q values can vary significantly between different network inputs and/or feature map locations. Finding a single scheme that controls the behavior of the C map for all conceivable input q pairs is likely impossible in general, and so we look to restrict the possible q values through some sort of active intervention. The one we propose in this section is a form of input data preprocessing, which ensures that all q values for a given layer are equal (across all possible locations in the feature map and inputs to the network). (Note that this condition does not require that q values be the same across different layers.) We will call this condition uniform q values. 10.1 A previous solution to this problem In Poole et al. (2016) it was observed that local Q maps can have stable fixed points, and that if the network consists of a composition of many combined layers of the same type, then its q values will converge with depth to such a point. Thus, a reasonable approximation, especially for deeper layers, is to assume that this convergence has already taken place, and that the q values over the entire network are equal. (Note that this is strictly stronger condition that uniform q values.) As discussed in Section 7, our setting is different from Poole et al.’s (2016) in that we consider more general architectures, and are interested in the precise behavior of a finite network architecture instead of its depth limiting behavior. Moreover, it may be a poor approximation to assume that q values are close to convergence in the earlier layers of the network, especially if there are no constraints placed on the initial q values (which are determined by the network’s input). 10.2 Uniform q values via Per-Location Normalization Our solution to the problem of unpredictable q values is a type of input data pre-processing which we call Per-Location Normalization (PLN ). This is related to the data nor- malization done in Daniely et al. (2016) for fully-connected networks, but generalized to convolutional networks. PLN ensures that each location vector in the network’s input fea- ture map has a dimension-normalized squared norm of 1, or in other words, that the q values for the network’s input layer are all 1. Because subsequent q values are fully determined by previous q values via location-agnostic computations (i.e. Q maps), it thus follows by induction that each layer will have uniform q values under PLN. PLN can be easily realized through a number of different possible transformations of the network’s input, although care must be taken not to destroy information. The naive approach of normalizing the vector at each location of the input feature map (and multiply- ing by the square root of the channel dimension) destroys information because the vector goes from having k degrees of freedom to k − 1 degrees of freedom (where k is the number 26
Deep Kernel Shaping of channels). This can be seen most starkly when k = 1, in which case all location-wise “vectors” are reduced to ±1 scalar values. The naive approach can however be repaired, by first adding an extra channel to the network’s input. In our experiments we used the value (cid:0) 1 E [(cid:107)x (cid:107)2](cid:1) 1 2 for this extra channel, k j j where the expectation is an average over location vectors x for the given input feature map j X. This results in a vector of the form (cid:34) (cid:35) (k + 1) 21 x i (cid:0) (cid:107)x i(cid:107)2 + k1 E j[(cid:107)x j(cid:107)2](cid:1) 1 2 (cid:0) k1 E j[(cid:107)x j(cid:107)2](cid:1) 1 2 for each location i. Note that this approach to PLN still destroys some information, although it’s only one degree of freedom across X, which includes all locations and channels. This can be seen most clearly in the case of only one location vector x, in which case the formula becomes √ 1 (cid:20) (cid:21) (cid:20) (cid:21) (k + 1) 2 x √ = kx/(cid:107)x(cid:107) , (cid:0) (cid:107)x(cid:107)2 + 1 (cid:107)x(cid:107)2(cid:1) 1 2 (cid:107)x(cid:107)/ k 1 k from which we cannot recover the norm of x. Thus, it only makes sense to use this form of PLN when there are a large number of locations and/or channels. For cases where there is only one location (i.e. in a fully-connected network) and the channel dimension is small, one possible alternative is to use a data-independent constant value for the extra channel. Using a value of 1 gives (k + 1) 21 (cid:20) x (cid:21) √ (cid:20) x/(cid:112) (cid:107)x(cid:107)2 + 1 (cid:21) = k + 1 (cid:112) , ((cid:107)x(cid:107)2 + 1) 21 1 1/ (cid:107)x(cid:107)2 + 1 which doesn’t destroy any information about x (since we can invert the last entry to get (cid:112) ((cid:107)x(cid:107)2 + 1)/(k + 1), and then multiply that by the other entries to recover x). The dis- advantage of this approach is that the scale of x could differ very significantly from 1, so that after normalization, the value of the extra channel could either dominate the overall vector, or be minuscule. Performing PLN may not always be important in practice, as we demonstrate later in our ablation experiments. Moreover, the version we have proposed seems to slightly degrade optimization performance in our benchmarks, possibly because of the extra parameters it adds to the first layer (for the extra channel dimension), or because of the nonlinear warping it applies to the input space. On the other hand, our experiments also demonstrate that very badly scaled input data can sometimes cause DKS to perform poorly, unless PLN is applied as a corrective measure. (See Appendix N.6 for the relevant results.) There are other ways we can produce normalized vectors without destroying information, such as those discussed in Daniely et al. (2016) for fully-connected networks. Of all of the aspects of DKS, our method of realizing PLN is the least explored, and we wouldn’t be surprised if there was a significantly better way of doing it. 27
Martens et al. 10.3 Assuming uniform q values going forward From this point forward we will assume that the uniform q value condition holds. This thus allows us to treat C maps as one dimensional functions, as the q value for each layer will be constant. As we will show in the next section, it also imbues C maps with a set of very useful properties which end up being crucial to our subsequent analysis of them in Section 13. 11. Additional consequences of uniform q values for C maps 11.1 Local C maps are positive definite functions and map c values of 1 to 1 For convenience, when we have uniform q values we will drop the formal dependence of the C map on its input q values, and instead treat these as known constants within the expression (which are both equal to the same value). This allows us to view C maps as essentially one dimensional functions, and we can use notation of the form “C (c)” for them f going forward. Under the assumption of uniform q values, several interesting and useful properties of local C maps emerge. Suppose f is a combined layer (or an isolated nonlinear layer) with activation function φ. We begin by setting q = q = q in Equation 11, which gives 1 2 qi (cid:104) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17)(cid:105) C(i) (c) = E φ(i) ( qx) φ(i) q cx + 1 − c2y . f Q (q) x,y∼N (0,1) f From this expression we can deduce that 1 (cid:104) √ (cid:105) Q (q) C (1) = E φ ( qx)2 = f = 1 f Q (q) x∼N (0,1) Q (q) f f and qi (cid:104) √ √ (cid:105) C(i) (0) = E φ(i) ( qx) φ(i) ( qy) f Q (q) x,y∼N (0,1) f qi (cid:104) √ (cid:105)2 = E φ(i) ( qx) ≥ 0. (14) Q (q) x∼N (0,1) f The second consequence implies two interesting properties. First, that for local maps, C (0) = 0 if and only if E (cid:2) φ (cid:0)√ qx(cid:1)(cid:3) = 0, where we note the interpretation of Ef (cid:2) φ (cid:0)√ qx(cid:1)(cid:3) from Apx∼ pN en(0 d, i1 x) A. And second, that the Taylor series expansion of x∼N (0,1) C (c) about c = 0, which is given by f ∞ (cid:88) 1 C(i) (0)ci, i! f i=0 will have all non-negative coefficients. Provided that the Taylor series converges and is equal to C (c) (which it will under mild technical conditions), it thus follows that C (c) is f f 28
Deep Kernel Shaping a positive definite function (Schoenberg, 1988; Daniely et al., 2016), which is defined as a function from [−1, 1] to R that can be written as ∞ (cid:88) b ci, i i=0 for non-negative coefficient b . i 11.2 Properties of positive definite functions Positive definite functions have many interesting and useful properties which thus carry over to C maps. These include: 1. The set of positive definite functions is closed under differentiation4. 2. Positive definite functions are non-negative, non-decreasing, and convex on the non- negative part of their domain. (This follows from the fact that their derivatives are also positive definite functions, and thus non-negative for non-negative inputs.) 3. The set of positive definite functions is closed under composition and weighted averages with non-negative weights5. 11.3 Extended C maps are positive definite functions and map c values of 1 to 1 Given that local C maps are positive definite functions and map c values of 1 to 1, it’s easy to show that the same applies to extended C maps. First, the property that c values of 1 map to 1 is clearly preserved under composition and weighted averaging, and thus carries over to extended maps since they are constructed from local maps this way. Second, the property of C maps being positive definite functions also carries over, since positive definite functions are closed under composition and non-negative weighted averages as mentioned above. Another way that one can show that extended C maps are positive definite is by ob- serving that they describe the exact one-dimensional kernel function [κ (z, z(cid:48))] of a fully- f 1,2 connected network f in the limit of infinite width (where we substitute convolutional layers in the original network with fully-connected layers). Because this kernel depends only on the inner product of its inputs via the function C , it is thus invariant to orthogonal trans- f formations of its input, and so by Schoenberg’s Theorem (Schoenberg, 1988) it is a positive definite function of this inner product (i.e. C is positive definite). Note that this argument f works even for non-smooth activation functions for which Equation 11 may not apply. See Daniely et al. (2016) for more details. 4. Closedness under differentiation can be easily verified by observing that the derivative of (cid:80)∞ b ci with respect to c is just (cid:80)∞ ib ci−1, which is also positive definite since ib (cid:62) 0 when b (cid:62) 0. i=0 i i=1 i i i 5. Closedness under composition can be easily verified by substituting one series into the other, expanding, and observing that the coefficients of the resulting series are non-negative combinations of coefficients from the two original series. Similarly, closedness under weighting averaging follows by observing that the coefficients of the series for the weighted average are just weighted averages of the corresponding coefficients from the original two series. 29
Martens et al. 11.4 A complementary perspective based on “dual activations functions” Assuming input q values of 1, local C maps are essentially equivalent to the “dual activation functions” defined in Daniely et al. (2016), with the only difference being that dual activation functions aren’t normalized by the output q values (as C maps are). Using the notation of Equation 10, the dual activation function φ˜ of φ can be written as φ˜(c) = Γ (c, 1, 1). φ Assuming that we normalize each activation function so that its output q value is 1, these dual activation functions can be composed and averaged in order to form what are called “compositional kernels”, which are approximations of the kernel function for the entire network, and are analogous to our network-level PKF approximation in the case where there is only one location (i.e. the fully-connected case). Given this connection, it may thus be an appealing prospect for us to adopt Daniely et al.’s (2016) framework instead of the one we’ve presented, as it’s very carefully laid out and rigorously developed, and comes packaged with the best known error bounds for initialization-time kernel approximations of neural networks (one of which we adapt in Section 5.8). However, while their framework can deal with local receptive fields, it cannot directly deal with the weight sharing used in convolutional layers, and it would likely require significant work to extend it in that direction. Indeed, to deal with convolutional layers in a way that allows kernel approximations for individual layers to be naturally composed, one seemingly must define something like our APKFs which keep track of approximations to entire IPMs (which contain inner products between every pair of locations in the feature maps of both inputs). Moreover, without assuming a Delta initialization, a decomposition of the kernel approximations into 1 di- mensional functions (such as Q/C maps) becomes impossible, since APKFs for standard initializations involve non-trivial interactions between all the locations in the feature map (as seen in Equation 2). While we do indeed restrict our attention to Delta initializations in this work, without the PKF/APKF formalism we would not be able to extend our analysis to mean pooling layers, since the kernel approximation for such layers also involves interactions between locations. As we will see later in Section 23, this extension will be necessary later in order to understand how DKS can be applied to standard random forest architectures. 30
Deep Kernel Shaping Part II Desirable Q/C map behavior and how to achieve it 12. C map behavior in deep networks and necessary requirements for trainability C maps approximate a network’s PKF at initialization time. In this view, c values ap- proximate the cosine similarity between pairs of vectors (corresponding to different loca- tions/inputs), and their evolution via C maps thus describes how these cosine similarities evolve in the network. Given uniform q values, the norms of these vectors are approxi- mately constant (for a given layer), and thus their relative distance is related to their cosine similarity c via (cid:107)x − y(cid:107) (cid:107)x − y(cid:107) (cid:112) = = 2(1 − c). 1 ((cid:107)x(cid:107) + (cid:107)y(cid:107)) (cid:112) (cid:107)x(cid:107)(cid:107)y(cid:107) 2 A (sub)network’s C map thus provides a complete description of how it warps the geometry of its input space at initialization time. As we will argue in this section, the preservation of some amount of this geometric information through the network, as indicated by a “well-behaved” C map, is a necessary condition for the network to be trainable. When C maps “degenerate” in certain ways, as we will show they do for standard deep neural networks, it means that the relative distances between the network’s (location-wise) input vectors are hard to infer from the network’s outputs, making gradient-based training difficult. 12.1 RELU networks The local C map of a combined layer f with a RELU activation function is given by √ 1 − c2 + (π − cos−1(c))c C (c) = . (15) f π This formula is stated in Daniely et al. (2016), and is based on a derivation by Cho and Saul (2009), where it corresponds to a normalized version of the “1st-order arc-cosine kernel function”. Note that while in general C maps depend on the input q value, this formula is valid for any q value, which is a consequence of the fact that RELUs are positively homogeneous (i.e. RELU(λu) = RELU φ(u) for all λ (cid:62) 0). One interesting fact about C is that C(cid:48) (1) = 1, which can be verified by taking the f f derivative of Equation 15 and letting c → 1. Moreover, because C (1) = 1 (which is true f for general C maps), we have that a deep RELU network g consisting of the composition of D combined layers will also have the property that C(cid:48) (1) = 1D = 1. g The following is a plot of C : f 31
Martens et al. From this we can see that the entire domain [−1, 1] of input c values is compressed to the range [0, 1], which makes intuitive sense since the RELU function is non-negative. Other than that, C resembles a slightly shifted and rescaled identity function, and so is f reasonably well-behaved. However, if we build a deep network as the composition of many RELU combined layers, compression of the C map’s output becomes much more extreme, with outputs rapidly concentrating around 1 as depth increases. This can be seen below in the plot of the C map for RELU networks of depths 5, 20 and 100 (which we obtain by iterating Equation 15 the required number of times): Here, the C map for depth 100 has the property that maps the entire interval [−1, 1] to [0.996, 1], which represents an extreme amount of compression. 32
Deep Kernel Shaping 12.2 Sigmoidal networks (using the erf activation) Another example of an activation function whose associated local Q and C maps have closed- form expressions is the classical “error function”, which is given by erf(u) = √2 (cid:82) u exp(−t2) d t. π 0 This function has a “sigmoidal shape”, which makes it a reasonable stand-in for the more common sigmoidal activation functions like tanh and the logistic sigmoid. The local Q map for a combined layer f with an erf activation function is given by (cid:18) (cid:19) 2 2q Q (q) = sin−1 , f π 1 + 2q and the local C map is given by (cid:18) (cid:19) 1 2 2cq C (c) = sin−1 . f Q (q) π 1 + 2q f These formulas follow from equation 11 of Williams (1997). Unlike in the RELU case, the local C map depends on the input q value, and so to plot it we must make an assumption about this value. One natural choice is q = 1, which gives the following plot: Visually, this function is almost indistinguishable from the identity function. To compute the C maps for deeper RELU networks we need to track both the q and c values through each layer, using the previously stated equations. Doing so for depths 50, 150, and 500 gives the following plot: 33
Martens et al. From this plot we can see that at high depths, the network’s C map has a tendency to compress nearly all input c values to a small region around 0. Moreover, this behavior only becomes more extreme as the depth increases. 12.3 C map degeneration in more general deep nonlinear networks The following proposition establishes that the C map degeneration we observed above for deep RELU and tanh networks happens for a larger class of deep networks. Moreover, the point c∗ towards which (nearly) all c values get mapped as the depth increases is unique. Proposition 10 Suppose f is a deep network consisting of a composition of D subnetworks, each with the same C map C. Then for all c ∈ (−1, 1) we have lim C (c) = c∗, f D→∞ for some c∗ ∈ [0, 1]. The proof of this proposition is a straightforward generalization6 of the proof of “Claim 1” from Daniely et al. (2016). While Proposition 10 describes the convergence of C (c) in the limit of infinite depth, f it is still informative about C (c) at finite depths (which is the setting we actually care f about). In particular, it essentially says that for any (cid:15) there is a constant D so that for (cid:15) when D (cid:62) D , nearly all input c values get compressed to a region of radius (cid:15) around c∗. (cid:15) We will call C maps exhibiting this compressive behavior (with a small (cid:15)) degenerate. 6. While the statement of their claim assumes that each of the D subnetworks is a combined layer, the only fact they use about C in their proof is that it is positive definite, which holds for more general subnetworks by Section 11.3. 34
Deep Kernel Shaping Remark 11 Note that Proposition 10 assumes that C is the same for all values of D. If, for example, we were to modify the network’s activation functions based on the value of D (which DKS will do), the convergence seen in the proposition may not occur. Remark 12 Also note that the hypothesis in Proposition 10 that each subnetwork has the same C map C will rule out many common cases. For example, it is violated for the deep erf network we looked at before, because the q values are different for each layer (which leads to different local C maps). However, because the q values converge rapidly to a fixed point in such networks, convergence of the c values will still occur. As shown in Appendix J, the “residual blocks” of ResNets (which are repeated many times in sequence) also violate this hypothesis, but their q values do not converge to a fixed point. In Appendix C (Theorems 40, 41, and 42) we give a much more detailed analysis of the convergence of C (c) in terms of the properties of C and the location of c in [−1, 1]. When f C(cid:48)(1) (cid:54)= 1 we prove exponential convergence (as a function of the depth D) with precise rates, thus establishing that degeneration can happen very quickly in deep networks. In contrast to the related analyses of Poole et al. (2016), our results apply pre-asymptotically. 12.4 Types of degeneration and their implications for trainability As we’ve seen above, degenerate C maps send a large range of input c values to a small (and sometimes point-like) region near some fixed point c∗. This means that the original geometric relationships between the corresponding input vectors are obscured by the action of the network, becoming essentially impossible to recover from its outputs. While it seems intuitively plausible that this would make gradient-based optimization of such networks difficult (as has been argued by Schoenholz et al. (2017)), it’s worth examining the situation in more detail. In this subsection we will give a detailed intuitive argument. A more rigorous argument which confirms these intuitions will appear later in Section 24. Suppose f is some subnetwork of the overall network that we wish to train. There are two basic cases, corresponding to different possible values for c∗. 12.4.1 c∗ = 1: the collapsing case If C sends nearly all input c values to a small neighborhood near c∗ = 1, this means that f regardless of the original distance of the two associated input vectors, their corresponding (cid:112) output vectors under f will be nearly identical (i.e. have a relative distance 2(1 − C (c)) ≈ f (cid:112) 2(1 − c∗) = 0). And because this holds for all pairs of vectors, it means that f is nearly constant, with only a very weak dependence on its input. This has a different set of consequences for layers in the network before f versus layers after. For layers before f there are two cases. If f ’s Jacobian is non-negligible for most inputs, then it will have to vary wildly over f ’s input space, as this is the only that a function can achieve a nearly constant output while having a non-negligible Jacobian. (Balduzzi et al. (2017) observed a similar phenomenon for early layers in deep RELU networks, likening the gradient function to a “random noise process”.) This will make learning difficult, or at the very least unlikely to generalize, as similar pairs of training cases will produce 35
Martens et al. very different gradients. If on the other hand f ’s Jacobian is negligible, this means that gradient magnitudes for layers before f will be very small compared to those for other layers. This makes simultaneous optimization of the network’s layers with gradient descent very difficult, and even sophisticated 2nd-order methods may struggle in the more extreme cases. (This is arguably related to the well-known “vanishing gradients” phenomenon identified in Hochreiter et al. (2001).) Meanwhile, layers after f won’t be able to learn anything more than a constant output prediction, as their inputs will be nearly constant. And even if the output produced by f has enough variance across the training data to overcome the limits of numerical precision, the part of the network after f would need to have a very large Lipschitz constant in order to produce well-separated outputs for different training cases. 12.4.2 0 (cid:54) c∗ < 1: the exploding case If C sends nearly all input c values to a small neighborhood around c∗ with 0 (cid:54) c∗ < 1, f then any two input vectors (that aren’t either almost identical or negations of each other) will be mapped by f to output vectors that are nearly a constant relative distance d = (cid:112) 2(1 − c∗) > 0 apart. Given this condition, and that f is differentiable, it follows that f ’s Jacobian must be very large in certain regions of the input space (and possibly everywhere). This means that the gradient magnitudes for layers in the network before f will be much larger than those for other layers, making it difficult to optimize them simultaneously. (This is arguably related to the well-known “exploding gradient” phenomenon discussed in Hochreiter et al. (2001).) A network containing such a subnetwork f possibly stands a greater chance of being trainable than in the previous “collapsing case”, since f ’s output vectors will still be dis- tinguishable for different inputs vectors. Optimization of the layers after f (or towards the end of f ) could even conceivably learn a map from these vectors to their associated targets in the training set. However, it is highly unlikely that the resulting model would generalize well, as the similarity between two such output vectors would have no discernible relationship to the similarity of the associated two input vectors. 12.5 Are well-behaved C maps sufficient? While a well-behaved (i.e. non-degenerate) C map seems like a necessary condition for trainability (as argued above), it should be noted that without additional hypotheses, no condition on the C map can be a sufficient. For example, because the C map is invariant to the network’s parameterization7, or even whether its parameters are considered trainable at all, it cannot completely predict the performance of a gradient-based optimizer. Even if we assume the standard network parameterization, and a model class which is equivalent to a standard deep nonlinear network, interesting counterexamples to sufficiency still exist, as we will show in Section 14.2. 7. This can be seen by observing that APKFs, from which local Q/C maps and ultimately extended Q/C maps are defined, only depend on the functional behavior of combined layers, and not which variables are formally considered “parameters” from the standpoint of the optimizer. Indeed, one could reparameterize a layer’s weights using any invertible function without changing what it computes at initialization time, or the form of its PKF/APKF. 36
Deep Kernel Shaping One way to incorporate hypotheses about the network’s parameterization, and the opti- mizer used, is to analyze gradient descent training from the perspective of Neural Tangent Kernel (NTK) theory (Jacot et al., 2018). Later in Section 24 we will argue that for a deep fully-connected network, a degenerate C map leads to a form for the NTK which implies very poor generalization and/or slow optimization under gradient descent. Conversely, we will also show that a network constructed using DKS has an NTK which is suggestive of good generalization and fast optimization (although doesn’t necessarily guarantee it). This NTK-based analysis can be viewed as a rigorous version of the intuitive argument given in the previous subsection. 13. Mathematical analysis of C maps As we saw in Section 12, C maps can become degenerate in deep networks by mapping nearly all input c values to a point-like region around some c∗ ∈ [0, 1], which leads to difficulty when training with standard optimization methods. In this section we will analyze C maps in closer detail, and show how their overall deviation from the identity function (which serves as a measure of degeneration) can be predicted from their slopes at c = 0 and/or c = 1. (We will ultimately design DKS to control these slopes in order to prevent degeneration.) We will also establish connections between the properties of a local C map and its associated activation function, and characterize the slope behavior of degenerate C maps over [−1, 1]. 13.1 Measures of deviation from the identity Suppose f is some subnetwork. The question of how to measure the deviation of C from f the identity function, which we will use as a measure of “degeneracy”, is an interesting one. Since we ultimately want to forbid extreme behavior of C for all input c values, it f makes sense to look at the worst-case ones. This suggests the following two options, which compare C to the identity function using either its values or its derivatives: f 1. max |c − C (c)| and c∈[−1,1] f 2. max |1 − C(cid:48) (c)|. c∈[−1,1] f The first of these, while a reasonable choice, could fail to detect small ranges of c values where geometric information is lost due to the slope of C getting close to zero. The second f option will detect such regions, but unlike the first measure, is insensitive to C being shifted f by an additive constant, and is only weakly sensitive to it being “angled” away from the identity function. Fortunately, we can avoid having to choose between the two, since as we will see next, they can be simultaneously bounded using a few easily-computed properties of C . f 13.2 Bounding deviation from the identity The following result relates the deviation of C from the identity function to the values of f C (0), C(cid:48) (0), and C(cid:48) (1). Its proof, which makes strong use of the fact that C is a positive f f f f definite function, is given in Appendix D.1. 37
Martens et al. Theorem 13 For any subnetwork f we have 1 (1 − C(cid:48) (0)) (cid:54) max |C (c) − c| (cid:54) 2(1 − C(cid:48) (0)) 4 f c∈[−1,1] f f and max |C(cid:48) (c) − 1| (cid:54) 2(1 − C(cid:48) (0)) + (C(cid:48) (1) − 1). f f f c∈[−1,1] If C (0) = 0, then we additionally have that f max |C (c) − c| (cid:54) 2(C(cid:48) (1) − 1) f f c∈[−1,1] and max |C(cid:48) (c) − 1| (cid:54) 3(C(cid:48) (1) − 1). f f c∈[−1,1] From this result we see that the first measure of deviation is within a factor 4 of 1−C(cid:48) (0) f (and also upper bounded by 2[C(cid:48) (1)−1] when C (0) = 0), and the second measure is within f f a factor 3 of C(cid:48) (1) − 1 when C (0) = 0. This suggests that we can control the deviation f f of C from the identity function by simply controlling the distance of C(cid:48) (0) and/or C(cid:48) (1) f f f from 1. Remark 14 Note that a simple consequence of this result is that C(cid:48) (1) (cid:62) 1 for subnetworks f f satisfying C (0) = 0. It is also true more generally that C(cid:48) (0) (cid:54) 1 (as shown in Appendix f f D.1). Remark 15 The lower bounds in Theorem 13 do not imply that C will look globally nonlin- f ear when C(cid:48) (1) is large (in contrast to the theorem’s upper bounds which do imply it will look f √ √ globally linear when C(cid:48) (1) is small). For example, the function (cid:0) 1 − 1/ j(cid:1) c + (cid:0) 1/ j(cid:1) cj is f √ a valid C map and has a derivative (cid:62) j at c = 1, and yet is very close to linear everywhere except near c = 1 and c = −1 when j is large. 13.3 The relationship between identity C maps and linear activations The local C map for a nonlinear layer with a linear activation function is the identity (which can be easily verified from Equation 7 by taking φ(u) = λu with λ (cid:54)= 0). However, from this observation it’s not immediately obvious that a local C map will converge to the identity function as its associated activation function becomes “more linear”, or vice versa. In this subsection we will show that this is indeed the case for certain carefully chosen measures on both function spaces, and we will give the rate of this convergence. We will assume that the activation functions live in a Hilbert H space with inner product given by (cid:104)φ, ψ(cid:105) = E [φ(x)ψ(x)], whose associated norm is (cid:107)φ(cid:107) = (cid:112) (cid:104)φ, φ(cid:105). (By H x∼N (0,1) H definition, elements of this Hilbert space are those functions φ for which (cid:107)φ(cid:107) exists.). H The standard measure on H, defined by (cid:113) (cid:107)φ − ψ(cid:107) = E [(φ(x) − ψ(x))2], H x∼N (0,1) 38
Deep Kernel Shaping is arguably the most natural one to use, as it employs a weighting over x that precisely reflects the input distribution we would expect given an input q value of 1. This measure is also closely related to Q/C maps in the sense that (cid:107)φ(cid:107)2 = Γ (1, 1, 1) = Q (1) (with Γ H φ f φ defined as in Equation 10), where f is a nonlinear/combined layer with φ as its activation function. φ is linear insofar as it’s close to a multiple of the identity function h . With this in 1 mind, and given the fact that (cid:107)h (cid:107) = 1, we will measure the level of nonlinearity of φ 1 H according to (cid:107)φ − (cid:104)φ, h (cid:105) h (cid:107) 1 H 1 H nl(φ) ≡ , (cid:107)φ(cid:107) H where we normalize by (cid:107)φ(cid:107) to keep nl(φ) invariant to changes in the overall scale of φ. H Note that with this definition, φ is perfectly linear (i.e. a multiple of h ) if and only if 1 nl(φ) = 0. It turns out that we can relate nl(φ) to properties of C , as is established in the following f proposition whose proof is given in Appendix D.2: Proposition 16 Suppose f is a nonlinear/combined layer with activation function φ and input q value 1. Then we have nl(φ)2 = 1 − C(cid:48) (0). f Moreover, 1 nl(φ)2 (cid:54) max |C (c) − c| (cid:54) 2 nl(φ)2. 4 c∈[−1,1] f This result shows a strong relationship between the level of linearity of φ, and the distance between C and the identity function (as measured by the infinity norm). Moreover, f one converges to 0 (as we vary φ) if and only if the other one does. Remark 17 Proposition 16 can be straightforwardly extended to the case of general input q values by modifying the definition of the inner product used for H. 13.4 The relationship between affine C maps and affine activations An affine function is, by definition, a linear function plus a constant term. Equivalently, it is a function whose derivative is constant (i.e. is a multiple of the constant function h (x) = 1). 0 From this second characterization, we can measure the “non-affineness” of φ by (cid:107)φ(cid:48) − (cid:104)φ(cid:48), h (cid:105) h (cid:107) 0 H 0 H na(φ) ≡ . (cid:107)φ(cid:48)(cid:107) H Note that with this definition, φ is perfectly affine if and only if nl(φ) = 0. It turns out that we can relate na(φ) to properties of C , as is established in the following f proposition (whose proof is given in Appendix D.3). Proposition 18 Suppose f is a nonlinear/combined layer with φ as its activation function. Then C(cid:48) (0) na(φ)2 = 1 − f . C(cid:48) (1) f 39
Martens et al. From the above expression we can see that φ becomes more affine as the ratio C(cid:48) (0)/C(cid:48) (1) f f approaches 1. Moreover, φ approaches an affine function if and only if C itself does, as f C(cid:48) (0)/C(cid:48) (1) is a measure of how affine C is. (To see this, note that C(cid:48) (0) (cid:54) C(cid:48) (c) (cid:54) C(cid:48) (1) f f f f f f for all c ∈ [0, 1] since C is convex on [0, 1] by Section 11.2, and thus C(cid:48) approaches a con- f f stant function on [0, 1] as C(cid:48) (0)/C(cid:48) (1) → 1, which extends to all of [−1, 1] since C is f f f analytic.) 13.5 Slope properties of degenerate C maps In subsection 13.2 we saw how certain conditions on the slope of a C map at c = 0 and/or c = 1 ensure that it is well behaved (i.e. not degenerate). In this subsection we will establish the converse: that degenerate C maps necessarily have extreme values for these slopes. As shown in Section 12, C maps in deep networks can become degenerate in the sense that they map almost their entire input domain (except points very close to ±1) to a small region around some limiting c value c∗. One way to quantify this behavior is to look at how “flat” the function is up to some c value c s.t. |c| < 1, which we can measure using C (|c|) − C (0) F (c) ≡ f f (cid:62) 0 f |c| for c (cid:54)= 0. When C is degenerate, or in other words very flat, F (c) will be very small (for f f values of c not too close to ±1). While the interpretation of F (c) is clear for c > 0, it is less clear for c < 0. In the f following proposition (proved in Appendix D.4) we show that F (c) does indeed work as a f measure of flatness for values of c less than 0. Proposition 19 Suppose f is a subnetwork, and c ∈ [−1, 1] with c (cid:54)= 0. Then (cid:12) (cid:12) (cid:12) (cid:12) C f (c) − C f (0) (cid:12) (cid:12) (cid:54) F f (c). (cid:12) c (cid:12) Note that because C is convex and non-decreasing on [0, 1] (by Section 11.2) and f C (1) = 1, we have that F (c(cid:48)) (cid:54) F (c) (cid:54) 1 for any valid c, c(cid:48) s.t. |c(cid:48)| (cid:54) |c|. Thus, F (c) f f f f being small implies flatness over the entire domain [−c, c], and not just at c. A more “analytic” way to measure flatness is to look |C(cid:48) (c)|, which intuitively should f be small in flat regions of C . It turns out that this intuition is basically correct, as we f establish in the following proposition (whose proof is given in Appendix D.5). Proposition 20 Suppose f is a subnetwork, and c ∈ (−1, 1) with c (cid:54)= 0. Then F (c) log F (c) |C(cid:48) (c)| (cid:54) f f (1 + |c|). f |c| log |c| This bound establishes that |C(cid:48) (c)| will be small whenever F (c) is (which is to say, when f f C is degenerate), provided that |c| is not too close to 1. f 40
Deep Kernel Shaping Remark 21 While Proposition 20 doesn’t address the value of |C(cid:48) (0)| directly, we can still f bound it by applying Proposition 20 with c = (cid:15) for some 0 < ε < 1 and then use the fact that 0 (cid:54) C(cid:48) (0) (cid:54) C(cid:48) (ε) = |C(cid:48) (ε)| for any 0 < ε < 1 (which is true because C is non-decreasing f f f f and convex on [0, 1] by Section 11.2). In general, we cannot say that much about C(cid:48) (1) or C(cid:48) (−1) when C is degenerate. f f f However, the following two propositions (proved in Appendices D.6 and D.7) give us some basic information about these values in certain special cases. Proposition 22 Suppose f is a composition of D subnetworks each having the C map C, and that c∗ = 1. Then we have that C(cid:48) (1) = C(cid:48)(1)D with 0 (cid:54) C(cid:48)(1) (cid:54) 1, and either f C(cid:48) (−1) = −C(cid:48) (1) or lim C(cid:48) (−1) = 0. f f D→∞ f Remark 23 Note that the claim made in Proposition 22 does not hold for more general types of networks. For example, if we have a sequence of networks (f )∞ such that n n=1 C (c) = 1 − 1/n + cn2/n, then for all c ∈ [−1, 1] we have C (c) → 1, F (c) → 0 fn fn fn and C(cid:48) (1) = n → ∞ as n → ∞. fn Remark 24 For RELU combined layers we have C(cid:48)(1) = 1 (which follows from Equation 15 by taking the derivative and letting c → 1), and thus the bound C(cid:48)(1) (cid:54) 1 in Proposition 22 is tight. Proposition 25 Suppose f is a subnetwork. For all 0 < (cid:15) < 1 we have 1 − C (0) − F (1 − (cid:15)) C(cid:48) (1) (cid:62) f f . f (cid:15) By taking a small value for (cid:15), Proposition 25 tells us that for degenerate C maps with c∗ < 1 (so that C (0) ≈ c∗ < 1), C(cid:48) (1) will be large provided that the flatness measure f f F (1 − (cid:15)) is small. See Section 12.2 for an example of degenerate C map where C(cid:48) (1) is f f indeed very large. 14. C map behavior in linear networks and the problem of being “too linear” 14.1 Linear networks have identity C maps and are easy to train In Section 12 we saw that deep nonlinear networks can easily have degenerate C maps, which makes them very hard to train with gradient-based methods. One might wonder if this pathology is reserved to nonlinear networks, or if deep linear networks8 also suffer from it. Given our assumption that the initial biases are zero, it turns out that the answer is no. The local C map for a combined layer with a non-zero linear activation function is equal to the identity. Because identity functions are preserved under composition and weighted 8. Here, a linear network is defined as one whose activation functions are a constant multiple of the identity function. 41
Martens et al. averages, it thus follows that the extended C map for any subnetwork is also the identity function, and is therefore well-behaved. Does this mean that linear networks are easy to train? Well, as discussed in Section 12.5, more hypotheses are required to say anything about trainability. But if one adopts the standard parameterization, very deep linear networks are surprisingly easy to train both in theory and practice using standard techniques (Saxe et al., 2014), provided that they are initialized using orthogonal weights. Linear networks thus represent an interesting example of where our necessary condition for trainability (i.e. having a well-behaved C map) also appears to be sufficient. Despite how easy they are to train, we obviously can’t use linear networks in practice, as their expressivity is fundamentally limited. But these observations do suggest a possible strategy to address problem of degenerate C maps in nonlinear networks: we can transform the network’s activation functions so that they appear “sufficiently linear” at initialization time. However, as we will see in the next subsections, going overboard on this idea will lead to a special type of untrainability that exists only in networks with very well-behaved C maps. 14.2 The problem of being “too linear” As suggested in the previous subsection, one way to achieve a well-behaved C map would be to transform the activation functions in a network to resemble the identity function (or a multiple thereof). We can do this for the RELU activation function (defined by RELU(u) = max(0, u)) by adding a large constant a to its input and subtracting the same constant from its output. In other words, we set φ(u) = RELU(u + a) − a = max(0, u + a) − a, which is equivalent to the identity function for all inputs u (cid:62) −a. If a is extremely large, say 10100, this means that all practically sized inputs φ will satisfy this constraint, and thus the function can be treated as the identity for all practical purposes. Moreover, the expectation formulas for local Q and C maps given in Section 6 will produce practically identical results to the identity function case, since the probability mass associated with inputs u < −a to φ will be vanishingly small. (See Section 13.3 for a formal justification of this.) Thus, the C map for networks consisting of the composition of many combined layers with these transformed RELUs will be equal to the identity function up to a vanishingly small approximation error. Meanwhile, because nonlinear layers are always preceded and followed by linear layers with learnable biases, the network can in principle learn to undo these transformations and thus simulate a standard deep RELU network. The model class is thus technically no different from a standard RELU network, assuming a perfect optimizer. But despite this, these transformed networks will never actually learn nonlinear behavior via standard gradient-based methods in a reasonable amount of time, and so their hypothetical expressive power will fail to be properly utilized. Indeed, unless the optimizer manages to change the parameters by a factor on the order of 10100, the network will behave nearly identically to the corresponding linear network (which computes only affine functions of its input) 42
Deep Kernel Shaping throughout the entire course of optimization, both in terms of its function values and its gradient/curvature estimates. The basic problem here is that the transformed network has become “too linear” in the sense that we require a very large change in its parameters to see any significant nonlinear behavior. While such a network may be readily trainable within the class of linear functions (as linear networks are), it will be severely limited compared to a standard RELU network in terms of its effective expressive power under gradient descent optimization. Thus, to achieve trainable networks that are also expressive, one must avoid this failure mode in addition to requiring a well-behaved C map. 14.3 How to avoid networks that are “too linear” If a subnetwork f has an C map which is very close to the identity function, this will usually mean the local C maps of its nonlinear layers are also very close to the identity function (and perhaps much more so). By Section 13.3, this implies that the activation functions must therefore be very close to linear, so that the network is at risk of being “too linear” (as defined above). We may thus hope to prevent this by insisting that the network’s C map isn’t too close to the identity function. However, this alone won’t be good enough, as shown in the following example. Consider modifying the transformed RELU activations in the previous example by √ adding 1 to their output and dividing the result by 2, so that they compute φ(x) = √ (x + 1)/ 2 over their high-probability range of inputs (which is an affine function of x). With this change, gradient-based learning will still be effectively restricted to the class of linear networks (which compute affine functions). Meanwhile, a straightforward calculation via Equations 5 and 7 shows that Q (q) ≈ q and C (c) ≈ (c + 1)/2 for nonlinear/combined f f layers f with activation function φ, and so C (c) differs significantly from the identity. f Fortunately, by leveraging the previous analysis, there is a simple way we can use C map properties to provably avoid networks that are “too linear”. From Section 13.4, the degree of “non-affineness” of φ, denoted na(φ), is given by C(cid:48) (0) na(φ)2 = 1 − f . C(cid:48) (1) f As we have C(cid:48) (0) (cid:54) 1 by Remark 14, it thus follows that f 1 na(φ)2 (cid:62) 1 − . C(cid:48) (1) f This shows that we can avoid activation functions that are too affine (which is sufficient to avoid networks that are “too linear”) by requiring that C(cid:48) (1) be sufficiently greater than 1 f for every nonlinear layer f . 15. Mitigating kernel approximation error Our analysis of the initialization-time behavior of deep neural networks via Q/C maps relies on the assumption that the APKF approximation, when applied over multiple layers in a 43
Martens et al. nested fashion, is a reasonable one to make. If this isn’t true, then Q/C maps will cease to describe the network’s PKF at initialization time, and our attempts to make the network trainable by controlling their properties will be doomed to failure. In Section 5.8 we discussed the error bounds from Daniely et al. (2016) in order to help justify our use of nested APKF approximations in deep networks. These bounds make high-probability statements about the error of initialization-time kernel approximations of neural networks, and give a maximum value which shrinks with the square root of the width and grows exponentially with depth. While they represent the best rigorous account of neural network kernel approximations, they are still too pessimistic to be useful in practical settings, either for predicting the approximation error or controlling it. In this section we will propose a heuristic way of looking at how approximation error originates and evolves across multiple layers which we have found to be quite predictive in practice, and which implies certain error mitigation strategies that we can incorporate into DKS. 15.1 Minimizing propagation of errors by controlling Q map derivatives Suppose f is a subnetwork consisting of a composition of many combined layers. A per- turbation to the input q value to Q , representing the error from approximations made at f previous layers, will manifest as a perturbation of Q ’s output. Up to first order, the size f of the latter will be approximated by that of the former, multiplied by Q ’s derivative. f As discussed in Section 8.2, the derivative of Q is equal to the product of the derivatives f for its constituent local Q maps (i.e. those for each of f ’s combined layers), and thus will grow or shrink in an exponential fashion as a function of the depth. Thus, it can easily be the case that deep networks will have very large Q map derivatives, which suggests a very large amplification of error through successive layers network. One way we can avoid this issue is by requiring that derivative of each local Q map, when evaluated at its expected input, is less than or equal to 1. A closely related perspective, which applies to networks consisting of a sequence of combined layers each with local Q map given by Q, is that a fixed point q∗ of Q will be attractive if Q(cid:48)(q∗) < 0. Attractive fixed points have the property that QD(q) will converge to q∗ as D → ∞ for all values of q sufficiently close to q∗, and are thus naturally “robust” to reasonably sized errors in q. See Appendix K for empirical evidence of the relationship between Q map derivatives and kernel approximation error. 15.2 Minimizing errors using large width and SUO-distributed weights In addition to controlling the propagation of errors across layers, another way to mitigate error is to increase quality of each of the layer-wise APKF approximations from which the errors first originate. In the case of Gaussian-distributed weights, APKFs use analytic expectations to approximate finite averages (over unit outputs), where each element being averaged is an iid unbiased estimator of the expectation. Increasing the width/channel dimension m will thus reduce the variance of the overall estimator, and thus reduce error (as predicted by Theorems 1 and 6). 44
Deep Kernel Shaping While not originally conceptualized as such, the use of SUO-distributed weights (as defined in Section 4) provides another way to mitigate the kernel approximation error that is essentially free. When using the SUO distribution, the weights are no longer statistically independent or Gaussian distributed, and so the unit outputs being averaged across are neither iid nor unbiased estimators of the kernel approximation formula. Nonetheless, their average is a consistent (but biased) estimator, whose variance goes to zero as m increases (as is established in Theorem 6). It is well-known that biased estimators can sometimes have lower variance than unbiased ones, and this does seem to be the case here. Recall the discussion from the end of Section 4.2, where it was observed that the distribution of an output vector from a linear layer is identical for the Gaussian and SUO-distributed cases, except that the former introduces a random multiplicative perturbation (with mean 1 and variance 1/m) on the vector’s dimension-normalized squared length (which is an estimator of the associated q value). While this perturbation is required for the implied estimator (after the nonlinearity) to be unbiased, it leads to additional variance. We conjecture that this extra variance is more significant than the bias, and thus SUO-distributed weights yield an overall lower approximation error for APKFs. Note that the upper bounds given in Theorems 1 and Theorem 6 do not reflect these intuitions, as they suggest an overall lower approximation error for Gaussian-distributed weights. However, because these are only upper bounds without matching lower bounds, and are likely quite loose/pessimistic, one cannot draw any conclusions. Indeed, we conjecture that tighter bounds could be obtained for SUO-distributed weights given a more careful analysis than the one in Martens (2021). Part III Specification and derivation of Deep Kernel Shaping 16. Conditions on Q/C maps that we will enforce Having established a detailed understanding of Q and C maps, and how their properties relate to network trainability, we are now in a position to state and justify the specific conditions which we will attempt to enforce with DKS. Our particular mechanism for doing this, which involves certain transformations of the network’s activation functions, will be described in later sections, and isn’t important for the present discussion. In the follow series of subsections we describe each of the conditions. Note that be- cause we want the entire network’s capacity to be utilized, and not just the subnetwork corresponding to the most direct input-output path, we will enforce these conditions for all subnetworks of the network. 45
Martens et al. 16.1 Q (1) = 1 for all subnetworks f f The uniform q condition ensures that the q values for a given layer are independent of location (in the feature map) and the network’s input. While this alone is a sufficient hypothesis to derive an approach similar to DKS, we will go a step further and standardize to a q value of 1, which will allow us to reuse local Q/C map computations across the entire network. Note that given our use of PLN (which ensures initial q values are 1), this is equivalent to the condition that Q (1) = 1 for all subnetworks f . f The choice to standardize to a q value of 1 (as opposed to some other positive constant) is somewhat arbitrary and not particularly important. 2 would have worked equally well, for example. However, the choice of 1 does lead to somewhat simpler expressions for the local Q/C map and their derivatives, and corresponds to an output scale which is in the range of “interesting behavior” for most typical loss functions (such as the commonly used softmax cross-entropy error). 16.2 Q(cid:48) (1) = 1 for all subnetworks f f As discussed in Section 15, the size of the error in our kernel approximations can be roughly predicted from the size of the derivatives of the Q maps. Thus, in order for Q/C maps to be an accurate description of the network’s true kernel function, we must keep the size of these derivatives under control. To this end, we will require that Q(cid:48) (1) = 1 for all subnetworks f f . We look at the derivative at q = 1 in particular since this is the input q value we expect in the absence of error, due to the condition Q (1) = 1. In principle, we also care about f the value of Q(cid:48) (q) for q’s close to 1, which would give us a more complete picture of the f approximation error (and perhaps let us to establish rigorous bounds on it). Unfortunately, we don’t yet have a powerful theory for the global properties Q maps like we do for C maps, and so the best we can do is to look at their properties at particular points. That being said, because we know that Q maps are smooth, it’s likely that Q(cid:48) (q) will be reasonably f well approximated by Q(cid:48) (1) for values of q close to 1. f The choice to make Q(cid:48) (1) equal 1, which corresponds to the error neither growing nor f shrinking, is somewhat arbitrary, and other choices for this value are possible. For example, we could minimize Q(cid:48) (1) instead of setting it to 1, thus suppressing the growth of errors f as much as possible. Minimizing Q(cid:48) (1) did seem to work in our experiments, however we f found that for certain activation functions (such as tanh) it resulted in slower optimization compared to using Q(cid:48) (1) = 1. (See Appendix N.4 for these experiments.) f We are currently not sure why setting Q(cid:48) (1) = 1 works better than minimizing it for f some activation functions. One possible explanation is that Q(cid:48) (1) = 1 allows f to transmit f information about the overall scale of its input vector as a roughly linear function of q. Meanwhile, networks where Q(cid:48) (1) is minimized will tend to “squash” the range around f q = 1, making it harder to recover the original input q value from the network’s output. One can perhaps draw a rough analogy between this and the preservation of “geometric information” by C maps as discussed in Section 12. 46
Deep Kernel Shaping 16.3 C (0) = 0 for all subnetworks f f In order to apply the analysis of Section 13 we require that C (0) = 0 for all subnetworks f . f While this might seem like an overly stringent requirement, it is worth noting that arbitrary deviation of C from the identity function is possible if we don’t place any restrictions on f the value of C (0). This is the case even when C(cid:48) (1) = 1 is enforced, as can be seen in f f Section 12.1 for deep RELU networks. 16.4 C(cid:48) (1) ≤ ζ for all subnetworks f f As discussed in Section 12, degenerate C maps correspond to networks that are difficult to train with gradient-based methods. Avoiding this degeneration is the central aim of DKS. As argued in Section 13, we can do this for a given C by bounding its maximum deviation f from identity function (which is the canonical non-degenerate C map). Given the condition C (0) = 0, Theorem 13 says that this deviation is roughly equal f to C(cid:48) (1) − 1. Thus, we will enforce the condition C(cid:48) (1) ≤ ζ for all subnetworks f , where f f ζ > 1 is a hyper-parameter which we will sometimes refer to as the global slope bound. (Note that C(cid:48) (1) (cid:62) 1 is true automatically as consequence of Theorem 13.) This condition f can be thought of as imposing a limit on how “non-linear” any given subnetwork is allowed to look. 16.5 min [C(cid:48) (1)] is maximized f f Even assuming that our kernel approximations are exact, a well-behaved C map is not a sufficient condition for a nonlinear network to be trainable. As discussed in Section 14.2, one way such a network can fail to be trainable is if it’s too far away in parameter-space from a significantly nonlinear function, or in other words is “too linear”. In such cases, a gradient-based optimizer will struggle to utilize the full expressive power of the network. For neural networks with standard parameterizations, this issue will manifest as nonlin- ear layers with activation functions that behave too much like affine functions. As argued in Section 14.3, this can be avoided by requiring that C(cid:48) (1) be sufficiently larger than 1 for f such layers f . Thus, it makes sense to balance the condition in Subsection 16.4 with one requiring that min [C(cid:48) (1)] is maximized, where the minimum is taken over all nonlinear f f layers f in the network. 16.6 Choosing the global slope bound ζ Given the above two conditions, the global slope bound ζ corresponds to the maximum value of C(cid:48) (1) over all subnetworks (and must be (cid:62) 1). Heuristically, the degree to which ζ f is greater than 1 tells us how nonlinear the network’s functional mapping is at initialization time. If ζ is too large, then the C map for the network (or one of its subnetworks) will experience the “exploding” type of degeneration discussed in Section 12.4.2, where c values are squashed towards c = 0. If it’s too close to 1, then the C map will be very close to the 0 identity, and we run the risk of making the network “too linear” (as per Section 14.2). In our experiments on 100 layer networks we tried only a few values of ζ before settling on ζ = 1.5, and in general we found that DKS is reasonably robust to significant variations 47
Martens et al. in ζ (or more precisely, log(ζ − 1)). For example, ζ = 1.01 and ζ = 100 both produced depth 100 networks that trained at competitive speeds, being only somewhat outperformed by networks that used ζ = 1.5. More extreme choices like ζ = 1.001 and ζ = 10000 meanwhile produced significantly slower training. See Appendix M.3 for these results. For depths 200 or greater we found that it was sometimes necessary to use a value of ζ less than 1.5 (such as 1.1) to achieve stable training. We speculate that this is because the kernel approximations underlying our Q/C maps tend to break down at very high depths (given our modest layer widths), but that this can be mitigated by making the network “more linear”. An interesting systematic trend we observed is that larger ζ values (up to a certain limit) tended to produce slightly faster optimization, whereas smaller ones led to slightly improved generalization, possibly because this made the inductive bias of the model (plus optimizer) favor a more linear solution. Relevant experimental data is presented in Appendix M.4. 17. From global map conditions to local ones In this section we will describe how the “global” map conditions given in the previous section can be achieved by enforcing an equivalent set of conditions on the local Q/C maps of the network. The particular mechanism we will use to enforce these “local” conditions will be discussed later in Section 18. 17.1 Slope polynomials and maximal slope functions Before we can write down the local map conditions we will define a special construction that allows us to relate the slope at 1 of extended C maps to the slope at 1 of local C maps. Let f be an arbitrary subnetwork. As discussed in Section 8.2, we may apply automatic differentiation to compute C(cid:48) (1) from the derivatives of the local C maps of f ’s constituent f layers, where composition corresponds to multiplication and weighted averages (due to concatenations or sum operations) correspond to weighted averages (with the same weights). Since c values of 1 always map to 1 (as argued in Section 11), the expression for the derivative will be a polynomial function of the local C map derivatives at c = 1. If we further assume that there is a constant ψ such that C(cid:48) (1) = ψ for each nonlinear g layer g in f , then we can express C(cid:48) (1) as a polynomial function of ψ, as the local maps for f all other layers are the identity function. We will call this function the slope polynomial of f and denote it by p (ψ). Note that since C(cid:48) (1) (cid:62) 1 whenever C (0) = 0 (which we are f g g enforcing), we may thus assume ψ (cid:62) 1 without loss of generality. Because slope polynomials can be constructed from products and weighted averages of lower degree slope polynomials, and the value of 1 is preserved under multiplication and weighted averages, it follows that p (1) = 1 for any subnetwork f . And since ψ (cid:55)→ ψ and f ψ (cid:55)→ 1 are positive definition functions of ψ (trivially), and positive definite functions are closed under multiplication and non-negative weighted averaging (as discussed in Section 11.2), it also follows that slope polynomials are positive definition functions, just like C maps. They are thus non-decreasing for ψ (cid:62) 0, and indeed strictly increasing provided that the subnetwork contains a nonlinear layer. From this it also follows that p (ψ) (cid:62) 1 for all f ψ (cid:62) 1. 48
Deep Kernel Shaping As we will be interested in computing the most extreme slope over a network, we will define a related function called the maximal slope function, which is given by µ(ψ) = max [p (ψ)], where the maximum is taken over all subnetworks f of the entire f f network. Because subnetworks with no nonlinear layers won’t influence the maximum, and the maximum of a set of strictly increasing functions is strictly increasing, we have that the maximal slope function is strictly increasing provided that the network contains at least one nonlinear layer. And because it is the maximum over a set of continuous functions, the maximal slope functions is also continuous, and therefore invertible, which is a fact we will make use of later. 17.2 Computing maximal slope functions The number of distinct subnetworks in a network can be very large, and so computing the maximal slope function naively from the definition can be laborious. Fortunately, we can eliminate most of these subnetworks from consideration immediately. Observe that if a subnetwork is formed by feeding the output of one subnetwork into the input of another, i.e. h = f ◦ g, then we have p (ψ) = p (ψ)p (ψ) by the chain rule. h f g And because p (ψ) (cid:62) 1 and p (ψ) (cid:62) 1 for all ψ (cid:62) 1, it thus follows that p (ψ) (cid:62) p (ψ) and f g h f p (ψ) (cid:62) p (ψ) for ψ (cid:62) 1. Therefore, any subnetwork that is part of another subnetwork in h g this particular sense can be ignored when computing the maximum. Moreover, without as- suming any relationship between f , g, and h, if p (ψ) is a factor of p (ψ), then p (ψ)/p (ψ) f h h f is also a valid slope polynomial, and therefore p (ψ) (cid:62) p (ψ) for all ψ (cid:62) 1, thus allowing h f us to ignore f in the maximum. Note this does not therefore imply that the maximal slope function is always the slope polynomial of the entire network9, as not every subnetwork can be related to the entire network in this way. For example, if we have a very deep nonlinear network with D nonlinear layers and a skip connection from the initial input to the final output, so that the final output √ √ is 1/ 2 times the initial input plus 1/ 2 times the output of the nonlinear subnetwork, then the slope polynomial for the nonlinear subnetwork is ψD, while the slope polynomial for the entire network is (cid:18) 1 (cid:19)2 (cid:18) 1 (cid:19)2 1 √ 1 + √ ψD = (1 + ψD), 2 2 2 which is strictly smaller than ψD for all ψ (cid:62) 1. (This formula can be derived by following the recipe given in Section 21.3.) For this network, the maximal slope function is in fact ψD. An even more interesting example is the same network, but with additional nonlinear layer added to the end, after the skip connection. The maximal slope function of this network is max{ψD, ψ(1 + ψD)/2}, which cannot be reduced to a polynomial as there are settings of ψ for which either input to the max is larger. 9. For the entire network to even have a slope polynomial requires that it be a valid subnetwork of itself, which is only the case for networks with a singular input and output. 49
Martens et al. 17.3 The equivalent local map conditions Having defined the maximal slope function, we are now in a position to derive the equiva- lent local map conditions to the global ones given in Section 16. First, observe that local Q/C maps for affine layers are identity functions, and can essentially be ignored when computing extended Q/C maps. What remains are nonlinear layers and weighted sum operations, and so we will concentrate on these. If we have that Q (1) = 1 for all nonlinear layers f , then the analogous property f automatically holds for all subnetworks that don’t contain weighted sums or constant scalar multiplications, as it is clearly preserved under composition and weighted averages (arising due to concatenations). The same reasoning also applies to the condition C (0) = 0. f While weighted sum operations are constructed from concatenation operations (which are accounted for in the above argument), the construction also introduces scalar multipliers which can affect the q values. To account for this, we must ensure that the output q value of each weighted sum operation is 1. By Equation 12, this is equivalent to requiring that the squares of the weights sum to 1, assuming that the inputs to the sum have q values of 1. We will call weighted sums satisfying this condition “normalized sums”. Given that a weighted sum is normalized, and that its input q values are 1, it additionally follows (by Equations 12 and 13) that the corresponding output q and c values will be weighted averages of the input q and c values, with weights given by the squares of the weights of the sum itself. To finally achieve Q (1) = 1 for all subnetworks f we must remove any constant scalar f multiplication operations from the network, except those that are part of the above normal- ized sums. With this done, a simple inductive argument then establishes that Q (1) = 1 f for all subnetworks f . Given constant q values of 1, and weighted sums that are all normalized, we may compute Q(cid:48) (1) using the same slope polynomials used to compute C(cid:48) (1), provided that Q(cid:48) (1) is the g g f same for all nonlinear layers f . Thus if we impose the condition Q(cid:48) (1) = 1 for all nonlinear f layers f , it will follow that Q(cid:48) (1) = p (1) = 1 for all subnetworks g. g g Finally, maximizing min [C(cid:48) (1)], while requiring that C(cid:48) (1) ≤ ζ for all subnetworks f , f f f is equivalent to setting C(cid:48) (1) = ψ for all nonlinear layers f (since a single nonlinear layer f is a subnetwork), where ψ = µ−1(ζ) and µ−1 is the inverse of the maximal slope function for the network (which exists as long as the network has at least one nonlinear layer). Summarizing, the equivalent local map conditions are: 1. Q (1) = 1, f 2. Q(cid:48) (1) = 1, f 3. C (0) = 0, and f 4. C(cid:48) (1) = µ−1(ζ), f for all nonlinear layers f , with the additional requirement that all weighted sum operations in the network are normalized (i.e. that the squares of their weights sum to 1). 50
Deep Kernel Shaping 18. Activation function transformations In addition to PLN and the use of Delta initializations, our main mechanism of control over the initialization-time behavior of neural networks will be to apply transformations to their activation functions. In particular, we will apply constant scalar multiplications and shifts to both their inputs and outputs. For most typical activation functions this will give us sufficient control over a combined/nonlinear layer’s local Q/C maps to enforce the “equivalent local map conditions” from the previous section. 18.1 Basic definition Suppose φ is some element-wise activation function in the network. We propose to make the following replacement: φ(u) −→ φˆ(u) ≡ γ(φ(αu + β) + δ) where α, β, γ, and δ are static scalar constants (that we do not train). Note that these constants are the same for all channels and feature map locations within a given layer, but can differ between layers. 18.2 Equivalent parameters and preservation of the model class Provided that each nonlinear layer is both preceded by and followed by an affine layer (which is true for most architectures), this way of transforming the activation functions has the property that it preserves the model class of the original network. By this we mean that for any network with transformed activation functions, there exists an equivalent network with untransformed activations that has precisely the same functional behavior. We will call the filter weights and biases of this second network the equivalent parameters. Computing the equivalent parameters is relatively straightforward. Because each non- linear layer is both preceded by and followed by an affine layer, we can essentially absorb the input/output scale and shift operations into these layers. To be more explicit, in the case of a standard fully-connected layer with weight matrix W and bias vector b, W (γx + δ1) + b becomes W (cid:48)x + b(cid:48) with W (cid:48) = γW and b(cid:48) = δW 1 + b (where 1 denotes the vector of ones). Similarly, α(W x + b) + β1 becomes W (cid:48)x + b(cid:48) with W (cid:48) = αW and b(cid:48) = b + β1. The construc- tion for convolutional layers is similar, and relies on the fact that the scaling and shifting constants are the same for each location (just as the filter weights and biases are). 18.3 Our method for transforming activation functions viewed as an initialization scheme The existence of equivalent parameters, and their relatively straightforward computation, makes it possible to turn our method for transforming activation functions into an initial- ization scheme for the network’s parameters. One simply computes the constants needed to appropriately transform the activation functions, and then uses them to instead compute the equivalent parameters, starting from a network initialized as per Section 4. If we view this process as a sampling procedure for the network’s parameters, then it corresponds to 51
Martens et al. a distribution with non-trivial correlations between the weights and biases of each affine layer. Note that while a transformed network and an untransformed network (with equivalent parameters) compute the same function, they correspond to different parameterizations of the same model class, and thus may give rise to different optimization dynamics. Stochastic gradient descent for example, is not invariant to reparameterizations of this type, and so we would expect it to behave differently on either network. The K-FAC optimizer (Martens and Grosse, 2015) on the other hand is approximately invariant reparameterizations involving affine transformations of layer inputs and outputs (Martens and Grosse, 2015; Grosse and Martens, 2016; Luk and Grosse, 2018). Our experimental results indicate that the transformed networks are easier to optimize with stochastic gradient descent than networks with equivalent parameters. Meanwhile, as predicted by the theory, the optimization performance with K-FAC is roughly the same for both versions. See Appendix N.7 for the relevant results. 18.4 Achieving local map conditions with activation function transformations Suppose f is a nonlinear layer with activation function φ which we propose to replace by φˆ(u) ≡ γ(φ(αu + β) + δ). The four equivalent local map conditions (from Section 17.3) give rise to a system of four nonlinear equations, with the four scalar constants (α, β, δ, and γ) as its unknowns. In this subsection we will show how to solve for these constants, assuming that a solution exists. By Equation 14, the condition C (0) = 0 holds if and only if E [φˆ(x)] = 0. Noting f x∼N (0,1) that E [φˆ(x)] = γ(E [φ(αx + β)] + δ), x∼N (0,1) x∼N (0,1) this becomes equivalent to δ = −E [φ(αx + β)]. x∼N (0,1) Thus δ is fully determined by α and β, which eliminates a single degree of freedom. From Equation 5, and basic properties of expectations, we have that Q (1) = E [φˆ(x)2] = γ2E [(φ(αx+β)+δ)2] = γ2 Var [φ(αx+β)]. (16) f x∼N (0,1) x∼N (0,1) x∼N (0,1) Thus the condition Q (1) = 1 is equivalent to f γ = (E x∼N (0,1)[(φ(αx + β) + δ)2])− 21 = Var x∼N (0,1)[φ(αx + β)]− 1 2 . This fully determines the value of γ in terms of the other constants, thus eliminating another degree of freedom. Given the above solutions for γ and δ, which we will treat as functions γ(α, β) and δ(α, β) of α and β, it remains to solve for the values of α and β which satisfy the final two conditions Q(cid:48) (1) = 1 and C(cid:48) (1) = µ−1(ζ). From the fact that Q (1) = 1 (for our choice of f f f γ), these two conditions can be written as: i. E [φˆ(x)φˆ(cid:48)(x)x] = 1, and x∼N (0,1) 52
Deep Kernel Shaping ii. E [φˆ(cid:48)(x)2] = µ−1(ζ) , x∼N (0,1) where the dependence on α and β is implicit in φˆ(x) = γ(α, β)(φ(αx + β) + δ(α, β)) and φˆ(cid:48)(x) = αγ(α, β)φ(cid:48)(αx + β). We are not aware of any closed-form solution for this two dimensional system. However, because it’s only two dimensional, and the expectations required to evaluate it are one dimensional (including those needed to compute δ and γ), we can readily solve it using black- box numerical software, assuming a solution exists. And because the system of equations only depends on the functional form of φ and no other details about f , we only need to solve it once for each distinct activation function in the network. Implementation details are given in Section 22. 18.5 When will solutions exist? While we found in our experiments that solutions for α and β exist for nearly all commonly used nonlinear activation functions, the popular RELU is a notable exception (which we will examine in the next subsection). Thus, it is worth delving deeper into the question of the existence of these solutions. Noting that µ−1(ζ) will typically be quite close to 1, if we can show that lim C(cid:48) (1) = α→0 f 1, this will suggest that C(cid:48) (1) = µ−1(ζ) is achievable by choosing a sufficiently small value f of α. Intuitively speaking, shrinking α allows us to effectively narrow the interval of typical inputs to φ, meaning that φ starts to resemble an affine function over this interval (since differentiable functions are, by definition, closely approximated by their 1st-order Taylor approximations within any sufficiently small neighborhood). As discussed in Section 13.4, this means that C(cid:48) (0)/C(cid:48) (1) → 1 as α → 0, which in turn implies that C(cid:48) (1) → 1 (as we f f f have by Remark 14 that C(cid:48) (0) (cid:54) 1 (cid:54) C(cid:48) (1) when C (0) = 0). f f f The following proposition formalizes this intuition, although is proved (in Appendix F.1) using a different technique. Proposition 26 Let f be a nonlinear layer with transformed activation function φˆ defined as above, with δ and γ chosen as per Section 18.4. If φ(cid:48)(β) (cid:54)= 0 then we have lim C(cid:48) (1) = 1. f α→0 The hypothesis that φ(cid:48)(β) (cid:54)= 0 is required here since otherwise φˆ will tend to the zero function as α → 0 (whose C map is undefined). Apart from this restriction, there is no obvious requirement on β for the condition C(cid:48) (1) = µ−1(ζ) to hold, and indeed in our f preliminary tests we found that we could satisfy this for nearly all reasonable choices of β for most activation functions. The role of β can thus be thought of selecting the position in φ’s graph to “zoom in on”, and gives us the extra flexibility needed to control the value of Q(cid:48) (1). f 18.6 The problem with positively homogeneous activation functions A positively homogeneous activation function φ(u) of degree k is one where φ(λu) = λkφ(u) for all non-negative scalars λ. A well-known example for k = 1 is the RELU activation function, which is given by φ(u) = max(u, 0). 53
Martens et al. Due to their defining property, positively homogeneous activation functions yield at most three effective degrees of freedom under our parameterized transformation, instead of the typical four. This can be seen by observing that φˆ(u) = γ(φ(αu + β) + δ) = γ(φ(|α|(sign(α)u + β/|α|)) + δ) = |α|kγ(φ(sign(α)u + β/|α|) + δ/|α|k) = γ˜(φ(sign(α)u + β˜) + δ˜), where we have defined γ˜ = |α|kγ, β˜ = β/|α|, and δ˜ = δ/|α|k. Apart from the sign of α, which can take only two discrete values, we effectively have only three free real-valued random variables: γˆ, βˆ, and δˆ. Because of this reduction in the degrees of freedom for positively homogeneous activation functions, we can only enforce at most three of our four local map conditions. The only one which is arguably optional is the condition that Q(cid:48) (1) = 1 for all combined layers f , which f corresponds to the equation E [φˆ(x)φˆ(cid:48)(x)x] = 1. Thus, in all of our experiments with x∼N (0,1) RELU networks we dropped this condition, and while it did produce networks that trained reasonably well, optimization performance was still slower compared to all other activation functions we tested, at least for skip connection-free networks trained with K-FAC. (See Section 28.4 for these results.) 18.7 Examples of transformed activation functions In this subsection we will give some examples of transformed activation functions produced by DKS. Our examples will assume a basic feedforward network of 100 combined layers, and a global slope bound ζ = 1.5. We will consider the standard tanh and RELU activation functions, as well as Swish (Prajit et al., 2017), SELU (Klambauer et al., 2017), and a commonly used smooth substitute for RELU called “softplus” (which is given by φ(x) = log(1 + exp(x))). The following table gives the approximate values for the activation function parameters found by DKS: Activation function α value β value δ value γ value tanh 0.090438 -0.56011 0.50500 14.9025 softplus 0.22802 0.40751 -0.92372 7.30325 relu 0.387604 1.0000 -1.0006 2.5916 swish 0.12945 0.349475 -0.20889 11.50455 selu 0.088294 -0.25244 0.38694 8.25434 In the following plots we compare the default and transformed activation functions over the input interval [−10, 10] for tanh, softplus, and RELU. Assuming uniform q values of 1, and that the error in our kernel approximations is relatively low, this interval contains all the inputs that our nonlinear units will see at initialization time with overwhelming probability. 54
Deep Kernel Shaping 55
Martens et al. We can see from these plots that the transformed activation functions tend to look more like the identity functions than the defaults ones do (over the relevant range of inputs). In fact, they all bare a resemblance to each other (especially tanh, softplus and swish), as can be seen in the following plot: 19. Addressing normalization layers 19.1 Batch Normalization layers Batch Normalization (BN) layers (Ioffe and Szegedy, 2015) are an important component in many neural network architectures, especially convolutional networks. For each unit scalar u in their input, BN layers compute a mean µ and variance σ2 of u over the training 56
Deep Kernel Shaping √ mini-batch, and then output a “normalized” version (u − µ)/ σ2 + (cid:15), where (cid:15) is a small constant. This is this sometimes followed by the application of per-channel learnable bias parameters, which are initialized to zero. Because they use statistics computed over the mini-batch, BN layers cannot really be described in Q/C map framework we have presented, and are therefore incompatible with DKS. In particular, our formalism assumes that the network’s computation for a single training input depends only on that input, and not on other elements of the mini-batch. To account for such interactions, one would have to introduce hypotheses on the size of the mini-batch and the statistical distribution of its vectors, as the behavior of BN layers are highly dependent on these factors. Moreover, the evolution of q and c values would not happen independently across the mini-batch, which would likely preclude a simple one- dimensional description like Q and C maps. 19.2 Layer Normalization layers Layer Normalization (LN) layers (Ba et al., 2016) are a popular ingredient in neural network architectures such as Transformers, and are sometimes used as an alternative to BN layers. For each location vector z in its input feature map, an LN layer computes the scalar mean µ = 1 1(cid:62)z and variance σ2 = 1 (cid:107)z − µ1(cid:107)2 over the k entries of z (where 1 denotes the vector k i k √ of 1’s), and outputs a “normalized” version (z − µ1)/ σ2 + (cid:15), where (cid:15) is a small constant. This is this sometimes followed by the application of learnable per-channel gain and bias parameters, which are initialized to 1 and 0 respectively. Note that LN layers were not explicitly defined for convolutional networks in the original paper. Thus, one could also conceivably define them as computing a mean µ and variance σ2 over both locations and channels, instead of individually per location. In this work we will assume our previous definition, and anything we say regarding LN layers from this point will apply only to that definition. Unlike BN layers, LN layers perform their computations and transformations individu- ally per training case, and do not involve any computations across the mini-batch. Aver- aging of statistics instead occurs over entries (i.e. channels) of the location vectors, and the same scale and shift is applied to all entries. In general, µ and σ2 will be different for each input to the network, so that the learnable gain and bias cannot ever actually “undo” the normalization for all training cases cases simultaneously. This means that introducing LN layers into a network will fundamentally change its model class. As we will show next, LN layers can be understood within our Q/C map framework, and are thus compatible with DKS. The formulas for their local Q/C maps are given below. 19.2.1 Q/C map computations for Layer Normalization layers As we are concerned with the network’s initialization-time behavior when computing Q/C maps, we will assume going forward that the LN layers f ’s learnable gain and bias parame- ters, if they are indeed used, are set to their initial values (1 and 0). Given this assumption, and taking (cid:15) = 0, the output of f will always have a dimension-normalized squared length of 1, as σ2 = 1 (cid:107)z − µ1(cid:107)2 by definition. As this is precisely the quantity approximated by q k values, we can thus define Q (q) = 1. f 57
Martens et al. To understand how f will affect c values, it suffices to analyze it as a mapping from z to z − µ1, since c values are invariant to scalar multiplications of the underlying vectors. Suppose z and z are two different vector inputs to f (for a particular location), with q 1 2 values q , q and c values and c , c (respectively), and define µ = 1 1(cid:62)z for i = 1, 2. Then 1 2 1 2 i k i we have 1 (z − µ 1)(cid:62)(z − µ 1) = 1 z(cid:62)z − µ µ for i, j ∈ {1, 2}. C (c, q , q ) approximates k i i j j k i j i j f 1 2 the cosine similarity of z − µ 1 and z − µ 1, which can therefore be written as 1 1 2 2 √ 1 z(cid:62)z − µ µ q q c − µ µ k 1 2 1 2 ≈ 1 2 1 2 . (cid:113) (cid:112) (cid:0) k1 (cid:107)z 1(cid:107)2 − µ2 1(cid:1) (cid:0) k1 (cid:107)z 2(cid:107)2 − µ2 2(cid:1) (q 1 − µ2 1)(q 2 − µ2 2) For a network initialized as per Section 4, we have by Equation 24 that µ ≈ E (cid:2) φ (cid:0)√ q x(cid:1)(cid:3) i x∼N (0,1) i for i = 1, 2, where φ is the activation function of the immediately preceding combined layer g (with φ being the identity if g is affine). If we have uniform q values (so that q = q = q), then by Equation 14 this implies 1 2 (cid:112) µ = µ = qC (0), so that the above expression for f ’s C map simplifies to 1 2 g qc − qC (0) c − C (0) g g C (c) = = . (17) f q − qC (0) 1 − C (0) g g When g is an affine layer (or a sum over multiple affine layers), or is a combined/nonlinear layer transformed via DKS, we have C (0) = 0. In this case, the above expression for f ’s C g map reduces to the identity function. More generally, we note that C (0) − C (0) g g C (0) = C (C (0)) = = 0, f◦g f g 1 − C (0) g and so the application of the LN layer f after g thus has the effect of ensuring that C (0) = f◦g 0 even when C (0) (cid:54)= 0. g 20. Addressing pooling layers Pooling layers are a type of layer used in certain convolutional network architectures to compress information from a larger feature map into a smaller one (with fewer locations). In this section we will discuss why standard pooling layers aren’t compatible with our Q/C map framework, and describe potential replacements for them which are. We will also give mathematical arguments and empirical evidence suggesting that it may nonetheless be okay to use them with DKS in practice. 20.1 (Global) mean-pooling layers Mean-pooling layers function similarly to convolutional layers, except that instead of com- puting a (learnable) affine function of each “patch” of activation vectors, they simply com- pute the average of those vectors. Typically these patches don’t overlap, and thus a mean pooling layer reduces the number of locations (while preserving the channels). 58
Deep Kernel Shaping In order to simplify the discussion we will restrict our attention to “global” mean-pooling layers, which average over all locations, and are the most common type used in practice. The same basic conclusions will apply to general mean-pooling layers, with somewhat more complicated formulas for the associated kernel functions. Formally, a global mean-pooling layer f computes 1 f (Z) = Z1, (18) (cid:96) where (cid:96) is the number of locations of the feature map Z, and 1 denotes a vector of 1’s of the appropriate dimension (which will change based on context). 20.1.1 The PKF for mean-pooling layers and associated difficulties From the above equation, the PKF associated with f is a (2 × 2)-matrix-valued function given by 1 (cid:20) (Z1)(cid:62)Z1 (Z1)(cid:62)Z(cid:48)1 (cid:21) 1 (cid:34) 1(cid:62) (cid:0) 1 Z(cid:62)Z(cid:1) 1 1(cid:62) (cid:0) 1 Z(cid:62)Z(cid:48)(cid:1) 1 (cid:35) κ (Z, Z(cid:48)) = = (cid:16) k (cid:17) (cid:16) k (cid:17) , f k(cid:96)2 (Z(cid:48)1)(cid:62)Z1 (Z(cid:48)1)(cid:62)Z(cid:48)1 (cid:96)2 1(cid:62) 1 Z(cid:48)(cid:62)Z 1 1(cid:62) 1 Z(cid:48)(cid:62)Z(cid:48) 1 k k (19) where k is the output channel dimension. Noting that the input and output channel dimension are equal for mean-pooling layers, we have (cid:20) 1 Z(cid:62)Z 1 Z(cid:62)Z(cid:48) (cid:21) Σ = k k , Z,Z(cid:48) 1 Z(cid:48)(cid:62)Z 1 Z(cid:48)(cid:62)Z(cid:48) k k and so κ (Z, Z(cid:48)) only depends on the inputs Z and Z(cid:48) via their IPM Σ . Thus, κ can f Z,Z(cid:48) f be composed with APKFs to form a network-level PKF approximation. Unfortunately, while κ depends only on input q and c values (i.e. the entries of Σ ), f Z,Z(cid:48) it cannot be broken down in terms of Q and C maps that operate independently across different locations. For example, the output q value is given by 1 1(cid:62) (cid:0) 1 Z(cid:62)Z(cid:1) 1, which (cid:96)2 k would be computed as an average of multiple input q and c values. Even if we assume that the input q values to κ are uniform, the output q values will differ for each input to the f network and each location due to their dependence on the c values. This invalidates our C map analysis for subsequent layers, which is predicated on uniform q values. 20.1.2 A possible replacement: weighted mean-pooling layers A possible solution to the issues associated with mean-pooling layers is to replace them with layers that can be more easily handled within our framework, and which ideally don’t shrink the model class. (Expansion of the model class is less objectionable, provided that it doesn’t significantly harm generalization performance.) One natural option is to use convolutional layers whose filter size equal is equal to that of the entire feature map. This won’t shrink the model class, as such layers can easily simulate (cid:16) √ (cid:17)−1 global mean-pooling layers by setting all filter weights to (cid:96) k . Unfortunately, such a layer will most likely have a very large filter bank matrix, and this may significantly increase the total number of parameters in the network. 59
Martens et al. Another option is something we call weighted mean-pooling layers, which are defined similarly to regular mean-pooling layers, except that the vector of 1’s in Equation 18 is replaced by a learnable vector of weights w, giving f (Z) = Zw. These can clearly also simulate regular mean-pooling layers (by setting w = (1/(cid:96))1). And because they don’t introduce as many new parameters as the previous option, they have a better chance of preserving the model’s generalization characteristics. Suppose f is a weighted mean-pooling layer. As with combined layers, we can compute an APKF κ which approximates f ’s PKF κ at initialization-time (with high probability), (cid:102)f f under the assumption that w ∼ N (0, (1/(cid:96))I). As shown in Appendix E, this is given by 1 (cid:34) tr (cid:0) 1 Z(cid:62)Z(cid:1) tr (cid:0) 1 Z(cid:62)Z(cid:48)(cid:1) (cid:35) κ (Σ ) = (cid:16) k (cid:17) (cid:16) k (cid:17) , (cid:102)f Z,Z(cid:48) (cid:96) tr 1 Z(cid:48)(cid:62)Z tr 1 Z(cid:48)(cid:62)Z(cid:48) k k and becomes a more precise approximation as (cid:96) grows, provided that the average absolute input c value across all pairs of locations simultaneously goes to zero. This latter condition could occur if the feature vectors for nearly all pairs of locations in the network’s input image have a small absolute cosine similarity, since c values always decrease with depth under DKS. It could also occur for more general input images if DKS is used with a large ζ parameter, and f is sufficient deep into the network (so that the C map up to f maps most inputs to a relatively small region near zero). κ has more favorable properties than the PKF for mean pooling layers given in Equation (cid:102)f 19. In particular, since the output q/c value is just the average across locations of the input q/c values, the property of uniform q values of 1 will be preserved, thus enabling our C map analysis to be valid for subsequent layers. Complicating the story somewhat is the fact that the c values for different locations are averaged together, as our analysis up to this point has assumed them to be separate and independently evolving. This means that geometric information about each individual location is no longer strictly preserved, as the averaging operation makes recovery of the individual c values impossible. It is true however that each location still has a proportional effect on the output, and thus the degeneration discussed in Section 12.4 can still be avoided, as long as the C map of the subnetwork up to f is sufficiently well-behaved. Because the input c values are given by C (c ) for locations i = 1, 2, . . . , (cid:96) (for some c ’s) g i i where g is the subnetwork up to f , and C is a convex and increasing function on [0, 1] (by g Section 11.2), we have that (cid:32) (cid:33) 1 (cid:88) 1 (cid:88) C c (cid:54) C (c ) (cid:54) C (max{c }), g i g i g i n (cid:96) i i i for c ∈ [0, 1]. The output c value associated κ , which is given by 1 (cid:80) C (c ), is thus i (cid:102)f (cid:96) i g i closely related to C as applied to a single input. It is on this basis that we will treat f as g having an identity C map in our computations which, for lack of a richer multi-dimensional theory, seems like a reasonable heuristic. 60
Deep Kernel Shaping In some of our experiments on convolutional networks we tried using a weighted mean- pooling layer in place of the usual global mean-pooling operation near the end of the net- work. While this worked well, we found that it didn’t provide any optimization benefit. (See Appendix N.9 for these experiments.) Thus in our main set of experiments with DKS we continued to use standard global mean-pooling layers, despite their apparent incompatibility with our theoretical framework. 20.2 Max-pooling layers A max-pooling layer is similar to a mean-pooling layer, but instead of taking the mean of a set of location vectors, it takes the coordinate-wise maximum. 20.2.1 PKFs for max-pooling layers and approximate map properties In the previous subsection we saw that the PKF for a mean-pooling layer, despite having a simple form that depended only on the IPM (Σ ) of its input, had unfavorable properties Z,Z(cid:48) that made it impossible to properly analyze within our Q/C map framework. The situation with max-pooling layers is arguably even worse, as its PKF has a more general dependence on its input, and thus cannot be composed with APKFs of combined layers to form a network-level PKF approximation. But despite this, we can still make some non-trivial statements about a max-pooling layer’s PKF that will be useful in understanding how DKS may possibly still apply to networks containing such layers. Consider a patch of locations over which the max operation is applied. If all the loca- tion vectors in the patch are nearly equal to each other, then the max operation simply outputs a close approximation of the vector in the center of the patch, and thus has a PKF approximated by the identity (for non-dropped locations). It is therefore reasonable to ap- proximate max-pooling layers as having local Q and C maps equal to the identity in this case. This situation is fairly common when max-pooling layers are used very early in the network, since nearby pixels tend to be similar to each other in natural image data, which, assuming well-behaved C maps, means that the corresponding vectors for subsequent layers will be similar too (as measured by their cosine similarly). Analogously, if the pixels within a patch fall into two tight clusters, which can happen if the patch overlaps an edge or the corner of an object, then the subsequent vectors will also fall into two tight clusters. If this is the situation, and we assume uniform q values and wide layers, then it can be shown that the output q value of a max pooling layer will be closely approximated by its input q value (so that we can treat the layer as having an identity Q map). This is shown in Appendix G, and relies on the somewhat surprising fact that E (cid:20) u1 (cid:21) ∼ N (cid:18) 0, (cid:20) 1 c (cid:21)(cid:19)[max{u 1, u 2}2] = 1 u2 c 1 for all c ∈ [−1, 1], along with the mild assumption that the max-pooling layer in question is directly preceded by a convolutional layer. Note that for clusters of 3 or more pixel values this approximation doesn’t work, although the output q value will only deviate from the input q value by a factor that grows slowly with the number of clusters. 61
Martens et al. 20.2.2 Possible replacements for max-pooling layers As with mean-pooling layers, we could consider replacing max-pooling layers with ones that are handled within our framework. However, unlike the mean operation, the max operation is difficult to elegantly simulate using our standard layer types, and so there are no obvious substitutions that would preserve the model class. In some architectures, max-pooling layers are used merely to reduce the size of a feature map, with the particular choice of pooling operation (max or mean) being unimportant from a modeling perspective. In such cases it may thus be quite reasonable to replace max-pooling layers with weighed mean-pooling layers. 21. Summary of our method 21.1 Architectural requirements In order to apply DKS we must observe certain architectural requirements on the network. These are summarized below: 1. The network must be constructed from combined layers (defined as an affine layer fol- lowed optionally by a nonlinear layer), weighted sums between the output of two or more affine layers (followed optionally by a nonlinear layer), concatenations of two or more feature maps along their channel dimension, mean-pooling layers, and max-pooling layers (although the latter should be used with caution as discussed in Section 20.2). 2. Batch Normalization layers must not be used. However, Layer Normalization layers are allowed, provided that their associated gain and bias parameters are initialized as per Item 5 of this list. (See Section 19 for additional discussion of normalization layers.) 3. Nonlinear layers must use element-wise activation functions. Positively homogeneous activation, such as RELU, are allowed but not recommended as they lead to a limited version of DKS (as discussed in Section 18.6). 4. Multiplication operations, such as those used in attention mechanisms, are also not allowed (although we hypothesize that DKS can be extended to handle these in the future). 5. The network should not contain any extraneous trainable parameters such as scalar multiplications or shift, unless these parameters have no effect at initialization time (e.g. a shift that is initialized to 0). Constant scalar multiplications are allowed, although these will typically be removed as part of the application of DKS. 6. Similarly, constant multiplications and shifts are not allowed, except as part of weighted sum operations. (Note that if such constants are normally required for the network to be trainable with standard optimization methods, it’s likely that DKS will render them unnecessary/obsolete.) 62
Deep Kernel Shaping 21.2 Execution steps To apply DKS to a given network one performs the following steps: 1. Initialize each bias vector to 0, and each weight matrix/filter bank using either a Gaussian Delta initialization, or an Orthogonal Delta initialization (which are both defined in Section 4). 2. Choose a value larger than 1 for the scalar hyperparameter ζ (such as 1.5 or 1.1). Note that ζ roughly corresponding to the “degree of nonlinearity” of the network. See Section 16.6 for additional discussion of this. As observed in Section 16.6, lower values of ζ tend to be associated with slightly better generalization, at the cost of somewhat slower optimization. 3. (optional) Apply some version of Per-Location Normalization (PLN) to the input data. Note that this can be done entirely online, as it only requires the current example, and not any aggregate statistics over the entire training set. (See Section 10.2 for more details.) 4. Remove any constant scalar multiply operations from the network. 5. Replace any weighted sums between features maps Y , . . . , Y with “normalized sums” of 1 n the form (cid:80)n w Y , for weights w satisfying (cid:80)n w2 = 1 (which may be chosen freely). i=1 i i i i=1 i Note that if w i = w j for all i and j this simplifies to √1 n (cid:80)n i=1 Y i, although other choices are permitted and may indeed be preferable (as demonstrated in Section 23.3). 6. (optional) Replace any mean-pooling layers with “weighted mean-pooling layers” as de- fined in Section 20.1.2. 7. Compute the network’s maximal slope function µ(ψ) (or some approximation of this). One can use the recipe given in Section 21.3. 8. Using the fact that µ is a 1D monotonically increasing function, compute µ−1(ζ) using binary search (or a similar such method). 9. For each distinct activation function φ in the network, do the following: i. Given µ−1(ζ), solve for α, β, γ, and δ as per Section 18.4, using the numerical methods described in Section 22 (or some alternative). ii. Replace all instances of φ in the network with φˆ(u) ≡ γ(φ(αu + β) + δ). 21.3 Recipe for computing slope polynomials and maximal slope functions As per Sections 17.1 and 17.2, the maximal slope function µ(ψ) is computed as µ(ψ) = max[p (ψ)], f f where p (ψ) is the network polynomial of f (whose computation we will describe below), f and the maximum is taken over all subnetworks f of the entire network. 63
Martens et al. Note while the number of distinct subnetworks may be quadratic (or worse) in the depth, when computing the maximum we may ignore any subnetwork that can be composed with another one to form a strictly larger subnetwork, or more generally, any subnetwork whose slope polynomial is a factor of the slope polynomial of another subnetwork. For a given subnetwork f , the computational graph of the network polynomial p (ψ) f may be obtained from the computational graph of f by recursively applying the following rules (which are essentially just the result of applying automatic differentiation to the graph of C (c) and then evaluating the result at c = 1 to obtain p (ψ) = C(cid:48) (1)): f f f 1. Composition g ◦ h of two subnetworks g and h maps to p (ψ)p (ψ). g h 2. Affine layers map to the constant 1. 3. Nonlinear layers map to ψ. 4. Concatenation operations (over the channel dimension) between the outputs of subnet- works g , g , . . . , g map to 1 2 n 1 (k p (ψ) + k p (ψ) + · · · + k p (ψ)), (cid:80)n k 1 g1 2 g2 n gn i=1 i where k is the number of output channels of g . i i 5. Normalized sums with weights w , w , . . . , w over the outputs of subnetworks g , g , . . . , g 1 2 n 1 2 n map10 to w2p (ψ) + w2p (ψ) + · · · + w2 p (ψ). 1 g1 2 g2 n gn 6. Layer normalization layers map to the constant 1. 7. Max-pooling and weighted mean-pooling layers map to the constant 1. (Standard mean- pooling layers can be heuristically mapped to 1, although they technically break our network polynomial formalism.) 8. The network’s input maps to the constant 1. Note that this recipe for computing can be generalized to compute C(cid:48) (1) for networks in f which C(cid:48) (1) may be different for each nonlinear layer (i.e. not equal to some common ψ) g by mapping nonlinear layers g to C(cid:48) (1) instead of ψ. g Provided that the network architecture is compatible with DKS, a quick way to compute slope polynomials is to count the number k of nonlinear layers in a given sequence of layers (to get a slope polynomial of ψk for that subnetwork), and then apply the rule for normalized sums where appropriate. See Sections 17.2 and 23.4 for instructive examples of how to the compute maximal slope function for certain architectures. 10. For reference, when computing the slope polynomial for a network whose q values may vary between layers (which won’t come up when applying DKS), normalized sums instead map to 1 (cid:80)n w2q (w 12q ip g1(ψ) + w 22q ip g2(ψ) + · · · + w n2 q ip gn(ψ)), i=1 i i where q is the output q value associated with g . i i 64
Deep Kernel Shaping 22. Some implementation details Through careful optimization and engineering, and a lot of trial and error, we were able to get the runtime of DKS down to a few seconds for typical large-scale networks. In this section we describe the aspects of this that were the most challenging and non-obvious. 22.1 Solving for the α and β constants As we saw in Section 18.4, finding the appropriate α and β constants for a particular activation function φ amounts to solving a system of two nonlinear equations. Since we don’t have a closed form solution for this, we must resort to numerical methods. After trying several possibilities, we got the best results using scipy.optimize.root(), which is part of the popular SciPy Python package (Jones et al., 2001–). We call this with the arguments method="hybr" and jac=False, and leave all other options at their defaults. This invokes an implementation of the modified Powell algorithm (Powell, 1964). Because the implicit regression loss of the system is non-convex in general, the solver sometimes fails to find a solution from its initial guess. Our solution to this is simply to call it repeatedly with different initial guesses until it returns successfully. In our experiments it never took more than 4 calls to find a solution for any of the eleven activation functions we tried. We took our first six initial guesses for (α, β) from the list [(1, 0), (1, 1), (1, −1), (0.1, 0), (0.1, 1), (0.1, −1)] and then generated subsequent ones randomly using numpy.random.uniform(low=0.0, high=2.0) for α, and numpy.random.uniform(low=-3.0, high=3.0) for β. 22.2 High-quality estimates of the expectations In order to guarantee fast and reliable convergence, the solver scipy.optimize.root re- quires the LHS values of the nonlinear equations to be computed to a very high precision. Moreover, the system we need to solve may be numerically sensitive in general, regardless of the particular solver algorithm used. Thus it is important that we compute high quality estimates of the four Gaussian expectations that determine these LHS values. A naive estimate based on sampling x’s from N (0, 1) will perform very poorly, as its variance scales as 1/n, where n is the number of sample points. Fortunately, all four of the Gaussian expectations are one dimensional, which opens the door to heavy-duty numerical integration methods capable of achieving near numerical precision in a reasonable amount of time. After experimenting with several such methods built into SciPy, we found that the best performing one by far was scipy.integrate.fixed_quad, which implements fixed- order Gauss-Legendre quadrature to compute one dimensional definite integrals. We used this method to approximate Gaussian expectations as 1 (cid:90) ∞ 1 (cid:90) 10 E [h(x)] = √ h(x) exp(−x2/2)dx ≈ √ h(x) exp(−x2/2)dx, x∼N (0,1) 2π 2π −∞ −10 which is justified by the fact that the integrand is negligible for values of x outside of [−10, 10] for all the h(x)’s we care about. To ensure high quality estimates, we set the “order” parameter n to 105 (which is considered very high). 65
Martens et al. By far the most expensive part of fixed_quad’s computation is the calculation of the sample points and weights (via the roots_legendre function), which can take around 15 minutes on a modern CPU when n = 105. Fortunately, because this part of the computation only depends on the order parameter n, it is cached by fixed_quad during the first call and reused in subsequent calls, allowing these calls to execute almost instantly. In our codebase we went a step further and stored the results of roots_legendre in a file which was then loaded and monkey-patched into the SciPy library before the first call to fixed_quad, thus eliminating this 15 minute overhead completely. 22.3 Computing and inverting the maximal slope function in software As they are determined by a network’s architecture, maximal slope functions can in principle be computed automatically and efficiently, according to recipe in Section 21.3. Automating this in software requires access to a high-level description of network’s structure, which could possibly be extracted from the API calls made to the neural network library. In our experiments we just computed them by hand, as we only experimented with a small handful of architectures. An alternative to manual computation or automation is to approximate the maximal slope function by a reasonable surrogate. For networks whose “deepest path” has D nonlin- ear layers, a natural approximation to use is ψD. However, for network architectures that involve extensive use of skip connections such as ResNets, this approximation may be very poor, as it fails to account for how the skip connections make the network’s computation “more linear”. (See Section 23.4 for more details.) As discussed in Section 17.1, maximal slope functions are continuous 1-dimensional functions of ψ that are strictly increasing (as long as the network has at least one nonlinear layer), and thus they can be inverted up to a fixed tolerance using a simple binary search. Our implementation started with an interval of [1, 2] for the solution, and kept doubling the maximum if the solution was determined to lie outside of it. We used a convergence tolerance of 10−6 on the function value, and used full precision 64-bit floating point numbers in all computations (which are cheap and can be done on the CPU). Note that for certain simple special cases, such as when m(ψ) = ψD, closed form solutions can be used if desired. 23. Application to various modified ResNets In this section we will discuss the very commonly used ResNet architecture (He et al., 2016a,b) and certain modified versions of it, and then go over the application of DKS to these different versions. In addition to being instructive in the application of DKS, these example will be the primary focus of our later experiments. 23.1 Standard ResNet-V2 architectures and terminology In this work we will only consider the “V2” version of the ResNet architecture (He et al., 2016b) as the opposed to the “V1” version (He et al., 2016a), as the former is conceptually simpler and is usually preferred by practitioners. We will also concentrate on the version of ResNet-V2 designed specifically for use in 224x224 MNIST classification, noting that 66
Deep Kernel Shaping versions of the architecture for other problems and datasets can differ, especially in terms of their first and last few layers. We will denote by D the “depth parameter” of the ResNet architecture, which corre- sponds to the total number of nonlinear layers plus 1. The standard values for D are 50, 101, and 152. The input is assumed to be 224x224 features maps with 3 dimensional pixel features (possibly extended to 4 dimensions if PLN is used as per Section 10.2). This is then fed into a 7x7 convolutional layer with 64 output channels and a stride of 2. Following this is a 3x3 max-pooling layer with a stride of 2. These two early layers are particular to the MNIST version of ResNet-V2, and have the purpose of reducing the dimension of the feature representation to a smaller size for processing by subsequent layers. Following this is a long sequence of residual blocks that form the large bulk of the network. Each of these is parameterized by an associated output channel dimension, a “bottleneck” channel dimension, and a stride, which can differ from block to block. The particular values for these parameters are determined by D. Let k be the input channel dimension, d the output channel dimension, b the bottleneck channel dimension, and s the stride associated with a particular residual block. The residual block contains two “branches” from its input that get summed together at the output. The first is called the residual branch , and consists of the following sequence of layers: a Batch Normalization (BN) layer, a RELU nonlinear layer, a 1x1 convolutional layer with stride 1 and output channel dimension b, a BN layer, a RELU nonlinear layer, a 3x3 convolutional layer with stride s and output channel dimension b, a BN layer, a RELU nonlinear layer, and finally a 1x1 convolutional layer with stride 1 and output channel dimension d. The second branch is called the shortcut branch , and consists of the identity map if k = d and s = 1, or a 2x2 max-pooling layer if k = d and s > 1. Otherwise, if k (cid:54)= d (which is the case for transition blocks), it consists of the following sequence of layers: a BN layer, a RELU nonlinear layer11, and a 1x1 convolutional layer with stride 1 and output channel dimension d. The shortcut branch is meant to act as an identity function or a reasonable approxi- mation to one, except when its input and output channel dimensions differ (which is only the case for transition blocks). Meanwhile, the residual branch, which always contains 3 nonlinear layers, is what performs the interesting nonlinear computation in the network. After the sequence of residual blocks, there is a BN layer and a RELU nonlinear layer, followed by a “global” mean-pooling layer which reduces the number of locations down to 1. The final layer of the network is a 1x1 convolutional layer operating on this single location (or equivalently a fully-connected layer), whose output channel dimension is the number of classes. Convolutional layers in ResNets typically do not have bias parameters, since these are made pointless by the mean-subtraction done by the BN layers that always immediately follow them. To compensate for this, BN layers will sometimes include trainable gain and/or bias parameters applied after their centering and normalization operations. 11. This nonlinear layer in the shortcut branch does not contribute to the total number of nonlinear layers for the purposes of computing D. Moreover, it can be identified with the first nonlinear layer of the residual branch (as they compute the same thing), in which case both the shortcut branch and residual branches can be seen as “branching off” from this layer’s output (instead of from the block’s original input). 67
Martens et al. 23.2 Modified ResNet architecture In this subsection we will describe the particular changes we made to the ResNet-V2 ar- chitecture in order to conform to the requirements listed in Section 21.1 and thus achieve compatibility with DKS. These changes don’t perfectly preserve the model class, although we tried to make them as innocuous as possible in order to facilitate the fairest comparison to standard ResNets in our experiments. Note that other modification schemes are possible, and the one we present here is not meant to be in any way “canonical” for this or any other architecture. As BN layers are incompatible with DKS we elect to remove them, while adding learnable bias parameters (initialized at zero) back into the convolutional layers. Another option would be to replace BN layers with Layer Normalization layers, as the latter are compatible with DKS. As discussed in Section 18.6, while technically supported, RELU activations force us to use a diminished version of DKS. Thus in our main experiments we often used alternative activation functions instead, including ones with a “RELU-like” shape, such as softplus. Max-pooling layers are provisionally supported by DKS, especially if they occur early in the network and have a relatively small kernel size. (See Section 18.6 for more details about this.) The 3x3 max-pooling layer near the beginning of the network meets these criteria, and so we elect to leave it in. 23.3 Further modifications made by DKS Having achieved compatibility by making the above changes, we can now apply DKS to the resulting modified ResNet architecture. In this subsection we will describe the subsequent changes made to the network as part of the execution of DKS itself , whose steps are outlined in Section 21.2. Note that some of these steps are optional, or involve degrees of freedom, and are all designed to preserve (or slightly expand) the model class. First, the mean-pooling layer near the end of the network can optionally be replaced by a weighted mean-pooling layer (as described in Section 20.1.2). While this replacement is necessary for the Q/C map computations to make sense, we found that it didn’t significantly improve optimization performance in our preliminary experiments, and so we didn’t do it in our main ones. One possible explanation for this finding is that because there is only a single nonlinear layer after the mean-pooling layer, the non-uniform q values produced by the latter can have only a limited effect on the network’s overall C map. Next, we must replace the sum operations, which occur in ResNets at the end of each residual block (where the residual and shortcut branches are combined together), with normalized sums. Each normalized sum involves two weights and one constraint (that the squares of the weights sum to 1), and so has one degree of freedom. A natural choice is √ to set both weights to 1/ 2, which naively seems like the best option for reproducing the behavior of an unmodified ResNet. However, as we will discuss in Section 26.6, for networks that forgo normalization layers and/or use bounded activation functions (as our modified ResNets do), placing more weight on the shortcut branch will result in better behavior that more closely matches that of a standard ResNet. This is confirmed in our experiments in Appendix M.1. 68
Deep Kernel Shaping The final modification made to the network as part of DKS is to replace all of the activation functions with their transformed versions, as described in Step 9 of Section 21.2. 23.4 Computing the maximal slope function for modified ResNets Having described the modifications we made to ResNets to achieve compatibility with DKS, and the further ones made by DKS itself, we are now in a position to compute the maximal slope function following the recipe in Section 21.3. For simplicity, we will assume that the normalized sums at the end of the residual blocks √ each have a weight of w on their residual branches. (A weight of 1 − w2 on the shortcut branches is then implied.) The subnetwork before the sequence of residual blocks is just a affine and max-pooling layer and so has a slope polynomial of 1. Consider any non-transition block. The slope polynomial for the residual branch is ψ3, as it has 3 nonlinear layers, and is 1 for the shortcut branch. Thus, the overall slope polynomial for the block is w2ψ3 + (1 − w2). Similarly, the slope polynomial for a transition block (which has a single combined layer in its shortcut branch) is w2ψ3 + (1 − w2)ψ. The subnetwork after the sequence of blocks consists of nonlinear layer, a (possibly weighted) mean-pooling layer, and then an affine layer, and so has a slope polynomial of ψ. Noting that the total number of residual blocks is (D − 2)/3, and the number transition blocks is 4 for all values of D, the overall slope polynomial for the network f (which has a single input and output and so is a subnetwork of itself) is p (ψ) = (w2ψ3 + (1 − w2))(D−2)/3−4(w2ψ3 + (1 − w2)ψ)4ψ, f which can be simplified to p (ψ) = (w2ψ3 + (1 − w2))(D−14)/3(w2ψ2 + (1 − w2))4ψ5. (20) f The only subnetworks of f that don’t compose with other subnetworks to form larger ones are the residual branches, and so their slope polynomials are the only other ones to consider when computing the maximal slope function µ(ψ). As they are simple compositions of layers with 3 (or 2) nonlinear layers total, their slope polynomials are ψ3 (or ψ2). Noting that ψ3 (or ψ2) is a factor of p (ψ), we may ignore them when computing the maximum, f and thus conclude that µ(ψ) = p (ψ). f It is worthwhile to note the dependency of µ(ψ) on the value of the residual branch weight w. For w = 0 we have µ(ψ) = ψ5, and for w = 1 we have µ(ψ) = ψD−1. More generally, µ(ψ) will be a degree D − 1 polynomial in ψ, where the coefficients (which must sum to 1) will more heavily favor high order terms as w approaches 1, and low-order terms as w approaches 0. Thus, much like ψ, w can be thought of as controlling the overall “degree of nonlinearity” of the network f (as quantified by C(cid:48) (1)). f 23.5 Equivalent standard convolutional networks To help demonstrate the power of DKS in our experiments, we will consider a skip-connection- free convolutional network architecture obtained from the above modified ResNet-V2 ar- chitecture by simple removal of the shortcut branches. The resulting architecture retains 69
Martens et al. the channel dimensions, strides, etc., of the original ResNet architecture, including it use of “bottleneck” layers in the residual branches, but is otherwise a standard deep convolutional network. Given the straightforward sequential structure of this architecture, with its D − 1 non- linear layers, its network polynomial and maximal slope function are simply ψD−1 (which corresponds to the w = 0 case above). 23.6 “Wide” ResNet variants for CIFAR-10 For our experiments involving the CIFAR-10 dataset (Krizhevsky and Hinton, 2009) we will make use of Wide Residual Networks (Zagoruyko and Komodakis, 2016), which are a well-known variant of the standard ResNet architecture. The Wide-ResNets we used in our experiments differ from standard ResNets in the following ways: • The initial subnetwork before the sequence of residual blocks consists of just a 3x3 convolutional layer with 16 output channels and a stride of 1. There is no max-pooling layer. • Given per-block parameters s and d, a residual branch consist of the following se- quence: a BN layer, a RELU nonlinear layer, a 3x3 convolutional layer with stride s and output channel dimension d, a BN layer, and RELU nonlinear layer, and finally a 3x3 convolutional layer with stride 1 and output channel dimension d. Note that there are only 2 nonlinear layers instead of the 3 normally present in standard ResNets. • There is a global “width” parameter which acts as multiplier on the output channel dimensions of all the residual blocks. In our experiments this was set to 2. • The scheme for mapping D to a configuration for the residual blocks is generalized to work with any value of D such that D − 4 is divisible by 6. Here, D represents the number of nonlinear layers plus 3, so that there are (D − 4)/2 total blocks, 3 of which are transition blocks. As we did for standard ResNets, to achieve compatibility of Wide-ResNets with DKS we will modify the architecture by removing the BN layers, adding back in learnable biases to the convolutional layers, and (possibly) replacing the RELU activation functions with various alternatives. Following a similar derivation to the one in Subsection 23.4, the maximal slope function for these networks is given by µ(ψ) = (w2ψ2 + (1 − w2))(D−10)/2(w2ψ + (1 − w2))3ψ4, √ where like before we have assumed weights w and 1 − w2 for the weighted sum operation at the end of each residual block. We can also define a skip-connection-free version of this architecture by removing the shortcut branches. The maximal slope polynomial associated with such a network is ψD−3, as there are D − 3 total nonlinear layers. 70
Deep Kernel Shaping Part IV Additional analysis of DKS and related methods 24. Neural Tangent Kernel analysis Recent advances in the theoretical understanding of neural network training have shown that highly overparameterized networks behave like linear functions of their parameters over the entire course of training by gradient descent (Jacot et al., 2018; Li and Liang, 2018; Du et al., 2019b,a; Allen-Zhu et al., 2019; Arora et al., 2019). This analysis works by approximating the network function by its own 1st-order Taylor series with respect to its parameters (centered at their initial values), and then showing that the parameters remain close enough to their initial values throughout training that the approximation remains a good one. Under this approximation, which becomes exact as the width of each layer goes to infinity, training a neural network with gradient descent resembles kernel regression, with a kernel12 known as the Neural Tangent Kernel (NTK) that is computed from the network’s Jacobian at initialization time. This enables one to accurately predict the functional form of the trained network, and precisely characterize the rate of convergence to this solution by gradient descent. While this type of analysis has been extended to exact and approximate natural gradient descent methods (Zhang et al., 2019b; Cai et al., 2019; Karakida and Osawa, 2020), we will only consider the gradient descent version in this work. Even though real networks trained on challenging datasets like MNIST are typically not wide enough to satisfy the formal requirements of NTK theory (especially when random dataset transformations are employed), the setting where this 1st-order Taylor approxima- tion works well – known colloquially as the “NTK regime” – may still serve as a rough analogy to more realistic training. It is thus interesting to consider what effect the network’s architecture, activation functions, and initialization has on the NTK, and what this says about training in the NTK regime. In this section we will review the basics of NTK theory, characterize the NTK in terms of the properties of the network’s C map, and show how the C map degeneration which happens naturally in deep networks (as shown in Section 12) leads to a form for the NTK which implies very slow optimization and/or very poor generalization. We will then show how the form of the NTK under DKS, assuming a reasonable choice for the global slope bound ζ, is much nicer, and leaves open the possibility of fast optimization and good generalization. (Although actually proving that it necessarily leads to these things would require assumptions on the dataset, and is beyond the scope of this work.) 24.1 Assumptions of this analysis For the remainder of this section we will assume that the network in question is a standard feed-forward MLP comprised of D fully-connected combined layers, where the last such 12. Note that the Neural Tangent Kernel is related to but distinct from the kernels we have been analyzing in this work so far. 71
Martens et al. layer has an identity activation function. We will represent the network as the function f (x, θ) for input vector x and parameter vector θ. For notational simplicity we will assume that the network’s output dimension is 1. The parameter vector θ will be split across layers into D segments denoted θ for i = √ i 1, 2, . . . , D. Each θ corresponds to d W (as opposed to W itself), where is W is the i i i i i weight matrix for layer i, and d its input dimension. This non-standard parameterization, i which is known as the NTK parameterization , is what we apply gradient descent on, and is required for the NTK to have its desired properties. We will not consider bias parameters in this analysis. The training set will consist of n input-output pairs (x , y )n satisfying (cid:107)x (cid:107)2 = d i i i=1 i 0 for all i (where d is the network’s input dimension), and the objective function used to 0 train the network will be the standard precision: 1 (cid:80)n (y − f (x , θ))2. We will 2 i=1 i i denote by θ(t) (or θ (t)) the parameters at iteration t of optimization. θ(0) (or θ (0)) will i i denote their random initial value, which is determined by a Gaussian fan-in initialization applied to the standard parameters (i.e. the original W ’s). i In addition to our global assumption that the activation functions are infinitely differen- tiable everywhere except for a finite set of points, we will also assume that they are Lipshitz continuous, which is required in order to apply the results in Jacot et al. (2018). 24.2 NTK definition The Neural Tangent Kernel (NTK) is given by Θ(x, x(cid:48)) = (cid:80)D Θ (x, x(cid:48)), where i=1 i Θ (x, x(cid:48)) ∈ R denotes the inner-product i (cid:42) ∂f (x, θ) (cid:12) (cid:12) ∂f (x(cid:48), θ) (cid:12) (cid:12) (cid:43) (cid:12) , (cid:12) . ∂ θ (cid:12) ∂ θ (cid:12) i θ=θ(0) i θ=θ(0) Given the NTK Θ(x, x(cid:48)) and training dataset (x , y )n , the NTK matrix K ∈ Rn×n is i i i=1 defined by   Θ(x , x ) Θ(x , x ) · · · Θ(x , x ) 1 1 1 2 1 n .  .  K i =     Θ(x 2 . . ., x 1) Θ(x 2, x 2) . . . .     . Θ(x , x ) · · · Θ(x , x ) n 1 n n We can similarly define the per-layer NTK matrix K ∈ Rn×n for layer i by i   Θ (x , x ) Θ (x , x ) · · · Θ (x , x ) i 1 1 i 1 2 i 1 n .  .  K i =     Θ i(x . . .2, x 1) Θ i(x 2, x 2) . . . .     , Θ (x , x ) · · · Θ (x , x ) i n 1 i n n noting that K = (cid:80)D K . i=1 i 72
Deep Kernel Shaping 24.3 Training in the NTK regime: a brief review There are various NTK-type results that bound the convergence rate of gradient descent in the case of finite width layers (e.g Du et al., 2019b). However, such results are complicated to prove, and seem to be fairly pessimistic in terms of the rate of convergence13 they predict, and the width they require. On the other hand, the situation simplifies considerably in the limit of infinite width (for all layers but input and output ones, whose width is fixed), and quite simple and elegant expressions exist for both the convergence rate of gradient descent, and the function computed at the converged solution (Jacot et al., 2018). While the infinite width limit is unrealistic, and totally ignores how kernel approximation error affects the theoretical predictions, we will nonetheless use it in our analysis for the sake of simplicity and clarity. 24.3.1 Notation and basic results Before we begin we must define some additional notation. Let       y f (x , θ(t)) Θ (x, x ) 1 1 i 1  y 2   f (x 2, θ(t))   Θ i(x, x 2)  y =   . .   ∈ Rn, f (t) =   . .   ∈ Rn, k i(x) =   . .   ∈ Rn,  .   .   .  y f (x , θ(t)) Θ (x, x ) n n i n and define k(x) = (cid:80)D k (x). i=1 i In the infinite width limit, provided that K is positive definite (i.e. non-singular), a standard result of NTK theory is that the t-th iterate θ(t) produced by gradient descent (with learning rate η) satisfies f (x, θ(t)) = k(x)(cid:62)K−1(I − (I − ηK)t)(y − f (0)) + f (x, θ(0)) (21) for all valid x. If 0 < η (cid:54) 1/λ (K), where λ (K) denotes the i-th largest eigenvalue of K, 1 i then θ(t) converges to some θ(cid:63). At this solution, the form of f is given by taking t → ∞ in the above equation, yielding: f (x, θ(cid:63)) = k(x)(cid:62)K−1(y − f (0)) + f (x, θ(0)). (22) 24.3.2 Convergence behavior on the training set Observing that k(x )(cid:62) is the i-th row of K, we can “stack” both sides of Equation 21 to i obtain f (t) = KK−1(I − (I − ηK)t)(y − f (0)) + f (0) = y − f (0) − (I − ηK)t(y − f (0)) + f (0) = y − (I − ηK)t(y − f (0)) n (cid:88) = y − (1 − ηλ (K))t(v(cid:62)(y − f (0)))v , i i i i=1 13. While these results predict exponential convergence, the associated rate constants are close enough to 1 that convergence requires a prohibitively large number of iterations. 73
Martens et al. where v denotes the eigenvector of K corresponding to the eigenvalue λ (K). i i Plugging this expression into the objective function and using the fact that the v ’s are i mutually orthogonal gives the following expression for the training loss: n n 1 (cid:88) 1 1 (cid:88) (y − f (x , θ ))2 = (cid:107)y − f (t)(cid:107)2 = (1 − ηλ (K))2t(v(cid:62)(y − f (0)))2. 2 i i t 2 2 i i i=1 i=1 When 0 < η (cid:54) 1/λ (K), this expression converges to 0 which implies that θ(cid:63) is indeed a 1 global minimizer of the objective. Moreover, if we employ early stopping, then directions in function space corresponding to eigenvectors with with smaller eigenvalues in K will have converged less than the others. As observed by Jacot et al. (2018), this may help explain early stopping’s regularization benefits. A complete picture of the convergence of the objective requires us to know the entire spectrum of K and the coefficients v(cid:62)(y − f (0)). However, assuming that (v(cid:62)(y − f (0)))2 i n is significantly large, the convergence speed will tend to (1 − ηλ (K))2t asymptotically. n With the optimal learning rate of η = 1/λ (K) this becomes (1 − 1/ cond(K))2t, where 1 cond(K) = λ (K)/λ (K) is the condition number of K. 1 n 24.3.3 Training only certain layers If we only optimize layer i, we may replace K by K and k(x) by k (x) in the above i i formulas (provided that it is positive definite) in order to obtain a description of the resulting convergence. As before, the training error will converge to zero at a speed determined by the eigenvalues of K . An analogous statement is also true if we optimize an arbitrary i (cid:80) (cid:80) subset S of the layers, in which case we replace K by K and k(x) by k (x). i∈S i i∈S i Note that because we are assuming infinitely wide layers there is no paradox here; each layer has enough capacity to memorize the training data entirely by itself. The form of the NTK matrix allows us to gain insight into the relative contribution of each layer to the overall solution. A layer whose per-layer NTK matrix is much smaller14 than the other layers will have much smaller gradients, and the changes to its weights made during training will have a much smaller effect on the overall solution. While training any single layer is sufficient to achieve zero error in the infinite width case, what this arguably means for realistically sized networks is that layers with very small per-layer NTKs will train much slower than other layers. 24.4 An elegant expression for the limiting NTK using C maps The inner-product which defines the per-layer NTK Θ (x, x(cid:48)) is a random variable that i depends on the random initial value θ(0) of the parameters θ. In the limit as the width of the network goes to infinity, Θ (x, x(cid:48)) converges in probability to a deterministic function i in much the same way that the network’s kernel function does. Indeed, an approximation result directly analogous to Theorem 1 exists for the NTK (Arora et al., 2019, Theorem 3.1). As we are performing our analysis in the infinite width limit, we will take Θ (x, x(cid:48)) to i be this limiting value going forward. 14. By “smaller” we mean that a PSD matrix A is smaller than B, written A ≺ B, if B − A is positive definite. 74
Deep Kernel Shaping Let g be the subnetwork that maps the network’s input to the input of the i-th combined i layer (which is the output of the (i − 1)-th combined layer when i (cid:62) 2). Jacot et al. (2018) show that D Θ (x, x(cid:48)) = (cid:2) κ (Σ )(cid:3) (cid:89) (cid:104) E [φ(cid:48) (u)φ(cid:48) (u)(cid:62)](cid:105) , i (cid:102)gi x,x(cid:48) 1,2 u∼N (0,κ (cid:103)gj (Σ x,x(cid:48))) j j 1,2 j=i where we note that κ (Σ ) = x(cid:62)x(cid:48)/d (since g is the identity), and that the quantities (cid:102)g1 x,x(cid:48) 0 1 inside of [·] are 2 × 2 matrices (so that [·] extracts their top corner entry). 1,2 1,2 Let f represent the i-th combined layer of the network, q its output q value (with q = 1 i i 0 being the q value for the network’s input), and φ its activation function. By Equations 7, i 10, and 11, we can write the above expression for the NTK as D (cid:89) Θ (x, x(cid:48)) = q C (c ) Γ (C (c ), q , q ) i i−1 gi 0 φ(cid:48) j gj 0 j−1 j−1 j=i D = q C (c ) (cid:89) q j C(cid:48) (C (c )) i−1 gi 0 q fj gj 0 j−1 j=i D (cid:89) = q C (c ) C(cid:48) (C (c )), D+1 gi 0 fj gj 0 j=i where c ≡ x(cid:62)x(cid:48)/d is the c value for the network’s input (recalling that (cid:107)x(cid:107)2 = (cid:107)x(cid:48)(cid:107)2 = d 0 0 0 by assumption). Denote by h the subnetwork that maps the input of f to the network’s final output. i i Since we have h = f ◦ f ◦ · · · ◦ f it follows that C = C ◦ C ◦ · · · ◦ C , and so i D D−1 i hi f D f D−1 fi by the chain rule we have C(cid:48) (C (c )) = (cid:81)D C(cid:48) (C (c )). Plugging this into the above hi gi 0 j=i fj gj 0 equation we arrive at the elegant formula Θ (x, x(cid:48)) = q C (c )C(cid:48) (C (c )). (23) i D gi 0 hi gi 0 While this formula has only been proven for deep MLPs (consisting of a composition of a sequence combined layers), we conjecture that it holds for more general architectures. 24.5 The form of the NTK matrix given a degenerate C map and implications for gradient descent training In this subsection we will consider the situation where a deep network f has a “degenerate” C map C that sends nearly all input c values to a small region around some value c∗ f (in the sense of Section 12), and argue that this implies slow optimization and/or poor generalization in the NTK regime. This analysis can be seen as a more rigorous version of the intuitive argument given in Section 12.4, and overlaps with the results of Xiao et al. (2020). 75
Martens et al. 24.5.1 Additional assumptions of this analysis To simplify the discussion, we will assume that each combined layer has the same activation function (except the last one, which is required to be linear), which means that the network’s C map C is just the composition of D − 1 copies of some local C map C . We will further f assume that C is itself “well-behaved” in the sense that C(cid:48)(1) is reasonably close to 1, so that the overall C map C is degenerate only because D is large. Moreover, any sufficiently f “deep” subnetwork of f is also degenerate, and any sufficiently “shallow” subnetwork is well-behaved. Additionally, we will assume that q = 1 (without loss of generality), and that there are D no two distinct inputs x and x(cid:48), from either the training or test set, for which x(cid:62)x(cid:48)/d is 0 very close to 1 or −1 (which would imply that either x ≈ x(cid:48) or x ≈ −x(cid:48) given our previous assumption that (cid:107)x(cid:107)2 = (cid:107)x(cid:48)(cid:107)2 = d ). 0 24.5.2 NTK matrix estimates As in Section 12.4 there are two main cases to consider for c∗: the “collapsing case”, where c∗ = 1, and the “exploding case”, where 0 (cid:54) c∗ < 1 with c∗ (cid:54)≈ 1. For the collapsing case we must have C(cid:48)(1) (cid:54) 1 since c∗ = 1 is an attractive fixed point of C. And for the exploding case we must have that 1 is a non-attractive fixed point (since C can only have one such point by Proposition 10), and so C(cid:48)(1) > 1. For the layer index i there are three cases to consider: i is small so that the layer is “early” in f , D − i is small so that the layer is “late” in f , and the default case where neither i nor D − i are small, so that the layer is the “middle” of the network. The following table gives estimates of the layer-wise NTK matrix for each combination of cases. These estimates are computed in Appendix H.1 using a semi-rigorous style of argument. The results of these computations have been checked numerically for the case of RELU and Erf activation functions (whose C maps have convenient analytic forms). Here, the symbol E denotes the matrix of 1’s. Type of degener- Early layers Middle layers Later layers ation Collapsing case K ≈ 0 K ≈ 0 K ≈ C(cid:48)(1)D−iE i i i (c∗ = 1) w/ C(cid:48)(1) < 1 Collapsing case K ≈ I K ≈ I + α (E − I) K ≈ E i i i i (c∗ = 1) w/ where 0 = α (cid:54) 1 C(cid:48)(1) = 1 α (cid:54) · · · (cid:54) α = 1 2 D (Observed em- pirically, and conjectured to be true in general.) Exploding case (0 (cid:54) K ≈ C(cid:48)(1)D−iI K ≈ C(cid:48)(1)D−iI K ≈ C(cid:48)(1)D−iI + i i i c∗ < 1, c∗ (cid:54)≈ 1) w/ (very large) (very large, but still c∗C(cid:48)(c∗)D−i(E − I) C(cid:48)(1) > 1 much smaller than (not very large) for early layers) 76
Deep Kernel Shaping From the above values we can compute an estimate of the overall NTK matrix. This is given the following table, which is computed in Appendix H.2: Type of degeneration Overall NTK matrix Collapsing case (c∗ = 1) w/ C(cid:48)(1) < 1 K ≈ 1 E 1−C(cid:48)(1) Collapsing case (c∗ = 1) w/ C(cid:48)(1) = 1 K ≈ D(I + α¯(E − I)) for some 0 (cid:54) α¯ (cid:54) 1 (Observed empirically for deep RELU net- works with α¯ = 1/4, and for other networks with α¯ = 1/3. Conjectured to be true in general.) Exploding case (0 (cid:54) c∗ < 1, c∗ (cid:54)≈ 1) w/ K ≈ 1−C(cid:48)(1)D I + c∗ (E − I) 1−C(cid:48)(1) 1−C(cid:48)(c∗) C(cid:48)(1) > 1 24.5.3 Implications for speed and generalization of gradient descent training There are several implications for gradient descent training that we can infer from the above estimates, all of which are bad. Firstly, in the collapsing case with C(cid:48)(1) < 1, and in the exploding case, the magnitude of the per-layer NTK matrices differ substantially over the network. This means that the layers whose per-layer NTKs are not amoung the largest will train very slowly. Given that such layers are only a small fraction of the total, this implies that only a few layers of the network will have the potential to train quickly. While this is technically sufficient to minimize the training loss in the NTK regime, in practice, our networks often won’t be highly overparameterized to the extent required by NTK theory, and so we actually will need to train all of the layers in order to fit the dataset. Insofar as the NTK regime is an analogy to this more realistic setting, this analysis thus predicts slow training. Secondly, in the collapsing case with C(cid:48)(1) < 1, we have that the per-layer and overall NTK matrices are approximately rank 1, which implies that they have a very high condition number. This means neither the individual layers, nor the overall network, will train quickly, and so the training loss will take a very long time to be minimized no matter what subset of layers we elect to train. Finally, in all cases we have that the approximate form of the per-layer and overall NTK matrices does not depend on the input training data. Additionally, we have that the vector k(x) does not depend on the training data, since by the derivations in Appendices H.1 and H.2, Θ (x, x(cid:48)) doesn’t depend on x or x(cid:48) (except to detect when x ≈ x(cid:48) or x ≈ −x(cid:48)). And i f (0) also won’t depend on the training data, since it will either look like a multiple of the ones vector in the collapsing case, or a completely random vector in the exploding case. It thus follows from Equation 22 that the predictions made by the fully trained network for a test point x will not actually depend on the input training data in any significant way, making it impossible for the network to generalize. 24.5.4 Comparison to the results of Xiao et al. (2020) The results of this subsection overlap with those of Xiao et al. (2020), who derive approxi- mations to the overall NTK matrix for deep networks (although not for individual layers) 77
Martens et al. using a different style of argument. Their results mostly agree with ours, except that for the case C(cid:48)(1) = 1 they predict a universal value of α¯ = 1/3 (whereas we observe α¯ = 1/4 for deep RELU networks), and for C(cid:48)(1) = 1 they estimate the second (and less significant) term of K to be 1 (E − I), whereas we predict c∗ (E − I). Numerical studies we 1−C(cid:48)(c∗) 1−C(cid:48)(c∗) performed on the Q/C maps of deep RELU/erf networks seem to confirm our predictions in these cases. 24.6 The form of the NTK under DKS The following theorem is proved in Appendix H.3 using Theorem 13 and Equation 23. Theorem 27 Suppose that Θ is the per-layer NTK (for layer i) of a network conforming i to the assumptions of Section 24.1 which has been transformed using DKS with global slope bound ζ. Then we have (cid:12) (cid:12) (cid:12) (cid:12)Θ i(x, x(cid:48)) − 1 x(cid:62)x(cid:48)(cid:12) (cid:12) (cid:54) 11(ζ − 1). (cid:12) d (cid:12) 0 The bound in this theorem establishes that each per-layer NTK matrix K converges to i the training data Gram matrix X(cid:62)X/d as ζ approaches 1, where X = (cid:2) x x · · · x (cid:3) . 0 1 2 n It also allows us to reason about larger values of ζ to a limited extent, although it arguably only becomes interesting when ζ < 1 + 1 . (We suspect that with a tighter and/or more 11 detailed analysis, interesting statements about the relationship of ζ and the layer-wise NTK could be made for larger values of ζ.) If X(cid:62)X/d is low rank, which it will be in the common case that dim(x) < n, this 0 means that the K will approach a low-rank matrix as ζ approaches 1, which corresponds to slow/impossible training under gradient descent. Intuitively this makes sense, since a value of ζ very close (or equal) to 1 corresponds to a network that looks almost perfectly linear at initialization time (by Theorem 13), and thus could fail to properly train as per the discussion in Section 14.2. Indeed, the foundational works on NTK only predict that the NTK will be positive definite (i.e. full-rank) when the activation functions are non- polynomial (and thus nonlinear) functions, and DKS makes them approach linear functions as ζ → 1. So while a value of ζ very close to 1 is clearly a bad choice, a value somewhat close to 1 (such as 1.5; which we use in most of our experiments) will allow K to retain some of the structure of X(cid:62)X/d , thereby ensuring that the network’s prediction (given in Equation 0 22) depends on the training data and thus has the potential to generalize. It will also allow K to deviate enough from X(cid:62)X/d to be full rank with a potentially small condition 0 number (which would imply fast training). Unfortunately, cond(K) is difficult to accurately estimate without full knowledge of both X(cid:62)X/d and the behavior of the C map over its 0 entire domain, and existing methods to bound cond(K) (e.g. Du et al., 2019b) seem unlikely to produce useful results in our context. We leave the problem of accurately estimating the value of cond(K) under DKS to future work. 78
Deep Kernel Shaping 25. Variance propagation, signal propagation, and their relationship to approximate kernel analysis Many previous methods for constructing and initializing neural networks are justified using analysis frameworks which attempt to characterize the initialization-time behavior of neural networks. The two most prominent examples of such frameworks are “variance propagation” and “signal propagation”. In this section we review these frameworks, highlight certain mathematical issues with them, and provide counterexamples to their general claims where possible. We also relate them and their predictions to the kernel approximation framework underlying DKS, and advocate for the latter as a more powerful and mathematically rigorous alternative. 25.1 The original variance propagation analysis of LeCun et al. (1998b) The earliest such analysis that we are aware of appeared in LeCun et al. (1998b, Section 4.6), which we will call “variance propagation”. It is based on the idea of computing the per-unit variance for each layer as a function of the per-unit variance of the previous layer, where the underlying distribution is over training cases. Typically, all units within a layer will have the same variance, and so only one scalar needs to be “propagated”. For fully-connected layers, LeCun et al. (1998b) argue that the variance for a particular output unit is equal to the (cid:96) -norm of that unit’s vector of weights, multiplied by the per- 2 unit variance of the input. For this to hold, they require that the input units are uncorrelated with each other and have the same variance. For nonlinear layers they assume the use of the activation functions that approximately preserve the mean and variance of their inputs. As an example, they give a transformed tanh activation function which resembles the identity function within a prescribed range of “typical” inputs around 0. Assuming that each weight vector has a norm of approximately 1, and that the training input data is whitened, one might try to apply this single-layer argument recursively to all layers of the network, starting from the input. Unfortunately, this doesn’t seem to work. While a whitening transform applied to the training data ensures that the input units of the first layer are uncorrelated with variance 1, uncorrelatedness will fail to hold for the output of this layer, making it impossible to apply the same argument for subsequent layers. As no assumption is made about the network’s parameters, beyond that the weight vectors must have a norm of ∼1, one can design a counterexample where these variance computations break down after the first fully-connected layer. For example, consider a linear neural network with a 1-dimensional input x, a 2-dimensional hidden layer, and a 1-dimensional output y, defined by the equations 1 1 h = x, h = −x, and y = √ h + √ h . 1 2 1 2 2 2 The input weight vector for each unit has norm 1, and yet y = 0 for all x, so that the network will not preserve the per-unit variance of x. (Intuitively, this is because h and 1 h have strong negative correlation.) Note that this example can easily be generalized to 2 arbitrarily wide layers. 79
Martens et al. Another more subtle issue with the analysis in LeCun et al. (1998b) is that the ap- proximation errors arising from the analysis of nonlinear layers can easily accumulate with depth, and may push the activation functions out of their assumed range of inputs. 25.2 Klambauer et al.’s (2017) modified variance propagation Klambauer et al. (2017) present a modified version of LeCun et al.’s (1998b) variance propagation analysis, which would seem to address the issues we’ve highlighted. They do this by arguing that as long as the inputs to a fully-connected layer are independent, its outputs (which are fixed linear combinations of its inputs as determined by the weights) will be approximately Gaussian distributed, thanks to the Central Limit Theorem (CLT) and the assumption of wide layers. Using this approximation they then compute the moments of the subsequent nonlinear layer using Gaussian integrals (similar to those that define Q maps), without having to make any strong assumptions on its activation function. Unfortunately, CLT is not actually applicable to arbitrary weighted sums of variables, even when those variables are perfectly iid. For example, if the weights of the sum are (1, 0, . . . , 0), then the output will have the same distribution as the first input unit, which won’t be Gaussian in general. Even if we somehow ruled out such weight vectors as having “low probability”, and focused only on weight vectors for which CLT would apply, there would still be major difficulties to overcome. Firstly, since we need to show that the output units of a fully-connected layer are approximately independent (in order to recursively apply the same analysis to subsequent layers), we would need to show that they are jointly Gaussian distributed with a diagonal covariance matrix. This would require the use of one of the multi-dimensional versions of CLT, all of which require significant additional hypotheses compared to the standard one-dimensional versions. Secondly, the approximate independence provided by CLT would not be sufficient to recursively apply the same argument to subsequent layers, as CLT typically requires exact independence of the variables under summation15. Thirdly, because CLT requires that the number of variables under summation is large, it usually won’t be applicable to the first layer of the network (where the input dimension is a fixed property of the training data). 25.3 The version of variance propagation in Glorot and Bengio (2010) Glorot and Bengio (2010) present a modified version of LeCun et al.’s (1998b) variance propagation analysis, which has formed the basis of many subsequent analyses over the years. The first change they make is to compute per-unit variances with respect to the joint distribution on network inputs and parameters (as opposed to just the inputs). Their second modification is to directly assume that the network’s activation functions behave like the identity function over typical inputs, thus implying that they preserve the mean and variance of their inputs. 15. In an attempt to preempt this criticism, Klambauer et al. (2017) refer to Bradley (1981), which proves a version of CLT that relaxes the independence assumption. However, this result assumes a very specific type of weak dependence which is unlikely to satisfied in this setting, and also only applies for one- dimensional variables. 80
Deep Kernel Shaping Assuming that the weights of a given fully-connected layer are iid with mean zero and variance σ2, and that its input units are mean zero with variance v, they show that the per-unit variance of the layer’s output is simply kσ2v, where k is the input dimension. Notably, by computing variances with respect to training cases and parameters, they do not require the input units to a layer to be uncorrelated. (Intuitively, this is because the multiplication by the independent mean-zero random weights causes any two random variable to become decorrelated.) This addresses one of the main problems of the original variance propagation analysis, and allows it to be recursively applied over the entire network without issue. Unfortunately, the modification also introduces a new issue not present in prior analysis: the variances no longer refer to any single network (with a particular parameter setting), but rather to a distribution over networks. This makes the interpretation of these variances unclear, and represents a subtle but serious issue in their analysis. One could possibly argue that, with high probability, a single network sampled from this distribution would have variances similar to those computed over the whole distribu- tion. However, this would require additional hypotheses, since otherwise there are simple counterexamples to the general claim. For example, consider a linear network with D (cid:29) 1 fully-connected layers of width 1, where the biases are zero and the weights are sampled iid from N (0, 1). The function computed by this network amounts to just multiplying its input by the product of D scalar weights drawn independently from N (0, 1). Variance propagation would predict that such a network will exactly preserve the variance of its input, or in other words, that the product of these D weights would be approximately 1. However, the distribution of the product of D independent samples from N (0, 1) is highly concentrated around zero for even moderate large values of D (which can be seen via Monte Carlo simulation), despite the fact that the variance of this product is 1. These counterexamples are not restricted to narrow networks either. If, for example, the weights are drawn iid from a heavy-tailed distribution that is highly concentrated around zero and has variance 1/k (where k is the width), then even for large k there will be an overwhelming probability that all the weights will be close to zero, leading to a network which “squashes” its input. This contradicts the prediction made by variance propagation, which is that such a network would approximately preserve the variance of its input. 25.4 Extension of variance propagation to RELUs He et al. (2015) extend the version of variance propagation in Glorot and Bengio (2010) to deal specifically with RELU activation functions, as there is no zero-centered range of inputs for which RELUs resemble the identity function. To do this, they introduce the additional hypotheses that the weights have a symmetric distribution around zero, that the biases are initialized to zero, and that each RELU layer is directly preceded by a fully- connected layer. Given these hypotheses, it follows that the input to each RELU layer is distributed symmetrically around zero, and thus the expected squared valued of a RELU unit will be exactly 1/2 times its input variance. One can then use this expected squared value in place of the input variance for the variance propagation calculation at the next layer, since multiplication by the mean-zero weights of said layer will restore a mean of zero. 81
Martens et al. While it deals with nonlinear layers in a cleaner fashion (at least for RELU networks), this analysis retains the central issue present in Glorot and Bengio’s (2010) analysis, which is that the variances do not necessarily describe the behavior of a single network. More- over, while the variance propagation formulas were originally derived for fully-connected networks, He et al. (2015) also applies them to convolutional networks without any addi- tional justification. (This is problematic since the weight sharing violates the iid weights assumption.) 25.5 Extensions of variance propagation to normalizer-free residual networks Zhang et al. (2019c)16, De and Smith (2020), and Shao et al. (2020)17 adopt Glorot and Ben- gio’s (2010) variance propagation framework to perform an analysis of ResNets (which are described in detail in Section 23.1) without normalization layers. While a mostly straight- forward application of the existing variance propagation formulas, they require an additional one which says that the output variance of a residual block is the sum of the output variances of its two branches. While the formula itself is correct (given the conceits of variance propagation), as far as we can tell it has never been properly justified. Zhang et al. (2019c) attempt to justify it using Var(x + y) = E[Var(y|x)] + Var(x), although this formula is shown to be incorrect by taking x = y . Shao et al. (2020) give a different argument which assumes that the input units to a residual block are uncorrelated (which won’t be true in general), and which treats the weights as fixed instead of random variables (which violates one of the core premises of variance propagation). For reference, we will give an argument here for fully-connected networks. Let x be the input to the residual block and z be the input to the final fully-connected layer of the residual branch, which has weight matrix W . Given that E[W ] = 0 and W is independent of x and z we have Cov(W z, x) = E[W z(x − E[x])(cid:62)] = E[W ]E[z(x − E[x])(cid:62)] = 0, where we have used E[W z] = E[W ]E[z] = 0. From this it follows that Var(W z + x) = Var(W z) + Var(x). 16. Zhang et al. (2019c) claim that their analysis actually describes the case of a constant network input, where only the network’s parameters are random variables. However, all of their variance propagation equations express the per-unit output variance of a layer as a function of its per-unit input variance. As this variance will be zero for the first layer when the network’s input is constant, this claim appears to be unsupported. 17. Technically, Shao et al. (2020) never actually specify what distribution they compute variances over. However, the only interpretation which makes sense, given the majority of their derivations, is that this is the distribution over network inputs and parameters. Despite this, they treat the parameters as fixed in some of their discussions. 82
Deep Kernel Shaping 25.6 Signal propagation (aka mean field analysis) Closely related to variance propagation is an approach for understanding the initialization- time behavior of neural networks commonly referred to as “signal propagation”18 or “mean field analysis” (Poole et al., 2016). In this approach, instead of propagating variances, one propagates per-unit expected squared values , or expected products between corresponding units from two copies of the same network (each fed different inputs). Here, expectations are taken with respect to the distribution on network parameters, and on the two network inputs (which may be correlated). In order to propagate through nonlinear layers, one approximates their input as being Gaussian distributed with mean zero and covariance matrix determined by the expectations from the previous layer. As will be explained in the next subsection, the expectations computed under signal propagation end up being equal to q and m values (or c values, after suitable normalization) as we have defined them in this work. Indeed, Q/C maps were originally derived by Poole et al. (2016) in the context of signal propagation. The mathematical justification of signal propagation given by Poole et al. (2016) in the case of a single fully-connected combined layer is roughly as follows. One starts from the assumption that the k entries of the input vector are iid random variables with expected squared values given by q. Then, multiplication by an m × k random matrix with mean-zero iid entries of variance σ2/k produces m outputs, each of which has bounded variance σ2q, and is a sum of k iid terms. For large k one applies the Central Limit Theorem (CLT) to get that these sums will be approximately iid Gaussian distributed, with mean zero and variance σ2q. It then follows that the entry-wise outputs of the nonlinear layer are approximately iid, with expected squared values given by Gaussian integrals. Expected products are then handled using a straightforward generalisation of this argument. In principle, this single layer argument can be applied recursively to a composition of combined layers, always starting from the hypothesis that the entry-wise inputs to a given layer are iid with some known expected squared value. Unfortunately, this recursive approach runs into the same problems with CLT discussed in the last paragraph of Section 25.2, which cannot be easily repaired19. As with variance propagation, the interpretation of the expectations computed under signal propagation isn’t clear. In particular, there is no obvious relationship between these expectations, and the properties of a single randomly initialized network. Signal propagation’s two main advantages over variance propagation are that it han- dles nonlinearities in a much more general and precise way (via Gaussian integrals), and that it also describes the propagation of expected unit products for correlated network inputs. These features make it a much more powerful framework for understanding the understanding the initialization-time behavior of neural networks, and for designing initial- 18. Note that some works (e.g. De and Smith, 2020) use the term “signal propagation” to refer to certain versions of what we have been calling “variance propagation”. In such works elements of both types of analysis often appear, and the precise distinction between them becomes a bit blurry. 19. To the best of our knowledge, the only mathematically rigorous CLT-based treatment of the width- limiting behavior of random networks is that of Matthews et al. (2018), which is given in the context of approximate kernel analysis. It’s not immediately obvious if/how Matthews et al.’s (2018) arguments can be used to rigorously justify signal propagation. 83
Martens et al. ization schemes. However, as we will discuss next, approximate kernel analysis has the same advantages while also being mathematically rigorous and more clearly interpretable. 25.7 Relationship of variance/signal propagation to approximate kernel analysis When σ2 = 1 (in the notation of the previous subsection), signal propagation’s defining equations for fully-connected combined layers are precisely equivalent the local Q and C maps computed under approximate kernel analysis. We may thus interpret the quantities propagated by signal propagation as q and m values, and their normalized versions as c values. And for other values of σ2, a similar statement holds for a slightly generalized notion of Q/C maps (as given in Poole et al. (2016)). In some sense, this equivalence acts as a mathematical justification of signal propaga- tion’s equations, although with a different meaning for the quantities being propagated. In particular, the expected squared values computed by signal propagation correspond to q values, and can thus be thought of as approximations of dimension-normalized squared norms of the associated feature map’s vectors. Similarly, the expected products computed by signal propagation correspond to m values, and can thus be viewed as approximations of the dimension-normalized inner-product between two such vectors (or the same vector for two different network inputs). Given this relationship between approximate kernel analysis and signal propagation, we can also relate approximate kernel analysis to Glorot and Bengio’s (2010) version of variance propagation (and its extensions). To so see this, note that insofar as the units in each layer have mean zero (under variance/signal propagation’s assumed distribution), their expected squared values are equal to their variances, in which case variance propagation also computes q values. Moreover, even when the means are not zero, as is the case for RELU networks, one can modify variance propagation to deal directly with expected squared values in a manner similar to He et al. (2015). While approximate kernel analysis provides the same level of description as signal prop- agation, it has several advantages. The first is that it is based on a rigorous mathematical theory with clearly defined hypotheses and probabilistic error estimates. This allows one to be confident in determining which architectures it can be applied to, and to have a rigor- ous pathway for extending it to new architectures (which we exploited in our treatment of normalization and pooling layers). The second advantage is that the quantities it computes have a clear relationship to the (high probability) initialization-time behavior of actual ran- domly initialized networks with definite inputs and weights. The third is that it applies to networks with low dimensional inputs, for which the CLT-based arguments commonly used to justify signal propagation are inapplicable. And while these advantages come at the cost of additional/stronger hypotheses (such as Gaussian or SUO-distributed weights), such hypotheses are likely required in order for the predictions made by the equations to be accurate in general. 84
Deep Kernel Shaping 25.8 Extensions of variance/signal propagation to networks with Batch Normalization layers De and Smith (2020) propose an extension of variance propagation to networks with Batch Normalization (BN) layers, in order to analyze standard ResNets. To do this, they argue that for large mini-batches, BN layers will compute a per-unit empirical variance which closely matches the per-unit variance computed under variance propagation. Thus, after normalization by the square root of this variance, the per-unit output variance of a BN layer will be always be 1, regardless of its per-unit input variance. There appears to be a subtle issue with this argument. As discussed above, variance propagation is a faithful description of a single randomly initialized network (with definite inputs) only insofar as the variances it computes correspond to q values. q values in turn are approximations of dimension-normalized squared norms of entire activation vectors, and have no clear relationship to the properties of individual units within a layer of such a network. So in general, the empirical unit-wise variances computed by a BN layer will not correspond to the variances computed by variance propagation, even approximately. It is conceivable that with additional hypotheses on the batch size and distribution, the network, and the initialization, the empirical distribution of the values of each input unit to a BN layer (taken across the mini-batch, for fixed parameters) might all have roughly the same variance with high probability, in which case the approximation in De and Smith (2020) would be a valid one. However, formalizing this would likely be quite difficult. Yang et al. (2019) propose an extension of signal propagation/mean field analysis to networks where BN layers are inserted between affine and nonlinear layers. To facilitate this, they propagate B × B matrices representing the expected products between different copies of the same unit for each of B possible inputs to the network, where B is the batch size. For networks without BN layers the propagation equations decomposes nicely in terms of low-dimensional Q/C maps, while for networks with BN layers no such decomposition exists, due to the way different elements of the mini-batch interact in BN layers. Yang et al. (2019) are nonetheless able to analyze the resulting high-dimensional propagation equations using various sophisticated approximations and characterize their asymptotic fixed point behavior. In their approach, the batch size, as well as the distribution used to generate the mini- batch, are encoded via the initial B × B matrix of expectations to the first layer. Thus, their analysis is not dependent on B being large, or on any strong distributional assumptions about the mini-batch. However, unlike for networks with element-wise nonlinearities (where approximate kernel analysis gives rise to the same equations as signal propagation), there is no mathematically rigorous derivation of their generalized equations for BN layers. Thus, it remains an open question as to whether these equations are accurate approximations in the sense of Section 5.8. Yang et al. (2019) provide empirical evidence that they are, at least for fairly wide networks with some commonly used activation functions. 85
Martens et al. 26. Review and analysis of related approaches for constructing and initializing deep neural networks In this section we will review some existing techniques, both standard and otherwise, for constructing and initializing neural networks in order to make them easier to train. We will further analyze these techniques from the perspective of approximate kernel theory by exploiting the latter’s connections with variance/signal propagation established in Section 25. 26.1 The fan-in initialization and related approaches derived from variance propagation The classical fan-in initialization (LeCun et al., 1998b) for fully-connected neural networks samples filter weights iid with mean zero and variance 1/k, where k is the total input dimension. Here, 1/k is precisely the value required for their version of variance propagation to predict constant per-unit variances throughout the entire network, with other values leading to an exponential increase or decrease with depth. However, as LeCun et al.’s (1998b) variance propagation analysis is only a reasonable approximation for activation functions that preserve the mean and variance of their input, their initialization will tend to fail in more realistic settings, especially as the network’s depth increases (He et al., 2015). Glorot and Bengio (2010) use their own version of variance propagation to motivate a similar initialization scheme, where the weight variance is 2/(k+m), with m being the output dimension. This choice is made as a “compromise” between the following two competing constraints: that the per-unit variances should be uniform across layers, and that the variances of the per-layer gradients should also be similarly uniform. We would argue that 2/(k + m) is not a good choice in general compared to 1/k. For example, if the layer widths alternate between between n and 2n for some n (cid:62) 1, running variance propagation across two consecutive combined layers would predict a decrease in the variance by a factor n(2n)(2/(n + 2n))2 = 8/9. This will lead to an exponential con- vergence of the variance towards zero as depth increases. Meanwhile, for the choice 1/k, variance propagation (or approximate kernel analysis) predicts no such exponential increase or decrease for any choice of widths. He et al. (2015) propose to use a weight variance of 2/k specifically in RELU networks, which compensates for how RELU nonlinear layers decrease the variance by a factor of 1/2 instead of preserving it. This is based on their expanded version of variance propagation that handles RELU activation functions. Setting aside issues of mathematical rigor and the interpretation of the quantities being propagated, variance propagation and approximate kernel analysis involve similar calcu- lations (as discussed in Section 25.7), and so these three initialization schemes can all be viewed as methods to control the q values of the network. When combined with the normalization of the input vectors (as per Section 10.2), and applied to standard feed- forward fully-connected networks with suitable activation functions20, the fan-in initializa- 20. Here, “suitable” means (approximately) mean and variance preserving for the standard fan-in initial- ization, or RELU for He et al.’s (2015) modified version. Note that the large majority of activation functions do not fall into the former category. 86
Deep Kernel Shaping tion method and its extensions achieve q values of ∼1 throughout the network, which is one of the four constraints enforced by DKS. Having q values of 1 ensures that local C maps are the same for each combined layer (assuming they all use the same activation function), and that the final output of the network falls within a reasonable range. If this is not done, q values can grow very large or small with increasing depth, leading to various problems. In particular, very large values can cause bounded monotonic activation functions like tanh to “saturate”, so that local C maps become increasing degenerate with depth. And very small values can cause most activation functions to behave in a way that is “too linear”, which may limit the effective expressivity of the network (as per the discussion in Section 14.2). Notably, the RELU activation function is immune both of these issues due to it being positively homogeneous, which perhaps explains its popularity. However, by not enforcing the other three conditions of DKS, networks using these initializations can still have degenerate network-level C maps, and can experience an expo- nential accumulation of kernel approximation errors with depth (so that the q values won’t actually be constant in practice). As a concrete example of the former problem, consider the example from Section 12.1 of a standard deep RELU network. This network’s C map doesn’t depend on the input q value at all (as long its uniform), but still develops degenerate behavior at very high depths, leading to a network that is essentially untrainable. Another more subtle issue with these initializations is that q values of 1 will work very badly for certain activation functions. For example, consider the activation function defined by φ(x) = tanh(αx). As α increases, an input q value of 1 becomes arbitrarily bad, leading to increasing levels of saturation and consequent C map degeneration. The reason that q values of one work reasonable well in practice is that the most commonly used activation functions in the literature happen to work well it, or have local C map behavior that is insensitive to q values (as is the case for RELUs). Notably, DKS does not suffer from this issue (despite also enforcing q values of 1), as its use of a multiplier on the input of each activation function ensures that 1 will always be optimal (since any other value can be effectively “simulated”). 26.2 Layer-Sequential Unit-Variance initialization and Within-Layer initialization Mishkin and Matas (2015) proposed an initialization method called Layer-Sequential Unit- Variance (LSUV), which uses an iterative procedure that starts from a standard random initialization and adjusts the scale of each weight matrix/filter bank to achieve the condition that the variance of the output of each affine layer – taken over the channels, locations and training cases – is approximately equal to 1. These variances are computed by evaluating the network empirically on random mini-batches of training data. By taking φ in Equation 24 to be the identity function, we have that the average value (across channels) for each location-wise output vector of an affine layer is approximately zero with high probability. The variances computed by LSUV can therefore be interpreted as estimates of the length-normalized squared norms of the location vectors for each layer, except that they are also averaged over locations and network inputs. Thus, we can think of LSUV as enforcing the condition that the “average q value” for the output of each affine 87
Martens et al. layer is equal to 1. This condition is similar to the one that the fan-in initialization (and its variants) are trying to achieve, and thus our discussion and critique of those methods (in Section 26.1) also applies to LSUV. In particular, a network initialized with LSUV can still have degenerate C maps, with all of their consequent problems. Because LSUV uses empirically computed statistics instead of canned formulas, it takes into account the given architecture and network topology, as well as the properties of the input training vectors. By contrast, variants of the fan-in initialization are usually only valid for the particular activation function and network topology they were derived for, despite often being applied more generally. They also implicitly assume that the training input vectors are appropriately normalized, which isn’t always the case in practice. (Note that DKS, while it is also based on formulas instead of empirical evaluations of the network, takes into account the activation functions and network topology, and is packaged with a data preprocessing technique for the input vectors.) The “Within-Layer Initialization” (WLI) of Kr¨ahenbu¨hl et al. (2016) can be viewed as a modification of LSUV where one enforces the condition that the mean and variance of the output of each layer and each channel is 0 and 1 respectively (as opposed to LSUV, which considers the average variance across channels). This is done by rescaling the filter bank weights separately for each output channel, and setting the bias appropriately. Because it enforces conditions per-channel instead of averaging across channels, this modification is harder to compare directly to fan-in initializations or DKS. It is perhaps more closely related to Batch Normalization (which is discussed later in this section), as it achieves the same conditions that BN does before the first step of optimization. 26.3 Self-normalizing neural networks Self-normalizing neural networks (Klambauer et al., 2017) use Scaled Exponential Linear Unit (SELU) activation functions to achieve a per-unit mean of 0 and variance of 1 asymp- totically with depth, as computed under variance propagation. Due to the relationship between variance propagation and Q/C maps (discussed in Section 25.7), this is equivalent to Q (g) having an attractive fixed point at q = 1, and C (c) having an attractive fixed g g point at c = 0, where g is a SELU nonlinear layer. Assuming the use of PLN, and a stan- dard feed-forward architecture (or normalized sums in more general architectures), these conditions imply that Q (1) = 1 and C (0) = 0 for all subnetworks f , which is two of the f f four conditions enforced by DKS. As previously discussed, Q (1) = 1 for all subnetworks f is a good condition to have, f and will prevent extreme q values from developing in deep networks (which can adversely affect C maps). However, it won’t in general guarantee a well-behaved C map in deep networks, even when combined with the condition C (0) = 0. f For a SELU nonlinear layer g we have C(cid:48) (1) ≈ 1.0716, which can be computed numeri- g cally using Equation 22.2 and the methods described in Section 22.2. Along with the condi- tion C (0) = 0, this guarantees a well-behaved C map up to a modest depth. For example, g given a standard 100 layer network f we have C (0) = 0 and C(cid:48) (1) = C(cid:48)(1)100 ≈ 1.005 · 103, f f so that C is reasonably well behaved according to Theorem 13. However, if f has 300 f layers then we have C(cid:48) (1) ≈ 1.0157 · 109, which indicates degenerate behavior with c∗ = 0 f (in the sense of Section 12.4). 88
Deep Kernel Shaping 26.4 The “Edge of Chaos” (EOC) method Consider a network f defined by a composition of D combined layers, each with the same nonlinear activation function φ. Every combined layer will have the same Q map, which we denote by Q. As discussed in Section 10.1, Q will typically have a fixed point q∗ which is rapidly converged to under repeated applications, and thus one may approximate the q values as uniform and constant across layers. This allows one to define a local C map C which only depends on the input c value, and which is the same for each combined layer. As argued by Schoenholz et al. (2017), C(cid:48)(1) will describe the asymptotic dynamics of the c values as they evolve through the layers of the network in the limit as D → ∞. C(cid:48)(1) > 1 indicates rapid convergence of the c values to 1, while C(cid:48)(1) < 1 indicates rapid convergence to a value c < 1. Because of its close proximity to these two undesirable 0 depth-limiting asymptotic behaviors, a network with C(cid:48)(1) = 1 is said to be “on the edge of chaos”, and will have its c values converge slowly towards 1 at an asymptotic rate which is sub-exponential. In the initialization method proposed by Schoenholz et al. (2017), which we will call the “Edge of Chaos” method (EOC), one initializes the weights using a standard Gaussian fan-in method, with the variance of the weights and biases chosen so that C(cid:48)(1) = 1. (Note that q∗ also depends on these variances, which is taken into account when computing C(cid:48)(1).) As observed by Schoenholz et al. (2017), there are typically infinitely many combinations for these two variances which achieve C(cid:48)(1) = 1 (assuming any exist for the given activation function), and so one is chosen arbitrarily. Notably, the condition C(cid:48)(1) = 1 is based entirely on the properties of C, and as such does not depend on the depth of the network. In Xiao et al. (2018), a version of EOC was proposed that used an Orthogonal Delta initialization technique for convolutional layers, with variances chosen so that C(cid:48)(1) = 1. In their experiments Xiao et al. (2018) showed that basic convolutional neural networks (without skip-connections or batch normalization) can be successfully trained on CIFAR-10 with the resulting initialization at depths of up to 10,000. This was a remarkable result, as such networks are considered essentially impossible to train at even modestly large depths when initialized using standard methods. 26.4.1 Relationship to DKS DKS is in many ways a spiritual successor to EOC, and is derived using an extended version of the same basic Q/C map analysis that underlies the latter. Like EOC, DKS also makes use of the Orthogonal Delta initialization technique. But despite these similarities, there are many important differences between the two methods, which we will discuss in sequence below. Firstly, DKS enforces uniform q values via data pre-processing instead of relying on the (presumed) convergent fixed-point behavior of the Q maps to achieve this asymptotically. See Section 10.1 for a more detailed discussion of this point. Secondly, instead of targeting the condition C(cid:48)(1) = 1 for each combined layer f , DKS targets C(cid:48)(1) = µ−1(ζ), so that the “degree of nonlinearity” is calibrated to the given architecture. This is motivated by looking at the overall C map behavior of the network, instead of the fixed point convergence behavior of its local C maps. From the perspective of 89
Martens et al. fixed point convergence, DKS achieves exponential rate towards c∗ = 0, while EOC achieves sub-exponential rate towards c∗ = 1. While this might seem like a point against DKS, one must remember that the precise rate of its exponential convergence will depend21 on the overall depth D, the effect of which is that the c values will be far from converged even by the D-th layer. The third difference between DKS and EOC is that DKS manipulate the network by transforming the input and output of the activation functions, as opposed to changing the variances of the weights and biases. If we consider the “equivalent parameters” (as per Section 18.3), DKS is implicitly searching over a space of distributions with more degrees of freedom than the two used by EOC, and one which allows for non-zero correlations between the weights and biases. Fourthly, despite having more degrees of freedom with which to manipulate the network, DKS makes use of all of them in order to enforce a total of four conditions on the Q and C maps of the network (which are listed in Section 17.3). EOC meanwhile only enforces a single condition (C(cid:48)(1) = 1), which leaves one of its two degrees of freedom unconstrained. (The effect of this is that EOC has a manifold of possible weight and biases variances from which to choose, and is therefore under-determined.) Fifthly and finally, DKS is applicable to a diverse set of architectures, thanks to our generalized notion of Q/C maps, analysis of pooling layers, and network polynomial con- struction. As it was originally developed, EOC assumes a strictly feedforward network consisting of a sequence of fully-connected or convolutional combined layers. 26.4.2 Possible failure modes of EOC It is worth pointing out that the condition C(cid:48)(1) = 1 enforced by EOC does not necessarily imply that the network will look perfectly linear at initialization time, as C(0) = 0 is not enforced in EOC as it is in DKS. Nonetheless, depending on the activation function, there may be choices for the weight and bias variances which can make the network look “too linear”, thus leading to very slow training as per the discussion in Section 14.2. As such choices are not explicitly forbidden in EOC, and are indeed compatible with the condition C(cid:48)(1) = 1, this may represent a failure mode of the method. Conversely, without C(0) = 0, the condition C(cid:48)(1) = 1 may not sufficient to ensure that the entire network’s C map C is well-behaved. For example, given a choice of 1 and 0 f for the variances of the weights and biases respectively, we have C(cid:48)(1) = 1 for unmodified RELU activation functions (by Section 12.1), and yet C quickly degenerates as D grows, f as can be seen from the figures in Section 12.1. Moreover, our experiments in Section 28.2 confirm that standard deep RELU networks (without BN layers or skip connections) are not readily trained at high depths, even with a high-powered optimizer like K-FAC. 21. In particular, the worst-case rate of convergence to the fixed point will be given by C(cid:48)(0), which by Equation 27 satisfies C(cid:48)(0) (cid:62) 2 − C(cid:48)(1) = 2 − µ−1(ζ). For a D layer convolutional network this is 2 − ζ1/D, which will be just slightly below 1.0 when D is large (given typical choices for ζ). 90
Deep Kernel Shaping 26.5 The Looks Linear method Balduzzi et al. (2017) use path-weight analysis (which we review in Appendix I) to argue that gradients with respect to the network’s input will decorrelate or “shatter” in deep RELU networks, leading to difficulties when training with gradient descent22. They also argue that this happens to a much lesser extent in ResNets. Motivated by these observations, and by the fact that this effect doesn’t occur in a purely linear network, Balduzzi et al. (2017) propose the Looks Linear (LL) method for initializ- ing/constructing RELU networks. This method exploits the fact that φ(x) − φ(−x) = x when φ is the RELU function in order to construct a network that behaves exactly like a linear one at initialization. In particular, one replaces φ by (φ(x), φ(−x)) for each RELU nonlinear layer (which effectively doubles its output channel dimension), and initializes the weights of the affine layer after each RELU layer according to (W, −W ), where W is sam- pled according to a Delta Orthogonal initialization. For fully-connected combined layers this produces an overall computation W φ(x) − W φ(−x) = W (φ(x) − φ(−x)) = W x, while for convolutional combined layers the computation is similarly linear (although harder to express in standard matrix notation). From the perspective of our analysis, perfectly linear-looking networks have (very) well- behaved C maps, and thus satisfy one of the main necessary conditions for trainability. With such networks there is always the danger that they may be “too linear” in the sense of Section 14.2, but LL-initialized networks avoid this because W φ(x)+W φ(−x) will become 1 2 highly nonlinear given a relatively small perturbations of W away from −W . 1 2 The main two obvious disadvantages to the LL approach are that it only works for RELU networks, and that it doubles the widths of RELU layers (without proportionally increasing the network’s expressivity/capacity). Beyond these things, the main difference between LL and DKS is the precise mechanism used to make the network “look linear”, and the implications that this has for optimization (which is not well understood in either case). In DKS, the degree of nonlinearity of a nonlinear layer, as measured by properties of its C map (such as the slope at c = 1), varies smoothly as a function of the parameters of the transformed activation functions, and so one could argue that it should also vary smoothly as a function of the network’s parameters, resulting in easier optimization. By contrast, the linearity property of the LL method depends on a delicate mirrored symmetry of the weights in each layer, so that relatively small perturbations in these could lead to large changes in the degree of nonlinearity of the network. This sensitivity may make optimization more difficult, and may explain the optimization difficulties we observed in our experiments with the LL approach. (See Section 28.5.5 for more details.) 26.6 Residual connections Residual Networks aka ResNets (He et al., 2016a,b), which are described in detail in Section 23.1, have become the dominant neural network architecture for computer vision problems. What makes ResNets so successful isn’t that they are more powerful or expressive than 22. Note that this prediction agrees with our NTK analysis in the sense that the per-layer NTK matrix of the first layer of a deep RELU network is approximately the identity, although the implications for training are somewhat different; see Appendix I 91
Martens et al. other more traditional deep convolutional architectures like VGG (Simonyan and Zisserman, 2015), but rather that they are easier to train with stochastic gradient descent at very high depths (He et al., 2016a; Szegedy et al., 2017). This easier training is owed to their use of skip connections (aka shortcut connections; which have been a feature of network architectures since the 1990s), Batch Normalization (BN) layers (Ioffe and Szegedy, 2015), RELU nonlinearities, and the surprising interplay between all three of these components (De and Smith, 2020). Moreover, popular new architectures such as Efficient Nets (Tan and Le, 2019) and Transformers (Vaswani et al., 2017) are based on the same high-level residual block structure, and differ only in terms the layers contained in their residual branches. ResNets, and their generalizations, thus represent a solution to the problem of how to achieve fast and stable training of very deep neural networks. And while the nature of this solution is still not totally understood, there has been progress in this direction (of which we will cover only a small subset). Veit et al. (2016) argued that residual networks behave like a ensemble of shallow net- works of varying depth throughout training. They gave evidence for this by showing that deep residual networks are highly robust to “lesion” operations which remove or rearrange layers, and that the network’s gradient is dominated by contributions made by paths through the network with fewer nonlinear layers Zhang et al. (2019c) observed that if one removes the BN layers from a ResNet-V2 network and initializes the last convolutional layer of each residual branch to zero (along with a few other smaller tweaks to the architecture and its initialization), the resulting network achieves training speed comparable to a standard ResNet, at least at modest batch sizes. In such networks, the residual blocks act as identity functions at initialization tune, only becoming nonlinear as training progresses. Subsequent work showed that one could achieve similar results in BN-free networks simply by using learnable weights on the residual branches that are initialized to zero (De and Smith, 2020; Bachlechner et al., 2020). More recently, it was found by Shao et al. (2020) that the branch sum can use static (non- learnable) weights, where the relative size of the weight on the residual branch is set to a small value (that can vary between blocks). To help explain these findings for BN-free networks, De and Smith (2020) applied a version of variance propagation to argue that the per-unit output variance of a residual block will be roughly 1 plus its per-unit input variance, so that the i-th residual block has a variance proportional to i. Then, because the output variance of each residual branch is constant (due to the use of BN), it follows that the relative contribution to the block’s output made by the residual branch shrinks as 1/i. This, they argue, leads to a network which behaves more like an linear function than it otherwise would. In Appendix J we make this argument more rigorous by computing q values in a (nearly) standard ResNet, and showing that their growth over layers leads to a better behaved C map. We also show that an identical C map can be obtained in a network without normalization layers via careful selection of weights on the residual and shortcut branches. 26.7 Normalization layers Normalization layers (the two most common types of which are defined in Section 19) have become a standard component in neural networks, since the introduction of Batch 92
Deep Kernel Shaping Normalization (BN) by Ioffe and Szegedy (2015). In addition to the important and specific role they play in ResNet-style architectures (as discussed in Section 26.6 and Appendix J), these layers have been observed to make deep neural networks easier to train on their own. In this subsection we will discuss possible explanations for this, with a particular focus on ones arising from Q/C map analysis. We will also give some arguments for why normalization layers alone are insufficient to enable fast training of deep networks. 26.7.1 Layer Normalization layers As discussed in Section 19.2.1, a Layer Normalization (LN) layer f has the property that Q (q) = 1 for all q, provided that its learnable gain and bias are set to their initial values. f Additionally, when applied after a combined/nonlinear layer g, the C map of the composition has the property that C (0) = 0, regardless of C map behavior of g. Thus, when used after f◦g each nonlinear layer in a network initialized as per Section 4, LN layers achieve uniform q values of 1 throughout the network, and also C (0) = 0 for all subnetworks h, which are h two of the four conditions enforced by DKS. (Note that if LN layers are instead inserted before each nonlinear layer, they will not achieve the latter condition.) The way LN layers achieve these conditions differs from DKS in at least two ways. First, they perform direct calculation of the relevant quantities instead of using q and c values as approximations. This allows them to work with arbitrary initializations of the parameters (including badly scaled ones), poorly scaled input data, and without any explicit knowledge of the network’s structure or activation functions. Second, LN layers continue to enforce a version of these conditions throughout training, or at least as long as their learnable gain and bias remain close to their initial values of 1 and 0. As discussed previously (e.g. Subsection 26.1), uniform q values of 1 is a useful property to have, but is far from sufficient to ensure trainability. The condition C (0) = 0 for all h subnetworks h is meanwhile only one half of the two conditions required by Theorem 13 to ensure well-behaved C maps, and arguably the less important of the two. To make this discussion more concrete, we will consider how putting LN layers after each nonlinear layer will effect the C map of a standard deep RELU network. For a RELU nonlinear layer g we have by Equation 15 that C (0) = 1/π and C(cid:48) (1) = 1 (which follows g g from Equation 15 by taking the derivative and letting c → 1). Taking the derivative in Equation 17 we have C(cid:48) (c) = 1/(1 − C (0)) = π/(π − 1), and so by the chain rule f g C(cid:48) (1) = C(cid:48) (C (1))C(cid:48) (1) = C(cid:48) (1)C(cid:48) (1) = π/(π − 1) ≈ 1.467. Thus we see that while f◦g f g g f g the use of an LN layer after a RELU layer gives us C (0) = 0, it comes at the price of f◦g increasing the C map slope from 1 to ∼1.467. The following plot shows the extended C map for a RELU network h with 20 combined layers, with and without LN layers used after each nonlinear layer: 93
Martens et al. From this plot we can see that the C map for the network with LN layers has a much larger output range. However, for the vast majority of its input domain, the output is restricted to a small region around 0, and it is still highly degenerate in the sense of Section 12.4 (and thus suggestive of poor training). Beyond their affect on the initial behavior of the network, LN layers may also have an independent and possibly beneficial effect on optimization, as they change the relationship of the loss and the parameters. Ba et al. (2016) argue that LN layers lead to a Fisher information matrix with more favorable properties for optimization. Another intuition is that an LN layer decouples the scale and direction of the weights of its immediately preceding affine layer, which may encourage faster optimization with gradient descent. Networks with LN layers may also be also be smoother when considered as functions of either their parameters or their inputs, since the change in the output of an LN layer is always bounded. Despite these intuitions, as far as we know there has yet to be strong theoretical or empirical evidence in favor of a specific optimization benefit to LN layers beyond their affect on the network’s initial behavior. 26.7.2 Batch Normalization layers As discussed in Section 19.1, Batch Normalization (BN) layers cannot be analyzed within the Q/C map framework we have presented. Despite this, we can still make some observations regarding their effect on network behavior in the context of our previous discussions. As shown in Section 12.4, one common way that a deep network f can become diffi- cult/impossible to train is when all input vectors map to approximately the same output vector (as measured by cosine similarity) at deeper layers of the network. This happens nat- urally in deep RELU networks, where BN is typically applied. Placing BN layers throughout the network may mitigate this particular pathology by ensuring that the empirical distribu- tion of each unit over the mini-batch has a large variance compared to its mean. However, this won’t obviously do anything to help with the opposite problem discussed in Section 94
Deep Kernel Shaping 12.4, where output vectors appear “random”, and in particular fail to reflect the geometric relationships between the original input vectors. These intuitions are confirmed by Yang et al.’s (2019) signal propagation analysis of RELU networks with BN layers (which we discuss in Section 25.8). In particular, Yang et al. (2019) predict that the distances between the output vectors (generated from different inputs) will converge to a constant as depth increases, and that this leads to an exponential increase in the norm of the gradient. Like with LN layers, placing BN layers after each affine layer makes the network insen- sitive to the scale of its weight parameters, which can thus correct for badly scaled initial weights. One can perhaps also view BN layers as ensuring that the “average q value” across the mini-batch is 1, although this is an imperfect analogy since BN layers operate on a per-channel basis instead of averaging over channels. Similar to LN layers, BN layers may also have an effect on optimization which is inde- pendent from their effect on the network’s initial behavior. Evidence for this includes the fact that various methods which modify networks and their initializations in an attempt to eliminate the need for BN (such as those we’ve previously discussed) fail to achieve the same optimization performance under SGD, except perhaps at small mini-batches sizes (where classical optimization considerations like curvature matter a lot less, as argued in Zhang et al. (2019a)). In support of the optimization-effect hypothesis, Santurkar et al. (2018) argue that BN layers make a network’s output a smoother function of its parameters, and that this helps improves the performance of gradient descent. Li and Arora (2019) argue that gradient descent applied to networks with BN layers behaves similarly to gradient descent applied to a normalizer-free network with a decaying learning rate, thus allowing gradient descent with a constant learning rate to converge in the stochastic setting (where it otherwise might not). Finally, Grosse (2021) argues that placing a BN layer after an affine layer g will make the network invariant to scaling and shifting of g’s input, and that this leads to a curvature matrix for f ’s parameters which is better conditioned. Part V Experiments and conclusions 27. Experimental setup In this section we will describe and justify the setup we will use in our experiments, which will depart somewhat from common practice. 27.1 Training problem and datasets The benchmark training problem we use in all of our experiments is image classification, on either the MNIST (Deng et al., 2009) and CIFAR-10 (Krizhevsky and Hinton, 2009) datasets. 95
Martens et al. The training objective is the average loss over the training set, with the loss given by the cross-entropy error between network’s output (interpreted as “logits” of a softmax) and the dataset labels. We also measure top-1 classification F1-score, and report this instead of the loss in our plots due to its higher interpretability. For MNIST, we use an image preprocessing and random augmentation pipeline similar to the one from Szegedy et al. (2015) to obtain images of size 224 × 224. The training set is obtained from the standard MNIST training set, minus the last 10000 cases (which is used as a new validation set), and the test set is obtained from the usual MNIST validation set. Training F1-score is reported using the examples actually used during training, which are subject to random augmentation. Test F1-score is meanwhile reported using examples from the test set without random augmentation. For CIFAR-10, we apply the standard preprocessing consisting of mean subtraction and normalization of each color channel. The training and test sets are their standard versions. For both datasets we apply Per-Location Normalization (as described in Section 10.2) as a final stage of processing before feeding the inputs to the network. This is done for all approaches and experiments unless stated otherwise, in the interest of fairness. 27.2 Focusing on optimization speed While we will report test set F1-score in many of our experiments, our primary focus will be on optimization speed, as measured using training F1-score. Moreover, the decisions we make while designing out experiments will be in the interest of obtaining the cleanest and fairest comparison for optimization speed, and we will tune various components (like the learning rate schedule, regularization, etc) with this in mind. In this subsection we will explain our rationale for this decision. The current standard approach to deep learning is to train normalized residual architectures with RELU nonlinearities, such as ResNets or Transformers, with basic op- timizers like SGD or Adam. Alternative approaches (such as normalization-free networks using Fix-up (Zhang et al., 2019c), standard deep convolutional networks initialized with EOC, or DKS) can underperform the standard approach in one of two basic ways. First, they can yield networks whose training plateaus earlier, resulting in underfitting, or whose training is just much slower overall. And second, they can yield a worse inductive bias for typical training problems (like MNIST classification), resulting in increased overfitting. In our initial experiments we found that while alternative deep learning approaches are typically affected by both of these problems, slower training is by far the more significant one, particularly for networks without skip connections. Moreover, the resulting underfitting problem (given a finite optimization step budget) led to a commensurate degradation in test set performance. (These findings echo those of He et al. (2016a) and Szegedy et al. (2017), who observed that the main benefit of adding skip connections was faster optimization.) Thus, by focusing on training speed, we are isolating what is the more serious problem currently affecting alternative methods, and the one which arguably should be addressed before attempting to close the generalization gap. We believe that the increased generalization gap we observed on MNIST for alter- native approaches such as ours is small enough that it can be overcome through the use of additional regularization strategies, dataset augmentation, scheduling of the optimizer 96
Deep Kernel Shaping hyperparameters, architectural tweaks, etc. We will leave this to future work. This position is echoed by Zhang et al. (2019c), and was arguably validated in the recent work of Brock et al. (2021) on “Normalizer-Free Networks”, which used a combination of these techniques to close the generalization gap for one such alternative approach. It is also our view that overfitting may become less of a concern as the machine learning community moves beyond supervised benchmark problems like MNIST classification, and towards giant/streaming datasets and unsupervised methods. 27.3 Network architectures and regularization In our experiments we train standard and modified ResNets and Wide-ResNet models for MNIST and CIFAR-10 image classification. (A detailed description of all the relevant architectures is given in Section 23.) We will place particular emphasis on “ablated” versions of ResNets, where Batch Nor- malization (BN) layers and/or the skip connections are removed, leaving everything else unchanged. The motivation for doing this is that we want to facilitate the fairest possible comparison to the standard deep learning approach. In particular, since we are focused mostly on optimization speed in our comparisons, we want to use models that are provably no more powerful than standard ResNets in terms of the class of functions they can express, so that the fundamental data fitting problem doesn’t become any “easier”. In the interest of making our experiments fair we also didn’t include the standard L2 regularization that is often used when training ResNets. This is because the effect of L2 regularization on the effective capacity of a model is highly dependent on the model’s parameterization, and this will vary significantly across the different approaches we consider. For example, due to the way BN layers are invariant to scalar multiplication of their inputs, one can rescale the weights of any affine layer that precedes a BN layer without changing the overall output of the network. Thus, networks with BN layers can effectively “cheat” the L2 regularization penalty in a way that networks without BN layers cannot. In our experiments we found that the removal of L2 regularization did have a small but still significant effect on the test set performance of standard ResNets, which is reflected in our reported results. Note that our purpose in experimenting with these modified ResNets is not to show they are a good replacement for standard ResNets in practice. Rather, our purpose is to determine the extent to which we can replace the ingredients of the standard deep learning approach with various alternatives (that preserve the model class), while retaining its fast training capabilities. If we were primarily interested in maximizing test set performance in our evaluations, then we would be free to design a network architecture best suited to DKS, and to include whatever regularization scheme we found to be most effective. And while this does seem like an interesting direction to explore, as has been done in the context of other alternative approaches to deep learning (Brock et al., 2021), it is beyond the scope of the present work. 27.4 Automatic learning rate schedules with Fire PBT Achieving a near optimal rate of convergence for standard ResNet training with SGD seems to require a carefully designed learning rate schedule (and not just a fixed value), especially 97
Martens et al. for more difficult datasets like MNIST. Through extensive and costly trial and error, the community has produced learning rate schedules which seem to work well on certain standard problems, such as MNIST classification with ResNets. These typically involve a quick “warm-up” of the learning rate from a moderate starting value to a larger one, followed by a decay or step-wise descent towards zero. In our experiments we consider a large variety of approaches for training deep networks, most of which depart from the standard one along directions such as architectural choices, optimizers, initialization, etc. There is no reason to think that a learning rate schedule tuned for standard ResNet training with SGD should perform well for all such approaches, and this was borne out in our initial experiments. (By contrast, it seemed like the momentum hyperparameter was much less important.) Thus, in order to conduct fair experiments, which are minimally confounded by data augmentation, we need a way of determining a near-optimal learning rate schedule for each approach. And this should ideally be done in an automatic way in order to reduce the role of experimenter bias. Recently, Dalibard and Jaderberg (2021) proposed an alternative version of Population Based Training (PBT, Jaderberg et al., 2017) called FIRE PBT, which is designed specifi- cally for the dynamic adjustment of optimizer hyperparameters. Like many other methods for automatically tuning the learning rate, standard PBT falls into the trap of being too greedy, and tends to lower the learning rate too quickly for the sake of short-term improve- ments in the loss (Wu et al., 2018). FIRE PBT is designed to tackle this issue, using a strategy which we will now briefly explain. Both PBT and FIRE PBT work by having many workers independently train neural networks, each with their own values for the hyperparameters. Both methods also asso- ciate a fitness to each of their workers which guides an evolutionary procedure. In PBT, this fitness is simply the current value of the objective function (which can be defined on the training or test sets). In FIRE PBT these fitnesses are altered in order to promote population members which may have a worse objective but are promising in other ways. In particular, a separate class of workers, called evaluators, periodically copy the model parameters of other workers, change the hyperparameters (e.g. decay the learning rate), and measure the rate at which the objective function improves while training with the new hyperparameters. The higher the rate of improvement as measured by the evaluator, the higher the fitness FIRE PBT will associate to the original worker whose model parameters were copied. This approach encourages workers to use “non-greedy” hyperparameters (such as high learning rates), if it is shown that doing so leads to better performance after training with different hyperparameters (such as lower learning rates) in the long run. In their experiments, Dalibard and Jaderberg (2021) showed that FIRE PBT worked very well at automatically generating learning rate schedules on the fly for standard ResNet training with SGD, matching or exceeding the performance of the previously mentioned community-tuned schedules. In our initial experiments we found that this capability carried over nicely to non-standard deep learning approaches as well, and so we decided to use it in all of our subsequent experiments. We now discuss the technical settings related to our use of FIRE PBT. We follow the presentation of Dalibard and Jaderberg (2021). Each experiment uses 36 workers. We divide them into three sub-populations P , P , P , each of size 8, and the evaluator set H 1 2 3 98
Deep Kernel Shaping which includes the remaining 12 workers. We train for a maximum of 200,000 steps when training on MNIST and for a maximum of 25,000 when training on CIFAR-10. Hyperparameters We optimise the learning rate hyperparameter. When using SGD or Adam, the learning rate is initially sampled log-uniformly in the range [10−5, 1]. When using K-FAC, we instead use the range [10−7, 10−3]. Objective function We evaluate the current model by reporting the current negated training loss. Ready A member of the population is deemed ready to exploit and explore every 500 steps when training on MNIST and 50 steps when training on CIFAR-10. Exploit We use a truncation selector: If a population member has a fitness in the bottom 25% of the population, it copies the neural network weights and hyperparameters of a random member in the top 25% of the population. Explore We multiply the learning rate by a value sampled at uniform random between the following two value: [0.8, 1.25]. We further set the FIRE PBT hyperparameter of max_eval_steps to 7200 when training on MNIST, and 360 when training on CIFAR-10. We set the hyperparameter min_steps_before_eval to 5000 and 10000 for P and P respectively when training on MNIST, and to 250 and 2 3 500 when training on CIFAR-10. The training curves plotted in our results section use the values recorded by the sequence of workers that led to the best eventual objective value (negative loss). In Appendix L we plot the learning rate schedule found by FIRE PBT for some of our main experiments. We note that apart from some small fluctuations, these schedules are fairly simple and natural looking, and typically involve an initial rapid increase in the learning rate, followed by a gradual decay. Thus, we don’t believe that the qualitative nature of our results is highly dependent on our use of FIRE PBT versus a simpler approach for learning rate tuning. 27.5 Optimizers In our experiments we used SGD (with momentum), K-FAC (Martens and Grosse, 2015), Adam (Kingma and Ba, 2014), and Shampoo (Gupta et al., 2018; Anil et al., 2020) as optimizers, with the majority just using SGD and K-FAC. Our motivation for considering stronger optimizers is that alternative deep learning approaches such as ours seem to benefit substantially from using them. For all optimizers we used a momentum parameter of 0.9, and adjusted the learning rate dynamically throughout training using FIRE PBT. For MNIST experiments we used a batch size of 512 with all optimizers, and for CIFAR-10 we used a batch size of 1024. For Adam we used a value of 10−5 for the “(cid:15)” parameter, which performed slightly better than the default value of 10−8. For K-FAC, we used a 0.99 exponential decay of the curvature matrix, and computed its inverse every 50 iterations. We initialized K-FAC’s damping parameter λ to 10−3, and exponentially decayed it at the rate 0.98 every 50 iterations to a minimum value of 10−6. Finally, we enforced a maximum norm of 10−2 on all updates, with the norm computed using K-FAC’s approximate curvature matrix (as in Ba et al. (2017)). 99
Martens et al. For Shampoo we used an epsilon parameter of 10−5 and an exponential decay factor if 0.99 for the second moments. In order to achieve optimization performance which was competitive with K-FAC, we used an “exponent multiplier” of 3 (which increases the expo- nent of all factors of the preconditioner by a factor of 3, with 1 being the default value), and enabled “grafting” (which uses Adagrad (Duchi et al., 2011) to compute the magnitude of the update for each parameter tensor, and the usual Shampoo formula to compute its direction). 27.6 Hardware and implementation details All of our experiments were implemented in TensorFlow (Abadi et al., 2015). Each of the 36 workers used by FIRE PBT ran on an 16 chip 32 core Cloud TPU v3 Pod (Google, 2018). For multi-core TPU Pods, each core ran a “replica” of the entire gradient computation on its assigned subset of the training mini-batch, with gradients and other key optimization quantities being averaged across the cores to simulate a single core computation. As long as training cases are independent of each other in the forward pass, this simula- tion is exact. However, this independence is slightly violated for networks with BN layers, and the resulting simulation is thus imperfect. Handling BN in this way in the multi-core setting has nonetheless become standard practice, and is even thought to be beneficial as it increases the “noise” originating from BN layers, which is thought to have a regularizing effect. 28. Experimental results In this section we will present our main experimental results as a series of plots of train- ing/test top-1 F1-score vs iteration number, with some discussion. Most of our experiments will use the standard RELU, tanh, and softplus activation functions, the latter of which is a smooth analogue of the RELU function defined by φ(x) = log(1 + exp(x)). Contents 28.1 Default hyperparameters and network configurations assumed for each ex- periment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 28.2 DKS with skip-free nets vs standard baselines . . . . . . . . . . . . . . . . . 101 28.3 DKS with and without skip connections . . . . . . . . . . . . . . . . . . . . 105 28.4 DKS with different activation functions . . . . . . . . . . . . . . . . . . . . 108 28.5 Comparisons to other approaches . . . . . . . . . . . . . . . . . . . . . . . . 109 28.5.1 Gaussian fan-in initialization . . . . . . . . . . . . . . . . . . . . . . 109 28.5.2 Glorot uniform initialization . . . . . . . . . . . . . . . . . . . . . . . 110 28.5.3 LSUV and WLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 28.5.4 Self-normalizing neural networks . . . . . . . . . . . . . . . . . . . . 112 28.5.5 Looks linear method . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 28.5.6 Edge of Chaos (EOC) method . . . . . . . . . . . . . . . . . . . . . 116 100
Deep Kernel Shaping 28.5.7 Fix-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 28.6 Meta-parameter studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 28.7 Ablations and modifications of DKS . . . . . . . . . . . . . . . . . . . . . . 120 28.1 Default hyperparameters and network configurations assumed for each experiment We use a value of 1.5 in all experiments for DKS’s global slope bound parameter ζ, unless stated otherwise. Except for DKS and the Looks Linear method, whenever using RELU activation func- √ tions we multiply the network’s initial weights by 2. This has become standard practice in the literature following He et al. (2015), and can be interpreted as making the local Q map of combined RELU layers equal to the identity. We don’t do this for DKS or the Looks Linear method since those methods achieve identity local Q maps through other means. Unless otherwise indicated, all result will be given for a skip connection-free BN-free modified ResNet-101 architecture trained on MNIST. “Standard ResNet” will refer to a standard unmodified ResNet with RELU activation functions, initialized with the standard √ Gaussian Fan-in initialization (with a 2 multiplier). For networks trained on CIFAR-10 we will use a modified Wide-Resnet with 250 layers and a width multiplier of 2. 28.2 DKS with skip-free nets vs standard baselines In this subsection we present our main results, in which we compare DKS networks without skip connections or BN layers, to both standard ResNets, and various “ablated” ResNets that are missing skip connections or BN layers (or both). From this first plot we can see that, with K-FAC, DKS enables skip-free BN-free net- works to train as fast as a standard ResNet on MNIST, which is the first time this has 101
Martens et al. been demonstrated to the best of our knowledge. Meanwhile, the ablated ResNets exhibit significantly slower optimization or underfitting. We also see that DKS underperforms for RELU compared to other activation functions, perhaps for the reasons discussed in Section 18.6. The story is somewhat different for SGD training. With SGD and no skip connections, DKS networks fail to match the training speed of standard ResNets, although they still outperform the ablated ResNets. Interestingly, RELUs give the same performance with DKS as the other activation functions do in this setting. 102
Deep Kernel Shaping For test set performance with K-FAC training we observe increased overfitting with DKS compared to standard ResNets, resulting in an overall lower test F1-score. Notably however, test F1-score is still higher than for the ablated ResNets. Once again the story is somewhat similar for SGD training, although with a larger performance gap vs standard ResNets due to the additional effect of underfitting from using SGD (without skip connections) instead of K-FAC. Note that the test error numbers for standard ResNet training with SGD are a few percentage points the commonly reported values. This is for a number of reasons, including the fact that we don’t include L2 regularization (as discussed in Section 27.3), that we configured FIRE PBT to maximize training speed and not test set performance, and that we use PLN to process the data. (Because these things affect DKS as well, we believe the comparison to still be fair.) The remaining results in this subsection are analogous to the previous ones, but use CIFAR-10 with modified/ablated Wide-ResNets models. The observations from these re- sults are similar, although we note that the performance gap between the DKS networks and ablated Wide-ResNets is considerably larger, likely due to the higher depth (250 vs 100) used in these experiments. 103
Martens et al. 104
Deep Kernel Shaping 28.3 DKS with and without skip connections In this subsection we compare the performance, with and without skip connections, of BN- √ √ free networks constructed with DKS. We use weights of 0.05 and 0.95 for the residual and shortcut branches respectively (so that the all sums in the network are normalized as √ per Section 21.2). The value 0.05 was selected from several candidate options in order to maximize training speed, as shown in Appendix M.1. 105
Martens et al. When using K-FAC we see that the training speed remains the same whether or not we use skip connections, except in the case of RELU activation functions. For RELUs, skip connections seem to help significantly, closing the performance gap with the other activation functions. With SGD the story is different, and skip connections allow us to match the training speed of standard ResNets with DKS, at least when using softplus or RELU activation functions. 106
Deep Kernel Shaping For K-FAC, the improvement to test set F1-score from using skip connections with DKS appears to be minimal, with the notable exception of RELU networks (where the improve- ment is likely due to improved fitting/optimization, as opposed to improved generalization). By contrast, in the context of SGD training we see a significant improvement to the test set F1-score from using skip connections with DKS. Although again, this is likely due to improved fitting enabled by the use of skip connections with SGD, rather than improved generalization. 107
Martens et al. 28.4 DKS with different activation functions In this subsection we compare performance of DKS with twelve different activation func- tions. In addition to certain well-known mathematical functions, we also include SELU (Klambauer et al., 2017), Softsign (Bergstra et al., 2009), Swish (Ramachandran et al., 2017; Elfwing et al., 2018), Elu (Clevert et al., 2016), and BentId (defined by φ(x) = √ x + ( x2 + 1 − 1)/2). For K-FAC we see fairly similar training speeds for each of the twelve activation func- tions, with RELU being the notable outlier. 108
Deep Kernel Shaping For SGD, there is a larger deviation in performance observed for the different options, and RELU is notably no longer an outlier. Results for test set F1-score were qualitatively very similar, and so we won’t report them here. 28.5 Comparisons to other approaches In this subsection we compare DKS to various other approaches for initializing and con- structing neural networks. We will focus primarily on skip connection-free BN-free networks, except when comparing to Fix-up (which requires the use of skip connections). We will omit test set F1-score in these comparisons, as we found that it gave qualitatively similar results to training F1-score. (This is likely because nearly all competing methods yield significant underfitting for skip-free BN-free networks, which overwhelms any possible advantage they might have in terms of generalization.) 28.5.1 Gaussian fan-in initialization The Gaussian fan-in initialization (aka “variance scaling initialization” or “Lecun initializa- tion”), which is discussed in Section 26.1, is the default initialization method used in many modern neural network frameworks, and is the first method we compare to. 109
Martens et al. From these results we can see that DKS significantly outperforms this canonical ap- proach, whose poor performance in this setting is not surprising given the analysis of Section 12. Note that it is common in practice to use a truncated Gaussian distribution or uniform distribution to sample the weights in a fan-in initialization, instead of the usual Gaussian distribution. When used with an appropriate rescaling term, these distributions produce weights with the same variance as the standard Gaussian distribution, although they won’t necessarily give rise to the same approximate kernel functions. We ran additional experi- ments using these distributions, and found that they gave similar results to those presented above. 28.5.2 Glorot uniform initialization Glorot initialization (aka “Xavier initialization”) is a commonly used modification of the Gaussian fan-in initialization which we discuss in Section 26.1. As with the Gaussian fan-in method, it is also often used with a truncated Gaussian or uniform distribution, the latter of which we will present results for. (We also performed experiments using truncated and non-truncated Gaussian distributions for the weights, which yielded similar findings.) 110
Deep Kernel Shaping From these results we can see that the Glorot approach is significantly outperformed by DKS, and completely fails to produce a trainable network for both the RELU and softplus activation functions. 28.5.3 LSUV and WLI The LSUV and WLI approaches, which are discussed in Section 26.2, represent the first gen- eration of methods which attempt to capture the benefits of Batch Normalization through initialization. They are fairly similar in their implementation, which is why we consider them together here. 111
Martens et al. From these plots we can see that these methods outperform simple initializations schemes like fan-in and Glorot, but are still significantly outperformed by DKS. 28.5.4 Self-normalizing neural networks Self-normalizing neural networks (which we discuss in Section 26.3) use SELU activation functions, together with a standard Gaussian fan-in initialization, to achieve certain condi- tions under variance propagation which are essentially equivalent to two of the four condi- tions enforced by DKS. 112
Deep Kernel Shaping From these results we see that DKS applied to a softplus network matches or exceeds the optimization performance of a self-normalizing network. DKS also improves the perfor- mance of a SELU network optimized with K-FAC, although slightly degrades it for SGD. 28.5.5 Looks linear method The Looks Linear method, which is discussed in Section 26.5, is an approach for constructing and initializing RELU networks which makes them behave like perfectly linear functions at initialization time, without the use of skip connections. The method is somewhat difficult to fairly compare to other ones, as it involves doubling the channel dimension of each layer, 113
Martens et al. while using a form of weight sharing which makes the resulting network less expressive than a standard one of the same dimensions. Our imperfect solution to this problem is to use the original dimensions when constructing networks with DKS, which will disadvantage DKS in the comparison. We had some trouble optimizing the networks constructed with the Looks Linear method. K-FAC would quickly diverge for all the hyperparameter settings we tried, perhaps because it broke the delicate symmetry of the initial weights too quickly, leading to extreme non- linear behavior. We had more luck with Adam and SGD, although we found that it was necessary to threshold the maximum update magnitude at 1 to achieve stable optimization (which is an approach known as “clipping” (Pascanu et al., 2013)). Because we couldn’t get K-FAC to work well with the Looks Linear method, we used it with Adam instead in our first comparison. We note that with Adam, DKS performs similarly to the Looks Linear method, but when used with K-FAC, DKS significantly out- performs it. 114
Deep Kernel Shaping For SGD both methods seem to perform similarly, and notably better than both the fan-in/Glorot initializations, and also the LSUV/WLI methods. We also conducted experiments with CIFAR-10, which yielded similar results. These are given below without commentary. 115
Martens et al. 28.5.6 Edge of Chaos (EOC) method The Edge of Chaos (EOC) method (described in detail in Section 26.4) is the closest ap- proach to ours in the existing literature, and the one which directly inspired it. The version in Xiao et al. (2018), which we will use here, involves two ingredients: choosing variances for the weight and bias distributions so that C(cid:48)(1) = 1 for each local C map C, and using the Delta Orthogonal initialization for the weights (which is rescaled to achieve the target variance). A clean comparison to EOC is somewhat difficult, as it is not fully specified. In par- ticular, for most activation functions there are infinitely many combinations of the two variances which achieve C(cid:48)(1) = 1. And for the RELU activation function, the condition C(cid:48)(1) = 1 holds for any weight variance (given zero bias variance), so that the method reduces to an Orthogonal Delta initialization. Xiao et al. (2018) focused their experiments on tanh networks, and following their advice we will take the variance of the weights and biases to be 1.01/k and 1.654355 · 10−7 (respectively) for tanh nets, where k is the input channel dimension for the given layer. We will also consider RELU networks, with a weight variance of 2/k and a bias variance of 0. 116
Deep Kernel Shaping 117
Martens et al. From these results we see that DKS significantly outperforms EOC in terms of opti- mization speed (for both MNIST and CIFAR-10), which in turn outperforms the simple Fan-in initialization method. 28.5.7 Fix-up Fix-up, which we briefly discuss in Section 26.6, is a recent method for constructing and initializing networks with residual connections which is designed to eliminate the need for normalization layers. It involves initializing the weights of the final convolutional layer in each residual block to zero (so that the residual blocks behave like identity functions at 118
Deep Kernel Shaping initialization), using a special formula for the variance of the weights distribution, as well as introducing learnable scalar multiplication and bias operations throughout the network. We were not able to get K-FAC to work well with Fix-up. This might have been due to a bad interaction with K-FAC and the extra parameters introduced by Fix-up (as K- FAC is designed specifically for the standard neural network parameters). Another possible explanation is that, like networks created with the Looks Linear method, the larger steps taken by K-FAC cause Fix-up networks to transition too quickly to extreme nonlinear behavior (after being essentially linear at initialization). As Fix-up requires the use a skip connections, for the sake of fairness we compared it to BN-free networks constructed with DKS that also used skip connections. And because we couldn’t get K-FAC to work well with Fix-up, we instead used Adam with Fix-up in our first comparison. (While this may seem unfair, we note that for networks with skip connections, K-FAC and SGD perform similarly, as shown in Subsection 28.3.) 119
Martens et al. From these results we see that Fix-up performs similarly to DKS for RELU activation functions, but falls behind for tanh and softplus. 28.6 Meta-parameter studies The influence of various training “meta-parameters” on the optimization and generalization performance of DKS networks is considered in Appendix M. These meta-parameters include the weight on the residual branch when using skip connections, DKS’s ζ parameter, and the choice of optimizer. Our conclusions from these studies are summarized as follows: √ • When using DKS with skip connections, a weight of 0.05 on the residual branch works the best overall among several other sensible options, although this is likely to be contingent on details of the architecture (such as depth). • In terms of optimization performance, ζ = 1.5 typically works better than values that are much larger, or much closer to 1, although the difference isn’t very big. In terms of generalization performance, somewhat smaller values (such as 1.1) may work slightly better. • For networks without skip connections, K-FAC is the best optimizer in terms of speed, followed closely by Shampoo. Following that are Adam and then SGD, which both perform significantly worse than Shampoo in this setting. For networks with skip connections, the gap between K-FAC and SGD narrows substantially. 28.7 Ablations and modifications of DKS Various ablations and modifications of DKS are considered in Appendix N. The overall conclusion of these studies is that each component of DKS, except perhaps for PLN (as- suming reasonably well scaled input data), is required to achieve the highest optimization 120
Deep Kernel Shaping speed. When considering test error the conclusions are similar but somewhat muted, with the single exception that using weighted mean-pooling layers with K-FAC seems to improve test set performance while degrading training set performance. 29. Conclusions In this work we developed Deep Kernel Shaping (DKS), a method for making neural net- works easier to train via model class preserving transformations. We showed how our method controls the shape of the network’s initialization-time kernel, by way of our gener- alized Q/C map analysis, in order to prevent certain common pathologies associated with slow optimization and poor generalization. In our experiments we showed that DKS allows deep networks without skip connections or normalization layers to be trained at similar speeds to ResNets on MNIST, assuming the use of K-FAC or Shampoo. To the best of our knowledge this is a unprecedented result. We also applied our generalized Q/C map analysis to explain the effectiveness of previously proposed methods for training deep net- works, such as skip connections, normalization layers, and popular initialization schemes. By demystifying trainability in deep networks, and disentangling it from model design, we hope that DKS will enable deep networks to reach new heights of performance, flexibility, and ease of use. There is even the potential that DKS may unlock a new class of neural mod- els untrainable with standard tools like normalization layers and skip connections, possibly when used in combination with strong optimizers like K-FAC or Shampoo. Finally, because of their sensitivity to the strength of the optimizer, deep skip connection- free networks constructed with DKS have the potential to serve as new benchmark prob- lem for neural network optimizers. This should be a welcome development to the area, as ResNets are often still used for benchmarking optimizers, despite the fact that it is known to be impossible to significantly outperform well-tuned SGD when training them at small/medium batch sizes (Zhang et al., 2019a). 30. Limitations and future directions We end by discussing some limitations of DKS, along with possible ways to overcome them in future work. • DKS currently doesn’t support layers with multiplicative units, such as the self- attention layers in Transformers (Vaswani et al., 2017). This is because we don’t have a kernel approximation for such layers that would yield one-dimensional Q/C maps (or something similar). A possible way around this would be to generalize C maps to higher dimensional inputs, and develop new theory along the lines of Sec- tion 13 to control their shape. Another possibility would be to find some weight initialization for self-attention layers which would give rise to one dimensional maps. • While DKS supports pooling layers in practice (based on our experiments), our theo- retical treatment of these layers in Section 20 is rudimentary and incomplete. More- over, mean-pooling layers aren’t really supported at all within our framework, since 121
Martens et al. they make it impossible to achieve uniform q values. And while weighted mean pool- ing layers can serve as a reasonable replacement (as discussed in Section 20.1.2), their Q/C map interpretation, and the quality of their kernel function approximations, are both somewhat dubious. One possible way to improve this situation would be to de- velop better replacements for pooling layers that are compatible with DKS. Another would be to extend our analysis to handle non-uniform q values, possibly as part of a generalized higher-dimensional version of Q/C map analysis. The fact that pooling layers seem to work reasonably well with DKS in practice hints that this should be possible. • To match the training speed of standard ResNets on skip connection-free networks using DKS we were required to train with K-FAC or Shampoo. From a practical perspective this is somewhat unsatisfying, as those methods are significantly more complex than SGD, and introduce additional computational overheads (although these can be largely mitigated through various strategies such as those proposed in Martens and Grosse (2015), Ba et al. (2017), and Anil et al. (2020)). An interesting direction for future work would be to try to achieve rapid training of skip connection-free networks with a much simpler optimizer like SGD or Adam (e.g. by modifying DKS somehow), or to explain the importance of stronger optimizers for training such networks. Recent work arguing for the optimality of approximate natural gradient methods like K-FAC in in the NTK setting (Zhang et al., 2019b; Karakida and Osawa, 2020) may be a good starting point for the latter direction. • In our experiments with DKS we consistently observed increased overfitting compared to standard ResNets, resulting in top-1 test set F1-score on MNIST that was lower by a few percent. This echoes similar observations made in related works such as Zhang et al. (2019c), and could by caused by a number of things, including the loss of noise from BN layers, or a subtle change in the inductive bias of the model. Addressing this remains an important direction for future work. • As discussed in Section 18.6, while RELU activation functions can be used with DKS, one can only enforce three of the four Q/C map conditions. And while RELU networks with DKS perform well in most settings in our experiments despite this limitation, they perform poorly with skip connection-free networks trained using K-FAC (relative to other activation functions). Fortunately, since we have consistently strong perfor- mance for other activation functions, including RELU-like ones such as softplus, this arguably isn’t a serious issue. Indeed, the main reason for using RELUs over other activation functions is that they are an important ingredient in the standard recipe for achieving fast and stable training of very deep networks, for which DKS is an alternative. • Recurrent neural networks (RNNs) are currently not supported by DKS. This is due to their sharing of parameters across time steps, which invalidates the kernel approx- imations that underlie our analysis. However, it is conceivable that a more advanced theory could be used to extend DKS to RNNs, and preliminary experiments we con- ducted with DKS on RNNs gave positive results, suggesting that it might already work well in practice. 122
Deep Kernel Shaping • Q/C map analysis is formally justified using kernel function approximations for neural networks at initialization time. The F1-score of these approximations is predicted by bounds such as those reviewed in Section 5.8. Currently, the best known bounds seem to be overly pessimistic, and in order to guarantee reasonable approximation error, require that the width (or channel dimension) grow exponentially with the network’s depth. We conjecture that much stronger bounds exist, although they might require the introduction of additional hypotheses, such as that the networks are constructed using DKS (or something similar). Acknowledgments We would like to thank Alex Botev, Alex Graves, Georg Ostrovski, Guodong Zhang, Ilja Kuzborskij, Koray Kavukcuoglu, Neil Rabinowitz, Soham De, Yann Dauphin, and Yee Whye Teh for their guidance, helpful discussions, and feedback on early drafts. We would also like to thank the entire team at DeepMind for supporting this project. 123
Martens et al. Part VI Appendix Appendix A. Approximating average unit values In addition to approximating the contents of the PKF Σ , which can be thought of f(Z),f(Z(cid:48)) as entry-averages (across channels) of element-wise products between pairs of vectors in f ’s output feature map, we may sometimes be interested in approximating the entry-averages of such vectors themselves. For a given k-dimensional vector y with associated q value q in f ’s output, we have the intuitive approximation 1 √ 1(cid:62)y ≈ E [φ ( qx)] , (24) k x∼N (0,1) where 1 denotes a vector of 1’s. As we will show below, this is an accurate approximation with high probability in the same sense that the APKFs are. To begin, we define a modified version g of f with activation function ψ(x) = φ(x) + 1. With this definition we have g(Z) = f (Z) + O, where O = 11(cid:62) is a matrix of 1’s. Observing that g(Z)(cid:62)g(Z) = (f (Z) + O)(cid:62)(f (Z) + O) = f (Z)(cid:62)f (Z) + O(cid:62)f (Z) + f (Z)(cid:62)O + O(cid:62)O it follows that Σ = Σ + S + S(cid:62) + Σ , g(Z),g(Z) f(Z),f(Z) O,O where 1 (cid:20) O(cid:62)f (Z) O(cid:62)f (Z) (cid:21) A = . k O(cid:62)f (Z) O(cid:62)f (Z) Meanwhile, under the APKF Condition we have that Σ = κ (Z, Z(cid:48)) g(Z),g(Z) g ≈ κ (Σ ) (cid:102)g Z,Z(cid:48) = E [ψ(u)ψ(u)(cid:62)] u∼N (0,ΣZ,Z) = E [φ(u)φ(u)(cid:62)] + E [φ(u)1(cid:62)] + E [1φ(u)(cid:62)] u∼N (0,ΣZ,Z) u∼N (0,ΣZ,Z) u∼N (0,ΣZ,Z) + E [11(cid:62)] u∼N (0,ΣZ,Z) = κ (Σ ) + v1(cid:62) + 1v(cid:62) + 11(cid:62) (cid:102)f Z,Z(cid:48) ≈ Σ + v1(cid:62) + 1v(cid:62) + 11(cid:62), f(Z),f(Z) where v = E [φ(u)]. Equating both expressions for Σ , and using the fact u∼N (0,ΣZ,Z) g(Z),g(Z) that Σ = 11(cid:62), we have O,O v1(cid:62) + 1v(cid:62) ≈ A + A(cid:62). Comparing diagonal entries of both sides implies 1 v ≈ (1(cid:62)f (Z)). k The RHS here is a vector (over locations) consisting of the vector-averages we are interested in estimating (i.e. 1 1(cid:62)y). For any given one of them with associated q value q we thus have the claimed approxk imation E (cid:2) φ (cid:0)√ qx(cid:1)(cid:3) . x∼N (0,1) 124
Deep Kernel Shaping Appendix B. Mathematical details for Section 8.1.2 In this section we will derive the formula √ Γ(cid:48) (c, q , q ) = q q Γ (c, q , q ), φ 1 2 1 2 φ(cid:48) 1 2 where (cid:104) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17)(cid:105) Γ (c, q , q ) ≡ E φ ( q x) φ q cx + 1 − c2y for x, y ∼ N (0, 1), φ 1 2 1 2 and the derivative of Γ is taken with respect to c. φ Taking the derivative inside of the expectation we have (cid:20) (cid:18) (cid:19)(cid:21) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17) √ cy Γ(cid:48) (c, q , q ) = E φ ( q x) φ(cid:48) q cx + 1 − c2y q x − √ φ 1 2 1 2 2 1 − c2 √ (cid:104) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17) (cid:105) = q E φ ( q x) φ(cid:48) q cx + 1 − c2y x 2 1 2 √ − √ q 2c E (cid:104) φ (√ q x) φ(cid:48) (cid:16)√ q (cid:16) cx + (cid:112) 1 − c2y(cid:17)(cid:17) y(cid:105) . 1 2 1 − c2 By Stein’s Lemma E [f (u)u] = E [f (cid:48)(u)], so that u∼N (0,1) u∼N(0,1) (cid:104) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17) (cid:105) √ (cid:104) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17)(cid:105) E φ ( q x) φ(cid:48) q cx + 1 − c2y x = q E φ(cid:48) ( q x) φ(cid:48) q cx + 1 − c2y 1 2 1 1 2 √ (cid:104) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17)(cid:105) − q cE φ ( q x) φ(cid:48)(cid:48) q cx + 1 − c2y 2 1 2 and (cid:104) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17) (cid:105) √ (cid:112) E φ ( q x) φ(cid:48) q cx + 1 − c2y y = q 1 − c2 1 2 2 (cid:104) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17)(cid:105) · E φ ( q x) φ(cid:48)(cid:48) q cx + 1 − c2y . 1 2 Plugging these expressions into the above equation for Γ(cid:48) (c, q , q ), and observing the can- φ 1 2 cellation of the √ q cE (cid:104) φ (cid:0)√ q x(cid:1) φ(cid:48)(cid:48) (cid:16)√ q (cid:16) cx + √ 1 − c2y(cid:17)(cid:17)(cid:105) term, we arrive at 2 1 2 √ (cid:104) √ (cid:16)√ (cid:16) (cid:112) (cid:17)(cid:17)(cid:105) √ Γ(cid:48) (c, q , q ) = q q E φ(cid:48) ( q x) φ(cid:48) q cx + 1 − c2y = q q Γ (c, q , q ). φ 1 2 1 2 1 2 1 2 φ(cid:48) 1 2 Appendix C. A detailed analysis of C map convergence in deep networks C.1 General purpose lemmas Unless specified otherwise, f n(x) will denote f ◦ . . . ◦ f (x), i.e. f composed with itself n (cid:124) (cid:123)(cid:122) (cid:125) n times. Lemma 28 Let f : R → R be given by f (x) = αx + β, α (cid:54)= 0. Then the explicit formula holds (cid:18) (cid:19) β β f n(x) = αn x − + . (25) 1 − α 1 − α 125
Martens et al. Proof Let ϕ(x) = x + β . Let us consider g = ϕ−1 ◦ f ◦ ϕ. Then 1−α (cid:20) (cid:18) (cid:19)(cid:21) β β αβ β β(1 − α) g(x) = f x + − = αx + + β − = αx + β − = αx. 1 − α 1 − α 1 − α 1 − α 1 − α Then gn(x) = αnx. Moreover, f n = ϕ ◦ gn ◦ ϕ−1, and thus (cid:18) (cid:19) β β f n(x) = αn x − + . 1 − α 1 − α Lemma 29 Let x < x and let the functions f : [x , x ] → R, f : [x , x ] → R satisfy 1 2 1 1 2 2 1 2 f (x) ≥ f (x) for every x ∈ [x , x ], and let f be an increasing function. Then for every 1 2 1 2 2 n = 1, 2, . . . such that f n−1(x) ∈ [x , x ] there holds f n(x) ≥ f n(x) 1 2 1 2 Proof We use induction. The inequality holds for n = 1 by assumption. Assume now that f k(x) ≥ f k(x) for some k and that f k(x) ∈ [x , x ]. Then f k+1(x) = f (f k(x)) ≥ 1 2 1 1 2 1 1 1 f (f k(x)) ≥ f (f k(x)), where the last inequality follows from the fact that f is an increas- 2 1 2 2 2 ing function. Lemma 30 Assume that an increasing, strictly convex map f satisfies f (1) = 1 and f (x ) = x for some x < 1. Then 0 < f(b)−x0 < 1 for every b in (x , 1). 0 0 0 b−x0 0 Proof The function f is increasing, so x = f (x ) < f (b), and it is strictly convex, so 0 0 f (tx + (1 − t)x ) < tf (x ) + (1 − t)f (x ) for all x < x and 0 < t < 1. Using x = x , 1 2 1 2 1 2 1 0 (cid:16) (cid:17) x = 1 and t = 1−b we obtain f (b) = f x · 1−b + 1 · b−x0 < 1−b f (x ) + b−x0 f (1) = 2 1−x0 0 1−x0 1−x0 1−x0 0 1−x0 1−b x + b−x0 1 = b. 1−x0 0 1−x0 C.2 Definitions and preliminaries We will call a C map nontrivial if and only if it is neither a constant function nor the identity function. As we are assuming uniform q values, we have by Section 11 that C maps are posi- tive definite functions, from which various properties immediately follow. The following proposition lists the ones we will use in this section: Proposition 31 A C map C satisfies the following properties: i) C(1) = 1 and C(0) ≥ 0, ii) for c ≥ 0 is an increasing, convex function, 126
Deep Kernel Shaping iii) if C is not a constant function it is strictly increasing for c ≥ 0, iv) if C is nontrivial, it is strictly increasing and strictly convex for c ≥ 0. Corollary 32 If C is a nontrivial c-map, then for all c ∈ [0, 1] there holds C(c) ≥ C(cid:48)(1)c + 1 − C(cid:48)(1). Proof Suppose that for some c˜ ∈ [0, 1] there holds C(c˜) < C(cid:48)(1)c + 1 − C(cid:48)(1). Then, by the Mean Value Theorem there exists c˜˜ ∈ (c˜, 1) such that 1 − C(c˜) 1 − (C(cid:48)(1)c˜ + 1 − C(cid:48)(1)) C(cid:48)(1) − C(cid:48)(1)c˜ C(cid:48)(c˜˜) = > = = C(cid:48)(1), 1 − c˜ 1 − c˜ 1 − c˜ which contradicts the strict convexity of C on [0, 1] (Proposition 31, iv)). Lemma 33 Suppose C is a nontrivial C map. Then we have C(−c) ≥ 2C(0) − C(c) (26) for every c ∈ (0, 1]. Moreover, if this inequality becomes an equality for any c ∈ (0, 1], then C is an odd function. Proof Because C is positive definite, it can be written as ∞ (cid:88) b ci i i=0 for b ≥ 0. We can then decompose this as i C(c) = C(0) + C (c) + C (c), e o ∞ ∞ where C (c) ≡ (cid:80) b c2i is an even function, and C (c) ≡ (cid:80) b c2i+1 is an odd function. e 2i o 2i+1 i=1 i=0 Thus, for c ≥ 0 C(−c) = C(0) + C (−c) + C (−c) = C(0) + C (c) − C (c) ≥ C(0) − C (c) − C (c) e o e o e o as C (c) = C (−c) and C (c) ≥ 0. Therefore e e e C(−c) ≥ C(0) − C (c) − C (c) = 2C(0) − (C(0) + C (c) + C (c)) = 2C(0) − C(c). e o e o This inequality can become an equality for any nonzero c if and only if C (c) = 0. But if e C (c) = 0 for any such c, then C ≡ 0, which ends the proof. e e C(γ)−C(0) Corollary 34 For γ ∈ (0, 1) there holds C(−c) ≥ − c + C(0) for c ∈ (0, γ). γ 127
Martens et al. Proof By convexity (Proposition 31), γ − c c γ − c c C(γ) − C(0) C(c) = C(0 · + γ · ) ≤ C(0) · + C(γ) · = c + C(0). γ γ γ γ γ Combining this and Equation 26 we get (cid:18) (cid:19) C(γ) − C(0) C(γ) − C(0) C(−c) ≥ 2C(0) − C(c) ≥ 2C(0) − c + C(0) = − c + C(0). γ γ Corollary 35 There holds C(−c) ≥ −(1 − C(0))c + C(0) for c ∈ (0, 1]. Proof By convexity (Proposition 31), C(c) ≤ (1 − C(0))c + C(0). Combining this and Equation 26 we get C(−c) ≥ 2C(0) − C(c) ≥ 2C(0) − (1 − C(0))c − C(0) = C(0) − (1 − C(0))c. Lemma 36 Let C be a C map and c ∈ (−1, 0). Then C(−c) − C(0) 0 < < 1. −c Proof The first inequality follows from the fact that C is an increasing function on [0, 1], C(0) ≥ 0, and that −c ∈ (0, 1). To show the second inequality, we consider two cases. If C(cid:48)(−c) ≥ 1, then C(−c) < −c, which is a consequence of the Mean Value Theorem. Indeed, C(cid:48) is an increasing function on the interval [0, 1], thus C(cid:48)(x) > 1 for all x ∈ [−c, 1]. By the Mean Value Theorem C(1) − C(−c) > 1 − (−c). But C(1) = 1, so this yields C(−c)−C(0) −C(−c) > c. Then −c > C(−c) ≥ C(−c) − C(0), as C(0) ≥ 0, and therefore < 1, −c because −c > 0. If, on the other hand, C(−c) < 1, then by monotonicity of C(cid:48) we have C(cid:48)(x) < 1 for C(−c)−C(0) x ∈ [0, −c], and the inequality < 1 follows from the Mean Value Theorem. −c Corollary 37 Let C be a nontrivial C map. Then either C has no fixed points on [−1, 0), or C is an odd function. 128
Deep Kernel Shaping Proof By Corollary 35, C(−c) ≥ −(1 − C(0))c + C(0) for c ∈ (0, 1]. Thus, if C(0) > 0, then C(c) ≥ (1 − C(0))c + C(0) > c + f (0) > c for all c ∈ [−1, 0]. In the case C(0) = 0 by Lemma 33 there holds C(−c) ≥ −C(c) for all c ∈ [0, 1]. But, by strict concavity, C(c) ≤ c for all c ∈ [0, 1]. Thus if C has a fixed point c ∈ [−1, 0), then −c = C(−c) ≥ −C(c) ≥ −c and thus C(−c) = 2C(0) − C(c), which (by Lemma 33) implies that C is an odd function. Corollary 38 Suppose C is a nontrivial C map. Then one of the three following alternatives holds: i) The map C has precisely one fixed point. This happens if and only if C(cid:48)(1) ≤ 1, and the fixed point is c∗ = 1. ii) The map C has precisely two fixed points. This happens if and only if C(cid:48)(1) > 1 and C is not an odd function, and the fixed points are c∗ ∈ [0, 1) and 1. iii) The function C has precisely 3 fixed points. This happens if and only if C is an odd function and the fixed points are −1, c∗ = 0 and 1. Proof We will treat each of i), ii) and iii) separately. i) By Proposition 31 iv), the function C is strictly convex on [0, 1]. We have C(1) = 1, so C(cid:48)(1) ≤ 1 implies that C(x) > x for x ∈ [0, 1). Indeed g(x) := C(x) − x satisfies g(1) = 0 and g is a decreasing function in [0,1], as g(cid:48)(x) = C(cid:48)(x) − 1 < 0 for x ∈ [0, 1). It remains to show, that C has no fixed points on [−1, 0) interval. We showed that C(0) > 0, so C cannot be an odd function, and thus, by Corollary 37 it has no fixed points on [−1, 0). ii) Assume, that C(cid:48)(1) > 1. Consider g(x) = C(x) − x. There holds g(1) = 0, and g(cid:48)(1) > 0. Thus g is an increasing function in some neighbourhood of x = 1 (by assumption C is an analytic function, hence all of its derivatives must be continuous). Then g(1 − ε) < 0 for ε sufficiently small. On the other hand, g(0) ≥ 0, and thus by continuity of g there exists x ∈ [0, 1) such that g(x) = x. The function C is strictly convex on [0, 1], (by Proposition 31 iv)), so it can have at most two fixed points on [0, 1]. By Corollary 37, these are the only fixed points of C, as we assumed that f is not an odd function. iii) We treat each of the implications in ”if an only if” separately ”⇐” By definition C(1) = 1, so if C is an odd function, f (−1) = −1 and C(0) = 0. By Proposition 31 iv), the function C is strictly convex on [0, 1], thus (because it is an odd function) it is strictly concave on (−1, 0), and thus the equation C(x) = x has no solutions on (−1, 0) ∪ (0, 1). ”⇒” Is a direct consequence of Lemma 33, as C(−1) = −1 = 2C(0) − f (1). 129
Martens et al. Corollary 39 The point c∗ from Corollary 38 ii) has to be an attractor. (In other words, 0 < C(cid:48)(c∗) < 1.) Proof As C is positive definite, C(cid:48)(c) > 0 for positive c’s. There holds C(1)−C(c∗) = 1−c∗ = 1 1−c∗ 1−c∗ thus, by the Mean Value Theorem there exists a point c¯ in the interval (c∗, 1), such that C(cid:48)(c¯) = 1. By convexity, C(cid:48) is an increasing function on (0, 1), so as c∗ < c¯, there holds C(cid:48)(c∗) < 1. The following theorems give bounds on the convergence rate of c values under repeated applications of a C map C satisfying i) or ii) of Corollary 38. Theorem 40 (Linear global attractor for C(cid:48)(1) < 1) Let a nontrivial C map C satisfy C(cid:48)(1) < 1. Then the unique (see Corollary 38) fixed point c∗ = 1 of C is a linear global attractor of the whole set [−1, 1], and the following set inequalities holds (cid:0) C(cid:48)(1)(cid:1)n (c − 1) + 1 ≤ Cn(c) ≤ 1 for all c ∈ [−1, 1]. Proof By Corollary 32 there holds C(x) ≥ C(cid:48)(1)x + 1 − C(cid:48)(1) for x ∈ [0, 1]. For val- ues of x ∈ [−1, 0) we use Lemma 33. This yields C(x) ≥ 2C(0) − C(−x) ≥ 2C(0) − (−C(cid:48)(1)x + 1 − C(cid:48)(1)) ≥ 2 − 2C(cid:48)(1) − (−C(cid:48)(1)x + 1 − C(cid:48)(1)) = 1 − C(cid:48)(1) + C(cid:48)(1)x. Thus we can apply Lemma 28 with α = C(cid:48)(1) and β = 1 − C(cid:48)(1) combined with Lemma 29 on the whole set [−1, 1] and the Theorem follows. Theorem 41 Let C be a non-trivial C map satisfying C(cid:48)(1) > 1 and C is not an odd function (i.e. satisfying ii) of Corollary 38), and let c∗ be the unique fixed point of C in the interval [0, 1). Then for all n = 1, 2, . . . i) for c ∈ (c∗, 1) there holds c∗ < Cn(c ) ≤ c∗ + (cid:16) C(c0)−C(c∗) (cid:17)n (c − c∗), 0 0 c0−c∗ 0 ii) for c ∈ [0, c∗) there holds c∗ + (C(cid:48)(c∗))n (c − c∗) < Cn(c ) < c∗, 0 0 0 (cid:16) (cid:17) iii) for c ∈ (−1, 0), if Cn−1(c ) ≤ 0 there holds αn c − C(0) + C(0) ≤ Cn(c ), where 0 0 0 1−α 1−α 0 α = C(−c0)−C(0) . −c0 Moreover, we have that C(c0)−C(c∗) , C(cid:48)(c∗), and α are all bounded strictly between 0 and 1. c0−c∗ Proof We are going to use an appropriate linear estimate in each of the three dynamical regimes, and then invoke Lemma 28 130
Deep Kernel Shaping i) Note that 0 < C(c0)−C(c∗) < 1 by Lemma 30. c0−c∗ There holds C(c) ≤ C(c0)−C(c∗) (c − c∗) + c∗ for c ∈ [c∗, c ], as C(c∗) = c∗ and C is c0−c∗ 0 a convex function in [0, 1] (by Proposition 31). First, let us note that the function C is increasing by Proposition 31. Let us take C (c) ≡ C(c0)−C(c∗) (c − c∗) + c∗ and 1 c0−c∗ C ≡ C. Such choice satisfies the assumptions of Lemma 29. Indeed, they are both 2 increasing functions, both satisfying C (c∗) = C (c∗) = c∗, and C (c ) = C (c ) < c , 1 2 1 0 2 0 0 so we get the inequality Cn(c ) ≥ Cn(c ) for all n = 1, 2, . . .. We apply Lemma 28 1 0 2 0 to function C , with α = C(c0)−C(c∗) and β = (1 − α)c∗, so that β = c∗. This yields Cn(c ) =1 c∗ + (cid:16) C(c0)−C(c∗c )0(cid:17)− nc∗ (c − c∗) for every n = 1, 2, . . ., whi1 c− hα together with 1 0 c0−c∗ 0 C(c) ≥ c∗ for c∗ ≤ c ≤ c finishes the proof in this case. 0 ii) Note that 0 < C(cid:48)(c∗) < 1 by Corollary 39. There holds C(c) ≥ C(cid:48)(c∗)(c − c∗) + c∗ for c ∈ [0, c∗), as C(c∗) = c∗ and C is a convex function in [0, 1] (by Proposition 31). Indeed, convexity implies, that C(cid:48)(c) < C(cid:48)(c∗) for c ∈ [0, c∗]. By Mean Value Theorem C(c∗)−C(c) ≤ C(cid:48)(c)(c∗−c). We have C(c∗) = c∗, so the inequality becomes c∗−C(c) ≤ C(cid:48)(c)(c∗−c), and finally, C(c) ≥ C(cid:48)(c)(c−c∗)+c∗. Let us take C ≡ C and C (c) ≡ C(cid:48)(c∗)(c−c∗)+c∗. Such functions satisfy the assumptions of 1 2 Lemma 29 (note, that C is increasing, because C(cid:48)(c ) > 0). Therefore Cn(c ) ≥ Cn(c ) 2 0 1 0 2 0 for all n = 1, 2, . . .. We apply Lemma 28 with α = C(cid:48)(c∗), β = (1 − C(cid:48)(c∗)) c∗, so that β = c∗, and obtain Cn(c ) = (C(cid:48)(c∗))n (c − c∗) + c∗, which finishes the proof in this 1−α 2 0 0 case. iii) Note, that 0 < α < 1 by Lemma 36. By applying Corollary 34 with γ = −c we obtain C(−c) ≥ − C(−c0)−C(0) c + C(0) for 0 −c0 c ∈ [0, −c ], and therefore C(c) ≥ C(−c0)−C(0) c + C(0) for c ∈ [c , 0]. Similarly to the 0 −c0 0 previous two regimes, we apply Lemma 29 to C ≡ C and C (c) ≡ C(−c0)−C(0) c + C(0), 1 2 −c0 and we get Cn(c ) ≥ Cn(c ) for all n = 1, 2, . . . such that Cn−1(c ) ≤ 0. Note, that C 1 0 2 0 0 2 is an increasing function, as C(−c0)−C(0) > 0. We apply Lemma 28 to function C with −c0 (cid:16) (cid:17) 2 α = C(−c0)−C(0) and β = C(0), which yields Cn(c ) = αn c − C(0) + C(0) for every −c0 2 0 0 1−α 1−α (cid:16) (cid:17) n = 1, 2, . . .. Thus we have Cn(c ) ≥ αn c − C(0) + C(0) whenever Cn−1(c ) ≤ 0, 1 0 0 1−α 1−α 0 which finishes the proof in this case. Theorem 42 (The case of an odd map) Let C be an odd nontrivial C map. Let 0 < cˆ < 1. Then c∗ = 0 is a linear global attractor of the whole set [−cˆ, cˆ] and for any c ∈ [−cˆ, cˆ] there holds −cˆn ≤ Cn(c) ≤ cˆn. Proof We only need to prove the inequality Cn(c) ≤ cˆn for positive c-s, due to symmetry and the fact that c > 0 ⇒ C(c) > 0 (the positive semi-axis is invariant under the map C). 131
Martens et al. By Proposition 31, iv) the map C is strictly convex for c ≥ 0, thus C(c) ≤ cˆ · c for c ∈ [0, cˆ] and the theorem follows from Lemma 28 combined with Lemma 29. Appendix D. Mathematical details for Section 13 D.1 Proof of Theorem 13 Given our running assumption of uniform q values, C maps are positive definite functions (as established in Section 11). This means that we can write ∞ (cid:88) C (c) = b ci, f i i=0 for some coefficients b (cid:62) 0, so that the derivative of C can similarly be written as i f ∞ (cid:88) C(cid:48) (c) = ib ci−1. f i i=1 Note that under this notation we have 1. b = C(cid:48) (0) , 1 f 2. (cid:80)∞ b = C (1) = 1 , and i=1 i f 3. 0 (cid:54) b (cid:54) 1 for all i. i Using these facts, it follows that (cid:12) (cid:12) ∞ (cid:12) (cid:88) (cid:12) max |C (c) − c| = max (cid:12)b + b ci − (1 − b )c(cid:12) f (cid:12) 0 i 1 (cid:12) c∈[−1,1] c∈[−1,1] (cid:12) (cid:12) i=2 (cid:34) ∞ (cid:35) (cid:88) (cid:54) max b + b |c|i + (1 − b )|c| 0 i 1 c∈[−1,1] i=2 ∞ (cid:88) (cid:54) b + b max |c|i + (1 − b ) max |c| 0 i 1 c∈[−1,1] c∈[−1,1] i=2 ∞ (cid:88) = b + b + 1 − b = 2(1 − b ) 0 i 1 1 i=2 = 2(1 − C(cid:48) (0)). f 132
Deep Kernel Shaping Observing that (cid:12) (cid:12)b 0 − 1 2 (cid:12) (cid:12) (cid:54) 21 (which follows from 0 (cid:54) b 0 (cid:54) 1) we also have that (cid:12) (cid:18) (cid:19) (cid:18) (cid:19)(cid:12) max |C f (c) − c| (cid:62) (cid:12) (cid:12)C f b 0 − 1 − b 0 − 1 (cid:12) (cid:12) c∈[−1,1] (cid:12) 2 2 (cid:12) (cid:12) (cid:12) (cid:12)(cid:88)∞ (cid:18) 1 (cid:19)i (cid:18) 1 (cid:19) (cid:18) 1 (cid:19)(cid:12) = (cid:12) b b − + b + b b − − b − (cid:12) (cid:12) i 0 2 0 1 0 2 0 2 (cid:12) (cid:12) (cid:12) i=2 (cid:12) (cid:12) (cid:12)(cid:88)∞ (cid:18) 1 (cid:19)i 1 (cid:12) = (cid:12) b b − + b b + (1 − b )(cid:12) (cid:12) i 0 2 1 0 2 1 (cid:12) (cid:12) (cid:12) i=2 (cid:62) b 1b 0 + 1 (1 − b 1) − (cid:88)∞ b i (cid:12) (cid:12) (cid:12)b 0 − 1 (cid:12) (cid:12) (cid:12)i 2 (cid:12) 2 (cid:12) i=2 1 (cid:88)∞ (cid:18) 1 (cid:19)i 1 1 (cid:88)∞ (cid:18) 1 (cid:19)i−2 (cid:62) (1 − b ) − b = (1 − b ) − b 1 i 1 i 2 2 2 4 2 i=2 i=2 ∞ 1 1 (cid:88) 1 1 (cid:62) (1 − b ) − b = (1 − b ) − (1 − b − b ) 1 i 1 0 1 2 4 2 4 i=2 1 1 1 = (1 − b ) + b (cid:62) (1 − b ) 1 0 1 4 4 4 1 = (1 − C(cid:48) (0)). 4 f Similarly, for the second measure of deviation we have (cid:12) ∞ (cid:12) (cid:34) ∞ (cid:35) (cid:12)(cid:88) (cid:12) (cid:88) max |C(cid:48) (c) − 1| = max (cid:12) ib ci−1 − (1 − b )(cid:12) (cid:54) max ib |c|i−1 + (1 − b ) f (cid:12) i 1 (cid:12) i 1 c∈[−1,1] c∈[−1,1] (cid:12) (cid:12) c∈[−1,1] i=2 i=2 ∞ (cid:88) (cid:54) ib + 1 − b i 1 i=2 = C(cid:48) (1) − b + 1 − b f 1 1 = 2(1 − C(cid:48) (0)) + (C(cid:48) (1) − 1). f f Finally, if b = C (0) = 0, we can relate the two key quantities C(cid:48) (1) − 1 and 1 − C(cid:48) (0) as 0 f f f follows: ∞ ∞ (cid:88) (cid:88) C(cid:48) (1) − 1 = C(cid:48) (1) − C (1) = ib − b f f f i i i=1 i=1 ∞ ∞ (cid:88) (cid:88) = (i − 1)b (cid:62) b = 1 − b i i 1 i=2 i=2 = 1 − C(cid:48) (0). (27) f The theorem then follows directly from the above inequalities. D.2 Proof of Proposition 16 It is well known that H has a basis h , h , h , . . . known as the Hermite polynomials (e.g. 0 1 2 Wikipedia contributors, 2021), which is orthonormal (i.e. (cid:104)h , h (cid:105) = 0 for i (cid:54)= j and i j H 133
Martens et al. (cid:107)h (cid:107) = 1), and has many other properties useful properties. Two of which we will make use i of here is that h is the identity function (i.e. h (x) = x), and that h is constant and equal 1 1 0 to 1 (i.e. h (x) = 1). 0 Because h , h , h , . . . form an orthonormal basis of H, we can represent φ in terms of 0 1 2 this basis as ∞ (cid:88) φ(x) = a h (x) where a = (cid:104)φ, h (cid:105) . i i i i H i=1 Daniely et al. (2016) showed that when its input q value is 1, the unnormalized C map φ˜(c) = Γ (c, 1, 1) of f (aka the dual activation function of φ; see Section 11.4) can be φ obtained from this representation as ∞ ∞ (cid:88) (cid:88) φ˜(c) = a2ci = (cid:104)φ, h (cid:105)2 ci. (28) i i H i=1 i=1 Using the bilinearity of inner products, and the fact that (cid:107)h (cid:107) = 1, we have 1 H (cid:107)φ − (cid:104)φ, h (cid:105)h (cid:107)2 (cid:107)φ(cid:107)2 − 2(cid:104)φ, h (cid:105) (cid:104)φ, h (cid:105) + (cid:104)φ, h (cid:105)2 (cid:107)h (cid:107)2 nl(φ)2 = 1 1 H = H 1 H 1 H 1 H 1 (cid:107)φ(cid:107)2 (cid:107)φ(cid:107)2 H H (cid:104)φ, h (cid:105)2 = 1 − 1 H . (cid:107)φ(cid:107)2 H Using the facts that φ˜(cid:48)(0) = (cid:104)φ, h (cid:105)2 (from Equation 28) and that C (c) = φ˜(c)/Q (1) = 1 H f f φ˜(c)/(cid:107)φ(cid:107)2 , we have H (cid:104)φ, h (cid:105)2 φ˜(cid:48)(0) 1 − 1 H = 1 − = 1 − C(cid:48) (0), (cid:107)φ(cid:107)2 (cid:107)φ(cid:107)2 f H H and thus nl(φ)2 = 1 − C(cid:48) (0) as claimed. f Plugging this into Theorem 13 it further follows that 1 nl(φ)2 (cid:54) max |C (c) − c| (cid:54) 2 nl(φ)2. 4 c∈[−1,1] f D.3 Proof of Proposition 18 Analogously to nl(φ), na(φ) can be written as (cid:104)φ, h (cid:105)2 na(φ)2 = 1 − 1 H . (cid:107)φ(cid:107)2 H Using Equation 11, and the identities from the previous subsection, we have (cid:107)φ(cid:48)(cid:107)2 Γ (1, 1, 1) H = φ(cid:48) = C(cid:48) (1). (cid:107)φ(cid:107)2 Q (1) f H f Combining this with (cid:104)φ(cid:48), h (cid:105)2 = φ˜(cid:48)(0) (from Equation 28), it follows that 0 H (cid:104)φ(cid:48), h (cid:105)2 φ˜(cid:48)(0) (cid:107)φ(cid:107)2 φ˜(cid:48)(0) C(cid:48) (0) 0 H = = H = f , (cid:107)φ(cid:48)(cid:107)2 (cid:107)φ(cid:48)(cid:107)2 (cid:107)φ(cid:48)(cid:107)2 (cid:107)φ(cid:107)2 C(cid:48) (1) H H H H f 134
Deep Kernel Shaping where f is a combined layer with φ as its activation function. Thus, C(cid:48) (0) na(φ)2 = 1 − f C(cid:48) (1) f as claimed. D.4 Proof Proposition 19 Because C is positive definite we can write it as C (c) = (cid:80)∞ b ci for some b (cid:62) 0 with f f i=0 i i (cid:80)∞ b = C (1) = 1. Using this we can rewrite F (c) as follows: i=0 i f f F (c) = C f (|c|) − C f (0) = (cid:80)∞ i=0 b i|c|i − b 0 = (cid:88)∞ b |c|i−1. f i |c| |c| i=1 We then have (cid:12) (cid:12) (cid:12) C f (c) − C f (0) (cid:12) (cid:12) (cid:12) = (cid:12) (cid:12) (cid:12) (cid:80)∞ i=0 b ici − b 0 (cid:12) (cid:12) (cid:12) (cid:12) c (cid:12) (cid:12) c (cid:12) (cid:12) (cid:12) ∞ (cid:12)(cid:88) (cid:12) = (cid:12) b ci−1(cid:12) (cid:12) i (cid:12) (cid:12) (cid:12) i=1 ∞ (cid:88) (cid:54) b |c|i−1 i i=1 = F (c) f as claimed. D.5 Proof of Proposition 20 As in Appendix D.4 we have C (c) = (cid:80)∞ b ci for some b (cid:62) 0 with (cid:80)∞ b = C (1) = 1, f i=0 i i i=0 i f and F (c) = (cid:80)∞ b |c|i−1. f i=1 i Let h(x) = x|c|x−1, so that h(cid:48)(x) = |c|x−1 + x|c|x−1 log |c| = |c|x−1(x log |c| + 1). We observe that h(cid:48)(x) (cid:54) 0 for x (cid:62) −1/ log |c|, and thus h(x) is a decreasing function for x (cid:62) −1/ log |c|. 135
Martens et al. Let y = log F (c)/ log |c|. Since F (c) (cid:54) 1, we have y (cid:62) −1/ log |c|. Using this fact, and f f that b (cid:62) 0 for all i, it thus follows that i (cid:12) (cid:12) ∞ (cid:12)(cid:88) (cid:12) |C(cid:48) (c)| = (cid:12) ib ci−1(cid:12) f (cid:12) i (cid:12) (cid:12) (cid:12) i=1 ∞ (cid:88) (cid:54) ib |c|i−1 i i=1 (cid:100)y(cid:101)−1 ∞ (cid:88) (cid:88) = ib |c|i−1 + ib |c|i−1 i i i=1 i=(cid:100)y(cid:101) (cid:100)y(cid:101)−1 (cid:32) ∞ (cid:33) (cid:88) (cid:88) (cid:54) y b |c|i−1 + b y|c|y−1 i i i=1 i=i0+1 ∞ (cid:32) ∞ (cid:33) (cid:88) (cid:88) (cid:54) y b |c|i−1 + b y|c|y−1 i i i=1 i=0 = y(F (c) + |c|y−1) f log F (c) = f (F (c) + |c|log F f (c)/ log |c|−1) f log |c| (cid:18) (cid:19) log F (c) F (c) F (c) log F (c) f f f f = F (c) + = (1 + |c|). f log |c| |c| |c| log |c| D.6 Proof of Proposition 22 Let C be the C map for each of the D subnetworks. Because C is positive definite we can write it as C(c) = (cid:80)∞ b ci for some b (cid:62) 0 with (cid:80)∞ b = C(1) = 1. i=0 i i i=0 i Since c∗ = 1, we have that 1 is an attractive fixed point of C, and thus 0 (cid:54) C(cid:48)(1) (cid:54) 1. (noting that C(cid:48)(1) (cid:62) 0 is true because C is positive definite and thus convex on [0, 1]). Meanwhile, since C(1) = 1, we have by the chain rule that C(cid:48) (1) = C(cid:48)(1)D. f Now because C(cid:48)(1) (cid:54) 1, we have by Corollary 38 that −1 cannot be a fixed point of C, and thus c ≡ C(−1) > −1. There are two cases for c to consider. 1 1 In that case that c = 1 we have 1 ∞ ∞ ∞ ∞ (cid:88) (cid:88) (cid:88) (cid:88) b = C(1) = 1 = C(−1) = b (−1)i = b − 2 b i i i i i=0 i=0 i=0 i odd and thus (cid:80) b = 0. Because b (cid:62) 0 for all i it thus follows that b = 0 for odd i, and so i odd i i i C is an even function and therefore C(cid:48)(−1) = −C(cid:48)(1). It remains to consider the case c (cid:54)= 1. Let g be a subnetwork of f consisting of D − 1 1 compositions of the subnetworks that define f . Under this definition we have C (c) = f C (C(c)). Since c∗ = 1 we have that C (c) → 1 as D → ∞ for any c ∈ (−1, 1) so g g that F (c) → 0. Thus by Proposition 20 (and Remark 21 to handle the case c = 0) it g 1 follows that |C(cid:48) (c )| → 0 (since c ∈ (−1, 1)). By the chain run we therefore have that g 1 1 |C(cid:48) (−1)| = |C(cid:48) (c )||C(cid:48)(−1)| → 0 as D → ∞. f g 1 136
Deep Kernel Shaping D.7 Proof of Proposition 25 Since 1 − (cid:15) > 0, we have by definition that C (1 − (cid:15)) − C (0) f f F (1 − (cid:15)) = . f 1 − (cid:15) Rearranging and bounding, this becomes C (1 − (cid:15)) = C (0) + (1 − (cid:15))F (1 − (cid:15)) (cid:54) C (0) + F (1 − (cid:15)). f f f f f Because C is positive definite and thus convex on [0, 1], we have for any c ∈ [0, 1] that f C (1) − C (c) C(cid:48) (1) (cid:62) f f . f 1 − c Taking c = 1 − (cid:15) and using using C (1) = 1 and the above inequality, we thus have f 1 − C (1 − (cid:15)) 1 − (C (0) + F (1 − (cid:15))) C(cid:48) (1) (cid:62) f (cid:62) f f f (cid:15) (cid:15) as claimed. Appendix E. Mathematical details for Section 20.1.2 A weighted mean-pooling layer f over (cid:96) locations is defined by f (Z) = Zw, where w is an (cid:96)-dimensional vector of parameters initialized using N (0, (1/(cid:96))I). In this section we will show that 1 (cid:34) tr (cid:0) 1 Z(cid:62)Z(cid:1) tr (cid:0) 1 Z(cid:62)Z(cid:48)(cid:1) (cid:35) κ (Σ ) = (cid:16) k (cid:17) (cid:16) k (cid:17) (cid:102)f Z,Z(cid:48) (cid:96) tr 1 Z(cid:48)(cid:62)Z tr 1 Z(cid:48)(cid:62)Z(cid:48) k k is an approximation of κ at initialization-time (with high probability), and that it becomes f more precise as (cid:96) grows, but only if the average absolute input c value to f (across pairs of locations) simultaneously goes to zero. Noting that E[ww(cid:62)] = (1/(cid:96))I, we have for arbitrary matrix M that E[w(cid:62)M w] = E[tr(w(cid:62)M w)] = E[tr(ww(cid:62)M )] = tr(E[ww(cid:62)]M ) = tr(M )/(cid:96). Thus, when conditioned on Z and Z(cid:48), we have E[f (Z)(cid:62)f (Z(cid:48))] = E[w(cid:62)Z(cid:62)Z(cid:48)w] = tr(Z(cid:62)Z(cid:48))/(cid:96). It then follows that 1 (cid:20) f (Z)(cid:62)f (Z) f (Z)(cid:62)f (Z(cid:48)) (cid:21) 1 (cid:34) tr(Z(cid:62)Z) tr(Z(cid:62)Z(cid:48)) (cid:35) E[κ (Z, Z(cid:48))] = = (cid:16) (cid:17) (cid:16) (cid:17) , f k f (Z(cid:48))(cid:62)f (Z) f (Z(cid:48))(cid:62)f (Z(cid:48)) k(cid:96) tr Z(cid:48)(cid:62)Z tr Z(cid:48)(cid:62)Z(cid:48) where k is the number of channels. This is the claimed formula for κ (Σ ). (cid:102)f Z,Z(cid:48) It remains to establish the conditions under which κ (Z, Z(cid:48)) will concentrate around its f expectation. This is more difficult than in the combined layer case, as the different output 137
Martens et al. channels of f (Z) are not independent given Z, meaning that the variance of κ (Z, Z(cid:48)) will f not necessarily shrink as k grows. Instead, our strategy going forward will be to compute 1 Var([κ (Z, Z(cid:48))] ) = Var(f (Z)(cid:62)f (Z(cid:48))) f 1,2 k2 as a function of Z and Z(cid:48), and argue that this goes to zero as (cid:96) grows, given certain conditions on the value of the product Z(cid:62)Z(cid:48) . (This will be sufficient to handle the other 3 entries of κ (Z, Z(cid:48)) by taking Z = Z(cid:48).) Given a shrinking variance, concentration then follows via f Chebyshev’s inequality, although a faster rate could likely be obtained by observing that [κ (Z, Z(cid:48))] is an average of independent sub-exponential random variables (for which f 1,2 better concentration bounds exists). By Appendix A of Cooijmans and Martens (2019) we have for an arbitrary matrix M that E [uu(cid:62)M uu(cid:62)] = σ4(tr(M )I + 2M ). u∼N (0,σ2I) Thus, by taking M = Z(cid:62)Z(cid:48) it follows that E[(f (Z)(cid:62)f (Z(cid:48)))2] = E[w(cid:62)Z(cid:62)Z(cid:48)ww(cid:62)Z(cid:62)Z(cid:48)w] = E[tr(w(cid:62)Z(cid:62)Z(cid:48)ww(cid:62)Z(cid:62)Z(cid:48)w)] = tr(E[ww(cid:62)Z(cid:62)Z(cid:48)ww(cid:62)]Z(cid:62)Z(cid:48)) 1 2 = tr(Z(cid:62)Z(cid:48))2 + tr((Z(cid:62)Z(cid:48))2). (cid:96)2 (cid:96)2 Given this, and the above fact that E[f (Z)(cid:62)f (Z(cid:48))] = tr(Z(cid:62)Z(cid:48)), we have 2 Var(f (Z)(cid:62)f (Z(cid:48))) = E[(f (Z)(cid:62)f (Z(cid:48)))2] − E[(f (Z)(cid:62)f (Z(cid:48)))]2 = tr((Z(cid:62)Z(cid:48))2), (cid:96)2 and so we conclude that 2 Var([κ (Z, Z(cid:48))] ) = tr((Z(cid:62)Z(cid:48))2). f 1,2 k2(cid:96)2 Note that the entries of Z(cid:62)Z(cid:48) will grow in proportion to k, so that tr((Z(cid:62)Z(cid:48))2) will grow in proportion to k2, and thus the variance won’t shrink as k grows. Moreover, by taking Z and Z(cid:48) to both be matrices of 1’s (so that their associated q values are 1 for each location), we have that tr((Z(cid:62)Z(cid:48))2) = k2(cid:96)2, and so the variance won’t always shrink as (cid:96) grows either. Nonetheless, we can identify situations where it will shrink with (cid:96), as we will explain next. Let q be the maximum dimension-normalized squared norm of the columns of Z and max Z(cid:48), or in other words, the maximal input q value to f . By the Cauchy-Schwarz inequality, the absolute values of the entries of Z(cid:62)Z(cid:48) are upper bounded by q . If Z(cid:62)Z(cid:48) is diagonal it max is easy to show that tr((Z(cid:62)Z(cid:48))2) (cid:54) k2(cid:96)q2 , and so the variance will indeed shrink at a rate max of 1/(cid:96). More generally, if c is the average absolute cosine similarity between columns avg of Z and Z(cid:48) (i.e. the average absolute input c value between Z and Z(cid:48) across all pairs of locations), then the average absolute value of the off-diagonal entries of Z(cid:62)Z(cid:48) will be bounded by q c . In this case it follows that tr((Z(cid:62)Z(cid:48))2) (cid:54) k2(cid:96)q2 (1 + (cid:96)c2 ), and so max avg max avg the variance will shrink as (cid:96) grows, provided that c simultaneously goes to zero. avg 138
Deep Kernel Shaping Appendix F. Mathematical details for Section 18.5 F.1 Proof of Proposition 26 By Equations 11 and 16 we have 1 C(cid:48) (1) = E [φˆ(cid:48)(x)2] f Q (1) x∼N (0,1) f E [(αγφ(cid:48)(αx + β))2] x∼N (0,1) = γ2 Var [φ(αx + β)] x∼N (0,1) E [φ(cid:48)(αx + β)2] x∼N (0,1) = . 1 Var [φ(αx + β)] α2 x∼N (0,1) Taking α → 0 in the numerator clearly gives φ(cid:48)(β)2. To handle the denominator, we will make use of the “delta method” from statistics (which is derived using a Taylor series argument), which says that if φ(cid:48)(αx + β) is a continuous with respect to x, φ(cid:48)(β) (cid:54)= 0, and 1 (αx − 0) = x is distributed as N (0, 1), then 1 (φ(αx + β) − φ(β)) converges in distribution α α to N (0, φ(cid:48)(β)2) as α → 0. It thus follows that (cid:20) (cid:21) 1 1 φ(cid:48)(β)2 = lim Var (φ(αx + β) − φ(β)) = lim Var [φ(αx + β)], α→0 x∼N (0,1) α α→0 α2 x∼N (0,1) and so we conclude that lim C(cid:48) (1) = 1. α→0 f Appendix G. Mathematical details for Section 20.2 In this section we will argue that a max pooling layer f approximately preserves its input q values, under the condition the vectors within a given patch of its input feature map always fall into two tight clusters. To simplify the argument, we will assume that these clusters have zero variance (or in other words that there are only two distinct input vectors in each patch), which means that our conclusions will only hold approximately. In addition, we will assume uniform input q values to f (which is guaranteed by DKS), and that f is directly preceded by a convolutional layer g initialized with a Gaussian Delta or Gaussian fan-in scheme. To begin, we observe that f (cid:0)√ qZ(cid:1) = √ qf (Z) for all q (cid:62) 0, and thus we may assume without loss of generality that the input q values are 1. We may also assume without loss of generality that the number of locations in each patch is two, since extra vectors that are duplicates of the first two will not affect the maximum. (cid:104) (cid:105) Consider a single patch in f ’s input. For each channel i, we denote by x(i) = x(i) x(i) 1 2 the two inputs in said patch for that channel. When conditioned on the input to g, we have (cid:20) (cid:21) 1 c that the x(i) are iid Gaussian vectors with mean zero and variance matrix Σ = for c 1 some c ∈ [−1, 1]. This follows from the fact the x(i)’s are linear combinations of iid mean zero Gaussian random variables (the weights), and that the g’s output q values are 1 by assumption. 139
Martens et al. The dimension-normalized squared norm of f ’s output vector associated with this patch (which is what the corresponding output q value approximates) is given by k 1 (cid:88) s = y2, k k i i=1 (i) (i) where y = max{x , x }, and where k is the number of channels (which is the same both i 1 2 for f ’s input and output). So, to argue that f ’s output q value is 1, it suffices to show that the mean of s is indeed 1, and that it concentrates around this mean as k grows. k To compute the mean we will make use of the following result from Nadarajah and Kotz (2008): Lemma 43 (Adapted from Nadarajah and Kotz (2008)) Suppose u and u are Gaus- 1 2 sian random variables wth means µ and µ , variances σ2 and σ2, and correlation coefficient 1 2 1 2 ρ. Then we have (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) µ − µ µ − µ µ − µ E[max{u , u }2] = (σ2+µ2)Φ 1 2 +(σ2+µ2)Φ 2 1 +(µ +µ )θφ 1 2 , 1 2 1 1 θ 2 2 θ 1 2 θ where φ and Φ are the pdf and cdf of the standard normal distribution, and where (cid:113) θ = σ2 + σ2 − 2ρσ σ . 1 2 1 2 In our case we have µ = µ = 0, σ2 = σ2 = 1, and ρ = c. Substituting these into the 1 2 1 2 above expression yields E[y2] = 2Φ(0) = 1. i As s is the average of the y2’s, it thus follows that E[s ] = 1. It remains to show that s k i k k concentrates around its mean as k grows. Observe that y4 = max{x(i) , x(i) }4 (cid:54) (x(i) )4 + (x(i) )4. We thus have i 1 2 1 2 Var(y2) (cid:54) E[y4] (cid:54) E[(x(i) )4] + E[(x(i) )4] = 3 + 3 = 6, i i 1 2 where we have used the fact that E [u4] = 3. u∼N (0,1) Because the y ’s are independent we have Var(s ) = (1/k2)6k = 6/k, and so the variance i k shrinks as k grows. Concentration of s around its mean then follows by Chebyshev’s k inequality. (Note that one could possibly obtain a faster rate than Chebyshev’s inequality gives by arguing that the y2’s are sub-exponential random variables, and then applying i concentration bounds for averages of such variables.) Appendix H. Mathematical details for Section 24 H.1 Estimating per-layer NTK matrices for deep nets with degenerate C maps In this section we will estimate the per-layer NTK matrices K for each of the different cases i described in Section 24.5. For the sake of simplicity we will argue in a semi-rigorous manner, 140
Deep Kernel Shaping employing fuzzy notions like “very large”, “small”, “(not) approximately equal”, “not too close”, “reasonably smooth”, etc. Note that in infinite depth limit these fuzzy notions all become precise, and in particular, “approximately equal” becomes “equal”, “reasonably smooth” becomes “smooth”, etc. Note that all of our conclusions from this analysis have been verified by our numerical studies. Let c ≡ x(cid:62)x(cid:48)/d for some pair of inputs x and x(cid:48) taken from the training set. Note 0 0 that by assumption we have c (cid:54)≈ ±1. 0 Following the notation of Section 24.4, for the i-th combined layer f of the network f i we will denote by g the subnetwork that maps f ’s input to the input of f , and by h the i i i subnetwork that maps the input of f to f ’s final output. i Suppose that f is early in the network, so that i is small and D − i is large. This i means that g is a shallow subnetwork of f , and so C is well-behaved, while h is a deep i gi i subnetwork, and so C is degenerate. Because C is well-behaved it is reasonably smooth hi gi (e.g. by Theorem 13), and so C (c ) (cid:54)≈ ±1 (since c (cid:54)≈ ±1). Then since C is degenerate, gi 0 0 hi this implies by Proposition 20 that C(cid:48) (C (c )) ≈ 0. Meanwhile, we trivially have that hi gi 0 C(cid:48) (C (1)) = C(cid:48) (1) (since C (1) = 1). Thus, Θ (x, x(cid:48)) = C (c )C(cid:48) (C (c )) ≈ 0 by hi gi hi gi i gi 0 hi gi 0 Equation 23, and also Θ (x, x) = C (1)C(cid:48) (C (1)) = 1 · C(cid:48) (1) = C(cid:48)(1)D−i. Since x and x(cid:48) i gi hi gi hi are general distinct inputs from the training set we thus have that K ≈ C(cid:48)(1)D−iI. And i because D − i is large this will be very small when C(cid:48)(1) < 1, very large when C(cid:48)(1) > 1, and equal to the identity matrix when C(cid:48)(1) = 1. Now suppose f is a layer later in the network, so that i is large and D − i is small. i This means that g is a deep subnetwork of f , and so C is degenerate, while h is i gi i a shallow subnetwork, and so C is well-behaved. Because C is degenerate we have hi gi C (c ) ≈ c∗ (as c (cid:54)≈ ±1 by assumption). Since C is well-behaved it is reasonably gi 0 0 hi smooth, and thus C(cid:48) (C (c )) ≈ C(cid:48) (c∗). By Equation 23 we therefore have that Θ (x, x(cid:48)) = hi gi 0 hi i C (c )C(cid:48) (C (c )) ≈ c∗C(cid:48) (c∗), and Θ (x, x) = C(cid:48)(1)D−i as in the previous case. Since x gi 0 hi gi 0 hi i and x(cid:48) are distinct general inputs from the training set we thus have that K ≈ C(cid:48)(1)D−iI + i c∗C(cid:48) (c∗)(E − I) = C(cid:48)(1)D−iI + c∗C(cid:48)(c∗)D−i(E − I), where E denotes the matrix of 1’s and hi we have used the fact that c∗ is a fixed point of C to get that C(cid:48) (c∗) = C(cid:48)(c∗)D−i. If c∗ = 1, hi then the estimate for K simplifies to K ≈ C(cid:48)(1)D−iE. i i Finally, suppose f is a layer in the middle of the network, so that both i and D − i are i large. This means that both g and h are deep subnetworks of f , and so C and C are i i gi hi degenerate. Because C is degenerate we have C (c ) ≈ c∗ (as c (cid:54)≈ ±1 by assumption). gi gi 0 0 Thus, by Equation 23 we have that Θ (x, x(cid:48)) = C (c )C(cid:48) (C (c )) ≈ c∗C(cid:48) (C (c )), and i gi 0 hi gi 0 hi gi 0 Θ (x, x) = C(cid:48)(1)D−i as in the previous case. There are three scenarios to consider. i If 0 (cid:54) c∗ < 1 and C(cid:48)(1) > 1, then since C (c ) ≈ c∗ (cid:54)≈ 1 by assumption and C(cid:48) (c) ≈ 0 gi 0 hi for all c (cid:54)≈ ±1 by Proposition 20, it follows that C(cid:48) (C (c )) ≈ 0. So in this case we have hi gi 0 K ≈ C(cid:48)(1)D−iI, which will be very large since C(cid:48)(1) > 1 and D − i is large. i If c∗ = 1 and C(cid:48)(1) < 1, then since as D − i is large and C(cid:48) is non-negative and non- hi decreasing (by Section 11.2) we have 0 (cid:54) C(cid:48) (C (c )) (cid:54) C(cid:48) (1) = C(cid:48)(1)D−i ≈ 0, and thus hi gi 0 hi C(cid:48) (C (c )) ≈ 0. So in this scenario we have K ≈ C(cid:48)(1)D−iI ≈ 0 since C(cid:48)(1) < 1 and D − i hi gi 0 i is large. 141
Martens et al. If c∗ = 1 and C(cid:48)(1) = 1, then we have C (c ) ≈ c∗ = 1, but this doesn’t help us estimate gi 0 C(cid:48) (C (c )), since the value of C(cid:48) (c) may be highly sensitive to the distance of c from hi gi 0 hi 1 (because C is degenerate and can thus have extreme behavior near c = 1). Since we hi have that C(cid:48) (1) = C(cid:48)(1)D−i = 1, and that C(cid:48) is non-negative and non-decreasing on [0, 1] hi hi (which contains C (c )), we do at least know that the entries of K are bounded between gi 0 i 0 and 1, and that the diagonal entries are 1. From numerical studies we conducted of the C map of deep RELU networks (which by Section 12.1 have c∗ = 1 and C(cid:48)(1) = 1) we observe that K ≈ I + α (E − I), where 0 = α (cid:54) α (cid:54) · · · (cid:54) α = 1 are constants, and we i i 1 2 D conjecture that this holds in general. H.2 Estimating the overall NTK matrix Having computed an estimate of the per-layer NTK matrix K in each case for c∗ and i, it i remains to estimate the full NTK matrix for each c∗. When c∗ = 1 and C(cid:48)(1) < 1 we have K ≈ 0 for early layers and middle layers, and i K ≈ C(cid:48)(1)D−iE for later layers. If L is the number of later layers we thus have i (cid:88)D (cid:88)D L (cid:88)−1 1 − C(cid:48)(1)L 1 K = K ≈ C(cid:48)(1)D−iE = C(cid:48)(1)iE = E ≈ E. i 1 − C(cid:48)(1) 1 − C(cid:48)(1) i=1 i=D−L+1 i=0 When 0 (cid:54) c∗ < 1 and C(cid:48)(1) > 1 we have K ≈ C(cid:48)(1)D−iI for early and middle layers, i and K ≈ C(cid:48)(1)D−iI + c∗C(cid:48)(c∗)D−i(E − I) for later layers. Moreover, since c∗ is an attractive i fixed point of C we have C(cid:48)(c∗) < 1. If L is the number of later layers we thus have D (cid:88) K = K i i=1 D D (cid:88) (cid:88) ≈ C(cid:48)(1)D−iI + c∗C(cid:48)(c∗)D−i(E − I) i=1 i=D−L+1 D−1 L−1 (cid:88) (cid:88) = C(cid:48)(1)iI + c∗C(cid:48)(c∗)i(E − I) i=0 i=0 1 − C(cid:48)(1)D 1 − C(cid:48)(c∗)L = I + c∗ (E − I) 1 − C(cid:48)(1) 1 − C(cid:48)(c∗) 1 − C(cid:48)(1)D c∗ ≈ I + (E − I). 1 − C(cid:48)(1) 1 − C(cid:48)(c∗) Finally, when c∗ = 1 and C(cid:48)(1) < 1, we have K ≈ I for early layers, K ≈ I + α (E − I) i i i for some 0 (cid:54) α < 1 for middle layers (which is only a conjectured formula), and K ≈ E i i for later layers. Or in general, we have K ≈ I + α (E − I) for some 0 (cid:54) α (cid:54) 1 for all i i i layers. This gives D D (cid:88) (cid:88) K = K ≈ I + α (E − I) = D(I + α¯(E − I)), i i i=1 i=1 142
Deep Kernel Shaping where α¯ = 1 (cid:80)D α (so that 0 (cid:54) α¯ (cid:54) 1). From our empirical studies of the C map of deep D i=1 i RELU networks we observe that α¯ = 1/4, and for some other example C maps we observe α¯ = 1/3 (the latter of which is consistent with the estimate given in Xiao et al. (2020)). H.3 Proof of Theorem 27 Let g and h for i = 1, 2, . . . , D be defined as in Section 24.4 for the network in question, i i and let x and x(cid:48) be two input data vectors. Under DKS we have C (0) = C (0) = 0, C(cid:48) (1) (cid:54) ζ(i−1)/(D−1), and C(cid:48) (1) (cid:54) ζ(D−i)/(D−1). gi hi gi hi By Theorem 13 this implies that |C (c ) − c | (cid:54) 2(ζ(i−1)/(D−1) − 1) gi 0 0 and |C(cid:48) (C (c )) − 1| (cid:54) 3(ζ(D−i)/(D−1) − 1), hi gi 0 where c = x(cid:62)x(cid:48)/d . 0 0 As q = 1 under DKS, it then follows from Equation 23 that D (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)Θ i(x, x(cid:48)) − d1 x(cid:62)x(cid:48)(cid:12) (cid:12) (cid:12) = |C gi(c 0)C h(cid:48) i(C gi(c 0)) − c 0| 0 (cid:54) |C(cid:48) (C (c )) − 1|(|c | + |C (c ) − c |) + |C (c ) − c | hi gi 0 0 gi 0 0 gi 0 0 (cid:54) 3(ζ(D−i)/(D−1) − 1)(1 + 2(ζ(i−1)/(D−1) − 1)) + 2(ζ(i−1)/(D−1) − 1) (cid:54) 6(ζ(D−i)/(D−1)ζ(i−1)/(D−1) − 1) + 3(ζ(D−i)/(D−1) − 1) + 2(ζ(i−1)/(D−1) − 1) = 6(ζ − 1) + 3(ζ(D−i)/(D−1) − 1) + 2(ζ(i−1)/(D−1) − 1) (cid:54) 11(ζ − 1), where we have used the general facts that |ab − c| (cid:54) |b − 1|(|c| + |a − c|) + |a − c| on the second line, and |a − 1||b − 1| (cid:54) |ab − 1| for a, b (cid:62) 1 on the third line. Appendix I. Path-weight analysis and its relationship to approximate kernel analysis Path-weight analysis is a method for analyzing deep fully-connected RELU networks de- veloped in a series works (Balduzzi et al., 2015; Balduzzi, 2016; Balduzzi et al., 2017). It is capable of approximating some of the same quantities computed by Q/C maps. Unlike those methods, it is not based on the “propagation” of anything through the layers of the network, but instead exploits the special structure of RELU networks to decompose their computation in terms of a collection of simple “paths” that are easier to analyze. In this section we will give a quick derivation of path-weight analysis, identify a possible issue with it, and then discuss how its predictions relate to those made by Q/C maps. For simplicity, we will assume that the sub-network f has only 1 output unit, and biases of zero. We will start by defining a path as a sequence of (scalar-valued) units chosen from each layer of the network (which includes the input layer). (So for example, we might 143
Martens et al. choose the 4-th unit from the input layer, the 2-nd unit from the next layer, etc). An active path p is one where every RELU unit in p is active in the sense of having a non-negative input value (i.e. so that the RELU function is in its “linear region”). We then define a path-weight W for a path p as the product of the weights that connect the units of p in p (cid:80) f ’s graph representation. Given these definitions, its not hard to see that f (x) = W x , p p p where x is the entry of x corresponding to the (single) input unit in p, and the sum is p taken over all active paths in f . Observe that the total number of paths P is simply the product of the input dimensions for each fully-connected layer. Thus, if the weights are chosen according to a standard Gaussian fan-in initialization (so that they are iid with mean zero and variance σ2/m, where m is the layer’s input dimension) it follows that (cid:26) σ2D/P if p = q E[W W ] = , p q 0 otherwise where D is the number of weights along p. Using the fact that the expected number of active paths is the same starting from any input unit (due to symmetry), we thus have that   (cid:34)(cid:32) (cid:33) (cid:32) (cid:33)(cid:35) E[f (x)f (x(cid:48))] = E (cid:88) W px p (cid:88) W qx(cid:48) q = σ2DE  (cid:88) x px(cid:48) p = σ2D E[A Px,x(cid:48)] k1 x(cid:62)x(cid:48), p q p co-active (29) where A is the number of paths that are active for both x and x(cid:48) (which are called co- x,x(cid:48) active paths), and k is the dimension of x. So, if one can estimate the expected fraction of paths which are co-active (i.e. E[A ]/P ), then one can effectively estimate per-unit x,x(cid:48) expected squared values (or products) under the weight distribution, which are precisely the quantities estimated by signal propagation (and thus Q/C maps, as discussed in Section 25.7). In order to estimate expected numbers of active and co-active paths, Balduzzi et al. (2017) make the key approximating assumption that for any setting of the weights, exactly half of the RELU units in each nonlinear layer are active for a typical input, and exactly one quarter of these units are active (or co-active) for a pair of typical distinct inputs. Given this assumption, and that the number of nonlinear layers is R, the fraction of all possible paths ending at y which are active for any x is thus (1/2)R, and the fraction which are co-active for any distinct x and x(cid:48) is (1/4)R. While this assumption is clearly not true in general, one can possibly argue that the fraction of the units that are active or co-active in a given layer will concentrate around its expectation as the layer widths go to infinity. However, while the expected fraction is indeed 1/2 for active units (e.g. because the input distribution to each unit is symmetric about 0), it won’t in general be 1/4 for co-active units. (For example, given two nearly identical inputs x and x(cid:48), the fraction of co-active units will be much closer to 1/2, especially in early layers.) Despite this potentially significant issue, Balduzzi et al. (2017) find that this assumption is a reasonable approximation in practice for certain well-behaved example networks with realistic inputs. When f is a standard feed-forward RELU networks with R nonlinear layers with σ2 = 2 (which is He et al.’s (2015) recommendation for RELU networks) and D = R, Equation 29 144
Deep Kernel Shaping thus gives E[f (x)2] ≈ (cid:107)x(cid:107)2/k and E[f (x)f (x(cid:48))] ≈ 1/2R(x(cid:62)x(cid:48))/k for x (cid:54)= x(cid:48). The former of these predictions agrees with Q/C maps in the sense that f is equivalent to a network g with √ σ2 = 1 and normalized RELUs (given by multiplication of a RELU by 2), for which we have Q (q) = q. The latter prediction does not however agree, as we have by Section 12.1 g that C (c) rapidly approaches 1 instead of 0 for all c as the depth grows. (This mismatch g is likely due to the approximating assumption discussed above.) (cid:80) As f (x) = W x , the derivative of f (x) with respect to some input unit is just the p p p sum over path-weights for all active paths starting at that unit. Using this fact, it follows that E[f (cid:48)(x)(cid:62)f (cid:48)(x(cid:48))] = σ2DE[A ]/P (using a derivation similar to Equation 29). For f x,x(cid:48) given as above, Balduzzi et al. (2017) use this identity to show that (cid:26) 1 if x = x(cid:48) E[f (cid:48)(x)(cid:62)f (cid:48)(x(cid:48))] ≈ 1/2R otherwise for standard feed-forward RELU networks with R nonlinear layers. They then argue that this decorrelation (or “shattering”) of the input gradients will make the network difficult or impossible to successfully train. As we saw in Section 24, Q/C map analysis can be used to compute the NTK matrix K (and per-layer NTK matrices K for i = 1, 2, . . . , D), which is a matrix of estimates of i the inner products between parameter gradients for different training inputs (assuming a squared error loss). For deep normalized RELU networks such as g we are in the collapsing case with c∗ = 1 and C(cid:48)(1) = 1 (by Section 12.1), and so assuming that D is large we have by Section 24.5 that K is approximately the identity for early layers, and approximately i equal to a matrix of ones for later layers. This predicts that substantial optimization will only occur in early layers, and that this is unlikely to yield any significant generalization. Note that this prediction is somewhat different than the one made by Balduzzi et al. (2017) insofar as the training loss will in fact be minimized given sufficiently wide layers. The per-layer NTK matrix for the first linear layer, denoted K , is closely related to the 1 matrix of input gradients estimated above under path-weight analysis. Reassuringly, the predictions agree in the sense that they are both (close to) an identity matrix. Appendix J. Analyzing (nearly) standard ResNets using Q/C maps As discussed in Section 26.6, ResNets represent a very popular solution to the same problem that DKS is aimed at solving: how to construct a very deep network that can be trained with a gradient-based optimizer. It thus worth understanding how the effectiveness of ResNets can be explained within the framework of Q/C maps. Since our analysis framework doesn’t handle BN layers, we can’t apply it directly to standard ResNets. As a compromise, we will instead analyze a ResNet which is modified to use Layer Normalization (LN) layers in their place, as these are handled within our framework, and perform a somewhat similar function. (See Section 19.2 for a discussion of normalization layers.) Note that Transformer models (Vaswani et al., 2017), which also employ a residual structure, already use LN layers in place of BN layers, although BN layers remain the more popular option for convolutional residual networks. In order to apply our Q/C map analysis to convolutional layers, we will assume the use of a Delta initialization (as opposed to a Gaussian fan-in initialization). And to simulate a 145
Martens et al. weight variance of 2 as used in He et al.’s (2015) initialization scheme we will use “normal- ized RELU” activations, which are obtained from standard RELU activation functions by √ multiplication of their input (or output) by 2. The Q map for a normalized RELU layer is the identity function, which can be straight- forwardly derived from Equation 5. So, since each (normalized) RELU nonlinear layer is immediately preceded by an LN layer, we have that its output q value will always be 1 (as the output q value of a LN layer is always 1). It thus follows that the output q value of a residual branch will always be 1 (since affine layers preserve q values). Meanwhile, for non-transition blocks (i.e. those with equal input and output channel dimensions), shortcut branches compute the identity function, so that their Q map is the identity. And for tran- sition blocks, shortcut branches consist of an LN layer, a RELU layer, and an affine layer, so that their output q value is always 1. By Equation 12, the output q value of residual block is the sum of the q values for the two branches, which will therefore be 1 plus the block’s input q value for non-transition blocks (mirroring De and Smith’s (2020) variance propagation analysis), and 1 + 1 = 2 for transition blocks. From these we observations it follows that q values will grow as a sequence 1, 2, . . . with each successive block, until a transition block is encountered, at which point the q value is reset to 2. For the standard values 50, 101, and 152 of the ResNet-V2 “depth” parameter D, there is a sequence of (D − 2)/3 residual blocks, 4 of which are transition blocks (which includes the first block). Thus, the sequence of input q values for the blocks are 1, 2, . . . , q , q + 1 1 1, 2, . . . , q , q + 1, 2, . . . , q , q + 1, 2, . . . , q , for some integers q (where the input q values 2 2 3 3 4 i to the transition blocks are q + 1, q + 1, q + 1, and q + 1). For D = 50 we have 1 2 3 4 (q , q , q , q ) = (3, 4, 6, 3), for D = 101 we have (q , q , q , q ) = (3, 4, 23, 3), and for D = 152 1 2 3 4 1 2 3 4 we have (q , q , q , q ) = (3, 8, 36, 3). 1 2 3 4 Let C denote the local C map for a RELU nonlinear layer, which is given in Equation 15, and notably doesn’t depend on the input q value (which will always be the case for positively homogeneous activation functions). Further, let B denote the C map for non- q transition blocks with input q value q, and T denote the C map for transition blocks (which doesn’t depend on the input q value). From the above analysis we have C = C ◦ E ◦ E ◦ E ◦ E , f 4 3 2 1 where E = B ◦ B ◦ · · · ◦ B ◦ B ◦ T for i ∈ {1, 2, 3, 4}. i qi qi−1 3 2 As discussed in Section 19.2, LN layers always output q values of 1. And because their inputs always comes directly from an affine layer in this network (or a sum over these), we have that their C maps are the identity function. Thus, by Equation 13 it follows that C(C(C(c))) + qc C(C(C(c))) + C(c) B (c) = and T (c) = . (30) q 1 + q 2 Given these identities (and Equation 15), we can compute and plot C for each possible D: f 146
Deep Kernel Shaping While there is some compression of the range of c values as depth increases in ResNets, it is much milder than what we see for standard deep RELU networks (e.g. in the plots of Section 12.1). For example, [−1, 1] is mapped to [0.94, 1] for a ResNet with D = 152, versus [0.996, 1] for a standard RELU network of depth 100. This observation is compatible the idea that ResNets are closer to linear/identity functions (which have identity C maps) than standard deep networks, at least at initialization time. It is also worth considering the C map behavior of a ResNet where the q values don’t grow throughout the network. For example, instead of having simple sums at the end of √ each residual block, we could use normalized sums, with a weight of 1/ 2 on both branches. Or, we could add an LN layer to the shortcut branch of every block. In either case, we have q values of 1 throughout the network, which leads to the following redefinition: C(C(C(c))) + c B (c) = . q 2 Intuitively, this definition places more weight onto the nonlinear contribution than we had previously. Plotting C in this scenario for different D’s yields the following: f 147
Martens et al. While less extreme than standard (non-residual) RELU networks, we still see significantly more compression than before, thus reinforcing the importance of growing q values for the trainability of ResNets. It’s worth noting one can achieve the same effect in a network with constant q values equal to 1 (such as the ones constructed with DKS) by careful choice of weights on the sums at the end of each residual block. For example, one can recover the original form of √ B (seen in Equation 30) by using a weight of w = 1/ q + 1 for the residual branch of the q √ corresponding block (and a weight of 1 − w2 on the shortcut branch). Doing this for all blocks exactly recovers the C map of a standard ResNet, as neither T nor C depend on the q values. Appendix K. Empirical evidence for the relationship between Q map derivatives and kernel approximation error In this section we will provide empirical evidence for the relationship between Q map deriva- tives and kernel approximation error that we posited in Section 15. To do this, we will examine the effect of changes to the local Q map conditions used in DKS on the F1-score of the predictions made by Q maps for an example network. In particular, we will consider the skip-free BN-free modified ResNet used in our main experiments (from Section 28.2) with the softplus activation function, and a depth param- eter of 50. The local Q maps of this network’s combined layers are equal to the same function Q, and we will consider the effect of using values of 0.95, 1.0, and 1.01 for Q(cid:48)(1) in DKS’s local map conditions. As we still have Q(1) = 1 with this change, and the network’s input q values are still 1 (due to our use of PLN), we thus have constant q values of 1 for all layers. We can empirically estimate the F1-score of this prediction by measuring how much (cid:107)v(cid:107)2/ dim(v) deviates from 1 for location vectors v from the network’s feature maps (computed at initialization time). 148
Deep Kernel Shaping The following plots show these “empirical q values”, averaged across locations and 192 training examples, versus the layer index for which they are computed. Vertical lines indi- cate the standard deviation. From these first two plots we can see that average empirical q value remains close to 1 for both the Q(cid:48)(1) = 0.95 and Q(cid:48)(1) = 1.0 cases. Although in the latter case we see higher variance, especially for deeper layers. In the next plot, we see that the empirical q values rapidly diverge from 1 when Q(cid:48)(1) = 1.01, thus confirming the intuitions given in Section 15. 149
Martens et al. Appendix L. Example learning rate schedules from FIRE PBT In this section we present the learning rate schedules that were found by FIRE PBT for our main MNIST experiments from Section 28.2. 150
Deep Kernel Shaping Appendix M. Meta-parameter studies In this section we will experimentally study the effect of various training “meta-parameters” on the optimization and generalization performance of networks constructed with DKS. These will include the weight on the residual branch when using skip connections, DKS’s global slope bound parameter ζ, and the choice of optimizer. Except when otherwise indicated, all experiments will use the same default settings (as stated in Section 28.1) as our main set of experiments. M.1 Sweeping residual weights In this subsection we compare different values for the weights of the residual branches BN- free networks with skip connections constructed using DKS. To satisfy the condition that the branch sums at the end of each residual block are normalized, we set their weights to √ w and 1 − w2 for the residual and shortcut branches (respectively). In addition to running experiments using the same weights for all blocks, we also tried setting the weights individually according to the recipe at the end of Appendix J, so as to recover the C map of an (almost) standard ResNet. (Note that this requires a generalized version of the maximal slope functions given for our modified ResNets in Section 23.4, but is otherwise a straightforward change.) 151
Martens et al. √ From these results we see that the value 0.05 seems to work best when using SGD. When using K-FAC, the difference in optimization speed between the three largest options √ is much smaller, and so we will use 0.05 as the default value for all optimizers. √ Note that while 0.05 is (arguably) the best amoung the values we tried for this net- work, there is no reason to think that this value will be the best choice for other residual architectures (or the same architecture for a different depth parameter). M.2 DKS with different optimizers In this subsection we compare the optimization performance of different optimizers on skip- free BN-free networks constructed with DKS. 152
Deep Kernel Shaping These results show that K-FAC and Shampoo have a large advantage over Adam and SGD in this setting. Moreover, Adam has a small advantage over SGD, and K-FAC has a small advantage over Shampoo. The picture looks different for networks with skip connections, and K-FAC and SGD yield fairly similar optimization speeds for two out of the three activation functions we tried. M.3 Sweeping ζ values In this subsection we study the influence of the global slope bound ζ on the optimization speed of skip-free BN-free tanh networks constructed with DKS. In particular, we compare 153
Martens et al. the default choice of ζ = 1.5 to various “extreme” values, which are either very large (corresponding to highly nonlinear network behavior), or are very close to 1 (corresponding to very linear behavior). From these results we can see that ζ = 1.5 gives the fastest optimization performance for K-FAC and the fastest short-term optimization performance for SGD. ζ = 1.00001, which corresponds to a very linear network, gives very slow optimization performance, perhaps for the reasons discussed in Section 14.2. Somewhat surprisingly, the choice ζ = 10000 yields quite respectable (although still suboptimal) performance. 154
Deep Kernel Shaping M.4 The influence of ζ on generalization In this subsection we study the influence of the global slope bound ζ on the generalization performance for networks trained with K-FAC. From these results we can see that while the value ζ = 1.5 tends to give faster optimiza- tion, slightly lower values are associated with improved generalization. 155
Martens et al. Appendix N. Experiments with ablations and modifications of DKS In this section we consider various ablations and modifications of DKS. The overall conclu- sion of these studies is that each component of DKS, except perhaps for PLN (assuming reasonably well scaled input data), is required to achieve the highest optimization speed. When considering test error the conclusions are similar but somewhat muted, with the sin- gle exception that using weighted mean-pooling layers with K-FAC seems to improve test set performance while degrading training set performance. For plots that contain solid and dotted lines of the same color, solid lines will corre- spond to the default unmodified version of DKS, while dotted lines will correspond to the ablated/modified version. Except when otherwise indicated, all experiments will use the same default settings (as stated in Section 28.1) as our main set of experiments. We will omit results for test error, except in those cases where it gives qualitatively different results from the training error. N.1 Alternative initializations for weights / filter banks In this subsection we consider replacing the Orthogonal Delta initialization used in DKS with various alternative weight initialization schemes. Note that while the use of Delta weight initializations is required by the Q/C map theory that underlies DKS, in practice one can still try DKS with any other weight initialization scheme. From these results we can see that while the Orthogonal Delta initialization gives the best results for both K-FAC and SGD, the results for the Orthogonal (non-Delta) and Gaussian Delta initializations are very close. One might be tempted to conclude from these findings that the weight initialization is relatively unimportant for skip-free BN-free networks in general. However, as we can see 156
Deep Kernel Shaping from the following plot, there is a much larger gap in performance between the different options when we don’t use DKS’s activation function transformations: N.2 Only enforcing the Q (1) = 1 condition f In this subsection we consider modifying DKS to only enforce the condition that Q (1) = 1 f for all subnetworks f (which is equivalent to doing the same for all nonlinear layers f ). Note that this conditions is roughly analogous to what normalization layers, the “He initialization method” for RELUs, and the LSUV/WLI initializations are trying to achieve. To achieve this condition we use only the output scale parameter (denoted γ in Section 18) in the transformed activation functions. 157
Martens et al. From these results we see that the condition Q (1) = 1 is clearly not enough by itself f to achieve fast optimization in BN-free networks, with or without skip connections. (And as we will see in Section N.5, enforcing this condition by itself may actually do more harm than good.) N.3 Removing the condition Q(cid:48) (1) = 1 f In this subsection we consider removing the condition Q(cid:48) (1) = 1 from the set of four f conditions that we enforce in DKS. The remaining three conditions are achieved by setting three of the four activation function parameters (defined in Section 18), with the input shift parameter β being left out (which is equivalent to taking β = 0). 158
Deep Kernel Shaping From these results we see that this condition appears to be important in most training scenarios, but not all of them. N.4 Minimizing Q(cid:48) (1) instead of enforcing Q(cid:48) (1) = 1 f f In this subsection we consider the effect of minimizing Q(cid:48) (1) for each subnetwork f in DKS f instead of enforcing the condition Q(cid:48) (1) = 1. This is accomplished by minimizing Q(cid:48) (1) for f f each nonlinear layer f in the network. This modification is motivated by the observation that minimizing Q(cid:48) (1) should, according to the reasoning of Section 16.2, minimize the f total kernel approximation error. 159
Martens et al. From these results we can see that minimizing Q(cid:48) (1) works overall worse than simply f setting Q(cid:48) (1) = 1. The reasons for this remain unclear. f N.5 Removing activation function transformations completely In this subsection we consider completely removing the activation function transformations from DKS. What remains is the Delta Orthogonal initialization for the weights (and a zero initialization of the biases), normalized sums between residual and shortcut branches, and the use of PLN. 160
Deep Kernel Shaping From these results we see that activation function transformations are important in all the scenarios we tested, except when training softplus networks with skip connections using K-FAC. Moreover, they seem to be especially important when training with SGD. Comparing the results here for tanh networks to those given in Subsection N.2, we can see that training speed becomes worse if we enforce the condition Q (1) = 1 by itself f (versus enforcing no conditions at all). While potentially counterintuitive, this isn’t actually surprising. Indeed, there is no reason to think that the local C map for an untransformed tanh layer will be more favorable given an input q value of 1 compared to some other value. (In this case, the “other value” is the fixed point of a tanh layer’s local Q map.) Note that for the fully transformed tanh layers generated by DKS this consideration is moot, since any fixed input q value is essentially equivalent to all other choices due to the flexibility afforded by the activation function’s input scale parameter (denoted α in Section 18). We also have results for skip-free BN-free networks trained on CIFAR-10, which tell a similar story. These are given below: 161
Martens et al. N.6 Removing Per-Location Normalization (PLN) In this subsection we consider using DKS without the Per-Location Normalization (PLN) data pre-processing step described in Section 10.2. From these results we can see that, somewhat surprisingly, the use of PLN actually harms optimization performance, especially for SGD. We speculate about possible reasons for this in Section 10.2. Whatever the reasons, we know that they are contingent on the default properties of the input training data. We can demonstrate this by rerunning the same experiment without 162
Deep Kernel Shaping PLN for input data that is scaled by a factor of 100 (after the usual MNIST pre-processing and augmentation). From these results we can see that if the original input data is badly scaled, using PLN will have a positive effect on optimization performance. N.7 Using equivalent parameters instead of activation transformations In this subsection we consider the effect of achieving the four conditions of DKS via “equiv- alent parameters” (as defined in Section 18.3), instead of explicit transformations to the activation functions. This change brings DKS much closer to being a pure initialization ap- proach, but introduces a reparameterization which can have implications for optimization. 163
Martens et al. From these results we can see that optimization performance with K-FAC is mostly unaffected by this change, while with SGD it becomes much worse. This difference in behavior between the two optimizers isn’t surprising, since as discussed in Section 18.3, K- FAC is essentially invariant to the kind of reparameterization being performed here, while SGD is not. N.8 Replacing max-pooling layers with convolutions As discussed in Section 20.2, max-pooling layers are not fully compatible with the Q/C map theory underlying DKS, and so we must be cautious when applying DKS to networks containing them. The networks used in our main experiments contain a max-pooling layer (near the beginning), and in this subsection we will justify this decision by considering the effect of replacing that layer with a standard convolutional layer of same kernel size, stride, etc. 164
Deep Kernel Shaping From these results we see roughly similar optimization performance with and without this replacement, with K-FAC becoming slightly slower, and SGD becoming slightly faster. N.9 Replacing mean pooling layers with weighted mean-pooling layers In this subsection we consider the effect using a weighted mean-pooling layer, as defined in Section 20.1.2, in place of the standard mean-pooling layer normally present near the end of our modified ResNet architecture. As discussed in Section 20.1.2, standard mean-pooling layers are not compatible with the theory underlying DKS, while weighted mean-pooling layers are, at least to some extent. 165
Martens et al. From these results we see that optimization performance gets slightly worse with K- FAC, and slightly better with SGD, and that generalization improves slightly as well for both optimizers. Thus, even though we didn’t use weighted mean-pooling layers in our main set of experiments, they are probably worth trying when using DKS. References M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. Warden, M. Wat- tenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org. Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over- parameterization. In International Conference on Machine Learning, pages 242–252, 2019. R. Anil, V. Gupta, T. Koren, K. Regan, and Y. Singer. Scalable second order optimization for deep learning. arXiv preprint arXiv:2002.09018, 2020. F. Anselmi, L. Rosasco, C. Tan, and T. Poggio. Deep convolutional networks are hierarchical kernel machines. arXiv preprint arXiv:1508.01084, 2015. S. Arora, S. S. Du, W. Hu, Z. Li, R. Salakhutdinov, and R. Wang. On exact computation with an infinitely wide neural net. Advances in Neural Information Processing Systems, 2019. 166
Deep Kernel Shaping J. Ba, R. Grosse, and J. Martens. Distributed second-order optimization using kronecker- factored approximations. In International Conference on Learning Representations, 2017. J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. T. Bachlechner, B. P. Majumder, H. H. Mao, G. W. Cottrell, and J. McAuley. Rezero is all you need: Fast convergence at large depth. arXiv preprint arXiv:2003.04887, 2020. D. Balduzzi. Deep online convex optimization with gated games. arXiv preprint arXiv:1604.01952, 2016. D. Balduzzi, H. Vanchinathan, and J. Buhmann. Kickback cuts backprop’s red-tape: Bi- ologically plausible credit assignment in neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, 2015. D. Balduzzi, M. Frean, L. Leary, J. Lewis, K. W.-D. Ma, and B. McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, pages 342–350. PMLR, 2017. J. Bergstra, G. Desjardins, P. Lamblin, and Y. Bengio. Quadratic polynomials learn better image features. Technical report, 1337, 2009. R. C. Bradley. Central limit theorems under weak dependence. Journal of Multivariate Analysis, 11(1):1–16, 1981. A. Brock, S. De, S. L. Smith, and K. Simonyan. High-performance large-scale image recog- nition without normalization. arXiv preprint arXiv:2102.06171, 2021. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. T. Cai, R. Gao, J. Hou, S. Chen, D. Wang, D. He, Z. Zhang, and L. Wang. Gram-gauss- newton method: Learning overparameterized neural networks for regression problems. arXiv preprint arXiv:1905.11675, 2019. Y. Cho and L. Saul. Kernel methods for deep learning. In Advances in Neural Information Processing Systems, volume 22, 2009. D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). In International Conference on Learning Representa- tions, 2016. T. Cooijmans and J. Martens. On the variance of unbiased online recurrent optimization. arXiv preprint arXiv:1902.02405, 2019. 167
Martens et al. V. Dalibard and M. Jaderberg. Faster improvement rate population based training. arXiv preprint arXiv:2109.13800, 2021. A. Daniely, R. Frostig, and Y. Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. Advances In Neural Information Processing Systems, 29:2253–2261, 2016. S. De and S. Smith. Batch normalization biases residual blocks towards the identity function in deep networks. Advances in Neural Information Processing Systems, 33, 2020. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. MNIST: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural networks. In International Conference on Machine Learning, pages 1675–1685, 2019a. S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over- parameterized neural networks. In International Conference on Learning Representations, 2019b. J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011. M. L. Eaton. Group invariance applications in statistics. In Regional conference series in Probability and Statistics, pages i–133. JSTOR, 1989. S. Elfwing, E. Uchibe, and K. Doya. Sigmoid-weighted linear units for neural network function approximation in supervised learning. Neural Networks, 107:3–11, 2018. L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International Conference on Machine Learning, pages 1407–1416. PMLR, 2018. K. Fukushima and S. Miyake. Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neural nets, pages 267–285. Springer, 1982. A. Garriga-Alonso, C. E. Rasmussen, and L. Aitchison. Deep convolutional networks as shallow gaussian processes. arXiv preprint arXiv:1808.05587, 2018. X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelli- gence and statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010. I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016. Google, 2018. Cloud tpu. https://cloud.google.com/tpu/. Accessed: 2021. 168
Deep Kernel Shaping R. Grosse. University of Toronto CSC2541, Topics in Machine Learning: Neural Net Train- ing Dynamics, Lecture Notes, Chapter 5: Adaptive Gradient Methods, Normalization, and Weight Decay, 2021. URL: https://www.cs.toronto.edu/~rgrosse/courses/ csc2541_2021/readings/L05_normalization.pdf. Last visited on 09/2021. R. Grosse and J. Martens. A kronecker-factored approximate fisher matrix for convolution layers. arXiv preprint arXiv:1602.01407, 2016. V. Gupta, T. Koren, and Y. Singer. Shampoo: Preconditioned stochastic tensor optimiza- tion. In International Conference on Machine Learning, pages 1842–1850, 2018. T. Hazan and T. Jaakkola. Steps toward deep kernel methods from infinite neural networks. arXiv preprint arXiv:1508.05133, 2015. K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human- level performance on MNIST classification. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a. K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630–645. Springer, 2016b. S. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber, et al. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001. S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by re- ducing internal covariate shift. In International conference on machine learning, pages 448–456, 2015. A. Jacot, C. Hongler, and F. Gabriel. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, 2018. M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue, A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017. E. Jones, T. Oliphant, P. Peterson, et al. SciPy: Open source scientific tools for Python, 2001–. URL http://www.scipy.org/. J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasu- vunakool, R. Bates, A. Zˇ´ıdek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, pages 1–11, 2021. R. Karakida and K. Osawa. Understanding approximate fisher information for fast conver- gence of natural gradient descent in wide neural networks. Advances in Neural Informa- tion Processing Systems, 33, 2020. 169
Martens et al. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2014. G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter. Self-normalizing neural net- works. In Proceedings of the 31st international conference on neural information process- ing systems, pages 972–981, 2017. P. Kr¨ahenbu¨hl, C. Doersch, J. Donahue, and T. Darrell. Data-dependent initializations of convolutional neural networks. In International Conference on Learning Representations, 2016. A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Tech- nical report, University of Toronto, 2009. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to docu- ment recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998a. Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Mu¨ller. Efficient backprop. In Neural networks: Tricks of the trade. Springer, 1998b. J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. In International Conference on Learning Repre- sentations, 2018. Y. Li and Y. Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. Advances in neural information processing systems, 2018. Z. Li and S. Arora. An exponential learning rate schedule for deep learning. In International Conference on Learning Representations, 2019. K. Luk and R. Grosse. A coordinate-free construction of scalable natural gradient. arXiv preprint arXiv:1808.10340, 2018. J. Mairal, P. Koniusz, Z. Harchaoui, and C. Schmid. Convolutional kernel networks. Ad- vances in neural information processing systems, 27:2627–2635, 2014. J. Martens. On the validity of kernel approximations for orthogonally-initialized neural networks. arXiv preprint arXiv:2104.05878, 2021. J. Martens and R. Grosse. Optimizing neural networks with kronecker-factored approximate curvature. arXiv preprint arXiv:1503.05671, 2015. A. G. d. G. Matthews, J. Hron, M. Rowland, R. E. Turner, and Z. Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations, 2018. E. S. Meckes. The random matrix theory of the classical compact groups, volume 218. Cambridge University Press, 2019. D. Mishkin and J. Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015. 170
Deep Kernel Shaping S. Nadarajah and S. Kotz. Exact distribution of the max/min of two gaussian random variables. IEEE Transactions on very large scale integration (VLSI) systems, 16(2):210– 212, 2008. R. M. Neal. Bayesian learning for neural networks. Lecture notes in statistics, 118, 1996. R. Novak, L. Xiao, J. Lee, Y. Bahri, G. Yang, J. Hron, D. A. Abolafia, J. Pennington, and J. Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018. R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310–1318, 2013. B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential expressivity in deep neural networks through transient chaos. Advances in neural information processing systems, 29:3360–3368, 2016. M. J. Powell. An efficient method for finding the minimum of a function of several variables without calculating derivatives. The Computer Journal, 7(2):155–162, 1964. R. Prajit, B. Zoph, and V. L. Quoc. Swish: a self-gated activation function. arXiv preprint arXiv:1710.059417, 2017. A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: replacing minimization with randomization in learning. In Nips, pages 1313–1320, 2008. P. Ramachandran, B. Zoph, and Q. V. Le. Swish: a self-gated activation function. arXiv preprint arXiv:1710.05941, 7:1, 2017. S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry. How does batch normalization help optimization? In Proceedings of the 32nd international conference on neural information processing systems, pages 2488–2498, 2018. A. Saxe, J. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Repre- sentations, 2014. I. Schoenberg. Positive definite functions on spheres. Duke Math. J, 1:172, 1988. S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propaga- tion. In International Conference on Learning Representations, 2017. J. Shao, K. Hu, C. Wang, X. Xue, and B. Raj. Is normalization indispensable for training deep neural network? Advances in Neural Information Processing Systems, 33, 2020. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. A general supervised learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. 171
Martens et al. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE confer- ence on computer vision and pattern recognition, pages 1–9, 2015. C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-first AAAI conference on artificial intelligence, 2017. M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pages 6105–6114, 2019. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L(cid:32) . Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017. A. Veit, M. J. Wilber, and S. Belongie. Residual networks behave like ensembles of relatively shallow networks. Advances in neural information processing systems, 29:550–558, 2016. Wikipedia contributors. Hermite polynomials — Wikipedia, the free encyclopedia, 2021. URL https://en.wikipedia.org/w/index.php?title=Hermite_polynomials&oldid= 1022750425. [Online; accessed 30-August-2021]. C. K. Williams. Computing with infinite networks. Advances in neural information pro- cessing systems, pages 295–301, 1997. Y. Wu, M. Ren, R. Liao, and R. Grosse. Understanding short-horizon bias in stochastic meta-optimization. In International Conference on Learning Representations, 2018. L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. Schoenholz, and J. Pennington. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. In International Conference on Machine Learning, pages 5393–5402, 2018. L. Xiao, J. Pennington, and S. Schoenholz. Disentangling trainability and generalization in deep neural networks. In International Conference on Machine Learning, pages 10462– 10472, 2020. G. Yang, J. Pennington, V. Rao, J. Sohl-Dickstein, and S. S. Schoenholz. A mean field theory of batch normalization. In International Conference on Learning Representations, 2019. S. Zagoruyko and N. Komodakis. Wide residual networks. In British Machine Vision Conference 2016. British Machine Vision Association, 2016. G. Zhang, L. Li, Z. Nado, J. Martens, S. Sachdeva, G. Dahl, C. Shallue, and R. B. Grosse. Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model. Advances in neural information processing systems, 32:8196–8207, 2019a. G. Zhang, J. Martens, and R. B. Grosse. Fast convergence of natural gradient descent for over-parameterized neural networks. In NeurIPS, 2019b. H. Zhang, Y. N. Dauphin, and T. Ma. Fixup initialization: Residual learning without normalization. In International Conference on Learning Representations, 2019c. 172
