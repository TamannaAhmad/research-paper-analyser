Input-level Inductive Biases for 3D Reconstruction Wang Yifan1* Carl Doersch2 Relja Arandjelovic´2 Joa˜o Carreira2 Andrew Zisserman2,3 1ETH Zurich 2DeepMind 3VGG, Department of Engineering Science, University of Oxford Queries for image 1 weiv elpitluM riap egami tupnI yrtemoeg sesaib evitcudni Output depth for image 1 Input matrix flatten() Generalist Perception Model Figure 1. Input-level inductive biases. We explore 3D reconstruction using a generalist perception model, the recent Perceiver IO [23] which ingests a matrix of unordered and flattened inputs (e.g. pixels). The model is interrogated using a query matrix and generates an output for every query – in this paper the outputs are depth values for all pixels of the input image pair. We incorporate inductive biases useful for multiple view geometry into this generalist model without having to touch its architecture, by instead encoding them directly as additional inputs. Abstract Eventually it seems desirable to build a more general vi- sual system that can deal with most perceptual problems. To Much of the recent progress in 3D vision has been driven get there, one option is to combine state-of-the-art systems by the development of specialized architectures that incor- for all of those problems, but this would be complex, inel- porate geometrical inductive biases. In this paper we tackle egant and not scalable. Another option is to employ mod- 3D reconstruction using a domain agnostic architecture and els without much customization or inductive biases for any study how to inject the same type of inductive biases directly particular task, but these models will by definition be less as extra inputs to the model. This approach makes it possi- data-efficient and hence less accurate than specialized ones ble to apply existing general models, such as Perceivers, given a fixed data budget. on this rich domain, without the need for architectural In this paper we explore the single-general-model route. changes, while simultaneously maintaining data efficiency We ask the following question: can the lack of architecture- of bespoke models. In particular we study how to encode level inductive biases be replaced by extra inputs which en- cameras, projective ray incidence and epipolar geometry code our knowledge about the problem structure? In other as model inputs, and demonstrate competitive multi-view words, can we feed those priors as inputs rather than hard- depth estimation performance on multiple benchmarks. wire them into the model architecture (Fig. 1), like a load- able software solution instead of a more rigid hardware so- lution. As the general model we employ the recently pub- 1. Introduction lished Perceiver IO [23] and as domain we focus on multi- view geometry and 3D reconstruction, an area of computer The focus of modern computer vision research is, to a vision where architectural specialization is particularly ex- large extent, to identify good architectures for each task of uberant [20, 22, 30, 36, 44, 66, 69]. interest. There are many tasks of interest, ranging from clas- Our main contribution is in mapping out and evaluating sical ones such as optical flow [19], to highly specialized some of the options for expressing priors for 3D reconstruc- (yet arguably important) ones such as recognizing equine tion as input features, in particular in the setting of depth action units [31]. Creating dedicated models for every pos- estimation from stereo image pairs. We consider concepts sible task naturally results in a sprawling catalog of archi- in multiview geometry such as camera viewpoint, light ray tectures. direction and epipolar constraints. Similar to the prior work ∗Work done during internship at DeepMind. we compare with [20, 22, 30, 56], we assume ground truth 1 2202 raM 91 ]VC.sc[ 2v34230.2112:viXra
cameras are given, but they could in principle be computed riety of approaches for integrating global and local infer- by the model as well and passed back as inputs recurrently. ence. Early approaches to deep optical flow and correspon- We experiment on multiple datasets—ScanNet [7], dence estimation [8, 11] used direct regression, as our ap- SUN3D [59], RGBD-SLAM [49] and Scenes11 [56]—and proach does, but later works found correlations and cost present results that are comparable or better to those ob- volumes more effective [11, 50, 55]. Perceiver IO [23], tained by state-of-the-art specialized architectures on all of however, shows strong flow performance with direct regres- them. This is achieved without using cost volumes, warp- sion. Transformers have also contributed to improvements ing layers, etc., and in fact (proudly) without introducing in more general scene correspondence [26,51,58], and even any architectural innovation. Instead, we propose powerful using learned correspondence to improve few-shot learn- input-level 3D inductive biases that substantially improve ing [9], though these transformers are still applied on fea- data efficiency. This paper reflects a new avenue for prob- ture grids with relatively complex mechanisms to represent lem solving in computer vision, in which domain knowl- correspondence explicitly. The grids are taken from prior edge is valued but applied in a flexible manner, as additional work on correspondence that uses deep learning, where ex- model inputs. plicit pairwise comparisons and cost volumes are a staple of top-performing methods [6, 36, 39, 43–45, 64]. 2. Related Work Our work also sits in the broader field of deep learn- ing for 3D reconstruction, where there have been a wide Our work is part of a long trend in computer vision variety of proposals for representing 3D inductive biases. of simplifying and unifying architectures. It was noted a Early works like DeepTAM [69] emphasize the importance decade ago that big data along with simple architectures of representing per-image depth maps and rays. More re- are “unreasonably effective” [15] at solving many percep- cent works have made use of deep implicit models to rep- tion problems, and subsequent progress has only reinforced resent 3D [4, 37, 42], introducing the idea that deep repre- this [53]. Computer vision has moved from architectures sentations should be queried with points. While this work like ConvNets, which are highly general image proces- has been extended to more complex scenes in NeRF [38] sors [29], to methods that are based on Transformers [57] and its many derivatives, these typically require many im- such as ViT [10] and Perceivers [23, 24], where the under- ages of the same scene and an expensive offline training pro- lying Transformer can be equally effective across multiple cess. Online methods typically rely on more explicit but ex- domains like sound and language. Unifying architectures pensive 3D representations like voxel grids [25, 40, 52, 67]. is useful because architectural improvements can be prop- Particularly relevant is TransformerFusion [1], which uses agated across tasks and domains trivially. It also enables Transformers to attend from its voxel grid representation to sharing and transferring information across modalities and the input images, although this approach still suffers from tasks [48, 65], which is critical for tasks with little data. problems with memory and resolution due to the voxel grid. However, seeking general-purpose architectures does not mean we should discard insights about geometry when 2.1. Review of Perceiver IO solving a geometric problem. Decomposing the problem into feature matching and triangulation was an early com- For the general perception model we use Perceiver ponent of stereo systems [18, 41]. More recent systems IO [23], and we briefly review it here. The model is based have relied on learning, especially for learning descrip- on Transformers [57] in that it treats its input as a simple tors which are compared across images to find correspon- series of tokens, and attention is the main workhorse. First, dence, either by directly searching for matches across im- cross-attention is performed between the input tokens and a ages [3, 33, 35, 60] or by computing 4D correlation vol- fixed-size set of internal vectors (‘latents’), thus obtaining umes [2, 5, 13, 27, 61, 62, 66], or a combination [14]; scaling a compressed representation of the input. Then, a series of these methods can be problematic as the number of consid- self-attentions is performed within the latents, enabling this ered matches grows. Several recent works [17, 32, 63] in- architecture to scale well to large inputs (e.g. high resolution ferred correspondences by aggregating sample points along images) and to stack many layers without hitting memory the epipolar line with a transformer; however matches are issues, since there are much fewer latents than input tokens. still represented and sampled explicitly. Similar to our The final step is another cross-attention, this time between work, Cam-Convs [12] leveraged input-level geometric pri- a set of externally specified ‘queries’ and the latents, which ors (camera intrinsics) for more robust single-view depth es- produces an output array of desired size (one element for timation under variable camera. Our work considers a more each query). Queries are typically some encoding of pixel general application - multi-view depth estimation, where we position and are quite dense (e.g. one per pixel). The archi- include also the camera relative poses and the epipolar em- tecture achieves strong results on a large variety of tasks and bedding. domains, such as image classification, optical flow, natural The broader field of correspondence learning has a va- language understanding, and StarCraft II, making it a nat- 2
Input Matrix main components of any general stereo system. Towards 1, we explore providing camera information in the form of R R R R R R encoded camera matrices, as well as the encodings of the Pixels G G G G G G rays at every pixel. Towards 2, we encode epipolar planes for each pixel, which tells the network which pixels might B B B B B B be in correspondence. Our main contribution in this work is to show that together, these geometric quantities can im- Geometrical prove the inferred 3D geometry without any changes to the channels network architecture. The geometric information is provided as input to the Image 1 Image 2 Geometrical network. We explore two main ways to operationalize this token (Fig. 2): 1) by fusing the information with all input ele- Figure 2. Geometrical Embeddings. The input to a Perceiver ments via concatenating it along the channel dimension, and model is a matrix. Instead of vanilla positional embeddings, we in- 2) by expanding the input set with additional ‘geometrical’ troduce geometrical embeddings that encode inductive biases from tokens. multiple view geometry. We form the input matrix by concatenat- ing pixel values with these embeddings: as extra per-pixel chan- 3.1. Featurizing Cameras nels and/or as extra tokens. A camera is one of the most important components for ural fit for the general perception model used in this work multiview geometry, providing the necessary information to (Fig. 1). perform triangulation [16]. We assume the commonly used pin-hole camera model parameterized with the intrinsic pa- 3. Featurizing Multiple View Geometry rameters K ∈ R3×3, which define the transformation from camera coordinates to image coordinates, and extrinsic pa- In this section, we demonstrate how to inject geometric rameters (cid:2) R ∈ R3×3, t ∈ R3(cid:3) , defining the 6-DOF camera inductive biases into a general perception model, Perceiver pose transformation from the world coordinates to the cam- IO [23] (Sec. 2.1), without changing its architecture. We era coordinates. In practice the intrinsic parameters can be consider the case of 3D reconstruction from an image pair – obtained by off-the-shelf calibration methods [68] and the the inputs are pixels and calibrated cameras, and the output extrinsic parameters can be estimated using structure-from- is depth at each pixel. motion algorithms such as COLMAP [47]. If we follow prior work, such as the optical flow network Next, we consider two alternatives to encode the camera from Perceiver IO [23], then we can treat each pixel (or, parameters, the first one is based on constructing viewing more generally, each vector in a feature grid) as an input rays connecting the camera with each pixel, and the second element. We then tag each pixel with an encoding for its is directly providing the projection matrix that maps the 3D position within the grid as input, and potentially with an world coordinates to the 2D pixel coordinates. additional tag to indicate which of the two input images the pixel belongs to. The output could be processed similarly: Option 1: Rays and camera center. Let x j,i ∈ R2 we use the same tagged pixels (or features) as queries in be the image coordinate of pixel i in image j. It can be order to get a depth value for each pixel. uniquely represented in the 3D space using the viewing ray, In practice, however, we expect this approach to over- which can be further parameterized using the camera cen- fit given the relatively small datasets that are available for ter, c j ∈ R3, and the unit-length ray direction, r j,i ∈ R3 training geometric inference. A high-capacity model can (Fig. 3). The projection matrix for the camera j is a 3 × 4 easily memorize the depth for each image, rather than learn- matrix P j = K j [R j|t j]. In homogeneous coordinates, the ing a procedure which matches features across images and camera center c˜ j = [c j, 1](cid:62), satisfies P jc˜ j = 0. Writing performs triangulation in a way that can generalize to unfa- the projection matrix as P j = [K jR j|K jt j], the camera miliar scenes. center in the world coordinate system is Our hypothesis is that we can create a more data-efficient c = −(K R )−1K t = −R−1t . (1) learning algorithm by simply providing the Perceiver IO j j j j j j j with information that describes the geometry as input. In The unnormalized viewing ray direction can be computed the ideal case, Perceiver IO can learn to use this informa- as tion correctly without the computational pipeline being pre- (cid:20) (cid:21) x scribed by a complex, restrictive architecture. r¯ j,i = (K jR j)−1 1j,i , (2) In particular, we explore providing information that lets the network more easily 1) represent 3D space to allow tri- since P [r¯ , 0](cid:62) = [x , 1](cid:62), which we normalize to unit j j,i j,i angulation, and 2) find correspondences, which are the two length to obtain r . j,i 3
Instead of providing c j and r j,i to the network in relative angle, θ 2,i t d i s Th c ui hae m i il ti sl er e y dnr isa sfi tw oo h drn a of ta fo nl ur et rm h tF hi bo s ea yu rs r hi p a3 e i r pgr- o pD h c lf e e ye rv s i-a ne sdt ic gu i nt mro ge er es ls bn, e, ys miw a o n ese n nea u tie -lt r wm ale iwb sn ne ea cd es o t mwdt sh i aoh ne prom g pkw is nnt io [ gs 3eh 8 xbmi ,g e 5p th 7t (cid:55)→ie er ]-r r . epipolar lii nm esage 1 3D point pixei lm , xa 2g ,ie 2 epipolar lines epipolar pl fa on r e pixel (j,i) ref eer pie pn oc le ar pl θa =n 0e [x, sin(f πx), cos(f πx), · · · , sin(f πx), cos(f πx)], 1 1 K K where K is the number of Fourier bands, and f is equally k c 1 ray, r spaced between 1 and µ , with µ corresponding to the 2,i 2 camera center, c sampling rate. The sampling rate µ and number of bands K 2 Figure 3. Geometric entities used to compute the geometrical em- are hyperparameters which can be set separately for c and j beddings that are passed as inputs to the perception module. For r . As a result, we obtain 6K + 3 and 6K + 3 Fourier j,i c r clarity, only entities related to one of the images are labeled. features for c and r respectively. j j,i Option 2: Pixel coordinates and projection matrix. Al- ment, we encode this image indicator as a D -dimensional ind ternatively, since the 3D position of each pixel can be deter- vector using either a Fourier mapping of the image index mined up to an unknown depth solely using the projection (0/1), or as a learnable parameter. The per-image inputs matrix P j, we can also uniquely embed each pixel directly contain the camera-specific geometry embedding, whereas with P j and the pixel coordinate x j,i. To this end, we flat- each per-pixel input is a concatenation along the channel ten P j to a 12-dimensional vector, then we map this 12-D dimension of the RGB-based inputs, the pixel-specific ge- vector as well as the 2-D x j,i again to Fourier features using ometry embedding, and the image indicator embedding. As K matrix and K x bands, and µ matrix and µ x sampling rates, re- a result, the inputs are comprised of 2 per-camera tokens spectively. The resulting 24K matrix+12 and 4K x+2 vectors with D cam dimensions, and 2 × (H × W ) per-pixel tokens uniquely determine the geometry for a given pixel. of (D + D + D ) dimensions. Finally, to ensure the rgb pix ind Injecting the camera information into the input of Per- two modes of input have the same channel dimension, we ceiver IO. The above geometric embeddings contain all the pad the smaller inputs with a learnable parameter. necessary information for the network to triangulate the pix- 3.2. Featurizing Epipolar Cues els. We now consider how to provide this information to the general perception model. Notice that in either afore- While the previous section uses on the geometric embed- mentioned options, there is a camera-specific part which is ding to facilitate view triangulation, now we take one step identical to all pixels in the a given image, namely c and further and exploit the given camera information to assist j P , and a pixel-specific part that is unique to each pixel, the search of correspondences between different images. j namely r and x . The pixel-specific part is most natu- Correspondence estimation is paramount to multi-view j,i j,i rally incorporated by concatenating it with the pixel’s RGB geometry. The epipolar constraint is a fundamental con- values along the channel dimension (‘geometrical channels’ straint in stereo vision, which prescribes that a pair of cor- in Fig. 2). There are two ways of assembling the camera- responding points in the two images (projections of the 3D specific part – again as geometrical channels, or as addi- point) must lie on the corresponding epipolar lines, which tional geometrical tokens. are defined as the intersections between the image planes The first way consists of simply duplicating the camera- and the plane defined by the two camera centers and the specific embedding for all the pixels of the correspond- 3D point (Fig. 3). In other words, a point in image 1 ly- ing image and again concatenating it along the channel ing on epipolar line l can only match to a point in image 1 dimension as geometrical channels. This results in a to- 2 that lies on the corresponding epipolar line l . Therefore, 2 tal of 2 × H × W inputs of (D + D + D ) dimen- for a known camera pair one can compute the correspond- rgb pix cam sions, where (H, W ) is the image dimension and D , D ing epipolar lines which can be used to restrict the search rgb pix and D are the total dimensions of the RGB-based in- for point correspondences. This enables a drastically faster cam puts, pixel-specific and camera-specific geometry embed- search while reducing the possibility of having outliers. dings respectively. Similarly to camera information, the epipolar constraint Alternatively, we can treat the camera-specific embed- is typically applied explicitly, e.g. by restricting the corre- ding as a separate input geometric token, alongside the per- spondence search only along epipolar lines. Instead, we pixel inputs, yielding a total of 2 × (H × W + 1) input to- provide the epipolar constraint directly as a part of the net- kens. In order to indicate which image is a pixel associated work inputs by tagging each pixel with its epipolar plane. with, we append an additional image indicator embedding Note that each pixel is assigned to a single epipolar plane, to the per-pixel tokens, that is unique per image and shared apart from a special case which occurs only when the pro- among all the pixels in the same image. In our experi- jection of the other camera (the epipole) falls inside the im- 4
age, since all epipolar planes pass through the epipole; how- Implementation details. We train our model with the ever, this degeneracy only potentially appears at a single commonly used L1LOG loss [21], L (d, d∗) = |log (d) − pixel, and it is practically impossible for the epipole to align log (d∗)|, where d and d∗ are the predicted and ground truth exactly with a pixel center, making this a non-issue. Next, depth values. Unless otherwise stated, we process images we consider two parameterizations of the epipolar plane. at 240 × 320 resolution. The raw RGB values are trans- The first option encodes the normal vector of the epipo- formed to 64-d (i.e. D = 64) color features by a standard rgb lar plane, which can be easily computed as the normalized convolutional preprocessor described in Perceiver IO [23], cross-product of c − c and r , where r is the ray di- which consists of 1-layer convolution with receptive field 7 2 1 j,i j,i rection in (2). Formally, for pixel i in image j, the normal and stride 2, followed by batch normalization, ReLU and vector, n , is: stride-2 max-pooling, resulting in a feature grid of dimen- j,i sion 60 × 80 × 64 for each image. Thess feature grids v = (c − c ) × r (3) are combined with the geometric embeddings to form the j,i 2 1 j,i n = sign (cid:0) [v ] (cid:1) v j,i , (4) inputs to the Perceiver IO model. We use a small ver- j,i j,i x (cid:107)v (cid:107) + (cid:15) sion of the original Perceiver IO architecture, which uses j,i a 2048 × 512 matrix for the latent representation, 1 cross- where the [v ] is the x-coordinate of v and the sign attention for the input, followed by 8 self-attention layers j,i x j,i disambiguates the direction of the normal vectors (opposite and 1 cross-attention for the output, where the self-attention normals denote the same plane). uses 8 heads and the cross-attention has only 1. The output The second option parameterizes the epipolar plane as a of the Perceiver IO model is two 60 × 80 depth maps. We relative angle, θ , between the epipolar plane and an ar- upsample it by 4 to the original resolution with a Convex j,i bitrarily chosen reference epipolar plane, where the angles Upsampling module [55] similar to Perceiver IO applied for are scaled such that θ ∈ [−1, 1] (Fig. 3): optical flow estimation. j,i For the geometric embeddings, we consider relative (cid:32) (cid:32) (cid:33) (cid:33) θ j,i = 2 π1 arccos (cid:107)n n(cid:62) (cid:62)j,in nref (cid:107) − 0.5 . (5) c Ka pm =era 1p 0o , s Ke ow. =r.t. 2t 0h ,e thfi ers mt c aa xm imer aa l. sW ame ps le int gK rr at= e µK tm oat ir six s= et j,i ref to 60 for the r , c and P , and to 120 for the epipolar j,i j j cue. These hyperparameters lead to the best evaluation in We choose the reference epipolar plane, which is fixed for our empirical study. both frames, as the plane associated with a randomly chosen We apply extensive augmentations, including random pixel from the first image. color jittering, which varies the brightness, contrast satu- Finally, for both parametrizations, the pixel-specific ration and hue of the RGB inputs, as well as random crop- epipolar encodings, n or θ , are embedded into Fourier j,i j,i ping, rotation, and horizontal flipping. We use the ADAM- features and treated as ‘geometrical channels’ (Fig. 2), i.e. W [28, 34] optimizer with standard parameters β = 0.9, concatenated to the per-pixel inputs along the channel di- 1 β = 0.999, and a cosine learning rate schedule without mension. 2 warmup, a weight decay of 1e−5, a maximal learning rate The epipolar embedding does not add new information of 2e−4, and train for 250 epochs with a batch size of 64. compared to camera geometric embeddings described in Sec. 3.1, but it provides an additional guidance for the net- 4.1. Geometrical Embeddings work to more efficiently leverage correspondence. We present results in a top-down matter, starting with the 4. Experiments higher-level questions: are camera and epipolar geometrical embeddings useful? Are they complementary? We then We evaluate our geometrical embeddings with the Per- trickle down and study finer-grained design decisions for ceiver IO model on the task of depth estimation from pairs each of these two families of geometrical embeddings. In of views, a central computer vision task. this subsection all experiments are done on ScanNet. For statistical robustness, we train three models using different Data. We use the ScanNet [7] and DeMoN [56] datasets random seeds and report the median result. for training and testing. For ScanNet we use the frame se- lection as provided by [30], which yields 94212 training Coarse-grained analysis. We consider the best-performing pairs and 7517 test pairs. The DeMoN dataset combines options (according to the fine-grained analysis in the next SUN3D [59], RGBD-SLAM [49] and Scenes11 [56]. It subsection) for camera and epipolar embeddings Secs. 3.1 has a total of of 166,285 training image pairs from 50420 and 3.2. scenes and 288 test image pairs. Both datasets contain in- Tab. 1 shows that, compared to using just standard pixel valid depth measurements, following the community com- positional embeddings, any of the geometric embeddings mon practice, we mask the depths out of [0.1, 10] as invalid. contribute to substantial depth estimation F1-score im- 5
camera epipolar training data proportion camera camera epipolar abs. parametrization assembling parametrization rel.diff embedding cue 30% 50% 100% c , r channel – 0.1234 0.2568 0.2423 0.2340 j j,i c , r token – 0.1249 (cid:88) 0.1350 0.1293 0.1234 j j,i P , x channel – 0.1345 (cid:88) 0.2084 0.2018 0.1853 j j,i P , x token – 0.1805 (cid:88) (cid:88) 0.1371 0.1304 0.1204 j j,i c , r channel n 0.1235 j j,i j,i Table 1. The effect of inputs on training efficiency and generaliza- c , r channel θ 0.1204 j j,i j,i tion (evaluated using absolute relative difference – lower is better), Table 2. Comparison between different parameterization options using the best option for each mode. for camera and epipolar embeddings (using absolute relative dif- ference). provement, with the camera embedding reducing the abso- lute relative difference almost by half. Interestingly, while error in Fig. 5. We observe from the validation curve (right), the epipolar embedding itself does not provide sufficient in- that when including the RGB information (green) in the formation to perform triangulation, the epipolar embedding queries, the network initially learns slightly faster, but as alone (row 3) can enhance the result as it provides additional the training progresses, this is outperformed by the queries guidance for correspondence estimation. that contain only the geometric-embedding. On the other When provided with both the camera and epipolar em- hand, the training loss of the RGB-included queries remains beddings (row 4), our model performs similarly as when smaller than that of the RGB-excluded queries, suggesting using the camera embedding alone. As the amount of train- that the RGB information eventually leads the network to ing data increases however, the epipolar embedding seems overfit by overly attending to the texture information. An to start contributing positively to the overall F1-score. example of such behavior is shown in Fig. 6. Fine-grained analysis. We now get down to more detailed 4.2. Comparison with State-of-the-Art Methods analysis and compare the different options introduced in Secs. 3.1 and 3.2. First, we compare the two proposed cam- We now compare our best model to the state-of-the-art era parameterizations, namely using camera center and ray on 4 different datasets: ScanNet, Sun3D, RGBD-SLAM direction, c j and r j,i, or directly using the projection ma- and Scenes11. The results are shown in Tab. 3 and indi- trix and the pixel positions, P j and x j,i, as well as the two cate that using geometrical embeddings with a very generic approaches for assembling this information into the input model matches and sometimes outperforms specialized via geometrical channels or geometrical tokens. As the up- state-of-the-art models. Note that the NAS model [30] uses per part of Tab. 2 shows, using camera center and ray di- additional ground truth normal information as supervision rection has a consistent advantage regardless of the assem- and enforces consistency between normals and depth. bling method, likely thanks to its compactness. At the same We also evaluate the generalization ability of our time, we observe that concatenating the geometric embed- method. As we demonstrate in the Appendix A, when test- ding channel-wise to the RGB inputs compares favorably ing on an unseen dataset of similar domain, our model per- with using the geometric embedding as a separate token. forms on par with state-of-the-art methods trained specif- This is likely due to the fact that the concatenation pro- ically for that dataset, but sees a performance drop un- vides a more direct association between the geometry and der significant domain shift. This is somewhat expected, the pixel-wise RGB information. since unlike conventional plane-sweep methods, our model Based on the best camera configuration, we evaluate the doesn’t have the frames aligned externally but rather learns two options for the parameterization of the epipolar cue. As the alignment from input cues. the lower part of Tab. 2 shows, the angle parameterization slightly outperforms the normal parameterization, likely be- 4.3. Camera Localization cause the randomness in choosing the reference epipolar To what extent does our algorithm understand camera plane reduces the overfitting. geometry, as opposed to simply memorizing depth [54]? Queries. We evaluate two types of queries. As the first One way we can find out is by asking our algorithm to per- option, the queries take the same form as the inputs, using form a useful task that it was never trained to perform. It both the RGB features and the constructed geometric em- turns out that our network can actually localize cameras beddings. Alternatively, we also experiment with discard- given 3D geometry, which is an important component of ing the RGB and querying using only the geometric embed- any SLAM system. dings. We assume that we have a pair of images and ground We show the progression of training loss and validation truth depth maps for both. We assume camera intrinsics are 6
Inputinimpuatg regpbairs Mproeddeilctperde ddiecptitohns Ggrorouunnddt-rtruutthh ddeepptthhmaps Figure 4. Examples of estimated depths using our best model on image pairs from ScanNet. Holes in the ground truth depth maps are masked out (shown in black). dataset methods abs.rel. ↓ rmse ↓ δ < 1.25 ↑ DPSNet [22] 0.1258 0.3145 - ScanNet NAS [30] 0.1070 0.2807 - IIB (ours) 0.1159 0.2807 0.9079 DeMoN [56] 0.2137 2.4212 0.7332 DeepMVS [20] 0.2816 0.9436 0.5622 SUN3D DPSNet [22] 0.1469 0.4489 0.7812 NAS [30] 0.1271 0.3775 0.8292 Figure 5. Queries with (green) and without (orange) RGB infor- IIB (ours) 0.0985 0.2934 0.9018 mation. We show the training loss (left) and the validation curve DeMoN [56] 0.1569 1.7798 0.8011 (right). The change of the relative performance ranking between DeepMVS [20] 0.2938 0.8684 0.5493 RGBD- the two options in the validation curve suggests that overfitting to SLAM DPSNet [22] 0.1508 0.6952 0.8041 NAS [30] 0.1314 0.6190 0.8565 the RGB information. IIB (ours) 0.0951 0.5498 0.9065 DeMoN [56] 0.5560 2.6034 0.4963 DeepMVS [20] 0.2100 0.8909 0.6881 Scenes11 DPSNet [22] 0.0500 0.4661 0.9614 NAS [30] 0.0380 0.3710 0.9754 IIB (ours) 0.0556 0.5229 0.9631 Figure 6. The effect of using RGB in the queries. Using RGB Table 3. Comparison with the state-of-the-art. Our method, here information in the query may cause artifacts due to overfitting the named IIB for Input-level Inductive Biases, performs competi- depth to texture. tively with these more specialized methods, doing best on two of the four datasets and coming close in the other two. available, but the camera position and orientation are un- known. We can randomly initialize the relative offset and orientation of the cameras, and then optimize them to mini- We evaluate on the SUN3D validation set. We treat the mize the L1LOG distance between the predicted depth and first camera as fixed at the origin, and evaluate the position ground truth depth. Our underlying assumption is that er- of the second camera relative to it. Following prior work on rors in cameras will result in incorrect triangulation when camera localization [46], we evaluate two metrics. First is the correspondences are correct. Therefore, if the algorithm translation error in cm, which is simply (cid:107)c − c (cid:107) where est gt 2 is doing correct 3D geometry, the error should be minimized c and c are the estimated and ground truth camera cen- est gt when the relative camera positions are correct. We give im- ters, respectively. Second is rotation error in degrees, which plementation details in Appendix B. is computed as arccos((trace(R−1R ) − 1)/2), where gt est 7
input t=0; l1=.094; t=5; l1=.066; t=10; l1=.068; t=50; l1=.053; t=200; l1=.042; ground truth rotation error=3.93; rotation error=5.21; rotation error=6.04; rotation error=6.53; rotation error=1.66; trans. error=17.6 trans. error=16.3 trans. error=14.8 trans. error=6.3 trans. error=2.0 Figure 7. Progression of our iterative camera localization algorithm across 200 timesteps of optimization for the input image (left) and ground-truth depth that we are fitting to (right). At each step we show the optimization timestep (t), l1 loss between the predicted depth and the ground truth depth, as well as the rotation and translation error between the estimated and ground truth cameras (which is not used in the optimization). 5. Discussion mean median mean median rotation rotation translation translation error (◦) error (◦) error (cm) error (cm) The 3D nature of space is a key aspect of our reality Identity 9.11 7.61 17.7 12.7 and should be incorporated as priors into our visual mod- Rand. init 9.18 7.68 17.8 12.8 els. Most models for 3D reconstruction currently incorpo- Optimized 6.67 4.38 2.5 1.9 rate 3D priors by tailoring the architectures. In this paper we investigated an alternative inspired by advances in mod- Table 4. Camera Localization performance on SUN3D. Lower is eling with Transformers: we featurize these priors and feed better. them as inputs to the model. We show that this incurs no sacrifice in performance and in fact we obtain results that are competitive with leading models on several datasets. R and R are the estimated and ground truth rotation Our exploration in the space of geometry parameterization est gt matrices respectively. This is the minimum rotation angle is non-exhaustive, more indicative priors may be derived to required to align both rotations. simplify the 3D reasoning. Having geometric priors as inputs also opens up new Tab. 4 shows our results. We report both the mean possibilities: given a pre-trained frozen model and ground and median across examples in the dataset. We see non- truth depth, one can finetune the geometrical inputs in case trivial improvements in both metrics, localizing the cameras they are unknown, e.g. for camera calibration or epipolar within a few centimeters and a few degrees of their true lo- geometry estimation. Input-level inductive biases may also cations. While we don’t expect this to be competitive with enable us to incorporate geometry into multimodal models, SOTA SLAM systems (which typically integrate informa- e.g., those that jointly process sound, touch or text. In such tion across many more images), these results clearly show a setting, the type of architecture engineering that is appro- that the algorithm is using geometry as expected: depth er- priate for vision would no longer apply, whereas input-level ror is minimized when the cameras are at the correct loca- biases still could. tions. This may serve as a starting point for more flexible On the other hand, since our model needs to learn the systems which don’t rely on access to ground truth cameras. 3D alignment, it expects the training and test data to have a Fig. 7 shows how the depth map progresses throughout similar distribution. Moreover, since the baseline architec- 200 steps of optimizing the camera. We see that the initial ture operates on a compressed latent space, how the network depth is quite poor, with little definition on the desk, and solves the problem is potentially less interpretable because a chair in the foreground which has been split in two, like there are no explicit correspondence establishing steps. double-vision in humans. These errors gradually resolve as the camera estimate gets better, with the algorithm able to 6. Acknowledgements correctly bind pixels across images using the geometry. In- terestingly, the translation error improves faster than the ro- We thank Yi Yang for providing advice regarding data tation error, suggesting that the algorithm may be using the processing and Jean-Baptiste Alayrac for his help with the camera centers more than the ray angles in order to perform training pipeline. We are also grateful for Ankush Gupta for triangulation. his insightful feedback on the paper. 8
References [15] Alon Halevy, Peter Norvig, and Fernando Pereira. The un- reasonable effectiveness of data. IEEE Intelligent Systems, [1] Aljazˇ Bozˇicˇ, Pablo Palafox, Justus Thies, Angela Dai, and 24(2):8–12, 2009. 2 Matthias Nießner. TransformerFusion: Monocular RGB [16] Richard I. Hartley and Andrew Zisserman. Multiple View scene reconstruction using transformers. arXiv preprint Geometry in Computer Vision. Cambridge University Press, arXiv:2107.02191, 2021. 2 ISBN: 0521540518, second edition, 2004. 3 [2] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo [17] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu. matching network. In CVPR, pages 5410–5418, 2018. 2 Epipolar transformers. In Proceedings of the ieee/cvf con- [3] Zhuoyuan Chen, Xun Sun, Liang Wang, Yinan Yu, and ference on computer vision and pattern recognition, pages Chang Huang. A deep visual correspondence embedding 7779–7788, 2020. 2 model for stereo matching costs. In ICCV, pages 972–980, [18] Heiko Hirschmuller. Stereo processing by semiglobal match- 2015. 2 ing and mutual information. IEEE TPAMI, 30(2):328–341, [4] Zhiqin Chen and Hao Zhang. Learning implicit fields for 2007. 2 generative shape modeling. In Proceedings of the IEEE/CVF [19] Berthold KP Horn and Brian G Schunck. Determining opti- Conference on Computer Vision and Pattern Recognition, cal flow. Artificial intelligence, 17(1-3):185–203, 1981. 1 pages 5939–5948, 2019. 2 [20] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra [5] Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Ahuja, and Jia-Bin Huang. DeepMVS: Learning multi-view Dai, Xiaojun Chang, Hongdong Li, Tom Drummond, and stereopsis. In CVPR, pages 2821–2830, 2018. 1, 7 Zongyuan Ge. Hierarchical neural architecture search for [21] Lam Huynh, Phong Nguyen-Ha, Jiri Matas, Esa Rahtu, and deep stereo matching. NeurIPS, 33:22158–22169, 2020. 2 Janne Heikkila¨. Guiding monocular depth estimation using [6] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep depth-attention volume. In European Conference on Com- global registration. In CVPR, pages 2514–2523, 2020. 2 puter Vision, pages 581–597. Springer, 2020. 5 [22] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So [7] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal- Kweon. DPSNet: End-to-end deep plane sweep stereo. arXiv ber, Thomas Funkhouser, and Matthias Nießner. Scannet: preprint arXiv:1905.00538, 2019. 1, 7 Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), [23] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, IEEE, 2017. 2, 5 Carl Doersch, Catalin Ionescu, David Ding, Skanda Kop- pula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier [8] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- He´naff, Matthew M. Botvinick, Andrew Zisserman, Oriol novich. Deep image homography estimation. arXiv preprint Vinyals, and Joao Carreira. Perceiver IO: A general ar- arXiv:1606.03798, 2016. 2 chitecture for structured inputs & outputs. arXiv preprint [9] Carl Doersch, Ankush Gupta, and Andrew Zisserman. arXiv:2107.14795, 2021. 1, 2, 3, 5 CrossTransformers: Spatially-aware few-shot transfer. In [24] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zis- NeurIPS, 2020. 2 serman, Oriol Vinyals, and Joao Carreira. Perceiver: General [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, perception with iterative attention. In ICML, 2021. 2 Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [25] Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- Fang. Surfacenet: An end-to-end 3d neural network for mul- vain Gelly, et al. An image is worth 16x16 words: Trans- tiview stereopsis. In ICCV, pages 2307–2315, 2017. 2 formers for image recognition at scale. arXiv preprint [26] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, arXiv:2010.11929, 2020. 2 and Kwang Moo Yi. COTR: Correspondence transformer for [11] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip matching across images. arXiv preprint arXiv:2103.14167, Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van 2021. 2 Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: [27] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Learning optical flow with convolutional networks. In ICCV, Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. pages 2758–2766, 2015. 2 End-to-end learning of geometry and context for deep stereo [12] Jose M Facil, Benjamin Ummenhofer, Huizhong Zhou, regression. In ICCV, pages 66–75, 2017. 2 Luis Montesano, Thomas Brox, and Javier Civera. Cam- [28] Diederik P Kingma and Jimmy Ba. Adam: A method for convs: Camera-aware multi-scale convolutions for single- stochastic optimization. arXiv preprint arXiv:1412.6980, view depth. In Proceedings of the IEEE/CVF Conference 2014. 5 on Computer Vision and Pattern Recognition, pages 11826– [29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 11835, 2019. 2 MNIST classification with deep convolutional neural net- [13] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong works. NeurIPS, 25:1097–1105, 2012. 2 Tan, and Ping Tan. Cascade cost volume for high-resolution [30] Uday Kusupati, Shuo Cheng, Rui Chen, and Hao Su. Normal multi-view stereo and stereo matching. In CVPR, pages assisted stereo depth estimation. In CVPR, pages 2189–2199, 2495–2504, 2020. 2 2020. 1, 5, 6, 7 [14] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and [31] Zhenghong Li, Sofia Broome´, Pia Haubro Andersen, and Hongsheng Li. Group-wise correlation stereo network. In Hedvig Kjellstro¨m. Automated detection of equine facial CVPR, pages 3273–3282, 2019. 2 action units. arXiv preprint arXiv:2102.08983, 2021. 1 9
[32] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, [47] Johannes Lutz Scho¨nberger and Jan-Michael Frahm. Francis X Creighton, Russell H Taylor, and Mathias Un- Structure-from-motion revisited. In Conference on Com- berath. Revisiting stereo depth estimation from a sequence- puter Vision and Pattern Recognition (CVPR), 2016. 3 to-sequence perspective with transformers. In ICCV, pages [48] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, 6197–6206, 2021. 2 and Stefan Carlsson. Cnn features off-the-shelf: an astound- [33] Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Wei ing baseline for recognition. In CVPR, pages 806–813, 2014. Chen, Linbo Qiao, Li Zhou, and Jianfeng Zhang. Learning 2 for disparity estimation through feature constancy. In CVPR, [49] Ju¨rgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram pages 2811–2820, 2018. 2 Burgard, and Daniel Cremers. A benchmark for the evalua- [34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay tion of rgb-d slam systems. In 2012 IEEE/RSJ international regularization. arXiv preprint arXiv:1711.05101, 2017. 5 conference on intelligent robots and systems, pages 573–580. [35] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, IEEE, 2012. 2, 5 Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A [50] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. large dataset to train convolutional networks for disparity, Pwc-net: Cnns for optical flow using pyramid, warping, and optical flow, and scene flow estimation. In CVPR, pages cost volume. In CVPR, pages 8934–8943, 2018. 2 4040–4048, 2016. 2 [51] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and [36] Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc Xiaowei Zhou. Loftr: Detector-free local feature matching Pollefeys, Esa Rahtu, and Juho Kannala. DGC-Net: Dense with transformers. In CVPR, pages 8922–8931, 2021. 2 geometric correspondence network. In WACV, pages 1034– [52] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, 1042. IEEE, 2019. 1, 2 and Hujun Bao. NeuralRecon: Real-time coherent 3D re- [37] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- construction from monocular video. In CVPR, pages 15598– bastian Nowozin, and Andreas Geiger. Occupancy networks: 15607, 2021. 2 Learning 3D reconstruction in function space. In CVPR, [53] Richard Sutton. The bitter lesson. http : pages 4460–4470, 2019. 2 / / www . incompleteideas . net / IncIdeas / [38] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, BitterLesson.html, 2019. Accessed: 2021-11-16. 2 Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: [54] Maxim Tatarchenko, Stephan R Richter, Rene´ Ranftl, Representing scenes as neural radiance fields for view syn- Zhuwen Li, Vladlen Koltun, and Thomas Brox. What do thesis. In ECCV, pages 405–421. Springer, 2020. 2, 4 single-view 3d reconstruction networks learn? In CVPR, [39] Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho. pages 3405–3414, 2019. 6 Hyperpixel flow: Semantic correspondence with multi-layer [55] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field neural features. In ICCV, pages 3395–3404, 2019. 2 transforms for optical flow. In ECCV, pages 402–419. [40] Zak Murez, Tarrence van As, James Bartolozzi, Ayan Sinha, Springer, 2020. 2, 5 Vijay Badrinarayanan, and Andrew Rabinovich. Atlas: End- to-end 3d scene reconstruction from posed images. In ECCV, [56] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Niko- pages 414–431. Springer, 2020. 2 laus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning [41] Yuichi Ohta and Takeo Kanade. Stereo by intra-and inter- monocular stereo. In Proceedings of the IEEE Conference scanline search using dynamic programming. IEEE TPAMI, on Computer Vision and Pattern Recognition, pages 5038– pages 139–154, 1985. 2 5047, 2017. 1, 2, 5, 7 [42] Jeong Joon Park, Peter Florence, Julian Straub, Richard [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- Newcombe, and Steven Lovegrove. DeepSDF: Learning reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia continuous signed distance functions for shape representa- tion. In CVPR, pages 165–174, 2019. 2 Polosukhin. Attention is all you need. In NeurIPS, pages 5998–6008, 2017. 2, 4 [43] Ignacio Rocco, Relja Arandjelovic´, and Josef Sivic. Efficient neighbourhood consensus networks via submanifold sparse [58] Ming Wei, Ming Zhu, Yi Wu, Jiaqi Sun, Jiarong Wang, and convolutions. In ECCV, pages 605–621. Springer, 2020. 2 Changji Liu. A fast stereo matching network with multi- cross attention. Sensors, 21(18):6016, 2021. 2 [44] Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovic´, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Neighbourhood con- [59] Jianxiong Xiao, Andrew Owens, and Antonio Torralba. sensus networks. In NeurIPS, 2018. 1, 2 Sun3d: A database of big spaces reconstructed using sfm [45] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and object labels. In Proceedings of the IEEE international and Andrew Rabinovich. SuperGlue: Learning feature conference on computer vision, pages 1625–1632, 2013. 2, matching with graph neural networks. In CVPR, pages 4938– 5 4947, 2020. 2 [60] Haofei Xu and Juyong Zhang. AANet: Adaptive aggregation [46] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, network for efficient stereo matching. In CVPR, pages 1959– Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi 1968, 2020. 2 Okutomi, Marc Pollefeys, Josef Sivic, et al. Benchmarking [61] Gengshan Yang, Joshua Manela, Michael Happold, and 6dof outdoor visual localization in changing conditions. In Deva Ramanan. Hierarchical deep stereo matching on high- CVPR, pages 8601–8610, 2018. 7 resolution images. In CVPR, pages 5515–5524, 2019. 2 10
[62] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. method train test abs.rel ↓ rmse ↓ δ < 1.25 ↑ Cost volume pyramid based depth inference for multi-view IIB (ours) ScanNet SUN3D 0.1291 0.3699 0.8298 stereo. In CVPR, pages 4877–4886, 2020. 2 IIB (ours) SUN3D SUN3D 0.0985 0.2934 0.9018 [63] Zhenpei Yang, Zhile Ren, Qi Shan, and Qixing Huang. NAS [27] SUN3D SUN3D 0.1271 0.3775 0.8292 Mvs2d: Efficient multi-view stereo via attention-driven 2d convolutions. arXiv preprint arXiv:2104.13325, 2021. 2 IIB (ours) ScanNet RGBD 0.2572 1.3102 0.5101 [64] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit, IIB (ours) RGBD RGBD 0.0951 0.5498 0.9065 Mathieu Salzmann, and Pascal Fua. Learning to find good NAS [27] RGBD RGBD 0.1314 0.6190 0.8565 correspondences. In CVPR, pages 2666–2674, 2018. 2 Table 5. Generalisation performance. [65] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lip- son. How transferable are features in deep neural networks? NeurIPS, 27:3320–3328, 2014. 2 maps, the goal is to infer the relative position and orienta- [66] Feihu Zhang, Victor Prisacariu, Ruigang Yang, and tion of the two cameras using the network. We use a per- Philip HS Torr. GA-Net: Guided aggregation net for end- ceiver with c , r for the camera embedding and n for to-end stereo matching. In CVPR, pages 185–194, 2019. 1, j,i j,i j,i the epipolar constraint. 2 Recall that cameras are parameterized with intrinsics K, [67] Jingyang Zhang, Yao Yao, and Long Quan. Learning signed and extrinsics R and t. We assume known K’s, and without distance field for multi-view surface reconstruction. In loss of generality that the first camera is fixed as R = I Proceedings of the IEEE/CVF International Conference on 1 Computer Vision, pages 6525–6534, 2021. 2 and t 1 = 0. Thus, the extrinsics of the second camera R and t are the optimization variables, and these are [68] Zhengyou Zhang. A flexible new technique for camera cali- 2 2 bration. IEEE Transactions on pattern analysis and machine parameterized as t 2 ∈ R3 and Rˆ 2 ∈ R3×3. To make intelligence, 22(11):1330–1334, 2000. 3 sure that the extrinsic matrix is a rigid transformation, we [69] Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. compute the final rotation matrix as R 2 = U V , where DeepTAM: Deep tracking and mapping. In ECCV, pages U , S, V = SVD(Rˆ 2), SVD is singular value decompo- 822–838, 2018. 1, 2 sition, U and V are orthonormal, and S is diagonal. There- fore we are overparameterizing R with 9 parameters but 2 A. Generalization on Out-Of-Domain Test only 3 degrees of freedom, but empirically we find this gives Data good results. Our optimization objective is L1LOG loss on both im- Our method generalizes to unseen datasets of a compa- ages, plus regularization terms to encourage both the trans- rable domain. However, when testing on a significantly lation and rotation to be small. Without regularization, we different domain, e.g. trained on indoor scenes and testing find that the optimization can get stuck in local minima on outdoor scenes, our framework will see a performance with very large rotation and translation, which we don’t drop. Table 5 shows the performance of a model trained expect will make sense to the network. Specifically, we on ScanNet and evaluated on RGBD and SUN3D datasets. let L (R) = arccos((trace(R) − 1)/2) in radians, and rot SUN3D is similar to ScanNet, our method (trained on Scan- L (t) = (cid:107)t(cid:107) . Then the final loss can be written as: trans 2 Net only) performs reasonably well and on par with meth- ods trained on SUN3D. RGBD datasets contain many ware- L(R 2, t 2) = L1LOG(R 2, t 2)+λ rotL rot(R 2)+λ transL trans(t 2) house scenes where the depth range is significantly different We set λ = 1 and λ = 20. Note that during opti- rot trans from the one seen during training. Our model shows overfit- mization, the translation coordinates use the default Sun3D ting in this case. This is because unlike conventional plane- scaling, which is 100× smaller than what we report in our sweep methods, where the network essentially computes a result table. cost-volume from RGB features that are explicitly aligned, We use the ADAM optimizer with a cosine learning rate our method has to learn the alignment itself. How to reduce schedule for 200 steps and an initial learning rate of 5e−3. such a gap in case of domain shift is a research direction We initialize t = (cid:15) where (cid:15) ∈ R3 and Rˆ = 2 trans trans 2 for future work. Besides introducing more augmentation I + (cid:15) where (cid:15) ∈ R3. Each element of both (cid:15) rot trans rot techniques, we think fine-tuning and scale-normalization and (cid:15) is distributed as N (0, 0.01). We find that the trans are some promising directions to pursue. network occasionally gets stuck in local optima, so we run with 5 different random initializations and take the solution B. Camera localization: implementation de- with the best total loss L. tails C. Qualitative Results In this section, we give implementation details for Sec- tion 4.3. Given a pair of images and ground truth depth Additional qualitative results are shown in Fig. 8. 11
Input image pairs Model predictions Ground truth depth maps Figure 8. Additional examples of estimated depths using our best model on image pairs from ScanNet. Holes in the ground truth depth maps are masked out (shown in black). 12
