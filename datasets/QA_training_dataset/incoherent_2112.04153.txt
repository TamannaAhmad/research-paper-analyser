Model-Value Inconsistency as a Signal for Epistemic Uncertainty Angelos Filos * 1 2 Eszter Ve´rtes * 1 Zita Marinho * 1 Gregory Farquhar 1 Diana Borsa 1 Abram Friesen 1 Feryal Behbahani 1 Tom Schaul 1 Andre´ Barreto 1 Simon Osindero 1 Abstract s vˆ vˆ m0 ˆ (s) v(s) Using a model of the environment and a value function, an agent can construct many estimates s Tmˆ vˆ vˆ m1 ˆ (s) s of a state’s value, by unrolling the model for dif- (b) At initialisation ferent lengths and bootstrapping with its value function. Our key insight is that one can treat s Tmˆ Tmˆ vˆ vˆ m2 ˆ (s) vv((ss)) this set of value estimates as a type of ensemble, which we call an implicit value ensemble (IVE). s Tmˆ · · · Tmˆ vˆ vˆ mk ˆ (s) ss Consequently, the discrepancy between these es- (a) Implicit Value Ensemble (IVE) (c) After training timates can be used as a proxy for the agent’s epistemic uncertainty; we term this signal model- Figure 1: Implicit value ensemble (IVE) estimated from value inconsistency or self-inconsistency for short. a single learned model mˆ and value function vˆ. (a) Unlike prior work which estimates uncertainty by Computation graph. The model-induced Bellman operator training an ensemble of many models and/or value T is repeatedly applied k times on the approximate mˆ functions, this approach requires only the single value function vˆ, i.e., vˆk (s) (cid:44) (T )kvˆ(s). (b-c) Didactic mˆ mˆ model and value function which are already being example with 1D state space: Value predictions (in learned in most model-based reinforcement learn- blue) for different values of k, i.e., {vˆk }10 , along with mˆ k=0 ing algorithms. We provide empirical evidence in the ensemble mean µ-IVE(10) and standard deviation both tabular and function approximation settings σ-IVE(10) (in orange), before (b) and after (c) training from pixels that self-inconsistency is useful (i) with value targets (black circles). The ensemble standard as a signal for exploration, (ii) for acting safely deviation is non-trivial at out-of-distribution (OOD) states under distribution shifts, and (iii) for robustifying and zero at in-distribution states. value-based planning with a learned model. agents’ uncertainty in both model-free (Dearden et al., 1998) and model-based (Dearden et al., 1999) RL approaches. 1 Introduction However, in complex RL problems, since exact Bayesian Agents that employ learning to improve their decision inference is intractable, proxy signals are often used instead, making should be equipped with mechanisms for repre- including prediction error (Lopes et al., 2012; Pathak et al., senting and using their acquired knowledge effectively. 2017), approximate state visitation counts (Bellemare et al., Learned models of the environment (Sutton, 1991) and 2016) and disagreement of samples from either approximate value functions (Sutton, 1988) are explicit ways that posterior distributions over learned parameters (Blundell supervised learning (RL, Sutton & Barto, 2018) agents et al., 2015) or explicit ensembles of value functions (Os- use to represent their knowledge about the environment. band et al., 2016) or world models (Chua et al., 2018). Equally important is the agents’ ability to reason about their In this work, we introduce a novel signal for capturing ignorance (i.e., epistemic uncertainty, Strens, 2000) and RL agents’ ignorance, termed model-value inconsistency factor it in their decisions (Milnor, 1951). In tabular settings, or self-inconsistency for short. A k-step self-inconsistency exact Bayesian inference can be used for quantifying the signal is constructed by applying the model-induced Bell- man operator T to learned value function vˆ, k times. mˆ *Equal contribution 1DeepMind 2University of Oxford. Corre- This produces k + 1 different estimates of the state value: spondence to: Angelos Filos <angelos.filos@cs.ox.ac.uk>. {vˆ, T vˆ, (T )2vˆ, . . . , (T )kvˆ}, as illustrated in Figures 1 mˆ mˆ mˆ and 2c. Our key insight is that these can be thought of as Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- predictions from an ensemble of value functions, which we right 2022 by the author(s). call the implicit value ensemble (IVE). 2202 nuJ 92 ]GL.sc[ 3v35140.2112:viXra
Model-Value Inconsistency as a Signal for Epistemic Uncertainty {vˆ (s)}n {T vˆ(s)}n {(T )ivˆ(s)}n i i=1 mˆ i i=1 mˆ i=1 T mˆ 1 vˆ 1 vˆ vˆ vˆ · · · · · · vˆ vˆ n T mˆ n T mˆ T mˆ T mˆ s s s (a) Value Ensemble (EVE) (b) Model Ensemble (EMVE) (c) Implicit Value Ensemble (IVE) Figure 2: Value computation in scalable epistemic uncertainty-aware RL agents. (a-b) Explicit ensemble of value func- tions (Osband et al., 2016) and world models (Chua et al., 2018), approximating samples from p(v|B) and p(m|B), respec- tively. The number of parameters grows linearly with the ensemble size. (c) Implicit value ensemble (IVE) make ensemble value predictions using a single learned value function and world model by exploiting the model-induced Bellman operator T and the Bellman consistency of the “true” model m∗ and value function v, keeping the number of parameters constant. mˆ Consequently, the disagreement of these predictions can tell vπ(s) (cid:44) E [(cid:80) γtR |S = s], where E [·] de- π,m∗ t≥0 t 0 π,m∗ us about the agent’s uncertainty in the value of a state. The notes the expectation1 over the trajectories induced by run- intuition behind this is based on the fact that the true model ning policy π in the environment m∗, starting from state s. and value are by definition Bellman-consistent. As a result, The computation of the value of a policy π, i.e., vπ, is for regions of the state space where the learned model and termed policy evaluation and can be concisely formulated value are accurate, we expect the self-inconsistency to be using Bellman evaluation operators (Bellman, 1957). Next, low. Conversely, high self-inconsistency can signal that the we define the one-step Bellman evaluation operator, applied learned model-value pair is inaccurate. on a state-(to-scalar) function v ∈ V (cid:44) {f : S → R}. In contrast to prior work that requires explicit ensembles of Definition 1 (Bellman evaluation operator). Given the learned value functions (Osband et al., 2016; Lowrey et al., model m∗ and policy π the one-step Bellman evaluation 2018) or ensembles of world models (Chua et al., 2018; operator T π : V → V is induced, and its application on a Sekar et al., 2020), self-inconsistency can be efficiently cal- state-function v ∈ V, for all s ∈ S, is given by culated by any RL agent that has a learned (approximate) model of the environment and value function, see Figure 2. T πv(s) (cid:44) E π,m∗ [R 1 + γv(S 1) | S 0 = s] . (1) Moreover, unlike model-ensembles, self-inconsistency cap- tures the agents’ ignorance about behaviourally-relevant The k-times repeated application of an one-step Bellman quantities, i.e., rewards and values, and hence is robust to ir- operator gives rise to the k-steps Bellman operator, relevant information for control noise (Schmidhuber, 2010). (T π)kv (cid:44) T π · · · T π v. (2) We provide empirical evidence that self-inconsistency pro- k-times vides a proxy of epistemic uncertainty (Section 4.1), and The Bellman evaluation operator, T π, is a contraction map- that this information can be used to guide exploration or act ping (Puterman, 2014), and its fixed point is the value of the safely (Section 4.2), and to robustify planning (Section 4.3). policy π, i.e., lim (T π)nv = vπ, for any v ∈ V. n→∞ 2 Background 2.1 Model-Based supervised learning We model the agent’s interaction with the environment as In the general RL formulation, it is assumed that the envi- a Markov decision process (MDP, Puterman, 2014), i.e., ronment model m∗ is unknown to the agent (Sutton & Barto, M (cid:44) (S, A, p, r). At any discrete time step t ≥ 0, the 2018) which thus cannot directly compute Eqn. (1). Model- agent is in state s ∈ S, takes an action a ∈ A, ac- free RL agents resolve this by estimating these expectations t t cording to a policy π : S → ∆(A), then receives re- through sampling. Model-based RL agents, the focus of ward R ∼ r(·|s , a ) ∈ R and transitions to the state this paper, learn an approximate model mˆ ≈ m∗, possi- t+1 t t S ∼ p(·|s , a ). For brevity, the “true” model is denoted bly together with a learned value function vˆ ≈ vπ (Sutton, t+1 t t by m∗ (cid:44) (p, r) and we write S , R ∼ m∗(·, ·|s , a ). t+1 t+1 t t 1In this work, we only construct estimates of the mean of the The agent’s goal is to find the policy that maximises the returns distribution (a.k.a value distribution Bellemare et al., 2017) value of each state, for a discount factor γ ∈ [0, 1), and hence environment and policy stochasticity is integrated out.
Model-Value Inconsistency as a Signal for Epistemic Uncertainty 1991), and use them to compute an estimate of the value, by is deep ensembles (Lakshminarayanan et al., 2016). Un- replacing model and function m∗, v with mˆ , vˆ in Eqn. (1). der certain assumptions (Pearce et al., 2020), the ensemble components can be seen as samples from the posterior dis- Model-induced Bellman operator. A model mˆ and policy tribution over NN parameters. It has been argued that the π induce a Bellman evaluation operator T π with a fixed diversity (i.e., de-correlation) of the ensemble components mˆ point vπ . Similar to Eqn. (2), a k-steps model-induced is important for better capturing epistemic uncertainty (Wil- mˆ Bellman operator is given by (T π)kv = T π · · · T π v. son & Izmailov, 2020) and various methods have been used mˆ mˆ mˆ k-times to achieve this, all of which inject noise into the learning algorithm, such as: (i) data bootstrapping (Tibshirani, 1996; Model learning principles. The agent interacts with the Osband et al., 2016); (ii) different loss function (iii) func- environment, generating a sequence of states, actions and tion form (Wenzel et al., 2020) or (iv) structured noise per rewards, which we denote with B (cid:44) {(s , a , r )} . t t t t≥0 ensemble component (e.g., priors, Osband et al., 2018). Maximum likelihood estimation (MLE, Kumar & Varaiya, RL agents with an ensemble of value functions or models 2015; Sutton, 1991) can be used for learning the model have been used to quantify their epistemic uncertainty e.g. parameters, given experience tuples (s, a, r(cid:48), s(cid:48)) ∼ B, (Osband et al., 2016; Kurutach et al., 2018), see Figure 2a mˆ = arg max E [log m(r(cid:48), s(cid:48) | s, a)] . (3) and 2b, respectively. We call these methods explicit ensem- MLE B m ble methods and their number of parameters grows linearly with the ensemble size. In contrast, implicit ensembles es- Action-conditioned hidden Markov models have been cape this linear scaling by sharing parameters between the used to scale MLE methods to high-dimensional environ- ensemble members but without sacrificing diversity. ments (Watter et al., 2015), e.g., with pixel observations. Value equivalence (VE, Grimm et al., 2021) is an alternative 3 Your Model-Based Agent is Secretly an principle for model learning. It selects the model that Ensemble of Value Functions induces the “best” approximation to the k-th order Bellman operator of the environment, applied on state-functions V, We now present a proxy signal for epistemic uncertainty, policies Π and state s, trained via samples (s, a, r(cid:48), s(cid:48)) ∼ B, computable by any model-based RL agent with a single (point) estimate of a world model and a value function2. mˆ VE = arg min E B (cid:88) (cid:12) (cid:12)(T mπ)kv(s) − (T π)kv(s)(cid:12) (cid:12) . (4) m π∈Π,v∈V 3.1 Implicit Value Ensemble A key component of our method is the value estimated by a 2.2 Epistemic-Uncertainty-Aware Agents k-step application of the model-induced Bellman operator We refer to learning agents that can quantify their uncer- on the learned value function, which we call k-steps model- tainty about their learned components, e.g., value function predicted value3 (k-MPV), given by or model, as epistemic uncertainty-aware (a.k.a. ignorance- vˆk (cid:44) (T π)kvˆ. (5) aware) agents. While aleatoric uncertainty captures the mˆ mˆ inherent and irreducible stochasticity of the agents’ environ- The k-MPV is a value estimator that interpolates be- ment, epistemic uncertainty is agent-centric (i.e., subjective, tween (i) a model-free value estimator, i.e., k = 0 and Savage, 1972) and reducible (Hutter, 2004). (ii) a purely model-based value estimator, i.e., k → ∞. Bayesian agents. A principled approach to quantifying epistemic uncertainty is by treating learned quantities as k-MPV and n-step returns. The k-MPV should not random variables and perform Bayesian inference given be confused with the n-step returns used in temporal the observed data. Bayesian RL agents maintain beliefs difference (TD, Sutton, 1988) learning. The former is an over value functions (Dearden et al., 1998) or world mod- agent’s estimate about its value, i.e., vˆk ≈ vπ that uses mˆ els (Dearden et al., 1999), which are updated upon interac- both the learned value function and model. The latter is tions with the environment. Exact inference is intractable a stochastic estimate of the environment’s n-step Bellman for most interesting problems and thus ensemble-based ap- proximations are used instead (Lu et al., 2021). 2In this section, we define everything in terms of the Bellman evaluation operator and an approximate on-policy value function. The Bellman optimality operator and an approximate optimal value Explicit ensemble methods. In deep RL, neural networks function could be used instead. For completeness, see Appendix C. (NNs) are used to approximate the value function (Mnih 3Similar quantities have been used in prior work, e.g., k- et al., 2013) or the model (Watter et al., 2015). A popular preturn (Silver et al., 2017) and MVE (Feinberg et al., 2018). approach to epistemic uncertainty quantification for NNs We discuss them and their differences in more detail in Section 5.
Model-Value Inconsistency as a Signal for Epistemic Uncertainty visited unvisited 1.0 0.8 0.6 0.4 0.2 (a) Dataset (b) σ-IVE(1) (c) σ-IVE(2) (d) σ-IVE(20) (e) σ-EVE(20) (f) σ-EMVE(20) 0.0 Figure 3: Model-value inconsistency (σ-IVE, see Section 3.2) as the standard deviation across the implicit value ensemble (IVE, see Section 3.1) for different numbers of ensemble components n. (a) The top left state of the gridworld is excluded from the data used to train the model mˆ and value function vˆ. (b-e) The disagreement between the IVE predictions diffuse for (b) 1-step; (c) 2-steps and (d) 20-steps model unrolls. The same disagreement across explicit ensembles (e) σ-EVE and (f) σ-EMVE built from different initialisation parameters. The standard deviation σ is normalised in range [0, 1] per figure. operator that can be used for constructing value target components, e.g., the standard deviation across the IVE estimators in TD learning with reduced bias. members, denoted by σ-IVE(n) for n members. Similarly, we define µ-IVE(n) as the value prediction, given by the en- An ensemble of k-MPV predictions can be made by varying semble mean, and µ + β · σ-IVE(n) as the weighted sum of k. We call this an implicit value ensemble4 (IVE), depicted the IVE mean and standard deviation, where β ∈ R. We can in Figure 1a and 2c and denoted by induce a self-inconsistency- (i) seeking; (ii) averse or (iii) {vˆi }n (cid:44) {vˆ, T πvˆ, . . . , (T π)nvˆ} . (6) neutral policy when β > 0, β < 0 and β = 0, respectively. mˆ i=0 mˆ mˆ n+1 value estimates 3.3 Practical Implementation Any agent with a model and value function is, in effect, We use parametric function approximators, in particular also equipped with an ensemble of value functions. neural networks, to approximate the model and value func- tion: θ are the model and φ are the value function param- eters, from hypotheses classes Θ and Φ, respectively, i.e., 3.2 Model-Value Inconsistency mˆ (·, ·|s, a; θ) ≈ m∗(·, ·|s, a) and vˆ(s; φ) ≈ vˆ(s). We term the disagreement of the IVE components as model- With small tabular models, such as the ones used for the value inconsistency or just self-inconsistency, for short, gridworld in Figure 3, we can calculate the k-MPVs exactly. since it quantifies the Bellman-inconsistency (Farquhar With neural network models, the calculation of the expec- et al., 2021) of the learned model and value function. tation in Eqn. (1) is generally intractable and hence we can As our learned model and value function better approxi- only approximate it, e.g., in the case of stochastic models, mate their “true” counterparts, the self-inconsistency re- via Monte Carlo (MC) sampling. An MC sample of the duces since the “true” model and value function are Bell- k-MPV of state s ∈ S is given by: man consistent, i.e., (T π )nvπ = (T π )lvπ, ∀n, l ∈ N. If m∗ m∗ the true model and value function are contained in the hy- k−1 (cid:88) pothesis classes of our approximators and the respective vˆ mk ˆ (s) = γi−1ri mˆ+1 + γkvˆ(sk mˆ ), (7) learning algorithms converge to the “true” solutions, then i=1 the self-inconsistency reduces to zero. where s0 = s and the samples from the model and policy mˆ In regions of state space where the learned model are in bold and subscripted with mˆ and π, i.e., ri+1, si+1 ∼ mˆ mˆ and value function are accurate, they are also self- mˆ (·, ·|si , ai ) and ai ∼ π(·|si ). mˆ π π mˆ consistent. With high self-inconsistency the learned In practice, to minimise the number of samples required to model or/and value should be inaccurate. calculate an IVE prediction, we reuse the samples used for estimating the different components of the ensemble. In par- Various metrics can be used to quantify the disagreement ticular, for every MC sample vˆn (s), we use the sampled re- mˆ between the IVE components. Since the k-MPVs are wards, states and actions trajectories {(ri+1, si+1, ai )}n−1 mˆ mˆ π i=0 scalars, we can use any measure of disagreement of its to also estimate the “preceeding” ensemble components {vˆi (s)}n−1. This makes the computation of IVE no 4Non-successive values of k can be used in the construction of mˆ i=0 an IVE, e.g., k ∈ {1, 7, 13}, but in practice this is less computa- more expensive than online sample-based planning meth- tionally efficient, see Section 3.3. ods (Hafner et al., 2019b; Schrittwieser et al., 2020).
Model-Value Inconsistency as a Signal for Epistemic Uncertainty Expectation models. Deterministic multi-step expectation a signal for epistemic uncertainty. Our goal is not to show models, e.g., the MuZero/Muesli model (Schrittwieser that the IVE is better than explicit ensembles. Instead, since et al., 2020), are especially well-suited for calculating IVEs IVE is present in any model-based RL agent, we want to in stochastic environments: they learn to predict expected empirically study its properties and validate its usefulness. rewards and values conditioned on a sequence of actions, thereby implicitly averaging over stochastic state transitions. Baselines. In the tabular experiments, we learn value To estimate the k-MPV in Eqn. (7), only policy samples functions with expected SARSA (Van Seijen et al., 2009) are needed. Empirically, in Appendix D, we found after an and use maximum likelihood estimation for model learning ablation that one sample from the policy sufficed. (see Section 2). The explicit ensemble components are trained independently, using exactly the same data. The 3.4 Diversity in the Implicit Value Ensemble only sources of variability are random initialisation of parameters and stochastic gradient descent. The components of the IVE form a heterogeneous ensem- ble (Wichard et al., 2003) since they differ in (i) functional In the deep RL experiments, we built on the following form, and (ii) learning algorithm. Next, we elaborate on model-based agents, that use either the MLE or VE how these can impact the diversity of the IVE predictions. model learning principles, described in Section 2: (i) Muesli (Hessel et al., 2021) is a policy optimisation method Functional form. While the ensemble components share with a learned multi-step expectation model. Muesli also the same model and value parameters, θ and φ, respectively, learns a state-value function, using Retrace (Munos et al., they make predictions by composing these parameters differ- 2016) to correct for the off-policiness of the replayed ently. For k = 0, only the parameters of the value functions experience. The learned model is used for representation are used for making predictions. As k grows, the contri- learning and for constructing action-value estimates, by bution of the model parameters to the prediction increases. one-step model unroll, used for policy improvement. The For instance, the 1-MPV and 5-MPV, i.e., vˆ1 and vˆ5 , are model parameters are trained to predict reward and value mˆ mˆ both functions parametrised by θ and φ but their functional k-steps into the future (corresponding to the individual dependence on θ and φ is generally different. This intro- terms in the k-MPV); (ii) Dreamer (Hafner et al., 2019a) duces diversity in the ensemble since different functions will is a policy optimisation method with an MLE model. The have different generalisation properties and their predictions model is an action-conditioned hidden Markov model, in out-of-distribution states are expected to differ, for an trained to maximise (a lower bound on) the likelihood of illustration, see Figure 1 and Appendix A for an exposition. the reward and observation sequences. Dreamer learns a value function using only rollouts from the learned model Variability between IVE members is also introduced by and its parameters are learned such that the learned value the training procedure. The exact details depend on the function becomes (self-)consistent with the model; (iii) algorithm used to learning algorithm. Next, we analyse the VPN (Oh et al., 2017) is a value-based planning method Muesli model and value learning algorithms (Hessel et al., with a multi-step expectation model. The action-value 2021) and their impact on the diversity on the IVE members. function and model are trained simultaneously with n-steps Q-learning (Watkins & Dayan, 1992). In this case, the Muesli learning algorithm. In training from a sequence k-MPV is the value estimate after applying k times the of interactions, the deterministic expectation model is un- model-induced Bellman optimality operator on the learned rolled from each state for K steps, following the actions value function (see Appendix C for a formal exposition). that were taken in the environment. The bootstrap target used to update the i’th resulting value estimate vˆ(s ), i ∈ t+i Environments. In the tabular experiments, we use an {0, . . . , K} uses the environment samples from t + i to empty 5 × 5 gridworld, and collect data by rolling out a t + i + n. This receding horizon means that each value esti- uniformly random policy, initialised at the bottom right cell. mate, and therefore each member of the IVE, is regressed We exclude from the dataset any transitions to the top left against a different target, furthering the diversity among cell, as illustrated in Figure 3a, in order to control for visited their predictions. See Appendix E for more details. (in-distribution) and unvisited (out-of-distribution) states. In the deep RL experiments, we use a selection of 5 tasks 4 Experiments from the procgen suite (Cobbe et al., 2019) to (i) control We conduct a series of tabular and deep RL experiments5 the number of distinct levels used for training the agent (i.e., #levels) and (ii) hold out a set of test levels that are to determine how effective model-value inconsistency is as not seen during training. We also use a modification of the 5Further experiments, details on the experimental protocol and walker walk task from the DeepMind Control suite (Tun- implementations can be found in Appendices D, A and B. yasuvunakool et al., 2020). The original walker task
Model-Value Inconsistency as a Signal for Epistemic Uncertainty has a per-step reward r bounded in [0, 1] which is com- Tabular. Figure 5a shows the probability of reaching the t puted based on the agent’s torso height and forward velocity. novel state in gridworld when a self-inconsistency- To parameterise exploration difficulty, we modify the re- seeking policy is followed (+σ-IVE). Seeking self- ward function to set any reward less than η to zero: r˜ = inconsistency improves upon a uniformly random or greedy t H(r − η)r , where H is the Heaviside step function. For policy and is on par with an explicit ensemble of values t t large η, agents that rely on naive exploration methods will (EVE) method (H4). For the experiment in Figure 5b, a struggle to find rewards and solve the task. Lastly, we use distribution shift is performed by raising the environment the original minatar (Young & Tian, 2019) suite for fast stochasticity from δ = 0.1 to δ = 0.5, and the probability of experimentation with value-based agents (Mnih et al., 2013). a self-inconsistency-avoiding policy (-σ-IVE) is illustrated. We observe that the self-inconsistency-avoiding policy is 4.1 Detecting Out-Of-Distribution Regimes with robust to the drift of the environment dynamics (H5). Self-Inconsistency Deep RL. Table 1 gives the performance of the Dreamer Based on the proposed role of self-inconsistency as agent and variants that use the model for online plan- a signal for epistemic uncertainty, and how epistemic ning (Ma et al., 2020) as we increase reward sparsity for the uncertainty changes between in- and out-of-distribution walker task, e.g., η = 0 is the original task and η = 0.5 regimes, we expect the following hypotheses to hold. H1: sets rewards below 0.5 to zero. We used the mean of IVE Self-inconsistency is low in in-distribution regions of components in place of the learned policy for acting (µ- the state-action space. H2: Self-inconsistency is high in IVE(5)), and combined the mean with the self-inconsistency out-of-distribution (OOD) regions. H3: Self-inconsistency signal for acting optimistically in the face of uncertainty (µ+ in an OOD test distribution is reduced by bringing the σ-IVE(5)). The self-inconsistency-seeking Dreamer-variant, training distribution closer to it. i.e., µ + σ-IVE(5), is performing well for η = 0.3 and η = 0.5 while the base agent fails, corroborating H4. Similar to Tabular. Figures 3b-3d show the self-inconsistency, mea- the tabular experiment results, the IVE is on par with the the sured as σ-IVE(n) for different values of n, in the tabular explicit value ensemble (EVE, Figure 2a) and outperforms gridworld. As n grows from 1 to 20, the standard devi- the explicit model value ensemble (EMVE, Figure 2b). ation across the IVE is qualitatively similar to the explicit value ensemble’s (EVE) in Figure 3e. We observe that Table 1: Pixel-based continuous control experiments. Re- the self-inconsistency is lower for visited states (H1) than sults for the Dreamer agent and IVE variants on a modified unvisited (OOD) ones (H2). version of the Walker Walk task with varying degrees of reward sparsity controlled by η, where higher η corresponds Deep RL. Figure 4 shows the Muesli agent’s performance to harder exploration. A “♦” indicates methods that use (left) and its self-inconsistency (right)—calculated after online-planning for acting. We report mean and standard training as the σ-IVE(5)—for the different procgen tasks error of episodic returns (rounded to the nearest tenth) over and for varying training #levels, after 100M envi- 3 runs after 1M steps. Higher-is-better and the performance ronment steps. The self-inconsistency for the training is upper bounded by 1000. The best performing method, (in-distribution) levels is always low, regardless of the per-task, is in bold. #levels used for training the agent (H1). We also observe that the self-inconsistency in the test (OOD) levels Methods η = 0.0 η = 0.2 η = 0.3 η = 0.5 is higher than the train ones (H2). Importantly, as the Dreamer 1000±00 720±10 570±60 80±50 number of training levels increases the self-inconsistency µ-IVE(5)♦ 1000±00 860±40 690±70 210±60 on the test levels decreases, which confirms H3. Also as µ + σ-EVE(5)♦ 1000±00 1000±00 980±10 280±50 expected, this reduced self-inconsistency correlates with µ + σ-EMVE(5)♦ 1000±00 910±20 730±40 210±60 improved test performance. µ + σ-IVE(5)♦ 1000±00 1000±00 1000±00 330±70 4.2 Optimism and Pessimism in the Face of 4.3 Planning with Averaged Model-Predicted Values Self-Inconsistency Bayesian model averaging (BMA), i.e., integrating over Epistemic uncertainty has been (i) sought to drive ex- epistemic uncertainty for making predictions, has been used ploration (Sekar et al., 2020) and (ii) avoided for acting to boost performance (Wilson & Izmailov, 2020). The inter- safely (Filos et al., 2020). This section addresses two hy- pretation of the IVE as an ensemble allows to justify prior potheses. H4: Self-inconsistency is an effective signal for methods in the literature that have argued for averaging exploration. H5: Avoiding self-inconsistency leads to ro- MPVs (Oh et al., 2017; Byravan et al., 2020) in order to ro- bustness to distribution shifts. bustify value-based planning, casting them as approximate BMA methods. This section addresses one hypothesis: H6:
Model-Value Inconsistency as a Signal for Epistemic Uncertainty 0.9 0.6 0.3 0 chaser climber coinrun fruitbot jumper nruter desilamron naeM 0.4 0.3 0.2 0.1 0 chaser climber coinrun fruitbot jumper EVI- #levels 10 100 Train 500 10 100 Test 500 Figure 4: Left: Normalised training and test performance for a Muesli agent evaluated on both training and unseen test levels of 5 procgen games after 100M environment frames, for different numbers of unique levels seen during training. Values are normalised by the min and max scores for each game. Right: σ-IVE(5) computed using the model of the Muesli agent while evaluating on both training and unseen test levels, for different numbers of unique levels seen during training. Bars, error-bars show mean and standard error across 3 seeds, respectively. σ-IVE(5) (ours) σ-EMVE(5) Uniform Random σ-EVE(5) Greedy Q 1.0 0.8 0.6 0.4 0.2 0.0 0 50 100 150 Number of steps gnihcaer fo ytilibaborP etats DOO na 0.10 0.08 0.06 0.04 0.02 0.00 0 50 100 150 Number of steps (a) Optimism gnihcaer fo ytilibaborP etats DOO na Table 2: Value-based planning experiments on minatar tasks, testing the impact of planning with the IVE ensembled mean. The original VPN(5) is the same with our µ-IVE(5). Non-ensembled value targets (vˆ1 , vˆ5 ) mˆ mˆ lead to significant deterioration in final performance. We report mean and standard error of episodic returns over 3 runs after 2M steps, higher-is-better. The best performing method, per-task, is in bold. Methods Asterix Breakout Freeway Seaquest S. Inv. DQN 14.7±0.4 12.1±1.2 49.6±0.3 2.3±0.6 47.2±1.3 VPN+vˆ1 15.1±0.6 13.8±0.8 49.1±0.7 4.7±0.9 53.9±1.8 mˆ VPN+vˆ5 7.1±2.3 4.2±2.3 24.3±4.2 1.2±1.4 28.6±8.3 (b) Pessimism mˆ µ-IVE(5) 18.3±0.2 22.0±0.7 49.4±0.5 8.6±0.3 97.3±9.6 Figure 5: Probability of reaching the out-of-distribution state in a tabular gridworld, starting from the bottom right cell (Figure 3a) by (a) seeking or (b) avoiding et al., 2020; Sekar et al., 2020); (iii) tackling distribution self-inconsistency (σ-IVE, see Section 3.2) or explicit value shifts (Lowrey et al., 2018; Kenton et al., 2019; Agarwal or model ensemble (EVE, EMVE) standard deviation. Error et al., 2020) and (iv) representation learning (Fedus et al., bars show standard error over 100 seeds. 2019; Dabney et al., 2020; Lyle et al., 2021). All of the above consider explicit ensemble methods (see Section 2) which can be graphically represented by Figures 2a and 2b Ensemble averaging of the IVE members is in general more or some combination of them. In contrast, IVE is an robust for value prediction than any component individually. implicit ensemble method that does not rely on an ensemble of either value functions or models but uses a single (point) Deep RL. Table 2 shows the final performance of a estimate. IVE could be combined with explicit ensembles, VPN(5) agent that uses µ-IVE(5) value targets and its vˆ1 this would break the correlation between its ensemble mˆ and vˆ5 variants’ on the minatar tasks. The ensembled components since parameters would not be shared, at the mˆ µ-IVE(5) value predictor is consistently better than the expense of growing the model size. single value predictors, supporting H6. Model-based RL. Learned models can be useful to RL 5 Related Work agents in various ways, such as: (i) action selection via plan- ning (Richalet et al., 1978; Hafner et al., 2019b); (ii) rep- Ensemble RL methods. Ensembles of deep neural net- resentation learning (Schmidhuber, 1990; Jaderberg et al., works have been used in value and model-based online RL 2016; Lee et al., 2019; Guez et al., 2020; Hessel et al., 2021); methods for (i) stabilising learning (Faußer & Schwenker, (iii) planning for policy optimisation or value learning (Wer- 2015; Anschel et al., 2017; Kalweit & Boedecker, 2017; bos, 1987; Sutton, 1991; Hafner et al., 2019b; Byravan et al., Kurutach et al., 2018; Chua et al., 2018); (ii) exploration by 2020); or (iv) a combination of all of them (Schrittwieser seeking epistemic uncertainty (Osband et al., 2016; Shyam et al., 2020). In this work, we use the learned model-induced et al., 2019; Pathak et al., 2019; Flennerhag et al., 2020; Ball Bellman operator and value function to construct an ensem-
Model-Value Inconsistency as a Signal for Epistemic Uncertainty ble of value estimators and interpret the disagreement of control tasks, respectively. Self-consistency regularisation their predictions as a proxy of epistemic uncertainty. has been used for learned world models by matching the predictions of a forward dynamics model with a backward Model-value expansion. Alternative methods predict val- dynamics model (Yu et al., 2021). Similar regularisation ues by unrolling the learned model for k-steps and boot- ideas have been used in other areas of machine learning, strapping from the model-free learned value function, see including offline multi-task inverse RL (Filos et al., 2021), Figures 1a and 2c. Feinberg et al. (2018); Buckman et al. computer vision (Bojar & Tamchyna, 2011; (2018); Byravan et al. (2020) follow a two-steps process: (i) Edunov et al., 2018) and generative modelling (Zhu et al., they learn a model by maximum likelihood (Section 2.1) 2017). All prior work directly “forces” self-consistency and then (ii) learn the value function by regressing it to MPV on modelled quantities as a form of regularisation, e.g., predictions/targets. Oh et al. (2017); Silver et al. (2017); applied on imagined data (Farquhar et al., 2021). Instead, Farquhar et al. (2017); Gregor et al. (2019); Schrittwieser we treat self-inconsistency as a proxy for epistemic et al. (2020); Nikishin et al. (2021) train the model and value uncertainty and, e.g., indirectly promote self-consistency function jointly, with a direct regression loss on the MPV. by actively guiding data collection/exploration with Both the IVE and self-inconsistency signal are compatible a self-inconsistency-seeking policy (see Section 4.2). with these learning approaches. Consequently, this avoids degenerate but self-consistent solutions since the learned model and value functions are Adapting k. With varying k, MPV interpolates between trained on real data (i.e., external consistency). the learned model and value function. In particular, for (i) k = 0 the value predictions are based only on the learned Implicit NN ensembles. Ensembles from a single NN have value function and for (ii) k → ∞ only the learned model been proposed and successfully used in supervised learn- contributes to the value predictions. The λ-predictron (Sil- ing but they require modifications to the learning algo- ver et al., 2017) uses a learned and adaptive mechanism for rithm (Huang et al., 2017; Maddox et al., 2019; Antora´n mixing the predictions for different ks. STEVE (Buckman et al., 2020) or architecture (Huang et al., 2016; Dusenberry et al., 2018) is an epistemic-uncertainty-informed mecha- et al., 2020). In contrast, IVE relies on the structure of the nism for weighting the different MPVs. It learns an explicit RL problem and leverages the Bellman consistency (Far- ensemble of models and value functions and weights the quhar et al., 2021) that the “true” model and value function MPV using an inverse variance weighting of the means, satisfy and hence their learned counterparts should also do. calculated across the explicit ensemble. This should not be confused with our σ-IVE(n) signal, which is the variance 6 Discussion across the MPVs and cannot be used for selecting the “best” k-th element but quantifies the model-value disagreement. We have introduced model-value self-inconsistency as a sig- nal for capturing RL agents’ epistemic uncertainty. Our key Novelty signals. Non-explicit ensemble methods have been insight is that a single (point) estimate of a world model and proposed for estimating the model prediction error and use value function can be used to generate multiple estimates of this as a proxy signal for novelty. Most of these meth- the state value, which can be combined to form an implicit ods make novelty predictions for a state s t, after observ- value ensemble (IVE). We showed empirically that self- ing a transition s t −a →t s t+1 (Stadie et al., 2015; Pathak inconsistency of the IVE—i.e., the disagreement amongst et al., 2017; Raileanu & Rockta¨schel, 2020) and therefore its members— is an effective signal for epistemic uncer- are termed retrospective novelty predictors in the litera- tainty in tabular and pixel-based deep RL settings. We then ture (Sekar et al., 2020). Lopes et al. (2012) assume that demonstrated that self-inconsistency can be used to guide the agent’s learning progress is a predictable process and exploration, increase an agent’s ability to handle distribution fit a model to it. While (s t, a t, s t+1) triplets are necessary shifts, and robustify value-based planning methods. for training the novelty predictor, after training, the signal can be calculated before observing s t+1 and hence can be Future work. We want to explore ways to: (i) Modify the used for planning purposes, which we term a plannable model, value-learning algorithms, or network architecture novelty predictor. The σ-IVE signal can be interpreted as to increase diversity in the IVE while keeping the model a prediction error estimate that quantifies how the learned size unchanged, such as using different sub-samples of the value function and model disagree in their predictions and data to train each IVE member or injecting k-dependent hence we can use it as a plannable novelty signal. structured noise (Osband et al., 2018). (ii) Integrate the self- inconsistency signal into more complex online planning Self-consistency regularisation. Silver et al. (2017) methods (e.g., MCTS, Coulom, 2006) since they already and Farquhar et al. (2021) regularised their learned value compute some “modification” of the IVE components. and model pairs to be self-consistent for prediction and
Model-Value Inconsistency as a Signal for Epistemic Uncertainty Acknowledgements. We thank Mark Rowland, Lo¨ıc Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, Matthey, Hado van Hasselt, Theophane Weber, Ioannis D. Weight uncertainty in neural network. In Interna- Antonoglou, Amin Barekatain, Junhyuk Oh, Abhinav Gupta, tional Conference on Machine Learning, pp. 1613–1622. Panagiotis Tigas, Yarin Gal, David Silver and Satinder Singh PMLR, 2015. for helpful discussions and feedback. Bojar, O. and Tamchyna, A. Improving translation model by monolingual data. In Proceedings of the Sixth Workshop References on Statistical Machine Translation, pp. 330–336, 2011. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Tensorflow: A system for large-scale machine learning. In Wanderman-Milne, S., and Zhang, Q. JAX: composable 12th {USENIX} symposium on operating systems design transformations of Python+NumPy programs, 2018. and implementation ({OSDI} 16), pp. 265–283, 2016. Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Agarwal, R., Schuurmans, D., and Norouzi, M. An opti- Lee, H. Sample-efficient supervised learning with mistic perspective on offline supervised learning. In stochastic ensemble value expansion. arXiv preprint International Conference on Machine Learning, pp. 104– arXiv:1807.01675, 2018. 114. PMLR, 2020. Byravan, A., Springenberg, J. T., Abdolmaleki, A., Hafner, Anschel, O., Baram, N., and Shimkin, N. Averaged-dqn: R., Neunert, M., Lampe, T., Siegel, N., Heess, N., and Variance reduction and stabilization for deep reinforce- Riedmiller, M. Imagined value gradients: Model-based ment learning. In International conference on machine policy optimization with tranferable latent dynamics mod- learning, pp. 176–185. PMLR, 2017. els. In Conference on Robot Learning, pp. 566–589. PMLR, 2020. Antora´n, J., Allingham, J. U., and Herna´ndez-Lobato, J. M. Chua, K., Calandra, R., McAllister, R., and Levine, S. Depth uncertainty in neural networks. arXiv preprint Deep supervised learning in a handful of trials us- arXiv:2006.08437, 2020. ing probabilistic dynamics models. arXiv preprint Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, arXiv:1805.12114, 2018. J., Buchlovsky, P., Budden, D., Cai, T., Clark, A., Dani- Clevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast helka, I., Fantacci, C., Godwin, J., Jones, C., Hennigan, and accurate deep network learning by exponential linear T., Hessel, M., Kapturowski, S., Keck, T., Kemaev, I., units (elus). arXiv preprint arXiv:1511.07289, 2015. King, M., Martens, L., Mikulik, V., Norman, T., Quan, J., Papamakarios, G., Ring, R., Ruiz, F., Sanchez, A., Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. Leverag- Schneider, R., Sezener, E., Spencer, S., Srinivasan, S., ing Procedural Generation to Benchmark Reinforcement Stokowiec, W., and Viola, F. The DeepMind JAX Ecosys- Learning. CoRR, abs/1912.01588, 2019. tem, 2020. Coulom, R. Efficient selectivity and backup operators in Ball, P., Parker-Holder, J., Pacchiano, A., Choromanski, Monte-Carlo tree search. In International conference on K., and Roberts, S. Ready policy one: World building computers and games, pp. 72–83. Springer, 2006. through active learning. In International Conference on Dabney, W., Barreto, A., Rowland, M., Dadashi, R., Machine Learning, pp. 591–601. PMLR, 2020. Quan, J., Bellemare, M. G., and Silver, D. The value- improvement path: Towards better representations for re- Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., inforcement learning. arXiv preprint arXiv:2006.02243, Saxton, D., and Munos, R. Unifying count-based ex- ploration and intrinsic motivation. Advances in neural 2020. information processing systems, 29:1471–1479, 2016. Dearden, R., Friedman, N., and Russell, S. Bayesian Q- learning. In Aaai/iaai, pp. 761–768, 1998. Bellemare, M. G., Dabney, W., and Munos, R. A distribu- tional perspective on supervised learning. In Interna- Dearden, R., Friedman, N., and Andre, D. Model-based tional Conference on Machine Learning, pp. 449–458. Bayesian exploration. arXiv preprint arXiv:1301.6690, PMLR, 2017. 1999. Bellman, R. A Markovian decision process. Journal of Dusenberry, M., Jerfel, G., Wen, Y., Ma, Y., Snoek, J., mathematics and mechanics, 6(5):679–684, 1957. Heller, K., Lakshminarayanan, B., and Tran, D. Efficient
Model-Value Inconsistency as a Signal for Epistemic Uncertainty and scalable bayesian neural nets with rank-1 factors. In Grimm, C., Barreto, A., Farquhar, G., Silver, D., and International conference on machine learning, pp. 2782– Singh, S. Proper Value Equivalence. arXiv preprint 2792. PMLR, 2020. arXiv:2106.10316, 2021. Edunov, S., Ott, M., Auli, M., and Grangier, D. Un- Guez, A., Viola, F., Weber, T., Buesing, L., Kapturowski, derstanding back-translation at scale. arXiv preprint S., Precup, D., Silver, D., and Heess, N. Value-driven arXiv:1808.09381, 2018. hindsight modelling. arXiv preprint arXiv:2002.08329, 2020. Farquhar, G., Rockta¨schel, T., Igl, M., and Whiteson, S. Treeqn and atreec: Differentiable tree-structured mod- Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to els for deep supervised learning. arXiv preprint control: Learning behaviors by latent imagination. arXiv arXiv:1710.11417, 2017. preprint arXiv:1912.01603, 2019a. Farquhar, G., Baumli, K., Marinho, Z., Filos, A., Hessel, Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., M., van Hasselt, H., and Silver, D. Self-consistent models Lee, H., and Davidson, J. Learning latent dynamics for and values. arXiv preprint arXiv:2110.12840, 2021. planning from pixels. In International Conference on Machine Learning, pp. 2555–2565. PMLR, 2019b. Faußer, S. and Schwenker, F. Neural network ensembles in supervised learning. Neural Processing Letters, 41(1): Hessel, M., Danihelka, I., Viola, F., Guez, A., Schmitt, 55–69, 2015. S., Sifre, L., Weber, T., Silver, D., and van Hasselt, H. Muesli: Combining improvements in policy optimization. Fedus, W., Gelada, C., Bengio, Y., Bellemare, M. G., and arXiv preprint arXiv:2104.06159, 2021. Larochelle, H. Hyperbolic discounting and learning over multiple horizons. arXiv preprint arXiv:1902.06865, Hochreiter, S. and Schmidhuber, J. Long short-term mem- 2019. ory. Neural computation, 9(8):1735–1780, 1997. Feinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, J. E., and Levine, S. Model-based value expansion for K. Q. Deep networks with stochastic depth. In European efficient model-free supervised learning. In Proceed- conference on computer vision, pp. 646–661. Springer, ings of the 35th International Conference on Machine 2016. Learning (ICML 2018), 2018. Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., and Filos, A., Tigkas, P., McAllister, R., Rhinehart, N., Levine, Weinberger, K. Q. Snapshot ensembles: Train 1, get m S., and Gal, Y. Can autonomous vehicles identify, re- for free. arXiv preprint arXiv:1704.00109, 2017. cover from, and adapt to distribution shifts? In Interna- tional Conference on Machine Learning, pp. 3145–3153. Hunter, J. D. Matplotlib: A 2D graphics environment. IEEE PMLR, 2020. Annals of the History of Computing, 9(03):90–95, 2007. Filos, A., Lyle, C., Gal, Y., Levine, S., Jaques, N., and Hutter, M. Universal artificial intelligence: Sequential Farquhar, G. PsiPhi-Learning: Reinforcement Learn- decisions based on algorithmic probability. Springer ing with Demonstrations using Successor Features and Science & Business Media, 2004. Inverse Temporal Difference Learning. arXiv preprint arXiv:2102.12560, 2021. Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., and Kavukcuoglu, K. Reinforce- Flennerhag, S., Wang, J. X., Sprechmann, P., Visin, F., ment learning with unsupervised auxiliary tasks. arXiv Galashov, A., Kapturowski, S., Borsa, D. L., Heess, N., preprint arXiv:1611.05397, 2016. Barreto, A., and Pascanu, R. Temporal Difference Un- certainties as a Signal for Exploration. arXiv preprint Kalweit, G. and Boedecker, J. Uncertainty-driven imagi- arXiv:2010.02255, 2020. nation for continuous deep supervised learning. In Conference on Robot Learning, pp. 195–206. PMLR, Garcia, C. E., Prett, D. M., and Morari, M. Model predictive 2017. control: Theory and practice—A survey. Automatica, 25 (3):335–348, 1989. Kenton, Z., Filos, A., Evans, O., and Gal, Y. Generalizing from a few environments in safety-critical reinforcement Gregor, K., Rezende, D. J., Besse, F., Wu, Y., Merzic, learning. arXiv preprint arXiv:1907.01475, 2019. H., and Oord, A. v. d. Shaping belief states with generative environment models for rl. arXiv preprint Kingma, D. P. and Ba, J. Adam: A method for stochastic arXiv:1906.09237, 2019. optimization. arXiv preprint arXiv:1412.6980, 2014.
Model-Value Inconsistency as a Signal for Epistemic Uncertainty Kumar, P. R. and Varaiya, P. Stochastic systems: Estimation, Nikishin, E., Abachi, R., Agarwal, R., and Bacon, identification, and adaptive control. SIAM, 2015. P.-L. Control-Oriented Model-Based supervised learning with Implicit Differentiation. arXiv preprint Kurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. arXiv:2106.03273, 2021. Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018. Oh, J., Singh, S., and Lee, H. Value prediction network. arXiv preprint arXiv:1707.03497, 2017. Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep ensembles. arXiv preprint arXiv:1612.01474, 2016. exploration via bootstrapped DQN. Advances in neural information processing systems, 29:4026–4034, 2016. Lee, A. X., Nagabandi, A., Abbeel, P., and Levine, S. Stochastic latent actor-critic: Deep reinforcement Osband, I., Aslanides, J., and Cassirer, A. Randomized prior learning with a latent variable model. arXiv preprint functions for deep supervised learning. arXiv preprint arXiv:1907.00953, 2019. arXiv:1806.03335, 2018. Lopes, M., Lang, T., Toussaint, M., and Oudeyer, P.-Y. Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Exploration in model-based supervised learning by Curiosity-driven exploration by self-supervised predic- empirically estimating learning progress. In Neural Infor- tion. In International conference on machine learning, mation Processing Systems (NIPS), 2012. pp. 2778–2787. PMLR, 2017. Loshchilov, I. and Hutter, F. Decoupled weight decay regu- Pathak, D., Gandhi, D., and Gupta, A. Self-supervised larization. arXiv preprint arXiv:1711.05101, 2017. exploration via disagreement. In International conference on machine learning, pp. 5062–5071. PMLR, 2019. Lowrey, K., Rajeswaran, A., Kakade, S., Todorov, E., and Mordatch, I. Plan online, learn offline: Efficient learning Pearce, T., Leibfried, F., and Brintrup, A. Uncertainty in and exploration via model-based control. arXiv preprint neural networks: Approximately bayesian ensembling. arXiv:1811.01848, 2018. In International conference on artificial intelligence and statistics, pp. 234–244. PMLR, 2020. Lu, X., Van Roy, B., Dwaracherla, V., Ibrahimi, M., Osband, I., and Wen, Z. supervised learning, Bit by Bit. arXiv Puterman, M. L. Markov decision processes: discrete preprint arXiv:2103.04047, 2021. stochastic dynamic programming. John Wiley & Sons, 2014. Lyle, C., Rowland, M., Ostrovski, G., and Dabney, W. On The Effect of Auxiliary Tasks on Representation Dynam- Raileanu, R. and Rockta¨schel, T. RIDE: Rewarding impact- ics. In International Conference on Artificial Intelligence driven exploration for procedurally-generated environ- and Statistics, pp. 1–9. PMLR, 2021. ments. arXiv preprint arXiv:2002.12292, 2020. Ma, X., Chen, S., Hsu, D., and Lee, W. S. Contrastive varia- Richalet, J., Rault, A., Testud, J., and Papon, J. Model tional model-based supervised learning for complex predictive heuristic control. Automatica (journal of IFAC), observations. arXiv e-prints, pp. arXiv–2008, 2020. 14(5):413–428, 1978. Maddox, W. J., Izmailov, P., Garipov, T., Vetrov, D. P., and Savage, L. J. The foundations of statistics. Courier Corpo- Wilson, A. G. A simple baseline for bayesian uncer- ration, 1972. tainty in deep learning. Advances in Neural Information Schmidhuber, J. An on-line algorithm for dynamic rein- Processing Systems, 32:13153–13164, 2019. forcement learning and planning in reactive environments. Milnor, J. Games against nature. Technical report, RAND In 1990 IJCNN international joint conference on neural Project Air Force Santa Monica CA, 1951. networks, pp. 253–258. IEEE, 1990. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Schmidhuber, J. Formal theory of creativity, fun, and in- Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing trinsic motivation (1990–2010). IEEE Transactions on atari with deep supervised learning. arXiv preprint Autonomous Mental Development, 2(3):230–247, 2010. arXiv:1312.5602, 2013. Schmitt, S., Hessel, M., and Simonyan, K. Off-policy actor- Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, critic with shared experience replay. In International Con- M. G. Safe and efficient off-policy supervised learning. ference on Machine Learning, pp. 8545–8554. PMLR, arXiv preprint arXiv:1606.02647, 2016. 2020.
Model-Value Inconsistency as a Signal for Epistemic Uncertainty Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Watter, M., Springenberg, J. T., Boedecker, J., and Ried- Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, miller, M. Embed to control: A locally linear latent D., Graepel, T., et al. Mastering atari, go, chess and shogi dynamics model for control from raw images. arXiv by planning with a learned model. Nature, 588(7839): preprint arXiv:1506.07365, 2015. 604–609, 2020. Wenzel, F., Snoek, J., Tran, D., and Jenatton, R. Hyperpa- Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., rameter ensembles for robustness and uncertainty quan- and Pathak, D. Planning to explore via self-supervised tification. arXiv preprint arXiv:2006.13570, 2020. world models. In International Conference on Machine Werbos, P. J. Learning how the world works: Specifications Learning, pp. 8583–8592. PMLR, 2020. for predictive networks in robots and brains. In Proceed- Shyam, P., Jas´kowski, W., and Gomez, F. Model-based ac- ings of IEEE International Conference on Systems, Man tive exploration. In International conference on machine and Cybernetics, NY, 1987. learning, pp. 5779–5788. PMLR, 2019. Wichard, J., Merkwirth, C., and Ogorzalek, M. Building Silver, D., Hasselt, H., Hessel, M., Schaul, T., Guez, A., ensembles with heterogeneous models, 2003. Harley, T., Dulac-Arnold, G., Reichert, D., Rabinowitz, Wilson, A. G. and Izmailov, P. Bayesian deep learning and a N., Barreto, A., et al. The predictron: End-to-end learning and planning. In International Conference on Machine probabilistic perspective of generalization. arXiv preprint Learning, pp. 3191–3199. PMLR, 2017. arXiv:2002.08791, 2020. Stadie, B. C., Levine, S., and Abbeel, P. Incentivizing ex- Young, K. and Tian, T. Minatar: An atari-inspired testbed ploration in supervised learning with deep predictive for thorough and reproducible supervised learning models. arXiv preprint arXiv:1507.00814, 2015. experiments. arXiv preprint arXiv:1903.03176, 2019. Strens, M. A Bayesian framework for reinforcement learn- Yu, T., Lan, C., Zeng, W., Feng, M., Zhang, Z., and Chen, ing. In ICML, volume 2000, pp. 943–950, 2000. Z. Playvirtual: Augmenting cycle-consistent virtual tra- jectories for supervised learning. Advances in Neural Sutton, R. S. Learning to predict by the methods of temporal Information Processing Systems, 34, 2021. differences. Machine learning, 3(1):9–44, 1988. Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired Sutton, R. S. Dyna, an integrated architecture for learning, image-to-image translation using cycle-consistent adver- planning, and reacting. ACM Sigart Bulletin, 2(4):160– sarial networks. In Proceedings of the IEEE international 163, 1991. conference on computer vision, pp. 2223–2232, 2017. Sutton, R. S. and Barto, A. G. supervised learning: An introduction. MIT press, 2018. Tibshirani, R. A comparison of some error estimates for neural network models. Neural Computation, 8(1):152– 163, 1996. Tunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez, S., Merel, J., Erez, T., Lillicrap, T., Heess, N., and Tassa, Y. dm control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020. Van Rossum, G. and Drake Jr, F. L. Python reference man- ual. Centrum voor Wiskunde en Informatica Amsterdam, 1995. Van Seijen, H., Van Hasselt, H., Whiteson, S., and Wiering, M. A theoretical and empirical analysis of Expected Sarsa. In 2009 ieee symposium on adaptive dynamic programming and supervised learning, pp. 177–184. IEEE, 2009. Watkins, C. J. and Dayan, P. Q-learning. Machine learning, 8(3-4):279–292, 1992.
Model-Value Inconsistency as a Signal for Epistemic Uncertainty A Experimental Details In this section, we describe the environments used in our experiments (see Section 4) and the experiment design. A.1 Environments In this section, we provide details on the specification of each task used in our experiments. A.1.1 TABULAR ENVIRONMENT We use an empty 5 × 5 gridworld (gridworld) environment for our tabular experiments. The task is specified by: visited unvisited 1. State space, S: A finite discrete state space, i.e., s ∈ {0, 1, . . . , 24}. 2. Action space, A: A finite discrete action space for moving the agent in the four cardinal directions (N, W, S, E), i.e., s ∈ {0, 1, 2, 3}. 3. Reward function, r(s, a): The zero function, i.e., r(s, a) = 0, ∀(s, a) ∈ S × A. 4. Transition dynamics, p(s(cid:48)|s, a): We consider the episodic setting, i.e., episode length = 20, and the dynamics are (optionally) stochastic. In particular, we use a single parameter that controls the stochasticity, called wind prob ∈ [0, 1] and implement stochastic dynamics as actuator noise, i.e., there is a wind prob probability that the agent action is ignores and an other action is applied to the environment by Figure 6: gridworld sampling randomly from the action space. A.1.2 PROCGEN (COBBE ET AL., 2019) We used 5 tasks from the Procgen (procgen, Cobbe et al., 2019) suite, shown at Figure 7. We used the default settings for the environments and we only varied the number of training levels used for learning, which we term #levels. The tasks are generally partially-observed (POMDPs) specified by: 1. Observation space, O: The original 64 × 64 RGB pixel-observations, i.e., o ∈ [0, 1]64×64×3. t 2. Action space, A: The original 15 discrete actions, i.e., a ∈ {0, . . . , 14}. t (a) chaser (b) climber (c) coinrun (d) fruitbot (e) jumper Figure 7: procgen tasks.
Model-Value Inconsistency as a Signal for Epistemic Uncertainty A.1.3 MINATAR (YOUNG & TIAN, 2019) We used all 5 tasks from the MinAtar (minatar, Young & Tian, 2019) suite, shown in Figure 8, with the default settings. The tasks are fully-observed and specified by: 1. State space, S: The original 10 × 10 × n channels symbolic observations, i.e., s ∈ [0, 1]10×10×n channels , where t n channels varies between tasks, from 4 to 10. 2. Action space, A: The original 6 discrete (non-minimal) actions, i.e., a ∈ {0, . . . , 5}. t 3. Transition dynamics, p(s(cid:48)|s, a): The default 0.1 probability for sticky actions is used. (a) asterix (b) breakout (c) freeway (d) seaquest (e) space invaders Figure 8: minatar environments. A.1.4 DEEPMIND CONTINUOUS CONTROL (TUNYASUVUNAKOOL ET AL., 2020) We use the walker walk task from the DeepMind Continuous Control (Tunyasuvunakool et al., 2020) suite and modified its reward function. Pixel-observations are used, and the problem is generally partially-observed. The task is specified by: 1. Observation space, S: A 64 × 64 RGB pixel-observation, where the robot body is in the centre of the frame, i.e., o ∈ [0, 1]64×64×3. t 2. Action space, A: A six-dimensional continuous action, i.e., a ∈ [−1, +1]6. t 3. Reward function, r(s, a): Originally, the reward is bounded in [0, 1], i.e., r ∈ [0, 1], t which is computed based on the robot’s torso height and forward velocity. We modify the original per-step reward, by setting to zero any reward below a parameter η, i.e., r˜ = H(r − η)r , where H is the Heaviside step function. For η = 0, we recover the t t t Figure 9: walker original reward, and for η > 0 we obtain an increasingly more difficult, in terms of exploration, walker task. A.2 Experiments In this section, we provide details on the experimental protocol we follow for each experiment. A.2.1 FIGURES 1 AND 10 We focus on the prediction problem (Sutton & Barto, 2018), modelled as a Markov reward process (MRP) with an one- dimensional state space, i.e., s ∈ S = [−3, +3] and a discount factor γ = 0.9. We are provided with state-value target pairs, i.e., {(s , v¯ )}N with N = 10 and learn (i) a representation function hˆ(s; ω), (ii) a value function vˆ(z; φ) and (iii) a model i i i=1 mˆ (·, ·|z; θ) (cid:44) (rˆ(z; θ), pˆ(z; θ)), represented as neural networks with parameters, ω, φ and θ, respectively. In particular: hˆ (s) = hˆ(s; ω) = tanh(MLP (s)) (cid:44) z ∈ [−1, +1]32 (8) ω ω vˆ (z) = vˆ(z; φ) = MLP (z) (cid:44) v ∈ R (9) φ φ pˆ (z) = pˆ(z; θ) = LSTM (z, 0) (cid:44) z1 ∈ [−1, +1]32 (10) θ θ rˆ (z) = rˆ(z; θ) = MLP (z) (cid:44) r1 ∈ R, (11) θ θ where all the multi-layer percepetrons (MLPs) have one hidden layer of 32 units with an ELU (Clevert et al., 2015) non-linearity and zk is the (latent) state after taking k steps with the model mˆ , starting from state z0 (cid:44) z (Silver et al., 2017).
Model-Value Inconsistency as a Signal for Epistemic Uncertainty μ ± 3σ-IVE(10) {vk m̂ ̂ }1 k0 = 0 targets v(s) v(s) vv((ss)) 100 5 5 0 0 0 s s ss −2 0 2 −2 0 2 −2 0 2 (a) At initialisation (i.e., prior) (b) Value targets (i.e., data) (c) After training (i.e., posterior) Figure 10: Expanded version of Figure 1. A value prediction problem of an implicit policy, modelled as a Markov reward process (MRP, Sutton & Barto, 2018) with an one-dimensional state space, i.e., s ∈ S = [−3, 3]. We learn a model mˆ and a value function vˆ and construct a k-steps model predicted value (k-MPV, Eqn. (5)) by applying the model induced Bellman operator T repeatedly k times on the learned value function vˆ, i.e., vˆk (s) (cid:44) (T )kvˆ(s). We visualise the k-MPVs, a.k.a mˆ mˆ mˆ components of the implicit value ensemble (IVE, Eqn. (6)) for k ∈ {0, . . . , 10} (in blue) along with the ensemble mean and standard deviation (in orange), constructed from a single (point) estimate of the value function and model. (a) The predictions at initialisation, i.e., before training. (b) The data, i.e., state and value target pairs. (c) The predictions after training every IVE member towards the value targets in (b), i.e., min (cid:80) (cid:80) (cid:107)vˆk (s ) − v (cid:107)2. We observe in (c) that the ensemble compo- m,v i k mˆ i i 2 nents fit the value targets and their standard deviation is zero at and around the observed (in-distribution) data but it is non-zero otherwise (out-of-distribution points). Therefore the IVE members’ disagreement can be used as a signal for epistemic uncer- tainty. In this example, the variability between the IVE members’ predictions is only due to their different functional forms. We make value prediction by repeatedly applying the mˆ model-induced Bellman operator T on the value function vˆ, i.e., mˆ constructing different k-steps model predicted values (k-MPVs, Eqn. (5)). In particular, the predictions are given by: vˆ0 (s) = (T )0vˆ (hˆ (s)) = vˆ (hˆ (s)) = vˆ ◦ hˆ (s) (12) mˆ mˆ θ φ ω φ ω φ ω vˆ1 (s) = (T )1vˆ (hˆ (s)) = (rˆ + γvˆ ) ◦ pˆ ◦ hˆ (s) (13) mˆ mˆ θ φ ω θ φ θ ω vˆ2 (s) = (T )2vˆ (hˆ (s)) = (rˆ + γ(rˆ + γvˆ ) ◦ pˆ ) ◦ pˆ ◦ hˆ (s) (14) mˆ mˆ θ φ ω θ θ φ θ θ ω . . .   k−1 vˆ mk ˆ (s) = (T mˆ θ )kvˆ φ(hˆ ω(s)) = (cid:88) γj−1rˆ θ ◦ (pˆ θ ◦ · · · ◦ pˆ θ) + γkvˆ φ ◦ (pˆ θ ◦ · · · ◦ pˆ θ) ◦ hˆ ω(s) (15) j=1 j-times k-times where ◦ denotes function composition. Obviously, the k-MPVs with different k have different functional forms, as the predictions at initialisation suggest at Figures 1b and 10a, too. Note that we do not use the bold notation introduced in Eqn. (7) to highlight that there is no Monte Carlo sampling—the learned model is deterministic and the policy is implicit. We learn the neural network parameters ω, φ and θ using the ADAM (Kingma & Ba, 2014) optimiser with decoupled weight decay (Loshchilov & Hutter, 2017) to minimise the empirical squared value prediction error for all k ∈ {0, . . . , 10}, i.e., N K (cid:88) (cid:88) min (cid:107)vˆk (s ) − v (cid:107)2. (16) mˆ i i 2 ω,θ,φ i=1 k=0 The only source of variability between the k-MPVs (implicit value ensemble (IVE) members) is their functional form, induced by different compositions of the learned parametric networks vˆ , pˆ and rˆ as Eqns. (12-15) show. φ θ θ A.2.2 FIGURE 3 Data. We collect experience/data B by running a uniformly random policy π for 500 steps (i.e., 25 episodes). We uniform exclude transitions from and to the top left cell, which we call the out-of-distribution (OOD) or unvisited state. Value learning. We learn a tabular action-value function qˆ ≈ qπuniform using expected SARSA (Van Seijen et al., 2009) and then we induce a (state-)value function, i.e., vˆ(s) (cid:44) E [qˆ(s, a)], ∀(s, a) ∈ S × A. a∼πuniform
Model-Value Inconsistency as a Signal for Epistemic Uncertainty Model learning. Maximum-likelihood estimation (MLE, see Section 2) with data B is used for learning the tabular model of the environment mˆ ≈ m∗. Visualisations. The mean and standard deviations are normalised in [0, 1], i.e., for given quantity x for state s and x s min (x the minimum (maximum) quantity across all states, we plot x¯ = (x − x )/(x − x ). We report the results max s s min max min for a single repetition of the experiment since it is a qualitative observation. IVE(n). We calculate the MPVs exactly, according to Eqn. (5). We vary the parameter n, i.e., maximum number of applications of the model-induced Bellman operator T on the learned value function vˆ. mˆ EVE(n) and EMVE(n). The explicit value ensemble (EVE, Figure 2a) and explicit model value ensemble (EMVE, Figure 2b) are also trained on the same data using the same value and model learning algorithms. The ensemble components different only in their (random) initialisation and seed used in stochastic gradient descent. A.2.3 TABLE 1 We use the walker task and train Dreamer (Hafner et al., 2019a) for 1M steps. An action repeat of 2 is used thus 0.5M agent-environment interaction steps are made per run. We repeat each experiment 3 times, varying the random seed in each one. We report the episodic returns (rounded to the nearest tenth) at the end of training by setting the agents in “evaluation” mode and average their performance across 10 episodes. A.2.4 FIGURE 4 We train Muesli (without any modification to its acting strategy or learning algorithm) for 100M environment frames. Figure 4 (left) reports the final performance of the agent evaluated on an additional 10M frames on the train and test levels. Mean episode returns are normalised as: R˜ = (R − R )/(R − R ), using min and max scores for each game (Cobbe min max min et al., 2019). The model-value self-inconsistency, reported in Figure 4 (right), is computed by unrolling the model for 5 steps using actions sampled from the policy and taking the standard deviation over the IVE: k−1 k-MVP(s) = vˆk (s) ( =7) (cid:88) γi−1ri+1 + γkvˆ(sk ) (17) mˆ mˆ mˆ i=1 σ-IVE(s) = std [k-MVP(s)], for k = 1, . . . , 5 (18) k where the “bold” notation refers to reward and value predictions given a single action sequence sampled from the policy π, as described in Eqn. (7). A.2.5 FIGURE 5 For training the values and model and calculating IVE and EVE, we follow the same protocol as in Figure 3. In this experiment, we use the learned action-value functions instead of the state-values, see Section C.2 for a formal discussion. We denote with σ-IVE(5) and σ-EVE(5) the standard deviation across the 5 ensemble members of the implicit and explicit ensembles of the action-values, respectively. Also, σ-IVE(5) ∈ RS×A and σ-IVE(5)[s, a] is the standard deviation of the implicit value ensemble at the state s for action a. We use the standard deviation across the ensemble of action-values for inducing policies that are novelty- seeking or avoiding: • In Figure 5a, the action that maximises the standard deviation across the value ensemble is selected, per-state, i.e., π (s) = arg max σ-XVE(5)[s, a], where XVE ∈ {IVE, EVE}. These are the novelty-seeking policies that seeking a∈A their probability of reaching the novel state is higher than a uniformly random policy. • In Figure 5b, the action that minimises the standard deviation across the value ensemble is selected, per-state, i.e., π (s) = arg min σ-XVE(5)[s, a], where XVE ∈ {IVE, EVE}. These are the novelty-avoiding policies that avoiding a∈A their probability of reaching the novel state is lower than a uniformly random policy. We calculate the probabilities by constructing a Markov chain, induced by the coupling of the policy under consid- eration π and the “true” environment model, m∗. The Markov chain’s transition kernel is given by pπ (s(cid:48)|s) (cid:44) m∗ (cid:80) p (s(cid:48)|s, a)π(a|s). We can write the transition kernel as a matrix P π ∈ RS×S , such that P π [i, j] = pπ (j|i). a∈A m∗ m∗ m∗ m∗
Model-Value Inconsistency as a Signal for Epistemic Uncertainty The (i, j) entry of the transition matrix, i.e., P π [i, j] is the probability of reaching the state j after one-step when starting m∗ from state i and following policy π in the environment with model m∗. The (i, j) entry of the l-th power of the transition matrix, i.e., (P π )l[i, j] is the probability of reaching the state j after l-steps when starting from state i and following policy m∗ π in the environment with model m∗. In Figure 5, we start from the bottom right cell, i.e., i = bottom right and plot the probability of reaching the top left cell, i.e., j = top right after l-steps, and we vary l from 1 to 150. We repeat each experiment 100 times, varying the random seed in each one. A.3 Table 2 We use the minatar tasks and train VPN (Oh et al., 2017) and some variants of it for 2M steps. The only modification to the original VPN(5) is the way value estimates are constructed: • vˆ1 is VPN variant that uses the 1-MPV for value estimation. mˆ • vˆ5 is VPN variant that uses the 5-MPV for value estimation. mˆ • µ-IVE(5) is the original VPN(5) agent that uses the mean over the implicit value ensemble with n = 5 for value estimation. The estimated values are used for value-based planning, as discussed in (Oh et al., 2017, Eqn. (1) & Appendix D.). B Implementation Details For our experiments we used Python (Van Rossum & Drake Jr, 1995). We used JAX (Bradbury et al., 2018; Babuschkin et al., 2020) as the core computational library for implementing Muesli (Hessel et al., 2021) and VPN (Oh et al., 2017). We used the official TensorFlow (Abadi et al., 2016) implementation of Dreamer (Hafner et al., 2019a). We also used Matplotlib (Hunter, 2007) for the visualisations. B.1 Tabular Methods We initialise the rewards, transition logits and action-values by sampling from a normal distribution with mean 0 and standard deviation 1. The ADAM (Kingma & Ba, 2014) optimiser with learning rate 5e-5 is used, and all losses converge after 10, 000 epochs of stochastic gradient descent with batch size 128. B.2 Dreamer (Hafner et al., 2019a) We use the Dreamer agent’s default hyperparameters, as introduced by (Hafner et al., 2019a). For the self-inconsistency- seeking variant, i.e., µ + σ-IVE(5), we used a scalar weighting factor β∗ = 0.1 to balance the mean and standard deviation IVE across the ensemble members, tuned with grid search in {0.05, 0.1, 0.2, 1.0, 10.0}. The same tuning procedure is used for the baselines. The reported scores are for β∗ = 0.2 and β∗ = 0.1. EVE EMVE B.3 Muesli (Hessel et al., 2021) We use the Muesli agent’s hyperparameters. In particular we use the ones from the large-scale Atari experiments by Hessel et al. (2021). Nonetheless, we set the fraction of replay data in each batch to 0.8 (instead of the original 0.95) to shorten train- ing time. To encourage diversity in value and reward predictions for unvisited states we have augmented the value and reward prediction heads of the model with untrainable randomized prior networks (Osband et al., 2018), using a prior scale of 5.0. Note that unlike in Osband et al. (2018), we did not introduce additional heads per prediction or modify the training procedure. B.4 VPN (Oh et al., 2017) We use the MinAtar DQN-torso (Young & Tian, 2019) and an LSTM (Hochreiter & Schmidhuber, 1997) with 128 hidden units and otherwise follow the original VPN(5) hyperparameters, as introduced by Oh et al. (2017).
Model-Value Inconsistency as a Signal for Epistemic Uncertainty C Extensions C.1 IVE with the Bellman Optimality Operator In Section 3, we defined the k-steps model-predicted value (k-MPV) in terms of the model-induced Bellman evaluation operator and a value function for a policy π, and constructed the implicit value ensemble (IVE) accordingly. In this section. we provide a brief presentation of MPVs and IVE in terms of the model-induced Bellman optimality operator and optimal value functions. Definition 2 (Bellman optimality operator). Given the model m∗, the one-step Bellman optimality operator T ∗ : V → V is induced, and its application on a state-function v ∈ V, for all s ∈ S, is given by T ∗v(s) (cid:44) max E [R + γv(S ) | S = s.A = a] . (19) m∗ 0 1 0 1 a∈A The k-times repeated application of an one-step Bellman optimality operator gives rise to the k-steps Bellman optimality operator, (T ∗)kv (cid:44) T ∗ . . . T ∗ v. (20) k-times The Bellman optimality operator, T ∗, is a contraction mapping (Puterman, 2014), and its fixed point is the value of the optimal policy π∗, i.e., lim (T ∗)nv = vπ∗ (cid:44) v∗, for any state-function v ∈ V (cid:44) {f : S → R}. n→∞ Model-induced Bellman optimality operator. A model mˆ induces a Bellman optimality operator T ∗ with a fixed point mˆ v∗ , i.e., the value of the optimal policy under the model (a.k.a. the solution of the model. Similar to Eqn. (20), a k-steps mˆ model-induced Bellman optimality operator is given by (T ∗)kv = T ∗ . . . T ∗ v. mˆ mˆ mˆ k-times Model-predicted values. The k-steps MPV, using the model-induced Bellman optimality operator is given by vˆk (cid:44) (T ∗)kvˆ (21) mˆ mˆ Implicit value ensemble. An ensemble of k-MPV predictions can be made by varying k, giving rise to {vˆi }n (cid:44) {vˆ, T ∗vˆ, . . . , (T ∗)nvˆ} . (22) mˆ i=0 mˆ mˆ n+1 value estimates The IVE with the Bellman optimality operator can be used for values learned with, e.g., Q-learning (Watkins & Dayan, 1992), or with other value-based agents, e.g., VPN (Oh et al., 2017). We use this idea in Appendix D. C.2 MPV with Action-Value Functions In order to be able to modulate action selection using the self-inconsistency signal, we have computed the k-MPV conditioned on both state and action: k−1 (cid:88) k-MVP(s, a) = qˆk (s, a) = γiri+1 + γkvˆ(sk ), (23) mˆ mˆ mˆ i=0 where now reward and value predictions are computed after unrolling the model using action a for one step, and actions sampled from the policy for the remaining k − 1 steps. D Additional Experiments D.1 Measuring Self-Inconsistency in OOD States To complement our results in Figure 4, we have also evaluated self-inconsistency by computing the IVE as an average over 100 action sequences sampled from the policy, see Figure 11. We observed only minor quantitative differences compared to the results presented in Figure 4 (where we were using a single action sequence to estimate the IVE).
Model-Value Inconsistency as a Signal for Epistemic Uncertainty 0.4 0.3 0.2 0.1 0 chaser climber coinrun fruitbot jumper EVI- #levels 10 100 Train 500 10 100 Test 500 Figure 11: σ-IVE(5) computed using the model of the Muesli agent while evaluating on both training and unseen test levels, for different numbers of unique levels seen during training. To estimate the IVE, we used 100 action sequences from the policy. Bars, error-bars show mean and standard error across 3 seeds, respectively. D.2 Measuring Explicit Ensemble (EVE) Variance in OOD States To complement our results in Figure 4 for IVE, we provide the EVE results in Figure 12. We observe that EVE behaves similar to IVE in terms of ensemble variance as a function of #levels for both training and testing levels. 0.8 0.4 0 chaser climber coinrun fruitbot jumper nruter desilamron naeM 0.5 0.4 0.3 0.2 0.1 0 chaser climber coinrun fruitbot jumper EVE- #levels 10 100 Train 500 10 100 Test 500 Figure 12: σ-EVE(5) computed using the Muesli agent augmented with an ensemble of 5 value heads (different random initialisation) while evaluating on both training and unseen test levels, for different numbers of unique levels seen during training. (Left:) Performance for training (green) and test (pink) for varying number of levels. (Right:) Explicit value ensemble inconsistency measured by standard deviation of the 5 different heads. Results are from a single seed. D.3 How to Use the IVE(5) Signal? In the following experiments we consider the self-inconsistency signal as an optimistic bonus to encourage better exploration during training, hence generalising better during evaluation. We test variants of the +σ-IVE(5) signal by mixing the policy with the self-inconsistency in probability space +σ-IVE(5)(cid:44) (1 − β)π + β · σ-IVE(5), and by mixing the signal with the policy logits: z + σ-IVE(5)(cid:44) softmax(z + β · σ-IVE(5)). We vary the number of MPV in the ensemble for n = 5, 10. π Use further test using a different metric for measuring the disagreement across the nMPVs that considers different weighting averages over k: (cid:32) (cid:33) d = JSD (IVE(n)) = H (cid:88) w vˆk − (cid:88) w H (cid:0) vˆk (cid:1) (24) JS w k mˆ k mˆ k k with three weighting schemes: a decreasing weight dec : w = rk/((cid:80) rj) such that the weight decreases to 1/3 over n, JS k j an increasing weight inc with the inverse trend, and a uniform weight uni that corresponds to the uniform mixing over JS JS n w = 1/n. k In Figure 13b we observe that learning with an optimistic bonus helps with generalisation at evaluation time. Figure13c we observe that mixing over probability space is less sensitive to re-scaling β, but yields higher variance. We notice a trade-off between the weighting scheme used vs. the size of the IVE, for higher ns the best performing metric has less
Model-Value Inconsistency as a Signal for Epistemic Uncertainty decJS incJS uniJS Vanilla Muesli Muesli, prior=5 +dJS-IVE(5) +dJS-EVE(5) 12 8 4 0 0 50M 100M nruter edosipe .gvA Train 9 6 3 0 0 50M 100M Number of frames (a) Train nruter edosipe .gvA Test Number of frames (b) Test )01(EVI- d+ SJ )5(EVI- d+ SJ )5(EVI- d+z SJ 300 200 100 0 CUA climber (c) Variations Figure 13: Model-value inconsistency (see Section 3.2) as the Jensen-Shannon divergence of the implicit value ensemble (see Section 3.1) for different numbers of ensemble components n, trained across 100 levels error bars show SE over 3 seeds. (a) Mean episode return during training with 100 levels, for Muesli baselines and for an agent trained with optimistic divergence over an explicit ensemble d -EVE(5) and over IVE(5), both with an increasing Jensen-Shannon disagreement. JS (b) Mean episode return for evaluation without the optimistic disagreement for the same methods. (c) Ablation study over d -IVE of varying length n = 5, 10 and by mixing in logit space z + d-IVE vs. mixing in probability space +d-IVE. JS weight on the larger k-MPVs. For the decreasing metric the results remain more robust, suggesting that the inconsistencies are higher for larger ks. We used β = 0.1 for mixing in probability and β = 1 for the logit case. D.4 Ablation on Pessimism for Evaluation We evaluate in Figure 14 how sensitive the self-inconsistency signal is to different re-scaling parameters β when acting pessimistically at test time z − β d-σ-IVE(5) with an increasing weight. We trained a vanilla Muesli agent using 10/100 levels over 150M frames and evaluated with a pessimistic bonus for the consecutive 20M frames over 3 seeds. β = 0.0 β = -1.0 β = -10.0 β = -100.0 β = -1000.0 2.0 1.5 1.0 0.5 0.0 nruteR edosipE naeM chaser climber coinrun fruitbot jumper 1.5 6 0 2.0 1.0 4 −1 1.5 −2 1.0 0.5 2 −3 0.5 0.0 0 0.0 6 4 2 0 nruteR edosipE naeM chaser climber coinrun fruitbot jumper 6 6 15 2 4 4 10 2 2 5 1 0 0 0 0 Figure 14: Mean episode return evaluated with pessimism bonus −d -IVE with increasing weights for each procgen JS environment on a trained vanilla Muesli using 10 levels (top) and 100 levels (bottom). Error bars show 95% CI. D.5 Dreamer Variants
Model-Value Inconsistency as a Signal for Epistemic Uncertainty In Section 4.2, we modified the Dreamer (Hafner et al., Table 3: Results for the Dreamer agent and IVE variants on a 2019a) agent to improve its exploration without having to modified version of the walker task with varying degrees of learn an explicit ensemble of value functions. We modify reward sparsity controlled by η, where higher η corresponds to the behavioural policy used for collecting data, using the harder exploration. A “♦” indicates methods that use gradient- mean and standard deviation of the implicit value ensem- based trajectory optimisation, while “♣” indicates methods ble, i.e., µ-IVE(5) and σ-IVE(5), respectively. We use the that use sample-based trajectory optimisation. We report mean original Dreamer setup otherwise. and standard error of episodic returns (rounded to the nearest In particular, for each time-step t, we sample action at π tenth) over 3 runs after 1M steps. Higher-is-better and the from the learned policy π and then calculate the IVE(5), performance is upper bounded by 1000. The best performing similar to Eqn. (7). Then, we can form the utility function method, per-task, is in bold. U = µ-IVE(5) + β · σ-IVE(5). (25) Methods η = 0.0 η = 0.2 η = 0.3 η = 0.5 We use online gradient-based or sample-based planning, Dreamer 1000±00 720±10 570±60 80±50 a.k.a. model-predictive control (MPC, Garcia et al., 1989) Dreamer♦ 1000±00 540±30 240±50 40±30 for selecting an action. µ-IVE(5)♦ 1000±00 860±40 690±70 210±60 We used β = 0.1, 10 gradient steps or 10 samples from µ + σ-EVE(5)♦ 1000±00 1000±00 980±10 280±50 µ + σ-EMVE(5)♦ 1000±00 910±20 730±40 210±60 the learned policy for guiding the search in all of our µ + σ-IVE(5)♦ 1000±00 1000±00 1000±00 330±70 experiments, shown in Table 3. µ + σ-IVE(5)♣ 1000±00 1000±00 1000±00 280±40
Model-Value Inconsistency as a Signal for Epistemic Uncertainty D.6 Qualitative Analysis of Different Value Ensembles In Figure 15, we plot the standard deviation across different types of value ensembles, as illustrated in Figure 2. 0.0 0.5 1.0 (a) σ-IVE(1) (b) σ-IVE(2) (c) σ-IVE(3) (d) σ-IVE(10) (e) σ-IVE(20) (f) σ-EVE(2) (g) σ-EVE(3) (h) σ-EVE(4) (i) σ-EVE(10) (j) σ-EVE(20) (k) σ-EMVE(2) (l) σ-EMVE(3) (m) σ-EMVE(4) (n) σ-EMVE(10) (o) σ-EMVE(20) Figure 15: Standard deviation across value ensembles. (i) Explicit value ensembles (EVE), as illustrated in Figure 2a; (ii) explicit model (value) ensembles (EMVE), as illustrated in Figure 2b and (iii) implicit value ensembles (IVE), as illustrated in Figure 2c. All values are normalised per-figure in range [0, 1].
Model-Value Inconsistency as a Signal for Epistemic Uncertainty E Muesli and its Implicit Value Ensemble In this section, we provide an exposition of how the (i) functional form and (ii) learning algorithm of the Muesli (Hessel et al., 2021) and MuZero (Schrittwieser et al., 2020) agents contribute to the diversification of their implicit value ensemble (IVE) members. This is to complement our analysis in Section 3.4. E.1 Functional Form The Muesli agent (see Figure 10, Hessel et al., 2021) is comprised of (i) an representation function hˆ(s; ω), (ii) a (latent) state-value function vˆ(z; φ) and (iii) an action-conditioned model mˆ (·, ·|z, a; θ) (cid:44) (rˆ(z, a; θ), pˆ(z, a; θ)), represented as neural networks with parameters ω, φ and θ, respectively. We omit the parametrisation and learning of the policy head πˆ(·|z) since it does not impact our analysis. In summary, the neural network functions are given by: hˆ (s) = hˆ(s; ω) (cid:44) z ∈ Z (26) ω vˆ (z) = vˆ(z; φ) (cid:44) v ∈ R (27) φ rˆ (z, a) = rˆ(z, a; θ) (cid:44) r1 ∈ R (28) θ pˆ (z, a) = pˆ(z, a; θ) (cid:44) z1 ∈ Z. (29) θ Note the similarity with Eqn. (8-11). The main difference is that the Muesli’s transition model and reward function, i.e., pˆ and rˆ , are action-conditioned. We also use the bold notation introduced in Eqn. (7). To ease the analysis, we define θ θ the state-to-state transition function and state-to-reward function by coupling the policy πˆ with the transition function pˆ θ and reward function rˆ , respectively, giving rise to a transition and reward kernel i.e., θ z1 ∼ pˆπˆ(z) (cid:44) pˆ (z, πˆ(·|z)) (30) θ θ r1 ∼ rˆπˆ(z) (cid:44) rˆ (z, πˆ(·|z)). (31) θ θ We construct the implicit value ensemble (IVE) by repeatedly applying the policy πˆ and mˆ model-induced Bellman operator θ T πˆ on the value function vˆ , i.e., constructing different k-steps model predicted values (k-MPVs, Eqn. (5)), given by mˆ θ φ vˆ0 (s) = (T πˆ )0vˆ (hˆ (s)) = vˆ (hˆ (s)) = vˆ ◦ hˆ (s) (32) mˆ mˆ θ φ ω φ ω φ ω vˆ1 (s) = (T πˆ )1vˆ (hˆ (s)) = (cid:0) rˆπˆ + γvˆ (cid:1) ◦ pˆπˆ ◦ hˆ (s) (33) mˆ mˆ θ φ ω θ φ θ ω vˆ2 (s) = (T πˆ )2vˆ (hˆ (s)) = (cid:0) rˆ + γ(rˆπˆ + γvˆ ) ◦ pˆπˆ(cid:1) ◦ pˆπˆ ◦ hˆ (s) (34) mˆ mˆ θ φ ω θ θ φ θ θ ω . . .   k−1 vˆ mk ˆ (s) = (T mˆπˆ θ )kvˆ φ(hˆ ω(s)) = (cid:88) γj−1rˆ θπˆ ◦ (pˆπ θˆ ◦ · · · ◦ pˆπ θˆ) + γkvˆ φ ◦ (pˆπ θˆ ◦ · · · ◦ pˆπ θˆ) ◦ hˆ ω(s) (35) j=1 j-times k-times where ◦ denotes function composition. Obviously, the functional form of the k-MPVs with different k is different since they compose differently mˆ and vˆ . For instance, vˆ0 (s) uses only vˆ , while vˆ1 (s) and vˆk (s) for k > 1 use both mˆ and vˆ θ φ mˆ φ mˆ mˆ θ φ but not in the same way. Instead, for k → ∞, we obtain a purely mˆ -based prediction (a.k.a. the fixed point of T πˆ ). θ mˆ θ For a stochastic policy in Eqn. (31) we sample from the policy πˆ(·|z). Hence we obtain stochastic estimates of the k-MPV in Eqn. (35), similar to Eqn. (7). Nonetheless, note that the Muesli model is a deterministic expectation model and hence we do not have to sample from it. Empirically, we found that the impact of using a stochastic policy on the estimation of the IVE members was negligible, see Appendix D. We illustrate the computational graph for the k-MPV in Figure 16. E.2 Learning Algorithm The value and model learning algorithms of Muesli further diversify the IVE member predictions. In our analysis, we consider two distinct cases: (i) when the model and value are trained with on-policy trajectories from πˆ and (ii) when off-policy trajectories from behavioural policies π are used. Note that the learning algorithm of the Muesli agent uses a β mix of on- and off-policy trajectories, similar to the LASER agent (Schmitt et al., 2020).
Model-Value Inconsistency as a Signal for Epistemic Uncertainty a0 a1 ak−1 s hˆ ω z0 mˆ θ z1 mˆ θ z2 · · · mˆ θ zk vˆ φ IVE vˆ m0 ˆ (s) v0 vˆ φ vˆ m1 ˆ (s) r1 + γv1 vˆ φ vˆ m2 ˆ (s) r1 + γr2 + γ2v2 vˆ φ vˆk (s) r1 + γr2 + · · · + γk−1rk + γkvk mˆ Figure 16: The computational graph of the implicit value ensemble (IVE) members for the Muesli (Hessel et al., 2021) agent. The action nodes {ai}k−1 are stochastic nodes from which we sample from the (latent-)state-conditioned policy ai ∼ πˆ(·|zi). i=0 On-policy trajectories. Provided a sequence of states, actions and rewards, collected from running policy πˆ in the true environment m∗, i.e., (s , a , r ) ∼ m∗ , we present the targets used for the IVE members for the first state, i.e., t:t+T t:t+T t:t+T πˆ {vk (s )}K , where in practice K = 5 (Hessel et al., 2021). We use the notation from Figure 16. n-step bootstrap value mˆ t k=0 estimates (Sutton, 1988) are constructed using vˆ and used as value targets vtarget , where φ t:t+T −1 n−1 (cid:88) vtarget = γj−1r + γnvˆ (s ) (36) t+i t+i+j φ t+i+n j=1 and n = 5. Alternative methods for constructing value targets, e.g., TD(λ) (Sutton & Barto, 2018) could be used, too. The “prediction-target” pairs for the different IVE members for state s and actions a are then given by6 t t:t+T vˆ0 (s ) = v0 ←− vtarget mˆ t t vˆ1 (s ) = r1 + γv1 ←− r + γvtarget (37) mˆ t t+1 t+1 vˆk (s ) = (cid:80)k−1 γi−1rj + γkvk ←− (cid:80)k−1 γi−1r + γkvtarget, mˆ t i=1 i=1 t+j t+k where the ←− indicates that an objective function (e.g., L2 loss) is minimised that makes the two sides of the arrow approximately equal. Importantly, Muesli and MuZero ground reward and value predictions independently, or in other words, the k-MPV is trained by minimising the following loss: k−1 (cid:88) L (cid:44) (r1 − r )2 + · · · + (rk−1 − r )2 + (vk − vtarget)2 = (ri − r )2 + (vk − vtarget)2. (38) t+1 t+k−1 t+k t+i t+k i=1 Eqn. (37) highlight that while all the IVE members are trained to approximate the value of the policy πˆ, i.e., vˆ0 ≈ vˆ1 ≈ mˆ mˆ · · · ≈ vˆk ≈ vπˆ, each one is regressed against a different target/estimate of the value, further diversifying the ensemble. mˆ 6We assume that T − 1 > K + n.
Model-Value Inconsistency as a Signal for Epistemic Uncertainty Off-policy trajectories. Provided a sequence of states, actions and rewards, collected from a behavioural policy π β interacting with the true environment m∗, i.e., (s , a , r ) ∼ m∗ , we construct off-policy corrected n-step t:t+T t:t+T t:t+T πβ bootstrap value target vtarget with the Retrace (Munos et al., 2016) algorithm, using vˆ . t:t+T −1 φ Next, we define the multi-step action-conditioned model-based reward and value estimates, i.e., ˆr (s , a , . . . , a ) (cid:44) ˆr (s , a ) and vˆ (s , a , . . . , a ) (cid:44) vˆ (s , a ), (39) mˆ t t t+k−1 mˆ t t:t+k−1 mˆ t t t+k−1 mˆ t t:t+k−1 where for on-policy actions, i.e., a ∼ πˆ, we note that vˆ (s , a ) is the k-MPV of Eqn. (35). As illustrated t:t+k−1 mˆ t t:t+k−1 in Figure 16, the multi-step action-conditioned reward and value estimates in Eqn. (39) are computed by setting the value of the nodes a0:k−1 to the replayed action sequence a from the behavioural policy π . The learning of the model and t:t+k−1 β value function proceeds as in the on-policy case, obtaining the following “prediction-target” pairs, analogous to Eqn. (37) vˆ (s ) ←− vtarget mˆ t t ˆr (s , a ) + γvˆ (s , a ) ←− r + γvtarget (40) mˆ t t mˆ t t t+1 t+1 (cid:80)k−1 γi−1ˆr (s , a ) + γkvˆ (s , a ) ←− (cid:80)k−1 γi−1r + γkvtarget, i=1 mˆ t t:t+j−1 mˆ t t:t+k−1 i=1 t+j t+k Eqn. (40) suggests that in the off-policy case, k-MPVs are not trained directly since the sampled sequence of actions that conditions the reward and value predictions does not (necessarily) come from πˆ. Nonetheless, the multi-step action- conditioned reward and value predictors are the building blocks for constructing the k-MPVs and hence we conjecture that the diversity induced by training these with different targets will lead to diversity in the IVE members too. Overall, the Muesli agent is trained with both on- and off-policy trajectories. In the case of on-policy trajectories, we showed in Eqn. (37) that that different targets are used for training each IVE member. When off-policy trajectories are used, instead of k-MPVs, model-based multi-step action-conditioned reward and value predictors are trained to regress different reward and value targets for different k. These predictors are used for constructing the IVE members at acting time and hence this diversity at training time can impact the diversity of the IVE members too.
