A Causal Bayesian Networks Viewpoint on Fairness Silvia Chiappa and William S. Isaac DeepMind, London, UK {csilvia,williamis}@google.com Abstract. We offer a graphical interpretation of unfairness in a dataset as the presence of an unfair causal path in the causal Bayesian network representing the data-generation mechanism. We use this viewpoint to re- visit the recent debate surrounding the COMPAS pretrial risk assessment tool and, more generally, to point out that fairness evaluation on a model requires careful considerations on the patterns of unfairness underlying the training data. We show that causal Bayesian networks provide us with a powerful tool to measure unfairness in a dataset and to design fair models in complex unfairness scenarios. 1 Introduction Machine learning is increasingly used in a wide range of decision-making scenarios that have serious implications for individuals and society, including financial lending [10,36], hiring [8,28], online advertising [27,41], pretrial and immigration detention [5,43], child maltreatment screening [14,47], health care [19,32], and social services [1,23]. Whilst this has the potential to overcome undesirable aspects of human decision-making, there is concern that biases in the training data and model inaccuracies can lead to decisions that treat historically discriminated groups unfavourably. The research community has therefore started to investigate how to ensure that learned models do not take decisions that are unfair with respect to sensitive attributes (e.g. race or gender). This effort has led to the emergence of a rich set of fairness definitions [13,16,21,24,38] providing researchers and practitioners with criteria to evaluate existing systems or to design new ones. Many such definitions have been found to be mathematically incompatible [7,13,15,16,30], and this has been viewed as representing an unavoidable trade-off establishing fundamental limits on fair machine learning, or as an indication that certain definitions do not map on to social or legal understandings of fairness [17]. Most fairness definitions express properties of the model output with respect to sensitive information, without considering the relations among the relevant variables underlying the data-generation mechanism. As different relations would require a model to satisfy different properties in order to be fair, this could lead The final publication is available at doi.org/10.1007/978-3-030-16744-8_1 [12]. 9102 luJ 51 ]LM.tats[ 1v03460.7091:viXra
2 S. Chiappa and W. S. Isaac to erroneously classify as fair/unfair models exhibiting undesirable/legitimate biases. In this manuscript, we use the causal Bayesian network framework to draw attention to this point, by visually describing unfairness in a dataset as the presence of an unfair causal path in the data-generation mechanism. We then use this viewpoint to raise concern on the fairness debate surrounding the COMPAS pretrial risk assessment tool. Finally, we show that causal Bayesian networks offer a powerful tool for representing, reasoning about, and dealing with complex unfairness scenarios. 2 A Graphical View of (Un)fairness Consider a dataset ∆ = {an, xn, yn}N , corresponding to N individuals, where n=1 an indicates a sensitive attribute, and xn a set of observations that can be used (possibly together with an) to form a prediction yˆn of outcome yn. We assume a binary setting an, yn, yˆn ∈ {0, 1} (unless otherwise specified), and indicate with A, X , Y , and Yˆ the (set of) random variables1 corresponding to an, xn, yn, and yˆn respectively. In this section we show at a high-level that a correct use of fairness defini- tions concerned with statistical properties of Yˆ with respect to A requires an understanding of the patterns of unfairness underlying ∆, and therefore of the relationships among A, X and Y . More specifically we show that: (i) Using the framework of causal Bayesian networks (CBNs), unfairness in ∆ can be viewed as the presence of an unfair causal path from A to X or Y . (ii) In order to determine which properties Yˆ should possess to be fair, it is necessary to question and understand unfairness in ∆. A Q D Y fair? Assume a dataset ∆ = {an, xn = {qn, dn}, yn}N correspond- n=1 ing to a college admission scenario in which applicants are unfair admitted based on qualifications Q, choice of department D, fair? and gender A; and in which female applicants apply more often to certain departments. This scenario can be represented by the CBN on the left (see Appendix A for an overview of BNs, and Sect. 3 for a detailed treatment of CBNs). The causal path A → Y represents direct influence of gender A on admission Y , capturing the fact that two individuals with the same qualifications and applying to the same department can be treated differently depending on their gender. The indirect causal path A → D → Y represents influence of A on Y through D, capturing the fact that female applicants more often apply to certain departments. Whilst the direct path A → Y is certainly an unfair one, the paths A → D and D → Y , and therefore A → D → Y , could either be considered as fair or as unfair. For example, rejecting women more often due to department choice could be considered fair with respect to college 1 Throughout the paper, we use capital and small letters for random variables and their values, and calligraphic capital letters for sets of variables.
A Causal Bayesian Networks Viewpoint on Fairness 3 responsibility. However, this could be considered unfair with respect to societal responsibility if the departmental differences were a result of systemic historical or cultural factors (e.g. if female applicants apply to specific departments at lower rates because of overt or covert societal discouragement). Finally, if the college were to lower the admission rates for departments chosen more often by women, then the path D → Y would be unfair. Deciding whether a path is fair or unfair2 requires careful ethical and so- ciological considerations and/or might not be possible from a dataset alone. Nevertheless, this example illustrates that we can view unfairness in a dataset as the presence of an unfair causal path from the sensitive attribute A to X or Y . Different (un)fair path labeling requires Yˆ to have different characteristics in order to be fair. In the case in which the causal paths from A to Y are all unfair (e.g. if A → D → Y is considered unfair), a Yˆ that is statistically independent of A (denoted with Yˆ ⊥⊥ A) would not contain any of the unfair influence of A on Y . In such a case, Yˆ is said to satisfy demographic parity. Demographic Parity (DP). Yˆ satisfies demographic parity if Yˆ ⊥⊥ A, i.e. p(Yˆ = 1|A = 0) = p(Yˆ = 1|A = 1), where e.g. p(Yˆ = 1|A = 0) can be estimated as N p(Yˆ = 1|A = 0) ≈ 1 (cid:88) 1 , N yˆn=1,an=0 0 n=1 with 1 = 1 if yˆn = 1 and an = 0 (and zero otherwise), and where N yˆn=1,an=0 0 indicates the number of individuals with an = 0. Notice that many classifiers, rather than a binary prediction yˆn ∈ {0, 1}, output a degree of belief that the individual belongs to class 1, rn, also called score. For example, in the case of logistic regression rn corresponds to the probability of class 1, i.e. rn = p(Y = 1|an, xn). To obtain the prediction yˆn ∈ {0, 1} from rn, it is common to use a threshold θ, i.e. yˆn = 1 . In this case, we can rewrite the estimate for rn>θ p(Yˆ = 1|A = 0) as N p(Yˆ = 1|A = 0) ≈ 1 (cid:88) 1 . N rn>θ,an=0 0 n=1 Notice that R ⊥⊥ A implies Yˆ ⊥⊥ A for all values of θ. In the case in which the causal paths from A to Y are all fair (e.g. if A → Y is absent and A → D → Y is considered fair), a Yˆ such that Yˆ ⊥⊥ A|Y or Y ⊥⊥ A|Yˆ would be allowed to contain such a fair influence, but the (dis)agreement between Y and Yˆ would not be allowed to depend on A. In these cases, Yˆ is said to satisfy equal false positive/false negative rates and calibration respectively. Equal False Positive and Negative Rates (EFPRs/EFNRs). Yˆ satisfies EFPRs and EFNRs if Yˆ ⊥⊥ A|Y , i.e. (EFPRs) p(Yˆ = 1|Y = 0, A = 0) = p(Yˆ = 2 A path could also be only partially fair — we omit this case for simplicity.
4 S. Chiappa and W. S. Isaac 1|Y = 0, A = 1) and (EFNRs) p(Yˆ = 0|Y = 1, A = 0) = p(Yˆ = 0|Y = 1, A = 1). Calibration. Yˆ satisfies calibration if Y ⊥⊥ A|Yˆ . In the case of score out- put R, this condition is often instead called predictive parity at threshold θ, p(Y = 1|R > θ, A = 0) = p(Y = 1|R > θ, A = 1), and calibration defined as requiring Y ⊥⊥ A|R. In the case in which at least one causal path from A to Y is unfair (e.g. if A → Y is present), EFPRs/EFNRs and calibration are inappropriate criteria, as they would not require the unfair influence of A on Y to be absent from Yˆ (e.g. a perfect model (Yˆ = Y ) would automatically satisfy EFPRs/EFNRs and calibration, but would contain the unfair influence). This observation is particularly relevant to the recent debate surrounding the correctional offender management profiling for alternative sanctions (COMPAS) pretrial risk assessment tool. We revisit this debate in the next section. 2.1 The COMPAS Debate Over the past few years, numerous state and local governments around the United States have sought to reform their pretrial court systems with the aim of reducing unprecedented levels of incarceration, and specifically the population of low-income defendants and racial minorities in America’s prisons and jails [2,25,31]. As part of this effort, quantitative tools for determining a person’s likelihood for reoffending or failure to appear, called risk assessment instruments (RAIs), were introduced to replace previous systems driven largely by opaque discretionary decisions and money bail [6,26]. However, the expansion of pretrial RAIs has unearthed new concerns of racial discrimination which would nullify the purported benefits of these systems and adversely impact defendants’ civil liberties. An intense ongoing debate, in which the research community has also been heavily involved, was triggered by an exposé from investigative journalists at ProPublica [5] on the COMPAS pretrial RAI developed by Equivant (formerly Northpointe) and deployed in Broward County in Florida. The COMPAS general recidivism risk scale (GRRS) and violent recidivism risk scale (VRRS), the focus of ProPublica’s investigation, sought to leverage machine learning techniques to improve the predictive F1-score of recidivism compared to older RAIs such as the level of service inventory-revised [3] which were primarily based on theories and techniques from a sub-field of psychology known as the psychology of criminal conduct [4,9]3. 3 While the exact methodology underlying GRRS and VRRS is proprietary, publicly available reports suggest that the process begins with a defendant being administered a 137 point assessment during intake. This is used to create a series of dynamic risk factor scales such as the criminal involvement scale and history of violence scale. In addition, COMPAS also includes static attributes such as the defendant’s age and prior police contact (number of prior arrests). The raw COMPAS scores are
A Causal Bayesian Networks Viewpoint on Fairness 5 Fig. 1. Number of black and white defendants in each of two aggregate risk categories [15]. The overall recidivism rate for black defendants is higher than for white defendants (52% vs. 39%), i.e. Y (cid:26)(cid:26)⊥⊥A. Within each risk category, the proportion of defendants who reoffend is approximately the same regardless of race, i.e. Y ⊥⊥ A|Yˆ . Black defendants are more likely to be classified as medium or high risk (58% vs. 33%) i.e. Yˆ(cid:26)(cid:26)⊥⊥A. Among individuals who did not reoffend, black defendants are more likely to be classified as medium or high risk than white defendants (44.9% to 23.5%). Among individuals who did reoffend, white defendant are more likely to be classified as low risk than black defendants (47.7% vs 28%), i.e. Yˆ(cid:26)(cid:26)⊥⊥A|Y . ProPublica’s criticism of COMPAS centered on two concerns. First, the authors argued that the distribution of the risk score R ∈ {1, . . . , 10} exhib- ited discriminatory patterns, as black defendants displayed a fairly uniform distribution across each value, while white defendants exhibited a right skewed distribution, suggesting that the COMPAS recidivism risk scores disproportion- ately rated white defendants as lower risk than black defendants. Second, the authors claimed that the GRRS and VRRS did not satisfy EFPRs and EFNRs, as FPRs = 44.9% and FNRs = 28.0% for black defendants, whilst FPRs = 23.5% and FNRs = 47.7% for white defendants (see Fig. 1). This evidence led ProP- ublica to conclude that COMPAS had a disparate impact on black defendants, leading to public outcry over potential biases in RAIs and machine learning writ large. In response, Equivant published a technical report [20] refuting the claims of bias made by ProPublica and concluded that COMPAS is sufficiently calibrated, in the sense that it satisfies predictive parity at key thresholds. Subsequent analyses [13,16,30] confirmed Equivant’s claims of calibration, but also demonstrated the incompatibility of EFPRs/EFNRs and calibration due to differences in base transformed into decile values by ranking and calibration with a normative group to ensure an equal proportion of scores within each scale value. Lastly, to aid practitioner interpretation, the scores are grouped into three risk categories. The scale values are displayed to court officials as either Low (1-4), Medium (5-7), and High (8-10) risk.
6 S. Chiappa and W. S. Isaac rates across groups (Y (cid:26)(cid:26)⊥⊥A) (see Appendix B). Moreover, the studies suggested that attempting to satisfy these competing forms of fairness force unavoidable trade-offs between criminal justice reformers’ purported goals of racial equity and public safety. As explained in Sect. 2, by requiring the rate of (dis)agreement between Y and Yˆ to be the same for black and white defendants (and therefore by not being concerned with dependence of Y on A), EFPRs/EFNRs and calibration are inappropriate fairness criteria if dependence of Y on A includes influence of A on Y through an unfair causal path. As previous research has shown [29,35,44], modern polic- A M ing tactics center around targeting a small number of neighborhoods — often disproportionately populated by non-white and low income residents — with recurring patrols and stops. This uneven distribution of police at- F Y tention, as well as other factors such as funding for pretrial Fig. 2. Possible CBN services [31,46], means that differences in base rates be- underlying the dataset tween racial groups are not reflective of ground truth rates. used for COMPAS. We can rephrase these findings as indicating the presence of a direct path A → Y (through unobserved neighborhood) in the CBN rep- resenting the data-generation mechanism (Fig. 2). Such tactics also imply an influence of A on Y through the set of variables F containing number of prior arrests. In addition, the influence of A on Y through A → Y and A → F → Y could be more prominent or contain more unfairness due to racial discrimination. These observations indicate that EFPRs/EFNRs and calibration are inappro- priate criteria for this case (and therefore that their incompatibility is irrelevant), and more generally that the current fairness debate surrounding COMPAS gives insufficient consideration to the patterns of unfairness underlying the training data. Our analysis formalizes the concerns raised by social scientists and legal scholars on mismeasurement and unrepresentative data in the US criminal justice system. Multiple studies [22,34,37,46] have argued that the core premise of RAIs, to assess the likelihood a defendant reoffends, is impossible to measure and that the empirical proxy used (e.g. arrest or conviction) introduces embedded biases and norms which render existing fairness tests unreliable. This section used the CBN framework to describe at a high-level different patterns of unfairness that can underlie a dataset and to point out issues with current deployment of fairness definitions. In the remainder of the manuscript, we use this framework more extensively to further advance our analysis on fairness. Before doing that, we give some background on CBNs [18,39,40,42,45], assuming that all variables except A are continuous. 3 Causal Bayesian Networks A Bayesian network is a directed acyclic graph where nodes and edges represent random variables and statistical dependencies. Each node X in the graph is i
A Causal Bayesian Networks Viewpoint on Fairness 7 associated with the conditional distribution p(X |pa(X )), where pa(X ) is the set i i i of parents of X . The joint distribution of all nodes, p(X , . . . , X ), is given by the i 1 I product of all conditional distributions, i.e. p(X , . . . , X ) = (cid:81)I p(X |pa(X )) 1 I i=1 i i (see Appendix A for more details on Bayesian networks). When equipped with causal semantic, namely when representing the data- generation mechanism, Bayesian networks can be used to visually express causal relationships. More specifically, CBNs enable us to give a graphical definition of causes and causal effects: if there exists a directed path from A to Y , then A is a potential cause of Y . Directed paths are also called causal paths. The causal effect of A on Y can be p(C) p(C) seen as the information traveling from C C A to Y through causal paths, or as the conditional distribution of Y given A G G →A=a restricted to causal paths. This implies A Y A Y that, to compute the causal effect, we p(A|C) p(Y |C, A) δ A=a p(Y |C, A) need to disregard the information that (a) (b) travels along non-causal paths, which occurs if such paths are open. Since Fig. 3. (a): CBN with a confounder C for paths with an arrow emerging from A the effect of A on Y . (b): Modified CBN re- are either causal or closed (blocked ) by sulting from intervening on A. a collider, the problematic paths are only those with an arrow pointing into A, called back-door paths, which are open if they do not contain a collider. An example of an open back-door path is given by A ← C → Y in the CBN G of Fig. 3(a): the variable C is said to be a confounder for the effect of A on Y , as it confounds the causal effect with non-causal information. To understand this, assume that A represents hours of exercise in a week, Y cardiac health, and C age: observing cardiac health conditioning on exercise level from p(Y |A) does not enable us to understand the effect of exercise on cardiac health, since p(Y |A) includes the dependence between A and Y induced by age. Each parent-child relationship in a CBN represents an autonomous mechanism, and therefore it is conceivable to change one such a relationship without changing the others. This enables us to express the causal effect of A = a on Y as the conditional distribution p (Y |A = a) on the modified CBN G of →A=a →A=a Fig. 3(b), resulting from replacing p(A|C) with a Dirac delta distribution δ A=a (thereby removing the link from C to A) and leaving the remaining conditional distributions p(Y |A, C) and p(C) unaltered — this process is called intervention on A. The distribution p (Y |A = a) can be estimated as p (Y |A = →A=a →A=a (cid:82) (cid:82) a) = p (Y |A = a, C)p (C|A = a) = p(Y |A = a, C)p(C). This is C →A=a →A=a C a special case of the following back-door adjustment formula. Back-door Adjustment. If a set of variables C satisfies the back-door criterion relative to {A, Y }, the causal effect of A on Y is given by p (Y |A) = →A (cid:82) p(Y |A, C)p(C). C satisfies the back-door criterion if (a) no node in C is a C descendant of A and (b) C blocks every back-door path from A to Y .
8 S. Chiappa and W. S. Isaac The equality p (Y |A = a, C) = p(Y |A = a, C) follows from the fact that →A=a G , obtained by removing from G all links emerging from A, retains all (and A→ only) the back-door paths from A to Y . As C blocks all such paths, Y ⊥⊥ A|C in G . This means that there is no non-causal information traveling from A A→ to Y when conditioning on C and therefore conditioning on A coincides with intervening. Conditioning on C to block an open back-door path may open a E X closed path on which C is a collider. For example, in the CBN of Fig. 4(a), C conditioning on C closes the paths A ← C ← X → Y and A ← C → Y , A Y M but opens the path A ← E → C ← X → Y (additional conditioning on X would close A ← E → C ← X → Y ). B Z A Y The back-door criterion can also be (a) (b) derived from the rules of do-calculus [39,40], which indicate whether and Fig. 4. (a): CBN in which conditioning on how p (Y |A) can be estimated us- →A C closes the paths A ← C ← X → Y and ing observations from G: for many A ← C → Y but opens the path A ← E → graph structures with unobserved con- C ← X → Y . (b): CBN with one direct and founders the only way to compute one indirect causal path from A to Y . causal effects is by collecting obser- vations directly from G — in this case the effect is said to be non-identifiable. →A Potential Outcome Viewpoint. Let Y be the random variable with distri- A=a bution p(Y ) = p (Y |A = a). Y is called potential outcome and, when A=a →A=a A=a not ambiguous, we will refer to it with the shorthand Y . The relation between Y a a and all the variables in G other than Y can be expressed by the graph obtained by removing from G all the links emerging from A, and by replacing Y with Y . If a Y is independent on A in this graph, then4 p(Y ) = p(Y |A = a) = p(Y |A = a). a a a If Y is independent of A in this graph when conditioning on C, then a (cid:90) (cid:90) (cid:90) p(Y ) = p(Y |C)p(C) = p(Y |A = a, C)p(C) = p(Y |A = a, C)p(C) , a a a C C C i.e. we retrieve the back-door adjustment formula. In the remainder of the section we show that, by performing different interven- tions on A along different causal paths, it is possible to isolate the contribution of the causal effect of A on Y along a group of paths. Direct and Indirect Effect Consider the CBN of Fig. 4(b), containing the direct path A → Y and one indirect causal path through the variable M . Let Y (M ) be the random variable a a¯ 4 The equality p(Y |A = a) = p(Y |A = a) is called consistency. a
A Causal Bayesian Networks Viewpoint on Fairness 9 with distribution equal to the conditional distribution of Y given A restricted to causal paths, with A = a along A → Y and A = a¯ along A → M → Y . The average direct effect (ADE) of A = a with respect to A = a¯, defined as ADE = (cid:104)Y (M )(cid:105) − (cid:104)Y (cid:105) , a¯a a a¯ p(Ya(Ma¯)) a¯ p(Ya¯) (cid:82) where e.g. (cid:104)Y (cid:105) = Y p(Y ), measures the difference in flow of causal a¯ p(Ya¯) Ya¯ a¯ a¯ information from A to Y between the case in which A = a along A → Y and A = a¯ along A → M → Y and the case in which A = a¯ along both paths. Analogously, the average indirect effect (AIE) of A = a with respect to A = a¯, is defined as AIE = (cid:104)Y (M )(cid:105) − (cid:104)Y (cid:105) . a¯a a¯ a p(Ya¯(Ma)) a¯ p(Ya¯) The difference ADE − AIE gives the average total effect (ATE) ATE = a¯a aa¯ a¯a (cid:104)Y (cid:105) − (cid:104)Y (cid:105) 5. a p(Ya) a¯ p(Ya¯) Path-Specific Effect C A M L Y C m θ c θm θl θy a m l A M L Y θl θy a m θy a lθ c To estimate the effect along a specific group of causal paths, we can generalize the formulas for the ADE and AIE by replacing the variable in the first term with the one resulting from performing the intervention A = a along the group of interest and A = a¯ along the remaining causal paths. For example, consider the CBN of θ cy Fig. 5 (top) and assume that we are interested in isolating the effect of A on Y along the direct path A → Y and the paths passing through M , A → M →, . . . , → Y , namely along the red links. The path-specific effect (PSE) of A = a with respect to A = a¯ for this group of paths is defined as Fig. 5. Top: CBN with the direct path from A to Y and the indirect PSE = (cid:104)Y (M , L (M ))(cid:105) − (cid:104)Y (cid:105) , paths passing through M high- a¯a a a a¯ a a¯ lighted in red. Bottom: CBN cor- where p(Y a(M a, L a¯(M a))) is given by responding to Eq. (1). (cid:90) p(Y |A = a, C, M, L)p(L|A = a¯, C, M )p(M |A = a, C)p(C) . C,M,L In the simple case in which the CBN corresponds to a linear model, e.g. A ∼ Bern(π), C = (cid:15) , c M = θm + θmA + θmC + (cid:15) , L = θl + θl A + θl C + θl M + (cid:15) , a c m a c m l Y = θy + θyA + θyC + θy M + θyL + (cid:15) , (1) a c m l y 5 Often the AIE of A = a with respect to A = a¯ is defined as AIEa = (cid:104)Y (cid:105) − a¯a a p(Ya) (cid:104)Y (M )(cid:105) = −AIE , which differs in setting A to a rather than to a¯ along a a¯ p(Ya(Ma¯)) aa¯ A → Y . In the linear case, the two definitions coincide (see Eqs. (2) and (3)). Similarly the ADE can be defined as ADEa = (cid:104)Y (cid:105) − (cid:104)Y (M )(cid:105) = −ADE . a¯a a p(Ya) a¯ a p(Ya¯(Ma)) aa¯
10 S. Chiappa and W. S. Isaac where (cid:15) , (cid:15) , (cid:15) and (cid:15) are unobserved independent zero-mean Gaussian variables, c m l y we can compute (cid:104)Y (cid:105) by expressing Y as a function of A = a¯ and the Gaussian a¯ variables, by recursive substitutions in C, M and L, i.e. Y = θy + θya¯ + θy(cid:15) + θy (θm + θma¯ + θm(cid:15) + (cid:15) ) a¯ a c c m a c c m + θy(θl + θl a¯ + θl (cid:15) + θl (θm + θma¯ + θm(cid:15) + (cid:15) ) + (cid:15) ) + (cid:15) , l a c c m a c c m l y and then take the mean, obtaining (cid:104)Y (cid:105) = θy + θya¯ + θy (θm + θma¯) + θy(θl + a¯ a m a l θl a¯ + θl (θm + θma¯)). Analogously a m a (cid:104)Y (M , L (M ))(cid:105) = θy + θya + θy (θm + θma) + θy(θl + θl a¯ + θl (θm + θma)) . a a a¯ a a m a l a m a For a = 1 and a¯ = 0, this gives PSE = θy(a − a¯) + θy θm(a − a¯) + θyθl θm(a − a¯) = θy + θy θm + θyθl θm . a¯a a m a l m a a m a l m a The same conclusion could have been obtained by looking at the graph annotated with path coefficients (Fig. 5 (bottom)). The PSE is obtained by summing over the three causal paths of interest (A → Y , A → M → Y , and A → M → L → Y ) the product of all coefficients in each path. Notice that AIE , given by a¯a AIE = (cid:104)Y (M , L (M ))(cid:105) − (cid:104)Y (cid:105) a¯a a¯ a a a a¯ = θy + θya¯ + θy (θm + θma) + θy(θl + θl a + θl (θm + θma)) a m a l a m a − θy + θya¯ + θy (θm + θma¯) + θy(θl + θl a¯ + θl (θm + θma¯)) a m a l a m a = θy θm(a − a¯) + θy(θl (a − a¯) + θl θm(a − a¯)) , (2) m a l a m a coincides with AIEa , given by a¯a AIEa = (cid:104)Y (cid:105) − (cid:104)Y (M , L (M ))(cid:105) a¯a a a a¯ a¯ a¯ = θy + θya + θy (θm + θma) + θy(θl + θl a + θl (θm + θma)) a m a l a m a − θy + θya + θy (θm + θma¯) + θy(θl + θl a¯ + θl (θm + θma¯)) . (3) a m a l a m a Effect of Treatment on Treated. Consider the conditional distribution p(Y |A = a¯). This distribution measures the information travelling from A a to Y along all open paths, when A is set to a along causal paths and to a¯ along non-causal paths. The effect of treatment on treated (ETT) of A = a with respect to A = a¯ is defined as ETT = (cid:104)Y (cid:105) − (cid:104)Y (cid:105) = a¯a a p(Ya|A=a¯) a¯ p(Ya¯|A=a¯) (cid:104)Y (cid:105) − (cid:104)Y (cid:105) . As the PSE, the ETT measures difference in flow a p(Ya|A=a¯) p(Y |A=a¯) of information from A to Y when A takes different values along different paths. However, the PSE considers only causal paths and different values for A along different causal paths, whilst the ETT considers all open paths and different values for A along causal and non-causal paths respectively. Similarly to ATE , a¯a ETT for the CBN of Fig. 4(b) can be expressed as a¯a ETT = (cid:104)Y (M )(cid:105) − (cid:104)Y (cid:105) −((cid:104)Y (M )(cid:105) − (cid:104)Y (cid:105)) . a¯a a a¯ a¯ a a¯ a (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) ADEa¯a|a¯ AIEaa¯|a¯
A Causal Bayesian Networks Viewpoint on Fairness 11 Notice that, if we define difference in flow of non-causal (along the open back-door paths) information from A to Y when A = a with respect to when A = a¯ as NCI = (cid:104)Y (cid:105) − (cid:104)Y (cid:105) , we obtain a¯a a¯ p(Ya¯|A=a) p(Y |A=a¯) (cid:104)Y (cid:105) − (cid:104)Y (cid:105) = (cid:104)Y (cid:105) − (cid:104)Y (cid:105) p(Y |A=a) p(Y |A=a¯) a¯ p(Ya¯|A=a) p(Y |A=a¯) − ((cid:104)Y (cid:105) − (cid:104)Y (cid:105) ) a¯ p(Ya¯|A=a) p(Y |A=a) = NCI − ETT = NCI − ADE + AIE . a¯a aa¯ a¯a aa¯|a a¯a|a 4 Fairness Considerations using CBNs Equipped with the background on CBNs from Sect. 3, in this section we further investigate unfairness in a dataset ∆ = {an, xn, yn}N , discuss issues that might n=1 arise when building a decision system from it, and show how to measure and deal with unfairness in complex scenarios, revisiting and extending material from [11,33,48]. 4.1 Back-door Paths from A to Y In Sect. 2 we have introduced a graphical interpretation of unfairness in a dataset ∆ as the presence of an unfair causal path from A to X or Y . More specifically, we have shown through a college admission example that unfairness can be due to an unfair link emerging (a) from A or (b) from a subsequent variable in a causal path from A to Y (e.g. D → Y in the example). Our discussion did not mention paths from A to Y with an arrow pointing into A, namely back-door paths. This is because such paths are not problematic. To understand this, consider the hiring scenario described by A E the CBN on the left, where A represents religious belief and E educational background of the applicant, which influences religious participation (E → A). Whilst Y (cid:26)(cid:26)⊥⊥A due to the Y open back-door path from A to Y , the hiring decision Y is only based on E. 4.2 Opening Closed Unfair Paths from A to Y In Sect. 2, we have seen that, in order to reason about fairness of Yˆ , it is necessary to question and understand unfairness in ∆. In this section, we warn that another crucial element needs to be considered in the fairness discussion around Yˆ , namely (i) The variables used to form Yˆ could project into Yˆ unfair patterns in X that do not concern Y . This could happen, for example, if a closed unfair path from A to Y is opened when conditioning on the variables used to form Yˆ .
12 S. Chiappa and W. S. Isaac As an example, assume the CBN in Fig. 6 representing the A M data-generation mechanism underlying a music degree sce- α β γ nario, where A corresponds to gender, M to music aptitude (unobserved, i.e. M ∈/ ∆), X to the score obtained from X Y an ability test taken at the beginning of the degree, and Y to the score obtained from an ability test taken at the Fig. 6. CBN under- end of the degree. Individuals with higher music aptitude lying a music de- M are more likely to obtain higher initial and final scores gree scenario. (M → X, M → Y ). Due to discrimination occurring at the initial testing, women are assigned a lower initial score than men for the same aptitude level (A → X). The only path from A to Y , A → X ← M → Y , is closed as X is a collider on this path. Therefore the unfair influence of A on X does not reach Y (Y ⊥⊥ A). Nevertheless, as Y (cid:26)(cid:26)⊥⊥A|X, a prediction Yˆ based on the initial score X only would contain the unfair influence of A on X. For example, assume the following linear model: Y = γM, X = αA + βM , with (cid:104)A2(cid:105) = 1 and (cid:104)M 2(cid:105) = 1. A linear predictor of the form Yˆ = θ X mini- p(A) p(M) X mizing (cid:104)(Y − Yˆ )2(cid:105) would have parameters θ = γβ/(α2 + β2), giving p(A)p(M) X Yˆ = γβ(αA + βM )/(α2 + β2), i.e. Yˆ(cid:26)(cid:26)⊥⊥A. Therefore, this predictor would be using the sensitive attribute to form a decision, although implicitly rather than explic- itly. Instead, a predictor explicitly using the sensitive attribute, Yˆ = θ X + θ A, X A would have parameters (cid:18) θ (cid:19) (cid:18) α2 + β2 α (cid:19)−1 (cid:18) γβ (cid:19) (cid:18) γ/β (cid:19) X = = , θ α 1 0 −αγ/β A i.e. Yˆ = γM . Therefore, this predictor would be fair. From the CBN we can see that the explicit use of A can be of help in retrieving M . Indeed, since M(cid:26)(cid:26)⊥⊥A|X, using A in addition to X can give information about M . In general (e.g. in a non-linear setting) it is not guaranteed that using A would ensure Yˆ ⊥⊥ A. Nevertheless, this example shows how explicit use of the sensitive attribute in a model can ensure fairness rather than leading to unfairness. This observation is relevant to one of the simplest fairness definitions, moti- vated by legal requirements, called fairness through unawareness, which states that Yˆ is fair as long as it does not make explicit use of the sensitive attribute A. Whilst this fairness criterion is often indicated as problematic because some of the variables used to form Yˆ could be a proxy for A (such as neighborhood for race), the example above shows a more subtle issue with it. 4.3 Path-Specific Population-level Unfairness In this section, we show that the path-specific effect introduced in Sect. 3 can be used to quantify unfairness in ∆ in complex scenarios. Consider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path A → D, and therefore A → D → Y , is considered unfair, unfairness overall population can be quantified with (cid:104)Y (cid:105) − (cid:104)Y (cid:105) p(Y |a) p(Y |a¯)
A Causal Bayesian Networks Viewpoint on Fairness 13 (coinciding with ATE = (cid:104)Y (cid:105) − (cid:104)Y (cid:105) ) where, for example, A = a and a¯a a p(Ya) a¯ p(Ya¯) A = a¯ indicate female and male applicants respectively. In the more complex case in which the path A → D → Y is considered fair, unfairness can instead be A Q quantified with the path-specific effect along the direct path A → Y , PSE , given by a¯a D Y (cid:104)Y (D )(cid:105) − (cid:104)Y (cid:105) . a a¯ p(Ya(Da¯)) a¯ p(Ya¯) Fig. 7. CBN underlying a college admission scenario. Notice that computing p(Y a(D a¯)) requires knowledge of the CBN. If the CBN structure is not known or estimating its conditional distributions is challenging, the resulting estimate could be imprecise. 4.4 Path-Specific Individual-level Unfairness In the college admission example of Fig. 7 in which the path A → D → Y is considered fair, rather than measuring unfairness overall population, we might want to know e.g. whether a rejected female applicant {an = a, qn, dn, yn = 0} was treated unfairly. We can answer this question by estimating whether the applicant would have been admitted had she been male (A = a¯) along the direct path A → Y from p(Y (D )|A = a, Q = qn, D = dn, Y = yn) (notice that the a¯ a outcome in the actual world, yn, corresponds to p(Y (D )|A = a, Q = qn, D = a a dn) = 1 ). Ya(Da)=yn To understand how this can be achieved, consider the following linear model associated to a CBN with the same structure as the one in Fig. 7 A ∼ Bern(π), Q = θq + (cid:15) , D = θd + θdA + (cid:15) , q a d Y = θy + θyA + θyQ + θyD + (cid:15) , (4) a q d y where (cid:15) , (cid:15) and (cid:15) are unobserved independent zero-mean Gaussian variables. q d y The relationships between A, Q, D, Y and Y (D ) in A a¯ a this model can be inferred from the twin Bayesian (cid:15) network [39] on the left resulting from the inter- d vention A = a along A → D and A = a¯ along D D a A → Y : in addition to A, Q, D and Y , the network (cid:15) y contains the variables Q∗, D a and Y a¯(D a) correspond- ing to the counterfactual world in which A = a¯ along Y Y a¯(D a) A → Y , with Q∗ = θq + (cid:15) , D = θd + θda + (cid:15) , and q a a d (cid:15) q Y a¯(D a) = θy + θ aya¯ + θ qyQ∗ + θ dyD a + (cid:15) y. The two Q Q∗ groups of variables are connected through (cid:15) d, (cid:15) q, (cid:15) y, indicating that the factual and counterfactual worlds share the same unobserved randomness. From this network, we can deduce that Y (D ) ⊥⊥ {A, Q, D, Y }|(cid:15) = {(cid:15) , (cid:15) , (cid:15) }6, and therefore that we can express a¯ a q d y 6 Notice that Y (D ) ⊥⊥ A, but Y (D )(cid:26)(cid:26)⊥⊥A|D. a¯ a a¯ a
14 S. Chiappa and W. S. Isaac p(Y (D )|A = a, Q = qn, D = dn, Y = yn) as a¯ a (cid:90) p(Y a¯(D a)|a, qn, dn, yn) = p(Y a¯(D a)|(cid:15), a(cid:1),(cid:0)q(cid:0)n,(cid:26)d(cid:26)n,(cid:0)y(cid:0)n)p((cid:15)|a, qn, dn, yn) . (5) (cid:15) As p((cid:15)n|a, qn, dn, yn) = δ , p((cid:15)n|a, qn, dn, yn) = δ , and p((cid:15)n y |a,q qn, dn, yn) = δ (cid:15)n y =yn−(cid:15)n q θ= y−qn θ− ayaθ −q θqyqn−d θ dydn, we obtain (cid:15)n d =dn−θd−θ ada p(Y a¯(D a)|A = a, Q = qn, D = dn, Y = yn) = 1 Ya¯(Da)=yn−θaya+θaya¯. Indeed, by expressing Y (D ) as a function of (cid:15)n, (cid:15)n and (cid:15)n, we obtain a¯ a q d y Y (D ) = θy + θya¯ + θyQ∗ + θyD + (cid:15)n a¯ a a q d a y = θy + θya¯ + θy(θq + (cid:15)n) + θy(θd + θda + (cid:15)n) + (cid:15)n = yn − θya + θya¯ . a q q d a d y a a Therefore, as Q is not a descendant of A and D is a descendant of A along a fair path, the outcome in the counterfactual world is obtained by correcting the outcome in the factual world through replacing θya with θya¯. a a Suppose that we want to post-process a learned model (4) to give a prediction yˆn for individual {an = a, qn, dn} based on the counterfactual distribution p(Y (D )|A = a, Q = qn, D = dn), e.g. yˆn = (cid:104)Y (D )(cid:105) — a¯ a a¯ a p(Ya¯(Da)|A=a,Q=qn,D=dn) the resulting model is said to satisfy path-specific counterfactual fairness [11]. By performing a similar reasoning to the one above for p(Y (D )|A = a, Q = a¯ a qn, D = dn), we obtain7 p(Y (D )|A = a, Q = qn, D = dn) = p(Y (D )|Q∗ = qn, D = dn) a¯ a a¯ a a = p(Y |A = a¯, Q = qn, D = dn) , i.e. the counterfactual prediction can be estimated by conditioning Y on qn and dn, and by replacing a with a¯ in the direct path A → Y . In the case in which A → D is also unfair, we have p(Y (D )|A = a, Q = qn, D = dn) = p(Y (D )|Q∗ = qn, D = dn − θda + θda¯) a¯ a¯ a¯ a¯ a¯ a a = p(Y |A = a¯, Q = qn, D = dn − θda + θda¯) , a a i.e. the counterfactual prediction can be estimated by conditioning Y on qn and the corrected version of dn, given by dn − θda + θda¯. In a more complex scenario a a in which e.g. D = f (A, (cid:15) ) for a non-linear function f we can sample (cid:15)n,m from d d p((cid:15) |a, qn, dn) and perform a Monte-Carlo approximation of Eq. (5), obtaining d M 1 (cid:88) p(Y (D )|A = a, Q = qn, D = dn) = p(Y |A = a¯, Q = qn, D = dn,m) , a¯ a¯ M m=1 7 Notice that (cid:104)Y a¯(D a)(cid:105) p(Ya¯(Da)|A=a,Q=qn,D=dn) = (cid:104)Y (cid:105) p(Y |A=a,Q=qn,D=dn) − PSE(cid:1)a a¯a. Indeed (cid:104)Y (cid:105) p(Y |A=a,Q=qn,D=dn) = θy + θ ay + θ qyqn + θ dydn and PSE a¯a = θ ay. This equivalence does not hold in the non-linear setting.
A Causal Bayesian Networks Viewpoint on Fairness 15 where dn,m = f (A = a¯, (cid:15)n,m) can be interpreted as a corrected version of dn. d From the discussion above we can deduce that the general procedure for computing the desired counterfactual outcome is to condition Y on the non- descendants of A, on the descendants of A that are only fairly influenced by A, and on corrected versions of the descendants that are (partially) unfairly influenced by A. 5 Conclusions We used causal Bayesian networks to provide a graphical interpretation of unfairness in a dataset as the presence of an unfair causal path. We used this viewpoint to revisit the recent debate surrounding the COMPAS pretrial risk assessment tool and, more generally, to point out that fairness evaluation on a model requires careful considerations on the patterns of unfairness underlying the training data. We then showed that causal Bayesian networks provide us with a powerful tool to measure unfairness in a dataset and to design fair models in complex unfairness scenarios. Our discussion did not cover difficulties in making reasonable assumptions on the structure of the causal Bayesian network underlying a dataset, nor on the estimations of the associated conditional distributions or of other quantities of interest. These are obstacles that need to be carefully considered to avoid improper usage of this framework. Acknowledgements The authors would like to thank Ray Jiang, Christina Heinze-Deml, Tom Steple- ton, Tom Everitt, and Shira Mitchell for useful discussions. Appendix A Bayesian Networks A graph is a collection of nodes and links connecting pairs of nodes. The links may be directed or undirected, giving rise to directed or undirected graphs respectively. A path from node X to node X is a sequence of linked nodes starting at X and i j i ending at X . A directed path is a path whose links are directed and pointing j from preceding towards following nodes in the sequence. A directed acyclic graph (DAG) is a directed graph with no directed paths X1 X3 X2 X1 X3 X2 starting and ending at the same node. For example, the directed graph in Fig. X4 X4 8(a) is acyclic. The addition of a link (a) (b) from X 4 to X 1 makes the graph cyclic (Fig. 8(b)). A node X with a directed i Fig. 8. Directed (a) acyclic and (b) cyclic link to X is called parent of X . In j j graph. this case, X is called child of X . j i
16 S. Chiappa and W. S. Isaac A node is a collider on a path if it has (at least) two parents on that path. Notice that a node can be a collider on a path and a non-collider on another path. For example, in Fig. 8(a) X is a collider on the path X → X ← X and a 3 1 3 2 non-collider on the path X → X → X . 2 3 4 A node X is an ancestor of a node X if there exists a directed path from X to i j i X . In this case, X is a descendant of X . j j i A Bayesian network is a DAG in which nodes represent random variables and links express statistical relationships between the variables. Each node X in the i graph is associated with the conditional distribution p(X |pa(X )), where pa(X ) i i i is the set of parents of X . The joint distribution of all nodes, p(X , . . . , X ), i 1 I is given by the product of all conditional distributions, i.e. p(X , . . . , X ) = 1 I (cid:81)I p(X |pa(X )). i=1 i i In a Bayesian network, the sets of variables X and Y are statistically indepen- dent given Z (X ⊥⊥ Y | Z) if all paths from any element of X to any element of Y are closed (or blocked ). A path is closed if at least one of the following conditions is satisfied: (a) There is a non-collider on the path which belongs to the conditioning set Z. (b) There is a collider on the path such that neither the collider nor any of its descendants belong to the conditioning set Z. Appendix B EFPRs/EFNRs and Calibration Assume that EFPRs/EFNRs are satisfied, i.e. p(Yˆ = 1|A = 0, Y = 1) = p(Yˆ = 1|A = 1, Y = 1) ≡ p and p(Yˆ = 1|A = 0, Y = 0) = p(Yˆ = 1|A = 1, Y = 0) ≡ Yˆ 1|Y1 p . From Yˆ 1|Y0 pY1|A0 (cid:122) (cid:125)(cid:124) (cid:123) p p(Y = 1|A = 0) p(Y = 1|A = 0, Yˆ = 1) = Yˆ 1|Y1 , p p + p (1 − p ) Yˆ 1|Y1 Y1|A0 Yˆ 1|Y0 Y1|A0 p p p(Y = 1|A = 1, Yˆ = 1) = Yˆ 1|Y1 Y1|A1 , p p + p (1 − p ) Yˆ 1|Y1 Y1|A1 Yˆ 1|Y0 Y1|A1 we see that, to also satisfy p(Y = 1|A = 0, Yˆ = 1) = p(Y = 1|A = 1, Yˆ = 1), (cid:24)w p Ye (cid:24) 1|n A(cid:24)e 0e )d (cid:1) p(cid:0) Y(cid:40)p 1|Yˆ A(cid:40) 1 1|Y ,(cid:40) 1 ip .(cid:40) eY .1|(cid:40) pA(cid:40) Y1 1|+ A0(cid:24)p =Yˆ(cid:24) 1| pY(cid:24) Y0( 1|1 A− 1. (cid:24)p Y(cid:24) 1|A(cid:24) 1)(cid:1) p Y1|A0 = (cid:0) (cid:40)p Yˆ(cid:40) 1|Y(cid:40) 1p(cid:40) Y1|(cid:40) A(cid:40) 0 + (cid:24)p Yˆ(cid:24) 1|Y(cid:24) 0(1 − References 1. AI Now Institute. Litigating algorithms: Challenging government use of algorithmic decision systems, 2018. 2. M. Alexander. The New Jim Crow: Mass Incarceration in the Age of Colorblindness. The New Press, 2012.
A Causal Bayesian Networks Viewpoint on Fairness 17 3. D. A. Andrews and J. Bonta. Level of Service Inventory – Revised. Multi-Health Systems Toronto, 2000. 4. D. A. Andrews, J. Bonta, and J. S. Wormith. The recent past and near future of risk and/or need assessment. Crime & Delinquency, 52(1):7–27, 2006. 5. J. Angwin, J. Larson, S. Mattu, and L. Kirchner. Machine Bias: There’s soft- ware used across the country to predict future criminals. And it’s biased against blacks., May 2016. https://www.propublica.org/article/machine-bias-risk- assessments-in-criminal-sentencing. 6. D. Arnold, W. Dobbie, and C. S. Yang. Racial bias in bail decisions. The Quarterly Journal of Economics, 133:1885–1932, 2018. 7. R. Berk, H. Heidari, S. Jabbari, M. Kearns, and A. Roth. Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research, 2018. 8. M. Bogen and A. Rieke. Help wanted: An examination of hiring algorithms, equity, and bias. Technical report, Upturn, 2018. 9. T. Brennan, W. Dieterich, and B. Ehret. Evaluating the predictive validity of the COMPAS risk and needs assessment system. Criminal Justice and Behavior, 36(1):21–40, 2009. 10. A. Byanjankar, M. Heikkilä, and J. Mezei. Predicting credit risk in peer-to-peer lending: A neural network approach. In IEEE Symposium Series on Computational Intelligence, pages 719–725, 2015. 11. S. Chiappa. Path-specific counterfactual fairness. In Thirty-Third AAAI Conference on Artificial Intelligence, 2019. 12. S. Chiappa and W. S. Isaac. A causal Bayesian networks viewpoint on fairness. In E. Kosta, J. Pierson, D. Slamanig, S. Fischer-Hübner, S. Krenn (eds) Privacy and Identity Management. Fairness, Accountability, and Transparency in the Age of Big Data. Privacy and Identity 2018. IFIP Advances in Information and Communication Technology, volume 547. Springer, Cham, 2019. 13. A. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2):153–163, 2017. 14. A. Chouldechova, E. Putnam-Hornstein, D. Benavides-Prado, O. Fialko, and R. Vaithianathan. A case study of algorithm-assisted decision making in child mal- treatment hotline screening decisions. Proceedings of Machine Learning Research, 81:134–148, 2018. 15. S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq. A computer program used for bail and sentencing decisions was labeled biased against blacks. It’s actually not that clear., October 2016. https://www.washingtonpost.com/news/ monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is- more-cautious-than-propublicas/?utm_term=.8c6e8c1cfbdf. 16. S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq. Algorithmic deci- sion making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, page 797–806, 2017. 17. S. Corbett-Davies and S.Goel. The measure and mismeasure of fairness: A critical review of fair machine learning. CoRR, abs/1808.00023, 2018. 18. P. Dawid. Fundamentals of statistical causality. Technical report, University College London, 2007. 19. J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Black- well, H. Askham, X. Glorot, B. O’Donoghue, D. Visentin, G. van den Driessche, B. Lakshminarayanan, C. Meyer, F. Mackinder, S. Bouton, K. Ayoub, R. Chopra,
18 S. Chiappa and W. S. Isaac D. King, A. Karthikesalingam, C. O. Hughes, R. Raine, J. Hughes, D. A. Sim, C. Egan, A. Tufail, H. Montgomery, D. Hassabis, G. Rees, T. Back, P. T. Khaw, M. Suleyman, J. Cornebise, P. A. Keane, and O. Ronneberger. Clinically appli- cable deep learning for diagnosis and referral in retinal disease. Nature Medicine, 24(9):1342–1350, 2018. 20. W. Dieterich, C. Mendoza, and T. Brennan. COMPAS risk scales: Demonstrating F1-score equity and predictive parity, 2016. 21. C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages 214–226, 2012. 22. L. Eckhouse, K. Lum, C. Conti-Cook, and J. Ciccolini. Layers of bias: A unified approach for understanding problems with risk assessment. Criminal Justice and Behavior, 46:185–209, 2018. 23. V. Eubanks. Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. St. Martin’s Press, 2018. 24. M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 259–268, 2015. 25. A. W. Flores, K. Bechtel, and C. T. Lowenkamp. False positives, false negatives, and false analyses: A rejoinder to "Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks.". Federal Probation, 80(2):38–46, 2016. 26. Harvard Law School. Note: Bail reform and risk assessment: The cautionary tale of federal sentencing. Harvard Law Review, 131(4):1125–1146, 2018. 27. X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising, pages 1–9, 2014. 28. M. Hoffman, L. B. Kahn, and D. Li. Discretion in hiring. The Quarterly Journal of Economics, 133(2):765–800, 2018. 29. W. S. Isaac. Hope, hype, and fear: The promise and potential pitfalls of artificial intelligence in criminal justice. Ohio State Journal of Criminal Law, 15(2):543–558, 2017. 30. J. Kleinberg, S. Mullainathan, and M. Raghavan. Inherent trade-offs in the fair determination of risk scores. In 8th Innovations in Theoretical Computer Science Conference, pages 43:1–43:23, 2016. 31. J. L. Koepke and D. G. Robinson. Danger ahead: Risk assessment and the future of bail reform. Washington Law Review, 93:1725–1807, 2017. 32. K. Kourou, T. P. Exarchos, K. P. Exarchos, M. V. Karamouzis, and D. I. Fotiadis. Machine learning applications in cancer prognosis and prediction. Computational and Structural Biotechnology Journal, 13:8–17, 2015. 33. M. J. Kusner, J. R. Loftus, C. Russell, and R. Silva. Counterfactual fairness. In Advances in Neural Information Processing Systems 30, pages 4069–4079, 2017. 34. K. Lum. Limitations of mitigating judicial bias with machine learning. Nature Human Behaviour, 1(7):1, 2017. 35. K. Lum and W. Isaac. To predict and serve? Significance, 13(5):14–19, 2016. 36. M. Malekipirbazari and V. Aksakalli. Risk assessment in social lending via random forests. Expert Systems with Applications, 42(10):4621–4631, 2015. 37. S. G. Mayson. Bias in, bias out. Yale Law School Journal, 128, 2019.
A Causal Bayesian Networks Viewpoint on Fairness 19 38. S. Mitchell, E. Potash, and S. Barocas. Prediction-based decisions and fairness: A catalogue of choices, assumptions, and definitions, 2018. 39. J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000. 40. J. Pearl, M. Glymour, and N. P. Jewell. Causal Inference in Statistics: A Primer. Wiley, 2016. 41. C. Perlich, B. Dalessandro, T. Raeder, O. Stitelman, and F. Provost. Machine learning for targeted display advertising: Transfer learning in action. Machine learning, 95(1):103–127, 2014. 42. J. Peters, D. Janzing, and B. Schölkopf. Elements of Causal Inference: Foundations and Learning Algorithms. MIT Press, 2017. 43. M. Rosenberg and R. Levinson. Trump’s catch-and-detain policy snares many who call the U.S. home. https://www.reuters.com/investigates/special-report/ usa-immigration-court, June 2018. 44. A. D. Selbst. Disparate impact in big data policing. Georgia Law Review, 52:109–195, 2017. 45. P. Spirtes, C. N. Glymour, R. Scheines, D. Heckerman, C. Meek, G. Cooper, and T. Richardson. Causation, Prediction, and Search. MIT Press, 2000. 46. M. T. Stevenson. Assessing risk assessment in action. Minnesota Law Review, 103, 2017. 47. R. Vaithianathan, T. Maloney, E. Putnam-Hornstein, and N. Jiang. Children in the public benefit system at risk of maltreatment: Identification via predictive modeling. American Journal of Preventive Medicine, 45(3):354–359, 2013. 48. J. Zhang and E. Bareinboim. Fairness in decision-making – the causal explanation formula. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018.
