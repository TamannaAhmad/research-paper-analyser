A Short Note on the Kinetics-700 Human Action Dataset Joa˜o Carreira Eric Noland Chloe Hillier Andrew Zisserman joaoluis@google.com enoland@google.com chillier@google.com zisserman@google.com Abstract of Kinetics YouTube videos and temporal intervals can be obtained from http://deepmind.com/kinetics. We describe an extension of the DeepMind Kinetics hu- man action dataset from 600 classes to 700 classes, where 2. Data Collection Process for each class there are at least 600 video clips from dif- The data collection process evolved from Kinetics-400 to ferent YouTube videos. This paper details the changes in- Kinetics-700, although the overall pipeline is the same: 1) troduced for this new release of the dataset, and includes action class sourcing, 2) candidate video matching, 3) can- a comprehensive set of statistics as well as baseline results didate clip selection, 4) human verification, 5) quality anal- using the I3D neural network architecture. ysis and filtering. In words, a list of class names is created, then a list of candidate YouTube URLs is obtained for each class name, and candidate 10s clips are sampled from the 1. Introduction videos. These clips are sent to humans who decide whether those clips contain the action class that they are supposed The goal of the Kinetics project is to provide a large scale to. Finally, there is an overall curation process including curated dataset of video clips, covering a diverse range of clip de-duplication, and selecting the higher quality classes human actions, that can be used for training and exploring and clips. Full details can be found in the original publica- neural network architectures for modelling human actions tion [7]. in video. This short paper describes the new version of the dataset, called Kinetics-700. The main differences in the data collection process be- tween Kinetics-400, Kinetics-600 and 700 is in steps 1, The new dataset follows the same principles as Kinetics- 2 and 4: how action classes are sourced, how candidate 400 [7] and Kinetics-600 [2]: (i) The clips are from YouTube videos are matched with classes, and human veri- YouTube videos, last 10s, and have a variable resolution fication. In the following we detail these differences and the and frame rate; (ii) for an action class, all clips are from dif- consequences of these changes on the dataset. Note, as well ferent YouTube videos. Kinetics-700 is almost a superset as producing clips for entirely new classes, it is necessary of Kinetics-600: the number of classes is increased from to ‘top up’ existing classes in Kinetics-600 since YouTube 600 to 700, with all but three of the Kinetics-600 classes videos are deleted or unlisted over time (about 3% per year). retained. As in the case of Kinetics-600, Kinetics-700 has It should be noted that the design of the collection pro- 600 or more clips per human action class – this represents cess is not well suited to finding action classes that progress a 30% increase in the number of video clips, from around over time. It is very well suited to continual actions that 500k to around 650k. The statistics of the three Kinetics exist over the length of the video (e.g. ‘juggling’, ‘drum- datasets are detailed in table 1. ming’), but not to those that have a progression from start In the new Kinetics-700 dataset there is a standard val- to middle to end (e.g. ‘dropping plates’, ‘getting out of car’). idation set, for which labels have been publicly released, and also a held-out test set (where the labels are not re- leased). We encourage researchers to report results on the 2.1. Action class sourcing standard validation set, unless they want to compare with The additional classes for Kinetics-700 over Kinetics- participants of the Activity-Net Kinetics challenge where 600 were partly sourced from the lists of actions (or verbs) the performance on the held-out test set can be be measured in recent human action datasets, such as EPIC-Kitchens [4] only through the challenge evaluation website1. The URLs and AVA [5]. Also, some existing classes in Kinetics-600 1http://activity-net.org/challenges/2019/ which were at quite a general level, e.g. ‘picking fruit’, were evaluation.html removed and replaced by a number of fine-grained varia- 1 2202 tcO 71 ]VC.sc[ 2v78960.7091:viXra
Version Train Valid. Test Held-out Test Total Train Total Classes Kinetics-400 [7] 250–1000 50 100 0 246,245 306,245 400 Kinetics-600 [2] 450–1000 50 100 around 50 392,622 495,547 600 Kinetics-700 450–1000 50 100 0 545,317 650,317 700 Table 1: Kinetics Dataset Statistics. The number of clips for each class in the various splits (left), and the totals (right). tions, for example: ‘picking apples’, ‘picking blueberries’. nation of the metadata of each video and the titles of re- We also introduced a number of more imaginative classes, lated ones. Importantly, these representations were compat- such as: ‘making slime’, ‘being in zero gravity’, ‘swimming ible with multiple languages. We combined this with stan- with sharks’. dard title matching to get a robust similarity score between a query and all YouTube videos. This meant that we never ran out of candidates, although the human-verification yield 2.2. Candidate video matching of the selected candidates became lower for smaller similar- In Kinetics-700 we formally separated the ‘class name’ ity values. This procedure generates a far larger candidate from the ‘query text’ used to search for that class. So, pool than simply a binary match between the query text and for example, to obtain the class ‘canoeing or kayaking’, YouTube video title, say. Since the target length of a clip is the query text could be canoeing and kayaking, and both 10s, videos longer than 5 minutes were discouraged. would be used. Another example is ‘abseiling’, which can be queried with both ‘abseiling’ or ‘rappelling’. Further 2.3. Candidate clip selection and yield more, the query text was translated into three languages. Within a video, candidate clips are selected by using im- In Kinetics-600 we had piloted this scheme by using both age classifiers. Image classifiers are available for a large English and Portuguese query texts, but in Kinetics-700 we number of human actions. These classifiers are obtained extended it. We describe next these multiple queries and by tracking user actions on Google Image Search. For ex- how they are matched to the YouTube corpus to obtain can- ample, for a search query “climbing tree”, user relevance didate videos. feedback on images is collected by aggregating across the multiple times that search query is issued. This relevance Multiple queries. In order to get a better and larger pool feedback is used to select a high-confidence set of images of candidates for a class, each query text was automatically that can be used to train a “climbing tree” image classifier. translated from English into three languages: French, Por- Classifiers corresponding to the class name are run at the tuguese, and Spanish. These are three out of six languages with the most native speakers in the world2, and have large frame level over the selected videos for that class, and clips extracted around the top k responses (where k = 2). In YouTube communities. We found that the machine transla- cases where we could not find classifiers for the class name, tion had adequate quality, though sometimes it introduced we used classifiers related to the query texts. ambiguity. The query texts in all four languages were used to obtain candidate videos. 2.4. Human verification Having multiple languages had the positive side effect of also promoting slightly greater dataset diversity by incor- The first and main annotation task in our pipeline asks porating a more well-rounded range of cultures, ethnicities human annotators if a clip contains a particular action. This and geographies. In terms of continents, more than 50% of step was the same as in previous years for Kinetics-700. the clips are sourced from North America. However, the A difference to previous years was in the final human fraction of clips from Latin America increased from 3% in annotation stage, which we previously did not crowdsource Kinetics-400 to 8% in Kinetics-700, thanks to adding Span- and instead did ourselves: we would go over each individ- ish and Portuguese language queries. Africa is still the least ual class and look at all its animated-gif thumbnails while represented continent, increasing from 0.8% in Kinetics- taking into account potentially confusing classes (derived 400 to 1% in Kinetics-700. These numbers are based on from classifier outputs). Sometimes class names may allow the 90% of videos that contained location information. for multiple types of videos – e.g. a class named “jumping into pool” could have people diving or just jumping. If we Matching query text to YouTube videos. Rather than had a competing “diving” class then we would try to remove matching directly using textual queries we found it bene- diving videos from “jumping into pool”. ficial to use weighted ngram representations of the combi- This was a painstaking manual effort, which we tried to 2According to https://www.babbel.com/en/magazine/the-10-most- crowdsource this year. Since crowdsourcing requires limit- spoken-languages-in-the-world/ ing the size of individual tasks, we divided class thumbnails
Rank Class Yield Acc. type Valid Test 1 busking 0.9227 Top-1 58.7 57.3 2 spinning poi 0.9227 Top-5 81.7 79.9 3 rope pushdown 0.9091 100.0 − avg(Top-1,Top-5) 29.8 31.4 4 front raises 0.8864 5 zumba 0.8864 Table 3: Performance of an I3D model with RGB inputs on 6 country line dancing 0.8727 the Kinetics-700 dataset, without any test time augmenta- 7 ice skating 0.8636 tion (processing a center crop of each video convolutionally 8 shearing sheep 0.8636 in time). The first two rows show F1-score in percentage, 9 arm wrestling 0.8636 the last one shows the metric used at the Kinetics challenge 10 bench pressing 0.8545 hosted by the ActivityNet workshop. 11 playing squash or racquetball 0.8455 12 playing accordion 0.8318 may have been replaced if the original videos have been Table 2: The classes that have the highest yield – measured deleted). For the other classes, we renamed one (“pass- as the proportion of candidate clips that were judged posi- ing american football (not in game)” to “passing American tive for that class by three or more annotators. football (not in game)”), and split “chopping vegetables” and “picking fruit” into multiple subclasses. In terms of the train/val/test split, there is a very small into panels of 16 elements and had human workers clean up overlap between the Kinetics-700 test set and Kinetics-600 the classes. Note that this provides them however with a train/val/test/hold out test (under 3%). tighter window into the contents of each class. It is therefore largely safe to use models that have been trained on Kinetics-600 to evaluate the Kinetics-700 test set Yield by class. It is interesting to see which classes gave (the activity-net evaluation website explicitly ignores the the highest and lowest yields in terms of the probability that predictions on those 3% clips when evaluating on the test a candidate clip was voted positive for that class by three set). It is however not safe to train on Kinetics-700 and then or more human annotators. The classes with highest yield evaluate on Kinetics-600 – many of the validation/test clips are given in table 2, and those with lowest yield are listed in from Kinetics-600 are in the training set of Kinetics-700 3! Appendix B. The full list of new classes in Kinetics-700 is given in There are multiple factors involved here: whether the Appendix A. query is text that is used to annotate videos; how general or specific the query text is for obtaining relevant videos 4. Benchmark Performance (for example, “acting in play” is already quite ambiguous in English, and the current automatic translations are to- As a baseline model we used I3D [3], with standard RGB tally off, e.g. for Portuguese it translates into what would videos as input (no optical flow). We trained the model translatee back as “acting in game”); how well the clip is from scratch on the Kinetics-700 training set, picked hyper- selected within a relevant video; and the actual numbers of parameters on validation, and report performance on vali- videos on YouTube for that action class. One notable com- dation and test set. We used 32 P100 GPUs, batch size 5 mon element of the highest yields is that they are the type videos, 64 frame clips for training and 251 frames for test- of actions where the temporal position selected is not im- ing. We trained using SGD with momentum, starting with portant – ‘playing guitar’ will be true at almost any point a learning rate of 0.1, decreasing it by a factor of 10 when over a long temporal period, and the video is easily spec- the loss saturates. Results are shown in table 3. Hardest and ified by the class name; in contrast ‘opening a letter’ only easiest classes are shown in fig. 1. occurs over a very specific and short time interval, and con- The top-1 F1-score on the validation set was 58.7 and sequently could easily be missed in a long video. on the test set was 57.3, which shows that both sets are In general the high yield classes are successful in being similarly hard. On Kinetics-400 the corresponding test set included in the Kinetics release, but conversely, only a small F1-score was 68.4 and on Kinetics-600 it was 71.7, hence proportion of the low yield classes survives. the task overall seems to have became considerably harder. This may partially have to do with the way we have now 3. From Kinetics-600 to Kinetics-700 crowdsourced the human verification stage – it may be that workers did not strive as hard as we previously did to make As mentioned above, Kinetics-700 is an approximate su- classes more unimodal. It is possible also that, since we perset of Kinetics-600 – overall, 597 out of 600 classes are exactly the same in Kinetics-700 (although some of the clips 3This is because we created a fresh held-out test set for Kinetics-700.
in this paper can be found online4. Acknowledgements: The collection of this dataset was funded by DeepMind. The authors would like to thank Jean-Baptiste Alayrac for checking and correcting the French machine translated queries. References [1] Y. Bian, C. Gan, X. Liu, F. Li, X. Long, Y. Li, H. Qi, J. Zhou, S. Wen, and Y. Lin. Revisiting the effectiveness of off- the-shelf temporal modeling approaches for large-scale video classification. arXiv preprint arXiv:1708.03805, 2017. 4 [2] J. Carreira, E. Noland, A. Banki-Horvath, C. Hillier, and A. Zisserman. A short note about Kinetics-600. arXiv preprint arXiv:1808.01340, 2018. 1, 2 [3] J. Carreira and A. Zisserman. Quo Vadis, Action Recogni- tion? New Models and the Kinetics Dataset. In IEEE Inter- national Conference on Computer Vision and Pattern Recog- nition CVPR, 2017. 3 [4] D. Damen, H. Doughty, G. Maria Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, Figure 1: List of 20 easiest and 20 hardest Kinetics-700 W. Price, and M. Wray. Scaling Egocentric Vision: The EPIC- classes sorted by class accuracies obtained using the I3D- KITCHENS Dataset. In Proc. ECCV, 2018. 1 RGB model. [5] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar, et al. AVA: A video dataset of spatio-temporally localized atomic visual actions. CoRR, abs/1705.08421, 4, 2017. 1 collected a full new test set, there is a little distribution shift [6] D. He, F. Li, Q. Zhao, X. Long, Y. Fu, and S. Wen. Exploiting between train and test (but the validation performance is not spatial-temporal modelling and multi-modal fusion for human very different from the test performance). action recognition. arXiv preprint arXiv:1806.10319, 2018. 4 [7] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vi- Kinetics challenge. There was a first Kinetics challenge at jayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M. Su- leyman, and A. Zisserman. The kinetics human action video the ActivityNet workshop in CVPR 2017, using Kinetics- dataset. arXiv preprint arXiv:1705.06950, 2017. 1, 2 400. The second challenge occurred at the ActivityNet workshop in CVPR 2018, this time using Kinetics-600. The performance criterion used in the challenge is the average of A. List of New Human Action Classes in Top-1 and Top-5 error. There was an improvement between Kinetics-700 the winning systems of the two challenges, with error get- This is the list of classes in Kinetics-700 that were not in ting down from 12.4% (in 2017) to 11.0% (in 2018) [1, 6]. Kinetics-600, or that have been renamed. The 2019 challenge featured the new Kinetics-700 dataset and had 15 participating teams. The top team was from JD 1. pouring wine AI Research and obtained 17.9% error, considerably below 2. walking on stilts our baseline – a single RGB I3D model – which obtained 29.3% error. 3. listening with headphones 4. dealing cards 5. sanding wood 5. Conclusion 6. splashing water We have described the new Kinetics-700 dataset, which 7. digging in terms of clip counts is 30% larger than Kinetics-600, 8. chasing and more than doubles the size of the original Kinetics- 9. tossing salad 400 dataset. It represents another step towards our original goal of producing an action classification dataset with 1000 4https://drive.google.com/file/d/164kU_ classes. Slides illustrating some of what has been described MFTKzmefbgOLntuiiTmADutl_x0/view
10. playing cards 50. eating nachos 11. moving baby 51. picking blueberries 12. bouncing ball (not juggling) 52. coughing 13. helmet diving 53. filling cake 14. vacuuming car 54. shouting 15. high fiving 55. playing mahjong 16. picking apples 56. spinning plates 17. swimming with sharks 57. spraying 18. cutting cake 58. pretending to be a statue 19. doing sudoku 59. moving child 20. swimming with dolphins 60. steering car 21. playing american football 61. baby waking up 22. pouring milk 62. treating wood 23. entering church 63. playing piccolo 24. carrying weight 64. letting go of balloon 25. taking photo 65. playing shuffleboard 26. saluting 66. playing road hockey 27. jumping sofa 67. using megaphone 28. exercising arm 68. squeezing orange 29. playing oboe 69. being in zero gravity 30. shooting off fireworks 70. walking with crutches 31. playing nose flute 71. polishing furniture 32. making latte art 72. closing door 33. carving wood with a knife 73. grooming cat 34. making slime 74. laying decking 35. looking in mirror 75. arresting 36. shoot dance 76. rolling eyes 37. checking watch 77. ski ballet 38. playing checkers 78. mixing colours 39. seasoning food 79. metal detecting 40. sieving 80. waxing armpits 41. gargling 81. peeling banana 42. pulling espresso shot 82. cooking chicken 43. curling eyelashes 83. carving marble 44. shredding paper 84. filling eyebrows 45. stacking dice 85. breaking glass 46. surveying 86. playing rounders 47. poaching eggs 87. petting horse 48. pulling rope (game) 88. putting wallpaper on wall 49. uncorking champagne 89. herding cattle
90. playing billiards 21. getting out of a car 0.0153 91. stacking cups 22. checking mail 0.0157 92. blending fruit 23. entering building 0.0177 93. lighting candle 24. signing document 0.0179 94. decoupage 25. cutting in line 0.0179 95. crocheting 26. waiting at crossing 0.0179 96. playing slot machine 27. dunking biscuit 0.0185 97. silent disco 28. checking tickets 0.0188 98. being excited 29. assembling bicycle 0.0196 99. brushing floor 30. exiting building 0.0198 100. opening coconuts 31. unloading the trunk of a car 0.0198 101. milking goat 32. setting up fish tank 0.0198 102. slicing onion 33. cutting squares 0.0201 103. flipping bottle 34. texting 0.0209 35. playing underwater frisbee 0.0212 B. List of Low Yield Classes 36. riding zebra 0.0212 This is the ranked list of classes that have lowest yield, where yield is the probability that a candidate clip was voted positive for that class by three or more human anno- tators. Bold indicates that the class was included in the final dataset; most of the low yield classes were not included. 1. opening letter 0.0019 2. adding fish to aquarium 0.0033 3. getting inside balloon 0.0034 4. comforting 0.0036 5. highlight text 0.0038 6. riding giraffe 0.0047 7. dropping plates 0.0057 8. contemplating 0.0061 9. whispering 0.0101 10. grooming (person) 0.0106 11. boarding train 0.0112 12. buying fast food 0.0114 13. Piling coins up 0.0114 14. looking through telescope 0.0116 15. breaking aquarium 0.0118 16. using a crowbar 0.0127 17. underlining 0.0128 18. instant messaging 0.0133 19. getting into a car 0.0134 20. tossing coin 0.0146
