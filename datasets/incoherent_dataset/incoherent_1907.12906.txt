Published in METRON, Springer, doi.org/10.1007/s40300-019-00155-4. Unsupervised Separation of Dynamics from Pixels Silvia Chiappa and Ulrich Paquet DeepMind, London, UK {csilvia,upaq}@google.com Abstract We present an approach to learn the dynamics of multiple objects from image sequences in an unsupervised way. We introduce a probabilistic model that first generate noisy positions for each object through a separate linear state-space model, and then renders the positions of all objects in the same image through a highly non-linear process. Such a linear representation of the dynamics enables us to propose an inference method that uses exact and efficient inference tools and that can be deployed to query the model in different ways without retraining. 1 Introduction The dynamics of moving objects can be described on a very low-dimensional manifold of positions and velocities, e.g. by using a state-space model represen- tation of Newtonian laws. A wealth of literature in probabilistic tracking [2,4] allows us to ask questions such as where objects are going to be in the future or where they have been in the past from noisy observations of their current positions, if the latent dynamics are known or can be learned. In this paper, we consider the more challenging task of learning the presence of multiple objects and their dynamics in an unsupervised way from image observations. The problem of learning dynamics from pixels has been studied for the simpler case of one object [11,20]. The multiple-object scenario is much harder as an object-to-identity assignment problem is introduced. The unsupervised learning method should be able to reliably disentangle objects from a sequence of images—this requires a recurrent attention mechanism which should learn to track each object separately over time. Deep recurrent neural networks have demonstrated impressive results on pixel-prediction and related tasks [1,8,9,10,19,22,23]. Whilst powerful and easy to design, in such methods the hidden states do not generally correspond to interpretable dynamics. Enforcing a desired hidden-state representation is challenging—attempts to get positions from pixels have so far succeeded only in the supervised scenario [24]. These methods also have shortcomings Silvia Chiappa and Ulrich Paquet contributed equally. 9102 luJ 02 ]VC.sc[ 1v60921.7091:viXra
2 Silvia Chiappa and Ulrich Paquet when asked to infer intermediate images from observed preceding and following images in a sequence. Probabilistic extensions, such as modern stochastic variational approaches to hidden Markov models [12,13,17], can achieve interpretable hidden-state representations and perform rich probabilistic reasoning. The capabilities of these methods can be further enhanced with the use of traditional probabilis- tic inference routines [11,15,18]. This paper contributes an approach in this direction. In Sec. 2.1, we introduce a model for generating images containing multiple objects in which positions are explicitly represented using auxiliary variables. In Sec. 2.2, we leverage this representation to introduce a method for performing inference and learning that makes use of exact and efficient inference techniques. Finally, in Sec. 3 we show how our approach performs on inferring latent positions from image sequences, and on image generation and interpolation, using an artificial dataset representing moving cannonballs. 2 A model for rendering and inferring multiple objects dynamics We wish to learn the dynamics of N objects from sequences of images v ≡ v , . . . , v in 1:T 1 T an unsupervised way. We restrict ourselves to the case in which the objects move indepen- dently in the two-dimensional plane, and N is known; and assume that each image is formed Fig. 1 Two sequences of images by white pixels representing objects positions overlaid in time, each containing and a black background. two cannonballs moving in oppo- Two examples of sequences, each containing site directions. two cannonballs moving in opposite directions, are given in Fig. 1—the images are overlaid in time such that lighter shades correspond to more recent images. The observed dynamics in the pixel space are high-dimensional and non- linear. However, the intrinsic dynamics can be described on the low-dimensional manifold of positions and velocities by simple dynamical systems. In the sections that follow, we show that the explicit representation of such latent dynamics enables us to infer positions using exact and efficient techniques. We also show that our approach enables us to answer different questions using the learned dynamics without the need to retrain or modify the model. 2.1 Generative model We assume that the generative process underlying the observed images consists of two main parts: A part that describes the objects dynamics in the low- dimensional manifold of positions and velocities through a linear state-space model, and a part that renders the latent positions of the N objects into the images through a highly non-linear process.
Unsupervised Separation of Dynamics from Pixels 3 2.1.1 Rendering latent object positions into images We use a set of auxiliary variables an to explicitly represent the latent positions 1:T of object n in the two-dimensional plane. Given the positions of all objects at time t, a1:N ≡ a1, . . . , aN , an image v is generated by recurrently rendering t t t t each an on v . We start with x0 = tanh(θ ), which represents a latent state t t x0 vector from which an empty image canvas is generated, with θ denoting an x0 unknown parameter vector. For n = 1, . . . , N , we iterate αn = sigm(W αan + bα) (latent attention mask) t xˆn = tanh(W xan + bx) (object n’s state contribution) t xn = (1 − αn) ◦ xn−1 + αn ◦ xˆn (state update) , (1) and finally generate the image as v ∼ p (v |a1:N ) = Bernoulli(sigm(W vxN + bv)) . (2) t θ t t The symbol ◦ indicates element-wise vector multiplication, and sigm(a) ≡ 1/(1 + e−a). The W ’s and b’s indicate weight matrices and biases, and are all included in the unknown parameters θ of the generative model. This rendering process is illustrated in Fig. 2(a). The final state xN , which is transformed in Eq. (2) so that v can be sampled, should contain information t from all N objects. To achieve that, the state is iteratively updated through Eq. (1) to incorporate the contribution from object n, xˆn. This is obtained through an attention mask vector αn with elements in the interval [0, 1], which specifies what information from object n should be included and what information from xn−1 should be retained. 2.1.2 Latent dynamics We model the latent positions of each object, an , using a hidden Markov 1:T model with linear Gaussian hidden-state and output, also known as linear Gaussian state-space model (LGSSM) [3,5], i.e. hn = Ahn + u + ηh, ηh ∼ N (ηh; 0, Σ ) , t t−1 t t t t H an = Bhn + ηa, ηa ∼ N (ηa; 0, Σ ) , (3) t t t t t A where N (x; µ, Σ) denotes the density of a Gaussian random variable x with mean µ and covariance Σ. We use the constraints A = [I, δI; 0, I] ∈ R4×4 (where [·, ·; ·, ·] indicates horizontal and vertical matrix concatenation, I ∈ R2×2 denotes the identity matrix, and δ denotes the sampling period) and B = [I, 0] ∈ R2×4 to obtain a description of Newtonian laws, such that the vector h represents positions and velocities, and u the force (which is assumed to be t t a constant u over time). We include δ, u, Σ and Σ in the generative model H A parameters θ. Objects may start moving from disjoint sets of initial positions and velocities; for instance from the left, the right but not the center of v . To allow the model 1
4 Silvia Chiappa and Ulrich Paquet x0 . . . h1 h1 . . . a2 a2 t t+1 t t+1 a1 t x1 z1 a1 t a1 t+1 a1 t a1 t+1 . . . h2 h2 . . . . . . s2 s2 . . . t t+1 t t+1 a2 t x2 z2 a2 t a2 t+1 . . . s1 t s1 t+1 . . . vt vt vt+1 s0 vt vt+1 (a) (b) (c) Fig. 2 (a) The generative model for an image vt, p θ(vt|a1 t:N ), for N = 2, see Eq. (2). Random variables are indicated with circles; observed variables are shaded. Diamond nodes indicate recurrent neural network hidden states. (b) The full generative model, where the initial hn depends on zn, see Eq. (4). Conditioned on zn, the backbone for each object n 1 is modelled by a LGSSM, see Eq. (5). The two red arrows correspond to the conditional density in Fig. 2(a). (c) The inference network for q φ(a1 1: :N T |v1:T ), given as a recurrent set of equations in Eqs. (8) and (9). to not put probability mass on initial positions and velocities that might never be helpful in explaining an , the initial positions and velocities hn are drawn 1:T 1 from a K component Gaussian mixture, i.e. K K (cid:88) (cid:88) hn ∼ p (hn) = p (zn = k) p (hn|zn = k) = π N (hn; µ , Σ ) . (4) 1 θ 1 θ θ 1 k 1 k k k=1 k=1 We additionally include π , µ and Σ in the generative model parameters 1:K 1:K 1:K θ. The joint density of all random variables factorizes as N (cid:26) T (cid:27) T (cid:89) (cid:89) (cid:89) p (a1:N , h1:N , z1:N ) = p (an|hn) p (hn|zn) p (zn) p (hn|hn ) , θ 1:T 1:T θ t t θ 1 θ θ t t−1 n=1 t=1 t=2 (5) where p (an|hn) = N (an; Bhn, Σ ) and p (hn|hn ) = N (hn; Ahn + u, Σ ). θ t t t t A θ t t−1 t t−1 H The full generative model combines Eq. (2) with Eq. (5) to yield (cid:26) T (cid:27) (cid:89) p (v , a1:N , h1:N , z1:N ) = p (v |a1:N ) p (a1:N , h1:N , z1:N ) . θ 1:T 1:T 1:T θ t t θ 1:T 1:T t=1 The backbone of the model is illustrated in Fig. 2(b). The advantage of this formulation for the latent dynamics is that quantities such as the smoothed distribution p (hn|an , zn), the likelihood p (an |zn), or θ t 1:T θ 1:T the most likely mixture component argmax p (zn = k|an ), can be computed k θ 1:T exactly in O(T ) operations, using message passing algorithms such as the Kalman filtering and Rauch-Tung-Striebel smoothing [3,5].
Unsupervised Separation of Dynamics from Pixels 5 2.2 Inference and Learning The non-linearity of the rendering process makes the computation of p (v ), θ 1:T p (a1:N , h1:N , z1:N |v ), and of quantities like p (an|v ) needed for estimat- θ 1:T 1:T 1:T θ t 1:T ing the positions of object n, intractable. We address this problem using a recent approach to variational methods known as variational auto-encoding (VAE) [16,21]. The basic principle of variational methods is to introduce a tractable approx- imating distribution1 q (a1:N , h1:N , z1:N |v ) to the intractable distribution φ 1:T 1:T 1:T p (a1:N , h1:N , z1:N |v ) via the Kullback-Leibler divergence θ 1:T 1:T 1:T (cid:16) (cid:13) (cid:17) KL q (a1:N , h1:N , z1:N |v ) (cid:13) p (a1:N , h1:N , z1:N |v ) . φ 1:T 1:T 1:T (cid:13) θ 1:T 1:T 1:T Given that (cid:28) (cid:29) (cid:13) q(a, h, z|v) KL(q(a, h, z|v) (cid:13) p(a, h, z|v)) = log p(a, h, z|v) q(a,h,z|v) (cid:10) (cid:11) = log q(a, h, z|v) − log p(a, h, z, v) + log p(v) ≥ 0 , q(a,h,z|v) (cid:10) (cid:11) where we omitted super and subscript indices and used the notation · q(·) to indicate averaging wrt q(·), we obtain a lower bound F on log p (v), i.e. θ,φ θ log p (v) ≥ F with θ θ,φ (cid:10) (cid:11) (cid:10) (cid:11) F = − log q(a, h, z|v) + log p(a, h, z, v) . θ,φ q(a,h,z|v) q(a,h,z|v) If F were tractable and we were able to perform marginalization on q (a, h, z|v), θ,φ φ we could find the optimal q (a, h, z|v) (parameters φ) and θ by maximizing φ the bound; see [6,7] for traditional approaches to variational methods in the temporal setting. However, this is not the case for our generative model choice, and thus we instead use the more recent VAE approach to variational methods, where a Monte-Carlo approximation of the intractable F is deployed. θ,φ The VAE approach consists in rewriting the bound in the form F = θ,φ (cid:10) (cid:11) f ((cid:15) ) for a parameter free distribution q ((cid:15) ), such that the θ,φ 1:T q(cid:15)((cid:15)1:T ) (cid:10)(cid:15) 1:T (cid:11) gradient of F with respect to φ is given by ∇ F = ∇ f ((cid:15) ) — θ,φ φ θ,φ φ θ,φ 1:T q(cid:15)((cid:15)1:T ) this is often called reparemetrization trick. We can then approximate the gradient with the Monte-Carlo estimate M (cid:10) ∇ f ((cid:15) )(cid:11) ≈ 1 (cid:88) ∇ f ((cid:15)m ), (cid:15)m ∼ q ((cid:15)m ) . (6) φ θ,φ 1:T q(cid:15)((cid:15)1:T ) M φ θ,φ 1:T 1:T (cid:15) 1:T m=1 In our case, the formulation of the latent dynamics described above enables us to avoid employing a full approximation of p (a1:N , h1:N , z1:N |v ), and instead θ 1:T 1:T 1:T 1 Whilst in practice we need to consider all observed sequences in the KL, to simplify the notation we focus the exposition on one sequence only.
6 Silvia Chiappa and Ulrich Paquet to decompose this distribution as a product of the exact tractable distribution p (h1:N , z1:N |a1:N ) and an approximation q (a1:N |v ) of p (a1:N |v ), i.e. θ 1:T 1:T φ 1:T 1:T θ 1:T 1:T p (a1:N , h1:N , z1:N |v ) = p (h1:N , z1:N |a1:N ,(cid:24)v (cid:24) ) p (a1:N |v ) θ 1:T 1:T 1:T θ 1:T 1:T 1:T θ 1:T 1:T (cid:124) (cid:123)(cid:122) (cid:125) =(cid:81)N n=1 pθ(hn 1:T ,zn|an 1:T ) (cid:26) N (cid:27) (cid:89) ≈ p (hn , zn|an ) q (a1:N |v ) . θ 1:T 1:T φ 1:T 1:T n=1 Thanks to this representation, the bound can be expressed as (cid:10) (cid:8) (cid:11) F θ,φ = − log[q(a|v) q(h, z|a, v(cid:1))] + log[p(v|a,(cid:8)h, z)p(a, h, z)] q(a|v)q(h,z|a,(cid:1)v) (cid:124) (cid:123)(cid:122) (cid:125) p(h,z|a) (cid:28) q(a|v) (cid:29) (cid:68) (cid:69) = − log + − log p(h, z|a) + log p(a, h, z) p(v|a) q(a|v) (cid:124) (cid:123)(cid:122) (cid:125) q(a|v)p(h,z|a) log p(a) (cid:28) (cid:29) q(a|v) (cid:10) (cid:11) = − log + log p(a) . p(v|a) q(a|v) q(a|v) This gives F = (cid:10) log p (v |a1:N )(cid:11) − KL(cid:16) q (a1:N |v ) (cid:13) (cid:13) p (a1:N )(cid:17) . (7) θ,φ θ 1:T 1:T qφ(a1 1: :N T |v1:T ) φ 1:T 1:T (cid:13) θ 1:T We model q (a1:N |v ) using a recurrent neural network with states s1:N as φ 1:T 1:T 1:T in Fig. 2(c). More specifically we assume an ∼ N (an ; µ (sn), σ2(sn)) and t t φ t φ t use the reparametrization an = µ (sn) + σ (sn) ◦ (cid:15)n, under the assumption t φ t φ t t q ((cid:15)1:N ) = (cid:81) q ((cid:15)n) with (cid:15)n ∼ N ((cid:15)n; 0, I). We describe this inference network (cid:15) 1:T n,t (cid:15) t t t in more details in the next section. 2.2.1 Inference network As shown in Fig. 2(c), the inference network iterates a latent state vector sn t over t = 1, . . . , T and n = 1, . . . , N . Starting with s0 = s0 = 0 at time-step t, t we recurrently iterate βn = sigm(W β[sn , sn−1, v ] + bβ) (latent attention mask) t t−1 t t sˆn = tanh(W s[sn , sn−1, v ] + bs) (object n’s state contribution) t t−1 t t sn = (1 − βn) ◦ sn−1 + βn ◦ sˆn (state update) , (8) t t t t t to compute a vector s1:N for each of the objects at time step t. Similar to the t generative model in Sec. 2.1.1, there is an attention mask βn that specifies t how much of each component of sn−1 we should keep. The mask is a function t of the visible image v , as well as the recurrently computed values for both t the previous object at this time-step, and this object and the previous time- step. There is also a contribution sˆn coming from image v for object n. The t t
Unsupervised Separation of Dynamics from Pixels 7 combination of sn−1 and sˆn is used to update sn. Samples from q (a1:N |v ) t t t φ 1:T 1:T are generated as an = µ (sn) + σ (sn) ◦ (cid:15)n , (9) t φ t φ t t where (cid:15)n ∼ N ((cid:15)n; 0, I). In this computation, external Gaussian noise is inserted t t in a computation graph, and transformed—this ensures that the Monte-Carlo estimate in Eq. (6) is fully differentiable. The W ’s and b’s denote weight matrices and biases, and are included in the inference network parameters φ. The recurrent process at time-step t = 1 depends on sn = tanh(φ ), and an initial state for each object n is learned 0 sn 0 through parameters φ 2. sn 0 Note that v appears in Eqs. (8) and (9) as input to every step n at time-step t t. This is important: To infer the position an, we need to consider the latent t representation sn of object n (actually 1 to n) in the previous image, as well t−1 as sn−1, which contains a rolled-up representation of objects 1 to n − 1 in this t image. Both these representations need to act on v to infer an. t t 2.2.2 Learning In the Kullback-Leibler divergence term in Eq. (7), (cid:10) log q (a1:N ||v )(cid:11) can be φ 1:T 1:T q expressed in analytic form. Both the first term in Eq. (7) and (cid:10) log p (a1:N )(cid:11) θ 1:T q in the KL divergence term can be stochastically estimated using a sample a1:N ∼ q (a1:N |v ). For such a sample, 1:T φ 1:T 1:T N (cid:32) K (cid:33) (cid:88) (cid:88) log p (a1:N ) = log p (zn = k)p (an |zn = k) . (10) θ 1:T θ θ 1:T n=1 k=1 To compute Eq. (10), we take an as “observations”, and for each mixture 1:T component zn = k we run a Kalman filter to obtain the log-likelihood log p (an |zn = k). The objective function is the sum of the bounds in Eq. (7) θ 1:T over all sequences in the training dataset. The negative of this objective is minimized via a stochastic gradient descent algorithm, using mini-batches from the dataset. 2.3 Limitations One of the main limitations of our approach is that, as q (a1:N |v ) does φ 1:t−1 1:t−1 not explicitly incorporate the LGSSM dynamics, the objective function can have many sub-optimal local maxima. As common in the VAE literature, we address such a decoupling between the generative model and variational distribution by annealing the KL term in the bound F to ensure that θ,φ the dynamics are correctly accounted for during training. More advanced 2 In practice, as the state sn 0 encodes which way we can interrogate v1 to infer an 1 , we have obtained better results by learning separate φ sn that depend on the number of objects 0 N in the image.
8 Silvia Chiappa and Ulrich Paquet methods in the literature consist in incorporating the unknown latent dynamics into q (a1:N |v ) by essentially letting the dynamics be a “regularizer” to φ 1:t−1 1:t−1 the parameters φ. “Structured inference networks” provide a framework for achieving this [18]. In [20] we show that this approach leads to better inference and more stable results than annealing for the case of learning the dynamics of one object from pixels—it not obvious how this can be extended to the multiple-object scenario. A more general limitation of our approach is its applicability to images that contain arbitrary backgrounds and objects, and nonlinear interactions between objects. 3 Results In this section, we evaluate our approach on inferring latent positions, and on image generation and interpolation using artificially generated images describing the movement of cannonballs, see the examples shown in Fig. 1. 3.1 Dataset We generated image sequences of length T = 30 describing the movement of up to three cannonballs. Noisy positions an were generated with an LGSSM 1:T formulation of Newtonian laws, as described in Sec. 2.1.2, with sampling period δ = 0.015; force u = −g(0 0.5δ2 0 δ), where g = 9.81 is the gravitational constant; Σ = 0; and Σ = 0.001I. H A Each ball was shot with random shooting angle γ in the interval [40◦, 60◦], from either the left side of the x-axis in the interval [−0.5, −0.1] or the right side of the x-axis in the interval [−0.5, −0.1] + 0.9 max , where max indicates x x the maximum possible displacement at T = 30 when starting in [−0.5, −0.1]. The initial position on the y-axis was sampled in the interval [−0.5, 0.5]. The initial velocity i was sampled in the interval [2, 3]. The resulting velocity on v the x-axis, i cos(γπ/180.0), was flipped in sign if the ball was shot from the v right side of the image. Some examples of trajectories are shown in black in Fig. 3, with circles indicating initial positions. To render the positions into white patches of radius R = 2 in the image, the generated positions a1:N were re-scaled to lie in the interval [R, H − 1 − 1:T R] × [R, W − 1 − R], where H = 48, W = 48 indicate the height and width of the image. This re-scaling ensured that each ball was always fully contained in the image. We also experimented with similar datasets with T = 50 and H = W = 32, obtaining similar results. With H = W = 32 the problem is easier in terms of dimensionality, but the latent positions are less identifiable, as close positions in the latent space might induce the same position in the image.
Unsupervised Separation of Dynamics from Pixels 9 Fig. 3 Ground-truth (black; left) and inferred (blue; right) trajectories from image sequences containing three balls. Initial positions are indicated with a circle. The ground-truth trajec- tories are also showed in red in each of the right figures, illustrating that the model can learn any arbitrary scaling of the dynamics, as long as it adequately explains the observed images. 3.2 Initialization and training The dataset consists of sequences with N ∈ {1, 2, 3} balls. Importantly, as the networks in Fig. 2 can dynamically unroll, the model was trained on all such sequences jointly. We used N to inform the networks of how many steps to unroll for each image sequence. As the initial cannonballs are roughly separated into two main clusters, we assumed two mixture components, i.e. K = 2. Although a higher K would induce a more refined grouping of initial positions and velocities, our experiments indicate that K = 2 was sufficient to obtain accurate results. We experimented with different types of 2 initialization for the LGSSM. Good results were obtained as long as smoothness in the 0 dynamics was enforced so that we could guide the inference network toward smooth trajec- -2 tories (as explained below). One example of such an initialization is to set A and B as -4 above with δ = 0.1, u = 0, Σ = 0.001I, and H Σ A = I; sample the part of µ k corresponding -6 -10 -5 0 5 10 to positions from a standard Gaussian distri- Fig. 4 Example of initialization. bution and set the part corresponding to the velocities to zero; and set Σ = I: This ensures symmetry breaking without k imposing any meaningful prior on the clusters. The absence of force gives rise to positions that form straights lines, as the dots in Fig. 4, which represent the first two dimensions of hn for n = 1, . . . , 3. 1:T The high emission noise gives rise to highly non-smooth an (crosses). This 1:T initialization induces very different dynamics from the ground-truth and does
10 Silvia Chiappa and Ulrich Paquet (a) (b) Fig. 5 (a) Initial, middle, and later stages of training, showing the inference network means µ (sn), . . . , µ (sn ), n = 1, 2, for a batch of image sequences. (b) Ground-truth (black; left), φ 1 φ 30 generated (red; right), and inferred (blue; right) trajectories for a case in which we fail to learn an accurate inference network despite having learned an accurate generative model. not assume any clustering, but encodes prior information that objects move smoothly in time. For the rendering and inference networks in Secs. 2.1.1 and 2.2.1, the weight √ matrices entries were randomly initialized from N ( · ; 0, 1/ d), where d is the number of matrix elements. All biases were initialized to zero. The dimension of the latent state sn was set to 1024. We used the Adam optimizer with t learning rate 0.001, mini-batch size 20, and default values β = 0.9, β = 0.999, 1 2 (cid:15) = 10−8. Training was stopped after 2 × 105 iterations. To guide the inference network toward smooth trajectories we kept the LGSSM parameters fixed to their initial values for the first 104 iterations, and only optimized for all other parameters. This initialization loosely gives temporal coherence between the inference network and the renderer, for different images. The model was then trained jointly, first by changing the objective function by multiplying the KL term in Eq. (7) with a weight β, starting at β = 100, and annealing β down to one. This avoids the LGSSM parameters from too quickly modelling the output of a (still very sub-optimal) q (a1:N |v ), φ 1:T 1:T and stagnating at a local maximum. This process initializes the model, after which end-to-end training proceeds. 3.3 Inferring latent positions from image sequences In Fig. 3 we show estimates of positions from the inference network. The ground- truth trajectories from which each image sequence is generated are shown in black. The inference network means µ (sn), . . . , µ (sn ) are plotted in blue. φ 1 φ T Notice that each plot is scaled differently to aid qualitative evaluation. For that reason, we also selected a run with minimum rotation, but the latent positions can only be retrieved up to re-scaling and rotation. The larger scale of the inferred trajectories is highlighted by re-plotting the ground-truth trajectories (red lines) together with the inferred positions. These figures demonstrate that our model can accurately infer latent positions from the sequences of images in an unsupervised way. In Fig. 5(a), we show the inference network means for a batch of image sequences at three stages of training (for a case in which N = 2), illustrating
Unsupervised Separation of Dynamics from Pixels 11 Fig. 6 Six examples of generated (left) versus ground-truth (right) images overlaid in time. Top row: Our model. Bottom row: ED-LSTM model. how the model learns over time. To highlight the challenge in learning to disentangle the objects dynamics, and the importance of initially strongly regularizing it to be close to the LGSSM during training, Fig. 5(b) shows an example in which we fail to learn an accurate inference network despite having learned accurate LGSSM dynamics. The ground-truth trajectories (black; left) and the trajectories generated from the learned LGSSM dynamics (red; right) are very similar. On the other hand, the inference network means (blue; right) swap the ball when reaching the middle part of the image: Rather than learning a successful attention mechanism, the inference network has learned to attend to the left part of the image for one ball and to the right part of the image for the other ball. 3.4 Multi-step ahead generation of images To benchmark against a standard deep-learning model for multi-step ahead generation of images, we compared our model to an encoder-decoder long-short term memory model (ED-LSTM) [14] on the task of generating the 25 images following five observed images v . 1:5 · · · σ τ σ τ+1 σ τ+2 · · · d. o c n vˆ τ e vˆ τ+1 vˆ τ+2 v τ v τ+1 v τ+2 decod. The structure of the ED-LSTM is represented in Fig. 7: At each time- step t, the hidden state σ generates an t image vˆ through a decoding transfor- t mation. For the first τ = 5 time-steps, σ receives an encoded version of the t previous ground-truth image v , as t−1 well as σ , as input. From time-step t−1 τ + 1 onward, σ receives an encoded Fig. 7 Encoder-decoder long-short term t version of the previous ground-truth memory structure. image v during training, or an encoded version of the previous generated t−1 image vˆ when the model is used in a multi-step ahead generation mode. t−1 We experimented with both convolutional and fully connected encoding and
12 Silvia Chiappa and Ulrich Paquet Fig. 8 We infer an 1:5 for each ball from v1:5, and use the filtered means of p(hn 1:5|an 1:5, zn) for the most likely zn to forward-generate the rest of the red trajectories an . In blue we 6:30 show the trajectories that would be obtained by the inference network if we observed the entire v1:30. The ground-truth trajectories underlying v1:30 are shown in (tiny) black lines. decoding transformations. The best results were obtained with one or two fully connected layers and dimension 2048 for σ . (All layers except the last t were followed by a ReLU activation. The decoder last layer was followed by a sigmoid activation.) We used the same weights and biases initialization and optimizer settings as for our model. To generate images with our model, we first inferred an as the means 1:5 µ (sn), . . . , µ (sn) of the inference network, for each object n. Using the inferred φ 1 φ 5 an , we computed the most likely mixture component as k∗ = argmax [p(zn = 1:5 k k|an ) ∝ p(an |zn = k)π ] by running a Kalman filter. The filtered means of 1:5 1:5 k p(hn|an , zn = k∗) were then used as initial conditions to generate an with 5 1:5 6:30 the learned LGSSM dynamics (Eq. (3)). Finally images v were generated 6:30 through rendering of a1:N (Eqs. (1) and (2)). 6:30 In Fig. 6, we show generated versus ground-truth images overlaid in time, for four sequences (more examples are given in the Appendix). For generation, our averaged test loss over the 25 time-steps was 0.691, using only eight as latent state dimension for the dynamics (h1:2). In contrast, the ED-LSTM’s t test loss was 0.693, with 2048 as latent state dimension for the dynamics. Some examples of trajectories generated by our model are shown in Fig. 8. 3.5 Inference using past and future observations Our model can interpolate missing images (and positions) from past and future images, a task that cannot be solved by the ED-LSTM without model adjustment and retraining. We evaluated how our model performs in inferring the latent positions and images in the intermediate time-steps t = 6, . . . , 25, based on observing the first and last five images v and v . The model should be able to use 1:5 26:30 information from the future to produce more accurate latent positions than the ones that would be estimated by forward generation as in Fig. 8. To solve the task, we first used v to obtain µ (sn), . . . , µ (sn) and the 1:5 φ 1 φ 5 most likely mixture component k∗; images at time-steps t = 6, . . . , 25 were then generated by the mechanism explained above. The inference network was run with the generated images to obtain a warmed-in state s , which was then 25
Unsupervised Separation of Dynamics from Pixels 13 a1 1:30 a2 1:30 Generated vs v1:30 | Interpolated vs v1:30 Fig. 9 Four examples of generation and interpolation. Top: Ground-truth (black), inferred (blue), generated (red), and interpolated (cyan) trajectories. Bottom: Generated (left) versus ground-truth (right) images, and interpolated (left) versus ground-truth (right) images overlaid in time. used as initial state for another run of the inference network with observed images v to infer µ (sn ), . . . , µ (sn ). We finally used µ (sn), . . . , µ (sn) 26:30 φ 26 φ 30 φ 1 φ 5 and µ (sn ), . . . , µ (sn ) as observations in a Rauch-Tung-Striebel smoother φ 26 φ 30 to interpolate the missing trajectories an , assuming that the observations at 6:25 the intermediate time-steps were missing (integrated out from the model). Some examples of obtained results are shown in Fig. 9 (more examples are given in the Appendix). We show the ground-truth trajectories in black, µ (sn), . . . , µ (sn ) in blue, the generated trajectories in red, and the interpo- φ 1 φ 30 lated trajectories in cyan. The interpolation corrects the generated trajectories by bringing them closer to µ (sn), . . . , µ (sn) (obtained by observing the images φ 1 φ 5 at all time-steps) whilst maintaining the smoothness of dynamics constraints. To show the results of this correction mechanism in the pixel space, below the trajectories, from left to right, we show the generated versus ground-truth images and the interpolated versus ground-truth images, overlaid in time. 4 Conclusions This paper describes an unsupervised approach to disentangle the dynamics of objects from pixels. We showed that it is possible for an inference network that is recurrent over both time and object number to sequentially parse each image
14 Silvia Chiappa and Ulrich Paquet to determine latent object positions. The model is regularized with a mixture of linear Gaussian state-space models, which encourages temporal coherence between latent positions extracted from images. Whilst the considered images of cannonballs are much simpler than those that would be encountered in most real- world applications, we nevertheless successfully demonstrated the usefulness of recovering interpretable latent structure in an unsupervised way and, more generally, of building structured generative models for high-dimensional visual stimuli. References 1. M. Babaeizadeh, C. Finn, D. Erhan, R. Campbell, and S. Levine. Stochastic variational video prediction. In 6th International Conference on Learning Representations, pages 1–14, 2018. 2. Y. Bar-Shalom and X. R. Li. Estimation and Tracking: Principles, Techniques, and Software. Artech House, 1993. 3. D. Barber, A. T. Cemgil, and S. Chiappa. Inference and estimation in probabilistic time series models. Bayesian Time Series Models, pages 1–31, 2011. 4. S. Blackman and R. Popoli. Design and Analysis of Modern Tracking Systems. Artech House, 1999. 5. S. Chiappa. Analysis and Classification of EEG Signals using Probabilistic Models for Brain Computer Interfaces. PhD thesis, EPF Lausanne, Switzerland, 2006. 6. S. Chiappa. A Bayesian approach to switching linear Gaussian state-space models for unsupervised time-series segmentation. In Proceedings of the Seventh International Conference on Machine Learning and Applications, pages 3–9, 2008. 7. S. Chiappa. Explicit-duration Markov switching models. Foundations and Trends in Machine Learning, 7(6):803–886, 2014. 8. S. Chiappa, S. Racanière, D. Wierstra, and S. Mohamed. Recurrent environment simulators. In 5th International Conference on Learning Representations, 2017. 9. E. L. Denton and V. Birodkar. Unsupervised learning of disentangled representations from video. In Advances in Neural Information Processing Systems 30, pages 4414–4423, 2017. 10. C. Finn, I. J. Goodfellow, and S. Levine. Unsupervised learning for physical interaction through video prediction. In Advances in Neural Information Processing Systems 29, pages 64–72, 2016. 11. M. Fraccaro, S. Kamronn, U. Paquet, and O. Winther. A disentangled recognition and nonlinear dynamics model for unsupervised learning. In Advances in Neural Information Processing Systems 30, pages 3604–3613, 2017. 12. M. Fraccaro, S. K. Sønderby, U. Paquet, and O. Winther. Sequential neural models with stochastic layers. In Advances in Neural Information Processing Systems 29, pages 2199–2207, 2016. 13. Y. Gao, E. W. Archer, L. Paninski, and J. P. Cunningham. Linear dynamical neural population models through nonlinear embeddings. In Advances in Neural Information Processing Systems 29, pages 163–171, 2016. 14. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997. 15. M. Johnson, D. K. Duvenaud, A. Wiltschko, R. P. Adams, and S. R. Datta. Composing graphical models with neural networks for structured representations and fast inference. In Advances in Neural Information Processing Systems 29, pages 2946–2954, 2016. 16. D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In 2nd International Conference on Learning Representations, 2014. 17. R. Krishnan, U. Shalit, and D. Sontag. Structured inference networks for nonlinear state space models. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, pages 2101–2109, 2017.
Unsupervised Separation of Dynamics from Pixels 15 18. W. Lin, N. Hubacher, and M. E. Khan. Variational message passing with structured inference networks. In 6th International Conference on Learning Representations, 2018. 19. J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks in Atari games. In Advances in Neural Information Processing Systems 28, pages 2863–2871, 2015. 20. M. Pearce, S. Chiappa, and U. Paquet. Comparing interpretable inference models for videos of physical motion. In Symposium on Advances in Approximate Bayesian Inference, 2018. 21. D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic meta-optimization synthesis and approx- imate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, pages 1278–1286, 2014. 22. N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using LSTMs. In Proceedings of the 32nd International Conference on Machine Learning, pages 843–852, 2015. 23. W. Sun, A. Venkatraman, B. Boots, and J. A. Bagnell. Learning to filter with predictive state inference machines. In Proceedings of the 32nd International Conference on Machine Learning, pages 1197–1205, 2016. 24. N. Watters, A. Tacchetti, T. Weber, R. Pascanu, P. Battaglia, and D. Zoran. Visual interaction networks. CoRR, abs/1706.01433, 2017.
16 Silvia Chiappa and Ulrich Paquet Appendix. Multi-step ahead generation of images & inference using past and future observations Fig. 10 Each plot shows generated (left) versus ground-truth (right) images at time-step 30 (top) and overlaid in time (bottom) for our model.
Unsupervised Separation of Dynamics from Pixels 17 Fig. 11 Each plot shows generated (left) versus ground-truth (right) images at time-step 30 (top) and overlaid in time (bottom) for the ED-LSTM.
18 Silvia Chiappa and Ulrich Paquet Fig. 12 Top: Ground-truth (black), inferred (blue), generated (red), and interpolated (cyan) trajectories. Middle: Generated versus ground-truth images and interpolated versus ground- truth images at time-step 30. Bottom: Generated versus ground-truth images and interpolated versus ground-truth images overlaid in time.
Unsupervised Separation of Dynamics from Pixels 19 Fig. 13 Top: Ground-truth (black), inferred (blue), generated (red), and interpolated (cyan) trajectories. Middle: Generated versus ground-truth images and interpolated versus ground- truth images at time-step 30. Bottom: Generated versus ground-truth images and interpolated versus ground-truth images overlaid in time.
