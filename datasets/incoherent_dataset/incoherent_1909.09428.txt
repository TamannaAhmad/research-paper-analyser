A Critical Analysis of Biased Parsers in Unsupervised Parsing Chris Dyer Ga´bor Melis Phil Blunsom DeepMind London, UK {cdyer,melisgl,pblunsom}@google.com Abstract proach to the classic problem of inferring hierar- chical structure from flat sequences. A series of recent papers has used a pars- In this paper, we make two primary contribu- ing algorithm due to Shen et al. (2018) to tions. First, we show, using the same methodol- recover phrase-structure trees based on prox- ogy, that phrase structure can be recovered from ies for “syntactic depth.” These proxy depths conventional LSTMs comparably well as from the are obtained from the representations learned “ordered neurons” LSTM of Shen et al. (2019), by recurrent language models augmented with which is an LSTM variant designed to mimic cer- mechanisms that encourage the (unsupervised) discovery of hierarchical structure latent in tain aspects of phrasal syntactic processing (§3). natural language sentences. Using the same Second, we provide a careful analysis of the pars- parser, we show that proxies derived from a ing algorithm (§4), showing that in contrast to conventional LSTM language model produce traditional parsers, which can recover any binary trees comparably well to the specialized archi- tree, the COO parser is able to recover only a small tectures used in previous work. However, we fraction of all binary trees (and further, that many also provide a detailed analysis of the parsing valid structures are not recoverable by this parser). algorithm, showing (1) that it is incomplete— that is, it can recover only a fraction of pos- This incomplete support, together with the greedy sible trees—and (2) that it has a marked bias search procedure used in the algorithm, results in for right-branching structures which results in a marked bias for returning right-branching struc- inflated performance in right-branching lan- tures, which in turn overestimates parsing perfor- guages like English. Our analysis shows that mance on right-branching languages like English. evaluating with biased parsing algorithms can Since an adequate model of syntax discovery inflate the apparent structural competence of must be able to account for languages with dif- language models. ferent branching biases and assign valid structural descriptions to any sentence, our analysis indicates 1 Introduction particular care must be taken when relying on this Several recent papers (Shen et al., 2018a; Htut algorithm to analyze the structural competence of et al., 2018; Shen et al., 2019; Li et al., 2019; language models. Shi et al., 2019) have used a new parsing algo- 2 COO Parsers rithm to recover hierarchical constituency struc- tures from sequiantial recurrent language models Fig. 1 gives the greedy top-down parsing algo- trained only on sequences of words. This parser, rithm originally described in Shen et al. (2018a) which we call a COO parser for reasons which we as a system of inference rules (Goodman, 1999).1 will describe below, was introduced by Shen et al. We call this a COO parser, which will be explained (2018a). It operates top-down by recursively split- below in the analysis section (§4). The parser op- ting larger constituents into smaller ones, based on erates on a sentence of length n by taking a vector estimated changes of “syntactic depth” until only of scores s ∈ Rn and recursively splitting the sen- terminal symbols remain (§2). In contrast to pre- vious work, which has mostly used explicit tree- 1Our presentation is slightly different to the one in Shen et al. (2018a), but our notational variant was chosen to facil- structured models with stochastic latent variables, itate comparison with other parsers presented in these terms this line of work is an interestingly different ap- and also to simplify the proof of Prop. 2 (see §4). 9102 peS 02 ]LC.sc[ 1v82490.9091:viXra
WSJ10 PTB23 tence into pairs of smaller and smaller constituents ON-LSTM-1† R 35.2 20.0 until only single terminals remain. Given a con- ON-LSTM-2† R 65.1 47.7 stituent [i, j] and the score vector, only a single ON-LSTM-3† R 54.0 36.6 inference rule ever applies. When the deduction completes, the collection of constituents encoun- LSTM-1 R 58.4 43.7 tered during parsing constitutes the parse. LSTM-2 R 58.4 45.1 Depending on the interpretation of s i, the con- LSTM-1,2 R 60.1 47.0 sequent of the MIDDLE rule can be changed to be LSTM-1 L 43.8 31.8 [i, k] [k + 1, j], which places the word triggering LSTM-2 L 47.4 35.1 the split of [i, j] in the left (rather than right) child LSTM-1,2 L 46.3 33.8 constituent. We refer to these variants as the L- and R-variants of the parser. We analyze the al- Table 1: F scores using the same evaluation setup as 1 gorithm, and the implications of the these variants Shen et al. (2019). Numbers in the model name give below (§4), but first we provide a demonstration the layer s was extracted from. R/L indicates which of how this algorithm can be used to recover trees variant of the parsing algorithm was used. Results with from sequential neural language models. † are reproduced from Table 2 of Shen et al. (2019). 3 Unsupervised Syntax in LSTMs supervised parse evaluation) are shown in Tab. 1, with the ON-LSTM of Shen et al. (2019) pro- Shen et al. (2019) propose a modification to vided as a reference. We note three things. First, LSTMs that imposes an ordering on the memory the F1 scores of the two models are comparable; cells. Rather than computing each forget gate in- however, the LSTM baseline is slightly worse on dependently given the previous state and input as WSJ10. Second, whereas the ON-LSTM model is done in conventional LSTMs, the forget gates in requires that a single layer be selected to com- Shen et al.’s ordered neuron LSTM (ON-LSTM) pute the parser’s scoring criterion (since each layer are tied via a new activation function called a cu- will have a different expected forget depth), the mulative softmax so that when a higher level for- unordered nature of LSTM forget gates means get gate is on, lower level ones are forced to be on our baseline can use gates from all layers jointly. as well. The value for s is then defined to be the i Third, the L-variant of the parser is substantially average forget depth—that is, the average number worse. of neurons that are turned off by the cumulative softmax—at each position i ∈ [1, n]. Intuitively, 4 Analysis of the COO Parser this means that closing more constituents in the tree is linked to forgetting more information about We now turn to a formal analysis of the parser, and the history. In this experiment, we operationalize we show two things: first, that the parser is able the same linking hypothesis, but we simply sum to recover only a rapidly decreasing (with length) the values of the forget gates at each time step (var- fraction of valid trees (§4.1); and second, that the iously at different layers, or all layers together) in parser has a marked bias in favour of returning a conventional LSTM to obtain a measure of how right-branching structures (§4.2). much information is being forgotten when updat- 4.1 Incomplete support ing the recurrent representation with the most re- cently generated word. To ensure a fair compari- Here we characterize properties of the trees that son, both models were trained on the 10k vocab- are recoverable with the COO parser, and describe ulary Penn Treebank (PTB) to minimize cross en- our reason for naming it as we have. tropy; they made use of the same number of pa- Proposition 1. Ignoring all single-terminal rameters; and the best model was selected based bracketings, the R-variant COO parser can on validation perplexity. Details of our LSTM lan- generate all valid binary bracketings that do not guage model are found in Appendix A. contain the contiguous string )((.2 Results on the 7422 sentence WSJ10 set (con- This avoidance of close-open-open leads us to sisting of sentences from the PTB of up to 10 call this the COO parser, where the notation x in- words) and the PTB23 set (consisting of all sen- tences from §23, the traditional PTB test set for 2Proofs of propositions are found in Appendix B.
Premises [1, n] Inference rules [i, j] BINARY j − i = 1 [i, i] [j, j] [i, j] LEFT j − i > 1 ∧ i = arg max s (cid:96) [i, i] [i + 1, j] (cid:96)∈[i,j] [i, j] RIGHT j − i > 1 ∧ j = arg max s (cid:96) [i, j − 1] [j, j] (cid:96)∈[i,j] [i, j] MIDDLE j − i > 1 ∧ k = arg max s (cid:96) ∧ k ∈ [i + 1, j − 1] [i, k − 1] [k, j] (cid:96)∈[i,j] Goals [i, i] ∀i ∈ [1, n] Figure 1: The COO parser as a system of inference rules. Inputs are a vector of scores s ∈ Rn where s is the score i of the ith word. An item [i, j] indicates a consistuent spans words i to j, inclusive. Inference rules are applied as follows: when a constituent matching the item above the line, subject to the constraints on the right is found, then the constituents below the line are constructed. The process repeats until all goals are constructed. dicates that x is forbidden. In Fig. 2 we give an ex- 1.0 ample of an unrecoverable parse for a sentence of 0.8 n = 5 (there are 14 binary bracketings of length-5 0.6 sentences, but the COO parser can only recover 13 of them). 0.4 0.2 0.0 0 5 10 15 20 25 30 35 40 Sentence Length some trees leftwards grow branches 1 2 3 4 5 Figure 2: Example of an unrecoverable tree structure and possible sentence with that structure, ((some trees) ((grow branches) leftwards), which includes the forbid- den string )((. Proposition 2. The number of parses of a string of length n that is recoverable by a COO parser is given by a 3, where n a = 1 a = 1 1 2 n−1 (cid:88) a = 2a + a × a . n n−1 k−1 n−k k=2 Although this sequence grows in Θ(2n), Fig. 3 shows that as the length n of the input increases, the ratio of the extractable parses to the num- ber of total binary parses, which is given by the 3This sequence is https://oeis.org/A082582, which counts a permutation-avoiding path construction that shows up in several combinatorial problems (Baxter and Pud- well, 2015). sesraP elbarevoceR Figure 3: The proportion of valid parses recoverable as a function of sentence length, i.e., a /C . n n−1 (n − 1)th Catalan number, C (Motzkin, 1948), n−1 converges logarithmically to 0. 4.2 Branching direction bias Since not every binary tree can be recovered by a COO parser, we explore to what extent this bi- ases the solutions found by the algorithm. We ex- plore the nature of the bias in two ways. First, in Fig. 4 we plot the marginal probability that [i, j] is a constituent when all binary trees are equiprob- able, and compare it with the probability that the R-COO parser will identify it as a span under a uniform distribution over the relative orderings of the s ’s (since it is the relative orderings that de- i termine the parse structure). As we can see, there is no directionality bias in the uniform tree model (all rows have constant probabilities), but in the R-COO parser, the probability of the right-most constituent [n − (cid:96) + 1, n] of length 1 < (cid:96) < n is twice that of the left-most one [1, (cid:96)], indicating
WSJ10 PTB23 1.0 .31 .31 Left-skew 31.6 (σ = 0.2) 16.8 (σ = 0.1) .20 .20 Uniform 33.7 (σ = 0.2) 18.3 (σ = 0.1) .16 .16 Right-skew 37.5 (σ = 0.2) 19.9 (σ = 0.2) .16 .16 .16 .16 .16 .20 .20 Table 2: Expected F1 score under different distribu- .31 .31 tions of trees. 1.0 1.0 phrases top down and to obtain any possible binary bracketing (Stern et al., 2017; Shen et al., 2018b). 1.0 The locus of the bias in the COO parser is the de- .13 .25 cision rule for splitting a span into two daughters, .14 .29 based on the maximally “deep” word. Since the .17 .33 maximum word in a larger span will still be max- .2 .1 .1 .1 .4 imal in a resulting smaller span, certain configu- .25 .5 rations will necessarily be unreachable. However, .33 .67 at least two alternative scoring possibilities sug- 1.0 1.0 gest themselves: (1) scoring transitions between Figure 4: Probability that [i, j] is a constituent when all words rather than individual words and (2) scor- binary trees are eqiprobable (above), and when all rel- ing spans rather than words. By scoring the O(n) ative orders of syntactic depths are equiprobable, when transitions between words, each potential division decoded by the R-COO parser (below). between i, i + 1 will lie outside of the resulting daughter spans, avoiding the problem of having the maximally scoring element present in both the a right-branching bias. The L-variant marginals parent and the daughter. By scoring the O(n2) (not shown) are the reflection of the R-variant ones spans rather than words or word gaps, a similar across the vertical axis, indicating it has the same argument holds. bias, only reversed in direction. Although these algorithms are well known, they Thus, while the R-variant COO parser fails to have only been used for supervised parsing. The reach a large number of trees, it nevertheless has challenge for using either of these decision rules a bias toward the right-branching syntactic struc- in the context of unsupervised parsing is to find a tures that are common in English. Since parse suitable linking hypothesis that connects the ac- evaluation is in terms of retrieval of spans (not tivations of a sequential language model to hy- entire trees), we may reasonably assume that the potheses about either changes in syntactic depth right-branching bias is more beneficial than the ex- that occur from one word to the next (i.e., scor- istence of unreachable trees (correct though they ing the gaps) or that assign scores to all spans in may be) is harmful. A final experiment supports a sequence. Candidates for such quantities in con- this hypothesis: we run a Monte Carlo simula- ventional networks do not, however, immediately tion to approximate the expected F1 score ob- suggest themselves. tained on WSJ10 and PTB23 under the uniform binary distribution, the left-skewed distribution, 6 Related Work and the right-skewed distribution. These estimates are reported in Tab. 2 and show that the R-variant Searching for latent linguistic structure in the ac- parser confers significant advantages over the uni- tivations of neural networks that were trained on form tree model, and the L-variant parser is worse surface strings has a long history, although the cor- again. relates of syntactic structure have only recently be- gun to be explored in earnest. Elman (1990) used 5 Potential Fixes entropy spikes to segment character sequences Is it possible to fix the parser to remove the biases into words and, quite similarly to this work, Wang identified above and to make the parser complete? et al. (2017) used changes in reset gates in GRU- It is. In general, it is possible to recursively split based autoencoders of acoustic signals to discover
phone boundaries. Hewitt and Manning (2019) question of how hierarchical structure is inferred found they could use linear models to recover the from unannotated sentences. However, an aware- (squared) tree distance between pairs of words ness of the biases of the algorithms being used to as well as depth in the contextual embeddings of assess this question is important. undirected language models. A large number of papers have also used recur- Acknowledgments sively structured networks to discover syntax, re- We thank Adhi Kuncoro for his helpful sugges- fer to a Shen et al. (2019) for a comprehensive list. tions on this writeup. 7 Discussion The learning process that permits children to ac- References quire a robust knowledge of hierarchical structure Andrew M. Baxter and Lara K. Pudwell. 2015. Ascent from unstructured strings of words in their envi- sequences avoiding pairs of patterns. The Electronic ronments has been an area of active study for over Journal of Combinatorics, 22(1):1–23. half a century. Despite the scientific effort—not to mention the reliable success that children learn- Roger A. Brown. 1973. A First Language: The Early Stages. Harvard. ing their first language exhibit—we still have no adequate model of this process. Thus, new model- Noam Chomsky. 1981. Lectures on Government and ing approaches, like the ones reviewed in this pa- Binding. Foris. per, are important. However, an understanding of Jeffrey L. Elman. 1990. Finding structure in time. what the results are telling us is more than sim- Cognitive Science, 14:179–211. ply a question of F1 scores. In this case, a biased parser that is well-matched to English structures Yarin Gal and Zoubin Ghahramani. 2016. A theoret- appears to show that LSTMs operate more syntac- ically grounded application of dropout in recurrent tically than they may actually do. Of course, the neural networks. In Advances in Neural Information Processing Systems, pages 1019–1027. tendency for languages to have consistent branch- ing patterns has been hypothesized to arise from Judit Gervain, Marina Nespor, Reiko Mazuka, Ry- a head-directionality parameter that is set during ota Horie, and Jacques Mehler. 2008. Bootstrap- learning (Chomsky, 1981), and biases of the sort ping word order in prelexical infants: A Japanese– Italian cross-linguistic study. Cognitive Psychology, imposed by the COO parser could be reasonable, 57(1):56–74. if a mechanism for learning the head-directionality parameter were specified.4 When comparing un- Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei supervised parsing algorithms aiming to shed light Wang, and Tie-Yan Liu. 2018. FRAGE: frequency- agnostic word representation. In Advances in Neu- on how children discover structure in language, we ral Information Processing Systems, pages 1334– must acknowledge that the goal is not simply to 1345. obtain the best F1-score, but to do so in a plausi- bly language agnostic way. If one’s goal is not to Joshua Goodman. 1999. Semiring parsing. Computa- understand the general problem of structure, but tional Linguistics, 25(4):573–605. merely to get better parsers, language specific bi- John Hewitt and Christopher D. Manning. 2019. A ases are certainly on the table. However, we argue structural probe for finding syntax in word represen- that the work should make these goals clear, and tations. In Proc. NAACL. that it is unreasonable to mix comparisons across Phu Mon Htut, Kyunghyun Cho, and Samuel Bow- models designed with different goals in mind. man. 2018. Grammar induction with neural lan- This paper has drawn attention to a potential guage models: An unusual replication. In Proc. confound in interpreting the results of experiments EMNLP. using COO parsers. We wish to emphasize again Diederik Kingma and Jimmy Ba. 2014. Adam: A that the works employing this parser represent a method for stochastic optimization. arXiv preprint meaningful step forward in the important scientific arXiv:1412.6980. 4Indeed, children demonstrate knowledge of word order from their earliest utterances (Brown, 1973), and even pre- Bowen Li, Lili Mou, and Frank Keller. 2019. An imi- lexical infants are aware of word order patterns in their first tation learning approach to unsupervised parsing. In language before knowing its words (Gervain et al., 2008). Proc. ACL.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and A Experimental Details Beatrice Santorini. 1993. Building a large annotated corpus of english: The Penn treebank. Computa- For our experiments, we trained a two-layer tional linguistics, 19(2):313–330. LSTM with 950 units and 24M parameters on the Penn Treebank (Marcus et al., 1993, PTB) lan- Toma´sˇ Mikolov, Martin Karafia´t, Luka´sˇ Burget, Jan Cernocky´, and Sanjeev Khudanpur. 2010. Recur- guage modelling dataset with preprocessing from rent neural network based language model. In Proc. Mikolov et al. (2010). Its lack of architectural in- Interspeech. novations makes this model ideal as a baseline. In particular, we refrain from using recent inventions Theodore Motzkin. 1948. Relations between hypersur- face cross ratios, and a combinatorial formula for such as the Mixture of Softmaxes (Yang et al., partitions of a polygon, for permanent preponder- 2018) and FRAGE (Gong et al., 2018). Still, the ance, and for non-associative products. Bulletin of trained model is within 2 perplexity points of the the American Mathematical Society, 54:352–360. state of the art (Gong et al., 2018), achieving 55.8 Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and and 54.6 on the validation and test sets, respec- Aaron Courville. 2018a. Neural language model- tively. ing by jointly learning syntax and lexicon. In Proc. As is the case for all models that are strong on ICLR. small datasets such as the PTB, this one is also Yikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessan- heavily regularized. An (cid:96) penalty controls the 2 dro Sordoni, Aaron Courville, and Yoshua Bengio. magnitude of all weights. Dropout is applied to 2018b. Straight to the tree: Constituency parsing the inputs of the two LSTM layers, to their re- with neural syntactic distance. In Proc. ACL. current connections (Gal and Ghahramani, 2016), Yikang Shen, Shawn Tan, Alessandro Sordoni, and and to the output. Optimization is performed by Aaron Courville. 2019. Ordered neurons: Integrat- Adam (Kingma and Ba, 2014) with β = 0, a set- ing tree structures into recurrent neural networks. In 1 Proc. ICLR. ting that resembles RMSProp without momentum. Dropout is turned off at test time, when we extract Haoyue Shi, Jiayuan Mao, Kevin Gimpel, and Karen values of the forget gate. Livescu. 2019. Visually grounded neural syntax ac- quisition. In Proc. ACL. B Proofs of Propositions Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A Although it was convenient to present the results minimal span-based neural constituency parser. In Proc. ACL. in reverse order, we first prove Proposition 2, the recurrence counting the number of recoverable Yu-Hsuan Wang, Cheng-Tao Chung, and Hung-Yi Lee. parses, and then Proposition 1 since it is used in 2017. Gate activation signal analysis for gated recurrent neural networks and its correlation with proving Proposition 1. phoneme boundaries. In Proc. Interspeech. B.1 Proof of Proposition 2 Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and To derive the recurrence giving the number of re- William W Cohen. 2018. Breaking the softmax bot- tleneck: a high-rank RNN language model. Proc. coverable parses, the general strategy will be to ICLR. exploit the duality between top-down and bottom- up parsing algorithms by reversing the inference system in Fig. 1 and parsing sentences bottom up. This admits a more analyzable algorithm. In this proof, we focus on the default R-variant, although the analysis is identical for the L-variant. We begin by noting that after the top-down MIDDLE rule applies, the new constituent [k, j] will be used immediately to derive a pair of con- stituents [k, k] and [k + 1, j] via either the LEFT or BINARY rule, depending on the size of [k, j]. This is because k is the index of the maximum score in [i, j], therefore it will also be the index of the maximum in the daughter constituent [k, j]. Since there is only a single derivation of [k, j] from its
subconstituents (and vice-versa), we can combine tion), with a single element [k, k], having a sin- these two steps into a single step by transforming gle derivation (by definition). Thus, for a possible the MIDDLE rule as follows: value of k, the contribution to a n is a k−1 × a n−k. Since k may be any value in the range [2, n − 1], MIDDLE(cid:48) [i, k − 1] [[ ki ,, kj] ] [k + 1, j] we have (cid:80)n k=− 21 a k−1 × a n−k−1 in aggregate as the contribution of MIDDLE(cid:48). Thus, combining the contributions of LEFT, RIGHT, and MIDDLE(cid:48), we when j − i > 1 ∧ k = arg max s obtain: l l∈[i,j] ∧ k ∈ [i + 1, j − 1]. n (cid:88)−1 a = 2a + a × a , n n−1 k−1 n−k This ternary form results in a system with the same k=2 number of derivations but is easier to analyze. for n > 2. In the bottom-up version of the parser, we start with the goals of the top-down parser, run the in- B.2 Proof of Proposition 1 ference rules backwards (from bottom to top) and derive ultimately the top-down parser’s premise We prove this in two parts, first we show that the as the unique goal. Since we want to consider string )(( cannot be generated by the parser. Sec- all possible decisions the parser might make, the ond we show that when brackets containing this bottom-up rules are also transformed to disregard string are removed from the set of all binary brack- the constraints imposed by the weight vector. Fi- ets of n symbols, its cardinality is a . n nally, to count the derivations, we use an inside algorithm with each item associated with a num- Part 1. We prove that the string )(( cannot be ber that counts the unique derivations of that item generated by the parser by contradiction. Assume in the bottom-up forest. When combining items that the bracketing produced by the parser con- to build a new item, the weights of the antecedent tains the string )((. To exist in a balanced binary items multiply; when an item may be derived mul- tree, there must be at least two symbols to the left tiple ways, they add (Goodman, 1999). (terminals are unbracketed, therefore the closing Let a refer to the number of possible parses for bracket that has not ended the tree will be at least n a sequence of length n. It is obvious that a = 1 a length-2 constituent); and three symbols to the 1 since the only way to construct single-length items right of the split (if the following material was in the bottom-up parser (i.e., [i, i]) is with the ini- length-2, then two opening brackets would result tialization step (all other rules have constraints that in a unary production, which cannot exist in a bi- build longer constituents). Likewise a = 1 since nary tree). 2 there is only one way to build a constituent of Thus, to obtain the split between ) and (, the length 2, namely the BINARY rule, which com- MIDDLE rule would have applied because of the bines two single-length constituents (each which size constraints on the inference rules. How- can only be derived one way). ever, as we showed in the proof of Prop. 1, after Consider the general case a n where n > 2. the MIDDLE applies, a terminal symbol must be Here, there are three ways of deriving [1, n]: the the left child of the resulting right daughter con- LEFT and RIGHT rules, and the more general stituent, thus we obligatorily must have the se- MIDDLE(cid:48) rule. Both LEFT and RIGHT combine a quence )(x(, where x is any single terminal sym- tree spanning n − 1 symbols with a single termi- bol. This contradicts our starting assumption. nal. The single terminal has, as we have seen, one derivation, and the tree has, by induction, a Part 2. It remains to show that the only unreach- n−1 derivations. Thus, the LEFT and RIGHT rules con- able trees are those that contain )((. Proving this tribute 2a derivations to a . formally is considerably more involved than Part n−1 n It remains to account for the contribution of the 1, so we only outline the strategy. First, we con- MIDDLE(cid:48) rule. To do so, we observe that MIDDLE(cid:48) struct a finite state machine that generates a lan- derives a span [1, n] by combining two arbitrarily guage in {(, ), x} consisting of all strings that do large spans: [1, k −1] and [k +1, n], having deriva- not contain the contiguous substring )((. Second, tion counts a and a respectively (by induc- we intersect this with the following grammar that k−1 n−k
generates all valid binary bracketed expressions: S → (S S) S → x Finally, we remove the bracketing symbols, and show that the number of derivations of the string xn is a . n
