Orthogonal Gradient Descent for Continual Learning Mehrdad Farajtabar Navid Azizan1 Alex Mott Ang Li DeepMind CalTech DeepMind DeepMind Abstract A typical neural network training procedure over a se- quence of different tasks usually results in degraded Neural networks are achieving state of the art performance on previously trained tasks if the model and sometimes super-human performance on could not revisit the data of previous tasks. This phe- learning tasks across a variety of domains. nomenon is called catastrophic forgetting (McCloskey Whenever these problems require learning in and Cohen, 1989; Ratcliff, 1990; French, 1999). Ide- a continual or sequential manner, however, ally, an intelligent agent should be able to learn con- neural networks suffer from the problem of secutive tasks without degrading its performance on catastrophic forgetting; they forget how to those already learned. With the deep learning renais- solve previous tasks after being trained on a sance (Krizhevsky et al., 2012; Hinton et al., 2006; Si- new task, despite having the essential capac- monyan and Zisserman, 2014) this problem has been ity to solve both tasks if they were trained on revived (Srivastava et al., 2013; Goodfellow et al., both simultaneously. In this paper, we pro- 2013) with many follow-up studies (Parisi et al., 2019). pose to address this issue from a parameter One probable reason for this phenomenon is that neu- space perspective and study an approach to ral networks are usually trained by Stochastic Gra- restrict the direction of the gradient updates dient Descent (SGD)—or its variants—where the op- to avoid forgetting previously-learned data. timizers produce gradients that are oblivious to past We present the Orthogonal Gradient Descent knowledge. These optimizers, by design, produce gra- (OGD) method, which accomplishes this goal dients that are purely a function of the current mini- by projecting the gradients from new tasks batch (or some smoothed average of a short window onto a subspace in which the neural network of them). This is a desirable feature when the train- output on previous task does not change and ing data is iid, but is not desirable when the train- the projected gradient is still in a useful direc- ing distribution shifts over time. In this paper, we tion for learning the new task. Our approach present a system where the gradients produced on a utilizes the high capacity of a neural network training minibatch can avoid interfering with gradi- more efficiently and does not require stor- ents produced on previous tasks. ing the previously learned data that might raise privacy concerns. Experiments on com- The core idea of our approach, Orthogonal Gradient mon benchmarks reveal the effectiveness of Descent (OGD), is to preserve the previously acquired the proposed OGD method. knowledge by maintaining a space consisting of the gradient directions of the neural network predictions on previous tasks. Any update orthogonal to this gra- 1 Introduction dient space change the output of the network min- One critical component of intelligence is the ability imally. When training on a new task, OGD projects to learn continuously, when new information is con- the loss gradients of new samples perpendicular to this stantly available but previously presented information gradient space before applying back-propagation. Em- is unavailable to retrieve. Despite their ubiquity in the pirical results demonstrate that the proposed method real world, these problems have posed a long-standing efficiently utilizes the high capacity of the (often over- challenge to artificial intelligence (Thrun and Mitchell, parameterized) neural network to learn the new data 1995; Hassabis et al., 2017). while minimizing the interference with the previously acquired knowledge. Experiments on three common continual learning benchmarks substantiate that OGD achieves state-of-the-art performance without the need Correspondence to farajtabar@google.com. to store the historical data. 1 Work done during an internship at DeepMind. 9102 tcO 51 ]GL.sc[ 1v40170.0191:viXra
Orthogonal Gradient Descent for Continual Learning 2 Preliminaries Consider a continual learning setting in which tasks {T , T , T , . . .} arrive sequentially. When a model is 1 2 3 being trained on task T , any data from previous tasks k {T | t < k} is inaccessible. Each data point (x, y) ∈ T t k is a pair consists of input x ∈ Rd and a label y. For a c- class classification, y is a c-dimensional one hot vector. The prediction of the model on input x is denoted by f (x; w), where w ∈ Rp are parameters (weights) of the low error low error model (neural network). For classification problems, for task B for task A f (x; w) ∈ Rc where f (x; w) is the j-th logit associated j low error to j-th class. for both The total loss on the training set (empirical risk) for task t is denoted by Figure 1: An illustration of how Orthogonal Gradient De- (cid:88) L (w) = L (w), (1) scent corrects the directions of the gradients. g is the origi- t (x,y) nal gradient computed for task B and g˜ is the projection of (x,y)∈Tt g onto the orthogonal space w.r.t the gradient ∇f (x; w∗ ) j A computed at task A. Moving within this (blue) space al- where the per-example loss is defined as lows the model parameters to get closer to the low error (green) region for both tasks. L (w) = (cid:96)(y, f (x; w)), (2) (x,y) and (cid:96)(·, ·) is a differentiable non-negative loss function. 3 Orthogonal Gradient Descent For classification problems, a softmax cross entropy loss is commonly used, i.e., Catastrophic forgetting happens in neural networks c (cid:88) (cid:96)(y, f (x; w)) = − y log a , (3) when the gradient updates with respect to a new task j j are applied to the model without considering previous j=1 tasks. We propose the Orthogonal Gradient Descent (cid:80) where a j = exp f j(x; w)/ k exp f k(x; w) is the j-th method for mitigating this problem, which is based on softmax output. modifying the direction of the updates to account for important directions of previous tasks. Figure 1 shows Two objects that frequently appear throughout the an illustration of the core idea of OGD which con- development of our method are the gradient of the loss, ∇L (w) ∈ Rp, and the gradient of the model, strains the parameters to move within the orthogonal (x,y) ∇f (x; w) ∈ Rp×c, which are both with respect to w, space to the gradients of previous tasks. and it is critically important to distinguish the two. In Suppose that the model has been trained on the task fact, the gradient of the loss, using the chain rule, can A in the usual way until convergence to a parame- be expressed as ter vector w∗ so that the training loss/error is small A or zero, and consider the effect of a small update to ∇L (x,y)(w) = ∇f (x; w)(cid:96)(cid:48)(y, f (x; w)), (4) W ∗ . In the high-dimensional parameter space of the A model, there could be update directions causing large where (cid:96)(cid:48)(·, ·) ∈ Rc denotes the derivative of (cid:96)(·, ·) with changes in the predictions from x ∈ T , while there A respect to its second argument, and ∇f (x; w) is the also exist updates that minimally affect such predic- gradient of the model f with respect to its second ar- tions. In particular, moving locally along the direction gument (i.e., the parameters). For the classification of ±∇f (x; w) leads to the biggest change in model j problem with cross entropy softmax loss, we have prediction f (x; w) given any sample x, while moving j orthogonal to ∇f (x; w) leads to the least change (or ∇f (x; w) = [∇f (x; w); . . . ; ∇f (x; w), ], (5) j 1 c no change, locally) to the prediction of x. Supposing task A has n data points in the stochastic gradient and the derivative of the loss becomes A descent setting, there will be n ×c gradient directions A (cid:96)(cid:48)(y, f (x; w)) = [a 1 − y 1, . . . , a c − y c](cid:62), (6) ∇f j(x; w). In order to guarantee the least change to the predictions on task A, for an update towards task where, ∇f (x; w) ∈ Rc is the gradient of the j-th logit B, the update has to be orthogonal to the n × c di- j A with respect to parameters. rections {∇f (x; w)} . j x∈TA,j=1...c
Orthogonal Gradient Descent for Continual Learning Denoting the gradient of the loss for task B (which can Algorithm 1 Orthogonal Gradients Descent be either stochastic, batch, or full) by g, we propose Input Task sequence T , T , T , . . . learning rate η 1 2 3 to “orthogonalize” it in a way that the new direction Output The optimal parameter w. g˜ satisfies the above requirement, i.e., 1: Initialize S ← {}; w ← w 0 2: for Task ID k = 1, 2, 3, . . . do g˜ ⊥ ∇f (x; w), ∀x ∈ T , j = 1 . . . c . (7) j A 3: repeat In this case, moving along direction g˜ makes the least 4: g ← Stochastic/Batch Gradient for T k at w (cid:80) change to the neural network predictions on the pre- 5: g˜ = g − v∈S proj v(g) vious task. As a result, we utilize the high capacity 6: w ← w − ηg˜ of neural networks more effectively. We know that a 7: until convergence neural network is part of a high dimensional param- 8: for (x, y) ∈ T t and k ∈ [1, c] s.t. y k = 1 do (cid:80) eter space (larger than or comparable to the number 9: u ← ∇f k(x; w) − v∈S proj v(∇f k(x; w)) of data points), so there always exist a direction that 10: S ← S ∪ {u} conforms to the orthogonality condition. In continual leaning, while processing T B, one does not creasing the collection size beyond this provides dimin- have access to T A anymore to compute ∇f j(x; w) at ishing returns. One downside of this method (similar the current parameter w. This means that, as an in- to other state-of-the-art methods such as (Chaudhry herent limitation, we are unable to compute the exact et al., 2018; Lopez-Paz and Ranzato, 2017; Riemer directions that will produce least changes on Task A et al., 2018)) is that the required storage size grows during training on Task B. To tackle this issue note with the number of tasks. An interesting extension to that the neural networks are often overparameterized, our method is to dynamically remove less significant which implies that in a close vicinity of the optimum directions from set S or perform principal component parameter for task A, there lies optimum parameters analysis on the gradient space, which are left for future for both tasks A and B (Azizan and Hassibi, 2018; Li work. and Liang, 2018; Allen-Zhu et al., 2018; Azizan et al., We now proceed to formally introduce OGD-GTL. 2019). For any parameter w in that neighborhood, we can basically approximate ∇f (x; w) ≈ ∇f (x; w∗ ) for Task B’s loss gradients should be perpendicular to the A all x ∈ T . Therefore, we can use ∇f (x; w∗ ) as proxy space of all previous model gradients, namely A A and satisfy S = span{∇f (x, w∗ ) | (x, y) ∈ T ∧k ∈ [1, c]∧y = 1}. k A A k g˜ ⊥ ∇f (x; w∗ ), ∀x ∈ T , j = 1 . . . c, (8) j A A We compute the orthogonal basis for S as {v , v , . . .} 1 2 using the Gram-Schmidt procedure on all gradients for all (batch) loss gradients g˜ of task B. One can com- w.r.t. samples (x , y ) ∈ T in task A. We iteratively pute and store ∇f (x; w∗ ) for all x ∈ T when training i i A A A project them to the previously orthogonalized vectors: on task A is done and task B is introduced. In practice, one does not need all n A × c direc- v 1 = ∇f k1(x 1; w A∗ ) , oti uo sn ls y l{ e∇ arf nj e( dx; iw n) fo} rx m∈T aA t, ij o= n1 ....c Fot ro exp are mse pr lev ,e pt eh re sap mre pv li e- v i = ∇f ki(x i; w A∗ ) − (cid:88) proj vj (∇f (x i; w A∗ )), j<i x, we can compute the gradient with respect to the average of the logits rather than use the individual where, k i represents the ground-truth index such that logits themselves. We call this OGD-AVE in contrast y i,ki = 1, and proj v(u) = (cid:104) (cid:104)u v,, vv (cid:105)(cid:105) v is the projection (vec- to OGD-ALL. Another alternative is to select the sin- tor) of u in the direction of v. Given the orthogonal gle logit corresponding to the ground truth label. For basis S = {v , . . . , v } for the gradient subspace of 1 nA data point x coming from k-th class (y = 1), we try task A, we modify the original gradients g of task B k to only keep ∇f (x; w) invariant. This alternative re- to new gradients g˜ orthogonal to S, i.e., k ferred to as OGD-GTL. Both OGD-GTL and OGD- AVE reduce the storage size by a factor of c. We use (cid:88)nA g˜ = g − proj (g) . (9) OGD-GTL in all of our following experiments and also vi i=1 empirically observe that OGD-GTL slightly outper- The new direction −g˜ is still a descent direction (i.e. forms the other variants of OGD (c.f appendix A.1). (cid:104)−g˜, g(cid:105) ≤ 0) for task B meaning that ∃ (cid:15) > 0 such To further control the amount of memory required for that for any learning rate 0 < η < (cid:15), taking the step this process, we store only a subset of gradients from ηg˜ reduces the loss. each task in our experiments (200 for the Mnist exper- iments). While this potentially misses some informa- Lemma 3.1. Let g be the gradient of loss function tion, we find in practice that it is sufficient and that in- L(w) and S = {v , . . . , v } is the orthogonal basis. 1 n
Orthogonal Gradient Descent for Continual Learning Let g˜ = g − (cid:80)k proj (g). Then, −g˜ is also a descent other. It can be seen as lower bound telling us what i vi direction for L(w). happens if we do nothing to explicitly retain infor- mation from the previous task(s). (4) MTL: Multi Proof. For a vector u to be a descent direction it Task Learning baseline using stochastic gradient de- should satisfy (cid:104)u, g(cid:105) ≤ 0. To begin with, we have scent with full access to previous data. In this setting, during task T , we trained the model on batches con- t k taining all data T . This can be considered a sort of (cid:88) ≤t (cid:104)−g˜, g(cid:105) = (cid:104)−g˜, g˜ + proj vi(g)(cid:105) (10) upper bound on the performance. i=1 Setup. We used a consistent training setup for all k = −(cid:107)g˜(cid:107)2 − (cid:104)g˜, (cid:88) proj (g)(cid:105) . (11) Mnist experiments so that we can directly compare vi the effects of the model across tasks and methods. We i=1 always trained each task for 5 epochs. The number of Since g˜ = g −(cid:80)k proj (g) is orthogonal to the space epochs was chosen to achieve saturated performance i=1 vi spanned by S and (cid:80)k proj (g) is a vector spanned on the first task classification problem. The per- by S, hence (cid:104)g˜, (cid:80)k i p= r1 oj (gv )i (cid:105) = 0. Substituting this formance numbers do not change substantially when i=1 vi trained for more epochs and, crucially, the relative per- into Eq. 11, we have (cid:104)−g˜, g(cid:105) = −(cid:107)g˜(cid:107)2 ≤ 0. There- formance between the different methods is identical fore, −g˜ is a descent direction for L(w) while being with more training epochs. We used a batch size of perpendicular to S. 10 similar to Chaudhry et al. (2018); Lopez-Paz and Ranzato (2017). We found that batch size was not a We can easily extend the method to handle multi- strong factor in the performance, other than in its in- ple tasks. Algorithm 1 presents this general case. In terplay with the number of epochs. For fewer than 5 this work, we apply the proposed Orthogonal Gradi- epochs, the batch size had a noticeable effect because ent Descent (OGD) algorithm to continual learning on it significantly changed the number of batch updates. consecutive tasks. Its application potentially goes be- yond this special case and can be utilized whenever Large learning rates do degrade the performance of one wants the gradient steps minimally interfere with OGD (the larger the learning rate, the more likely a the previous learned data points and potentially re- gradient update violates the locality assumption). We duce the access or iterations over them. chose a learning rate of 10−3, consistent with other studies (Kirkpatrick et al., 2017; Chaudhry et al., It is worth reiterating the distinction made in Sec- 2018), and small enough that decreasing it further did tion 2 between using the gradient of the logits—as not improve the performance. For all experiments the OGD does—and using the gradient of the loss—as same architecture is used. The network is a three-layer many other methods do, including the A-GEM base- MLP with 100 hidden units in two layers and 10 logit line (Chaudhry et al. (2018)) in the next section. outputs. Every layer except the final one uses ReLU As Equation (4) indicates, the gradient of the loss activation. The loss is Softmax cross-entropy, and the ∇L (w) can be zero or close to zero for the exam- (x,y optimizer is stochastic gradient descent. This setting ples that are well fitted ((cid:96)(cid:48)(y, f (x; w)) ≈ 0) carrying is similar to previous works (Chaudhry et al., 2018; effectively low information on the previous tasks. In Kirkpatrick et al., 2017). At the end of every task contrast, OGD works directly with the model (through boundary we performed some processing required by its gradeint ∇f (x; w)) which is the essential informa- the method. For OGD, this means computing the or- tion to be preserved. thogonal gradient directions as described in Section 3. For A-GEM, this means storing some examples from 4 Experiments the ending task to memory. For EWC, this means freezing the model weights and computing the fisher We performed experiments in this section on three con- information. Both OGD and A-GEM need an storage. tinual learning benchmark: Permuted Mnist, Rotated A-GEM for actual data points and OGD for the gradi- Mnist, and Split Mnist. ents of the model on previous tasks. We set the storage Baselines. We considered the following baselines for size for both methods to 200. Last but not least, in comparison purposes. (1) EWC (Kirkpatrick et al., all the experiments the mean and standard deviation 2017): one of the pioneering regularization based of the test error on the hold out Mnist test set are methods that uses fisher information diagonals as im- demonstrated using 10 independent random runs for 2 portant weights. (2) A-GEM (Chaudhry et al., 2018): and 3 task experiments and 5 independent runs for 5 using loss gradients of stored previous data in an in- task experiments. equality constrained optimization. (3) SGD: Stochas- tic Gradient Descent optimizing tasks one after the
Orthogonal Gradient Descent for Continual Learning 0.95 0.90 0.85 0.80 0.75 ycaruccA 1 ksaT 0.95 0.90 0.85 0.80 0.75 ycaruccA 2 ksaT 0.95 0.90 0.85 0.80 0.75 10000 20000 30000 40000 50000 60000 70000 Training Steps ycaruccA 3 ksaT F1-score ± Std. (%) Task 1 Task 2 Task 3 Task 4 Task 5 mtl 93.2 ± 1.3 91.5 ± 0.5 91.3 ± 0.7 91.3 ± 0.6 88.4 ± 0.8 ogd 79.5 ± 2.3 88.9 ± 0.7 89.6 ± 0.3 91.8 ± 0.9 92.4 ± 1.1 a-gem 85.5 ± 1.7 87.0 ± 1.5 89.6 ± 1.1 91.2 ± 0.8 93.9 ± 1.0 ewc 64.5 ± 2.9 77.1 ± 2.3 80.4 ± 2.1 87.9 ± 1.3 93.0 ± 0.5 sgd 60.6 ± 4.3 77.6 ± 1.4 79.9 ± 2.1 87.7 ± 2.9 92.4 ± 1.1 Table 1: Permuted Mnist: The F1-score of models for test examples from the indicated class after being trained on all tasks in sequence, except the multi-task setup (mtl). The best continual learning results are highlighted in bold. mtl We extended this experiments to 5 permutations and ogd tasks in the same manner. For this experiment, we agem evaluated the classifier after training had completed ewc (at the end of task 5) and measured the F1-score for sgd examples from each of task 1. . . 5. Table 4.1 reports these accuracies for OGD and the baseline training methods. The results suggest that the overall perfor- Figure 2: Performance of different methods on permuted mance of OGD is significantly better than EWC and Mnist task. 3 different permutations (p , p , and p ) are 1 2 3 SGD while being on par with A-GEM. used and the model is trained to classify Mnist digits under permutation p for 5 epochs, then under p for 5 epochs 1 2 and then under p 3 for 5 epochs. The vertical dashed lines 4.2 Rotated Mnist represent the points in the training where the permutations switch. The top plot reports the F1-score of the model on We further evaluated our approach on identifying ro- batches of the Mnist test set under p ; the middle plot, 1 tated Mnist digits. The training setup is similar to under p ; and the bottom plot under p . The y-axis is 2 3 truncated to show the details. Note that MTL represents Permuted Mnist except that instead of arbitrary per- a setting where the model is directly trained on all previous mutation, we used fixed rotations of the Mnist digits. tasks. Because we keep constant batch size and number of Here we started with a two task problem: task 1 is epochs, the MTL method effectively sees one third of the to classify standard Mnist digits and then task 2 is to task 3 data that other methods do. This is the reason that classify those digits rotated by a fixed angle. MTL learns slower on task 3 than other methods. Figure 3 shows the F1-score of the model when classi- fying task 1 examples (normal, un-rotated Mnist dig- 4.1 Permuted Mnist its) after the end of training on task 2 (rotated Mnist We tested our method on the Permuted Mnist setup digits). We report this as a function of the angle of described in (Goodfellow et al., 2013) and utilized rotation. One can see that, as the angle of rotation in- in (Kirkpatrick et al., 2017; Chaudhry et al., 2018) creases, the task becomes harder. Even in this harder too. In this setup we generated a series of 3 permuta- setting, we still observe that OGD and A-GEM exhibit tions p , p , and p that shuffle the pixels in an Mnist similar levels of performance. 1 2 3 image. We designated task T as the problem of clas- i In the same way as the previous experiment, we ex- sifying Mnist digits that have been shuffled under per- tended the rotated Mnist experiment to more tasks mutation p . We chose these permutations randomly i by training a classifier on 5 rotated Mnist tasks with so each task is equally hard and so the difference in increasing angle of rotation. We defined the tasks F1-score between examples from task 1 and examples as classification under angles of T = Rot(0◦), T = from task 3 after task 3 has been trained is a measure 1 2 Rot(10◦), . . . , T = Rot(40◦), and train the models in of how much the network is able to remember p . 5 1 that order. Table 2 shows the F1-score of the fully- Figure 2 shows the F1-score of OGD and baselines on trained model at classifying examples from each tasks. the Permuted Mnist task. The plot shows that OGD We can observe that OGD outperforms other methods retains performance on task 1 examples as well as A- on 10, 20, and 30 degree rotations. GEM even after training on task 3. Both methods perform slightly worse than a model that is able to 4.3 Split Mnist train on all previous tasks (MTL), but significantly better than the naive sequential model (SGD) and We also tested OGD in a setting where the labels be- than EWC. tween task 1 and task 2 are disjoint. We followed the
Orthogonal Gradient Descent for Continual Learning 100 90 80 70 60 50 mtl ogd gem ewc sgd a ycaruccA rotation = 10° rotation = 20° rotation = 30° rotation = 40° mtl ogd gem ewc sgd mtl ogd gem ewc sgd mtl ogd gem ewc sgd a a a Figure 3: Rotated Mnist: Accuracies of multiple continual Figure 4: Split Mnist: Accuracies of multiple continual learning methods. Every classifier is trained for 5 epochs learning methods. The training regime is the same as that on standard Mnist and then trained for another 5 epochs of Figure 2. The reported value is the F1-score on task 1 on a variant of Mnist whose images are rotated by the after the model being trained on task 2. Different plots cor- specified angle. The F1-score is computed over the entire respond to different configurations, i.e., different partitions original (un-rotated) Mnist test set after the model being of the Mnist labels into task 1 and task 2. trained on the rotated dataset. Each bar represents the mean F1-score over 10 independent runs and the error bars reflect their standard deviations. MTL represents the (non- F1-score ± Std. (%) Task 1 Task 2 Task 3 Task 4 Task 5 continual) multi-task learning setting where the model is trained with the combined data from all previous tasks. mtl 99.6 ± 0.2 99.8 ± 0.1 98.8 ± 0.2 98.2 ± 0.4 99.1 ± 0.2 ogd 98.6 ± 0.8 99.5 ± 0.1 98.0 ± 0.5 98.8 ± 0.5 99.2 ± 0.3 a-gem 92.9 ± 2.6 96.3 ± 2.1 86.5 ± 1.6 92.3 ± 2.3 99.3 ± 0.2 F1-score ± Std. (%) ewc 90.2 ± 5.7 98.9 ± 0.2 91.1 ± 3.5 94.4 ± 2.0 99.3 ± 0.2 Task 1 Task 2 Task 3 Task 4 Task 5 sgd 88.2 ± 5.9 98.4 ± 0.9 90.3 ± 4.5 95.2 ± 1.0 99.4 ± 0.2 mtl 92.1 ± 0.9 94.3 ± 0.9 95.2 ± 0.9 93.4 ± 1.1 90.5 ± 1.5 ogd 75.6 ± 2.1 86.6 ± 1.3 91.7 ± 1.1 94.3 ± 0.8 93.4 ± 1.1 Table 3: Split Mnist2: The F1-score of models for test a-gem 72.6 ± 1.8 84.4 ± 1.6 91.0 ± 1.1 93.9 ± 0.6 94.6 ± 1.1 examples from the indicated class after being trained on all ewc 61.9 ± 2.0 78.1 ± 1.8 89.0 ± 1.6 94.4 ± 0.7 93.9 ± 0.6 tasks in sequence, except the multi-task setup (mtl). The sgd 62.9 ± 1.0 76.5 ± 1.5 88.6 ± 1.4 95.1 ± 0.5 94.1 ± 1.1 best continual learning results are highlighted in bold. Table 2: Rotated Mnist: The F1-score of models for test examples from the indicated class after being trained on all each containing 5 labels. The tasks are then just to tasks in sequence, except the multi-task setup (mtl). The classify examples from the set associated with each best continual learning results are highlighted in bold. task. Figure 4 shows the F1-score of the fully-trained model to classify images from task T . We report the 1 results for 5 different partitions of the labels into the setup for split Mnist laid out in Zenke et al. (2017) with task sets, to ensure that the partition does not have a some variations. We defined a set of tasks T . . . T , 1 N strong effect on the results. In all cases, we observe the with task T defined by a series of integers t1 . . . tki i i i OGD performs the best, beating A-GEM again. We with 0 ≤ tj ≤ 10 and tj = tj(cid:48) if and only if i = i(cid:48) and i i i(cid:48) also observe that the performance order is preserved j = j(cid:48). For each task T , then, the task is to classify i across different configurations of the experiment. Mnist digits with labels in {tj}. i We again generalized this experiment to a longer se- Because a given task does not contain all labels, we quence of tasks by splitting Mnist into 5 tasks, each used a slightly different architecture for this task com- with two classes. We used a multi-headed architec- pared to other tasks. Instead of having a single output ture as in the 2 task case. We report the F1-score layer containing 10-logits for all the Mnist classes, we of the fully trained model on examples from each of used separate heads for each task, where each head the 5 classes in Table 3. As in the previous case, has the same number of logits as there are classes in we evaluated this on multiple partitions of the labels; the associated task. This means that, for each task T , i the results from other partitions are shown in the ap- the softmax and cross-entropy calculation only runs pendix. In this setting, OGD performs very closely over the logits and labels {tj}. We found that this i to the multi-task training benchmark and consistently model has higher performance under all methods than outperforms the other baselines. a model using a joint head. We began with a two task classification problem, 2The F1-score for different assignments of labels to where the Mnist dataset is split into two disjoint sets tasks in Table 3 can be found in Appendix A.3.
Orthogonal Gradient Descent for Continual Learning 5 Related Work bine online variational inference and recent advances in Monte Carlo methods. Ritter et al. (2018) recur- There is a growing interest in measuring catastrophic sively approximate the posterior after every task with forgetting (Toneva et al., 2018; Kemker et al., 2018), a Gaussian Laplace approximation of the Hessian for evaluating continual learning algorithms (Farquhar continual learning, and Ebrahimi et al. (2019) used un- and Gal, 2018; Hayes et al., 2018; D´ıaz-Rodr´ıguez certainty measures to help continual learning. Schwarz et al., 2018; De Lange et al., 2019; Hsu et al., 2018), et al. (2018) proposed a cycle of active learning (pro- and understating this phenomenon (Nguyen et al., gression) followed by consolidation (compression) that 2019; Farquhar and Gal, 2019). The existing work on requires no architecture growth and no access to or alleviating catastrophic forgetting can be divided into storing of previous data or tasks which is similar in a few categories. spirit to distillation based methods of continual learn- ing Li and Hoiem (2017); Hu et al. (2018). Knowledge The expansion based methods allocate new neurons or Distillation (Hinton et al., 2015) and its many vari- layers or modules to accommodate new tasks while uti- ants(Romero et al., 2014; Mirzadeh et al., 2019) are lizing the shared representation learned from previous useful to retain the previous information. ones. Rusu et al. (2016) proposed progressive neural networks in which parameters for the original task are In (Zenke et al., 2017) each parameter accumulates untouched while the architecture is expanded by al- task relevant information over time, and exploits this locating new sub-networks with fixed capacity to be information to rapidly store new tasks without forget- trained on the new task. Similarly, Xiao et al. (2014) ting old ones. Lee et al. (2017) incrementally match proposed a method in which the network not only the moment of the posterior distribution of the neural grows in capacity, but forms a hierarchical structure network trained on the first and the second task to as new tasks arrive at the model. Yoon et al. (2018) regularize its update on the latter. Other works along proposed a dynamically expanding network that ei- this line are (Aljundi et al., 2018; Kolouri et al., 2019) ther retrain or expand the network capacity upon ar- which penalize the weights based on a Hebbian like rival of a new task with only the necessary number update rule. These approaches are well motivated by of units by splitting/duplicating units and timestamp- neuro-biological models of memory Fusi et al. (2005); ing them. Draelos et al. (2017) used auto-encoder to Kaplanis et al. (2018) and are computationally fast dynamically decide to add neurons for samples with and do not require storing data. However, these con- high loss and whether the older data needs to be re- solidated weights reduce the degree of freedom of the trained or not. Along this idea Jerfel et al. (2019) neural network. In other words, they decrease the ef- proposed to use Dirichlet process mixture of hierar- fective volume of parameter space to search for a con- chical Bayesian models over the parameters of neural figuration that can satisfy both the old and new tasks. networks to dynamically cope with the new tasks. Re- The repetition based methods employ memory systems cently, Li et al. (2019b) proposed to utilize the neu- that store previous data or, alternatively, train a gen- ral architecture search to find the optimal structure erative model for the first task and replay them inter- for each of the sequential tasks. These methods avoid leaved with samples drawn from the new task. Shin storing data and are aligned with neurogenesis in the et al. (2017); Kamra et al. (2017); Zhang et al. (2019); brain Aimone et al. (2009) but may be complex for the Rios and Itti (2018) learned a generative model to cap- current neural network libraries. ture the data distribution of previous tasks, along with In the regularization based approaches, catastrophic the current task’s data to train the new model so that forgetting is tackled by imposing constraints on the the forgetting can be alleviated. Lu¨ders et al. (2016) weight updates of the neural network according to used a Neural Turing Machine that enables agents to some importance measure for previous tasks. The store long-term memories by progressively employing difference lies in the way how importance weights additional memory components. In the context of Re- are computed. In Elastic Weight Consolidation inforcement learning Rolnick et al. (2018) utilized on- (EWC) (Kirkpatrick et al., 2017) the importance policy learning on fresh experiences to adapt rapidly weights are the diagonal values of the Fisher informa- to new tasks, while using off-policy learning with be- tion matrix which approximates the posterior distribu- havioral cloning on replay experience to maintain and tion of the weights. Along this Bayesian perspective, modestly enhance performance on past tasks. Lopez- Titsias et al. (2019) proposed to work over the function Paz and Ranzato (2017) proposed Gradient Episodic space rather than the parameters of a deep neural net- Memory (GEM) to efficiently use an episodic storage work to avoid forgetting a previous task by construct- by following loss gradients on incoming task to the ing and memorizing an approximate posterior belief maximum extent while altering them so that they do over the underlying task-specific function. Employing not interfere with past memories. While minimizing other Bayesian techniques, Nguyen et al. (2017) com-
Orthogonal Gradient Descent for Continual Learning the loss on the current task GEM treats the losses on 6 Conclusion and Outlook the episodic memories of previous tasks as inequality constraints, avoiding their increase but allowing their In this paper, we propose to project the current gra- decrease. Chaudhry et al. (2018) improved GEM by dient steps to the orthogonal space of neural net- changing the loss function and proposed dubbed Aver- work predictions on previous data points. The goal aged GEM (A-GEM), which enjoys the same or even is to minimally interfere with the already learned better performance. Riemer et al. (2018) combined ex- knowledge while gradually stepping towards learning perience replay with optimization based meta-learning new tasks. We have demonstrated that our method to enforce gradient alignment across examples in or- matches or exceeds other state-of-the-art methods on der to learn parameters that make interference based a variety of benchmark experiments. We have ob- on future gradients less likely. A few other works served that OGD is able to retain information over have utilized gradient information to protect previous many tasks and achieved particularly strong results knowledge. He and Jaeger (2018) proposed a variant on the split Mnist benchmark. of the back-propagation algorithm named conceptor- There are several avenues for future study based on aided backprop that shields gradients against degrada- this technique. Firstly, because we cannot store gra- tion of previously learned tasks. Zeng et al. (2018) en- dients for the full datasets there is some forgetting sure that gradient updates occur only in the orthogo- happening. Finding a way to store more gradients nal directions to the input of previous tasks. This class or prioritize the important directions would improve of methods also have their root in neuroscience (Ku- the performance. One can also maintain higher-order maran and McClelland, 2012) making training samples derivatives of the model for a more accurate repre- as identically distributed as possible. However, they sentation of previously learned knowledge, at the ex- need to store a portion of the data or learning a gen- pense of more memory and computation. Secondly, erative model upon them, which might not be possible we have observed that all methods (including ours) in some settings, e.g., with user data when privacy fail considerably when the tasks are dissimilar (for ex- matters. Moreover, many of these methods work with ample rotations larger than 90 degrees for the Mnist the gradients of the loss, which can be close to zero for task). This calls for a lot more future research to be many samples and therefore convey less information invested in this important yet under-explored problem on previous tasks. In contrast, we work with the gra- of continual learning. Thirdly, it is observed that our dients of the model (logits or predictions) which is the method is sensitive to the learning rate and it some- actual knowledge to be preserved on the course of con- times fail to produce comparable results to A-GEM tinual learning. By providing more effective shield of for large learning rates. It’s aligned with our expecta- gradients through projecting to the space of previous tion that the learning rate is determining the locality model gradients, we achieve better protection to previ- and the neighborhood of the search. The gradients of ously acquired knowledge, yielding highly competitive the model predictions at optimal points are a good ap- results in empirical tests compared to others. proximation for the gradients on others if they lie in a Continual Learning as a sub-field in AI has close con- close neighborhood. Further work on coping with this nection and ties to other recent efforts in machine would allows OGD to apply to settings where higher learning. Meta learning algorithms use a data-driven learning rates are desired. Another interesting direc- inductive bias to enhance learning new tasks (Jerfel tion for future research is to extend this idea to other et al., 2019; He and Jaeger, 2018; Vuorio et al., 2018; types of optimizers such as Adam or Adagrad. Al-Shedivat et al., 2017; Riemer et al., 2018). Few Finally, it is worth noting that the implications of the shot learning also serves the same purpose and can be proposed Orothogonal Gradient Descent goes beyond leveraged in continual learning (Wen et al., 2018; Gi- the standard continual learning setup we described. daris and Komodakis, 2018) and vice versa. The way Firstly, it does not require tasks to be identified and we treat previous knowledge (i.e. through the model distinguished. OGD minimally interfere with previ- prediction gradients not the actual data) is also related ously seen data points no matter what class they be- differential private learning (Wu et al., 2017; Li et al., long to. This makes it applicable when the task shift 2018; Pihur et al., 2018; Han et al., 2018) and feder- does not arrive as a distinct event, but rather a grad- ated learning (Bonawitz et al., 2019; Smith et al., 2017; ual shift (He et al., 2019). Moreover, it is applicable to Vepakomma et al., 2018). Multi-task learning (Sorokin standard learning paradigm where one does not have and Burtsev, 2019), curriculum learning (Bengio et al., the luxury of iterating over numerous epochs, as it can 2009), and transfer learning (Pan and Yang, 2009; Li preserve information that has not yet been strongly et al., 2019a) are other related areas helpful to develop encoded in the weights of the network. A principled better continual learning machines that do not catas- extension and verification on common gradient neural trophically forget previous experiences. network training methods is left as future work.
Orthogonal Gradient Descent for Continual Learning Acknowledgements Draelos, T. J., Miner, N. E., Lamb, C. C., Cox, J. A., Vineyard, C. M., Carlson, K. D., Severa, W. M., The authors would like to thank Dilan Gorur, James, C. D., and Aimone, J. B. (2017). Neuro- Jonathan Schwarz, Jiachen Yang, and Yee Whye Teh genesis deep learning: Extending deep networks to for the comments and discussions. accommodate new classes. In International Joint Conference on Neural Networks, pages 526–533. References Ebrahimi, S., Elhoseiny, M., Darrell, T., and Aimone, J. B., Wiles, J., and Gage, F. H. (2009). Com- Rohrbach, M. (2019). Uncertainty-guided contin- putational influence of adult neurogenesis on mem- ual learning with bayesian neural networks. arXiv ory encoding. Neuron, 61(2):187–202. preprint arXiv:1906.02425. Al-Shedivat, M., Bansal, T., Burda, Y., Sutskever, Farquhar, S. and Gal, Y. (2018). Towards robust I., Mordatch, I., and Abbeel, P. (2017). Contin- evaluations of continual learning. arXiv preprint uous adaptation via meta-learning in nonstation- arXiv:1805.09733. ary and competitive environments. arXiv preprint Farquhar, S. and Gal, Y. (2019). A unifying arXiv:1710.03641. bayesian view of continual learning. arXiv preprint Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, arXiv:1902.06494. M., and Tuytelaars, T. (2018). Memory aware French, R. M. (1999). Catastrophic forgetting in con- synapses: Learning what (not) to forget. In Pro- nectionist networks. Trends in cognitive sciences, ceedings of the European Conference on Computer 3(4):128–135. Vision (ECCV), pages 139–154. Fusi, S., Drew, P. J., and Abbott, L. F. (2005). Cas- Allen-Zhu, Z., Li, Y., and Song, Z. (2018). A cade models of synaptically stored memories. Neu- convergence theory for deep learning via over- ron, 45(4):599–611. parameterization. arXiv preprint arXiv:1811.03962. Gidaris, S. and Komodakis, N. (2018). Dynamic few- Azizan, N. and Hassibi, B. (2018). Stochas- shot visual learning without forgetting. In Proceed- tic gradient/mirror descent: Minimax optimal- ings of the IEEE Conference on Computer Vision ity and implicit regularization. arXiv preprint and Pattern Recognition, pages 4367–4375. arXiv:1806.00952. Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., Azizan, N., Lale, S., and Hassibi, B. (2019). Stochas- and Bengio, Y. (2013). An empirical investigation tic mirror descent on overparameterized nonlinear of catastrophic forgetting in gradient-based neural models: Convergence, implicit regularization, and networks. arXiv preprint arXiv:1312.6211. generalization. arXiv preprint arXiv:1906.03830. Han, B., Tsang, I. W., Xiao, X., Chen, L., Fung, S.-f., Bengio, Y., Louradour, J., Collobert, R., and We- and Yu, C. P. (2018). Privacy-preserving stochastic ston, J. (2009). Curriculum learning. In Proceed- gradual learning. arXiv preprint arXiv:1810.00383. ings of the 26th annual international conference on machine learning, pages 41–48. ACM. Hassabis, D., Kumaran, D., Summerfield, C., and Botvinick, M. (2017). Neuroscience-inspired artifi- Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., cial intelligence. Neuron, 95(2):245–258. Ingerman, A., Ivanov, V., Kiddon, C., Konecny, J., Mazzocchi, S., McMahan, H. B., et al. (2019). To- Hayes, T. L., Kemker, R., Cahill, N. D., and Kanan, wards federated learning at scale: System design. C. (2018). New metrics and experimental paradigms arXiv preprint arXiv:1902.01046. for continual learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- Chaudhry, A., Ranzato, M., Rohrbach, M., and El- nition Workshops, pages 2031–2034. hoseiny, M. (2018). Efficient lifelong learning with A-GEM. arXiv preprint arXiv:1812.00420. He, X. and Jaeger, H. (2018). Overcoming catastrophic interference using conceptor-aided meta-optimization synthesis. De Lange, M., Aljundi, R., Masana, M., Parisot, S., In ICLR 2018. Jia, X., Leonardis, A., Slabaugh, G., and Tuyte- laars, T. (2019). Continual learning: A compara- He, X., Sygnowski, J., Galashov, A., Rusu, A. A., Teh, tive study on how to defy forgetting in classification Y. W., and Pascanu, R. (2019). Task agnostic con- tasks. arXiv preprint arXiv:1909.08383. tinual learning via meta learning. arXiv preprint arXiv:1906.05201. D´ıaz-Rodr´ıguez, N., Lomonaco, V., Filliat, D., and Maltoni, D. (2018). Don’t forget, there is more than Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling forgetting: new metrics for continual learning. arXiv the knowledge in a neural network. arXiv preprint preprint arXiv:1810.13166. arXiv:1503.02531.
Orthogonal Gradient Descent for Continual Learning Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A Li, X., Zhou, Y., Wu, T., Socher, R., and Xiong, C. fast learning algorithm for deep belief nets. Neural (2019b). Learn to grow: A continual structure learn- computation, 18(7):1527–1554. ing framework for overcoming catastrophic forget- ting. arXiv preprint arXiv:1904.00310. Hsu, Y.-C., Liu, Y.-C., and Kira, Z. (2018). Re- evaluating continual learning scenarios: A catego- Li, Y. and Liang, Y. (2018). Learning overparameter- rization and case for strong baselines. arXiv preprint ized neural networks via stochastic gradient descent arXiv:1810.12488. on structured data. In Advances in Neural Informa- Hu, W., Lin, Z., Liu, B., Tao, C., Tao, Z., Ma, J., tion Processing Systems, pages 8157–8166. Zhao, D., and Yan, R. (2018). Overcoming catas- Li, Z. and Hoiem, D. (2017). Learning without for- trophic forgetting for continual learning via model getting. IEEE transactions on pattern analysis and adaptation. machine intelligence, 40(12):2935–2947. Jerfel, G., Grant, E., Griffiths, T. L., and Heller, K. A. Lopez-Paz, D. and Ranzato, M. (2017). Gradient (2019). Reconciling meta-learning and continual episodic memory for continual learning. In Advances learning with online mixtures of tasks. In NeurIPS. in Neural Information Processing Systems, pages Kamra, N., Gupta, U., and Liu, Y. (2017). Deep gen- 6467–6476. erative dual memory network for continual learning. Lu¨ders, B., Schl¨ager, M., and Risi, S. (2016). Con- arXiv preprint arXiv:1710.10368. tinual learning through evolvable neural turing ma- Kaplanis, C., Shanahan, M., and Clopath, C. (2018). chines. In NIPS 2016 Workshop on Continual Continual supervised learning with complex Learning and Deep Networks (CLDL 2016). synapses. arXiv preprint arXiv:1802.07239. McCloskey, M. and Cohen, N. J. (1989). Catastrophic Kemker, R., McClure, M., Abitino, A., Hayes, T. L., interference in connectionist networks: The sequen- and Kanan, C. (2018). Measuring catastrophic for- tial learning problem. In Psychology of learning and getting in neural networks. In Thirty-second AAAI motivation, volume 24, pages 109–165. Elsevier. conference on artificial intelligence. Mirzadeh, S.-I., Farajtabar, M., Li, A., and Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, Ghasemzadeh, H. (2019). Improved knowledge J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., distillation via teacher assistant: Bridging the Ramalho, T., Grabska-Barwinska, A., et al. (2017). gap between student and teacher. arXiv preprint Overcoming catastrophic forgetting in neural net- arXiv:1902.03393. works. Proceedings of the national academy of sci- Nguyen, C. V., Achille, A., Lam, M., Hassner, T., Ma- ences, 114(13):3521–3526. hadevan, V., and Soatto, S. (2019). Toward under- Kolouri, S., Ketz, N., Zou, X., Krichmar, J., and Pilly, standing catastrophic forgetting in continual learn- P. (2019). Attention-based structural-plasticity. ing. arXiv preprint arXiv:1908.01091. arXiv preprint arXiv:1903.06070. Nguyen, C. V., Li, Y., Bui, T. D., and Turner, Krizhevsky, A., Sutskever, I., and Hinton, G. E. R. E. (2017). Variational continual learning. arXiv (2012). MNIST classification with deep convolu- preprint arXiv:1710.10628. tional neural networks. In Advances in neural infor- Pan, S. J. and Yang, Q. (2009). A survey on transfer mation processing systems, pages 1097–1105. learning. IEEE Transactions on knowledge and data Kumaran, D. and McClelland, J. L. (2012). General- engineering, 22(10):1345–1359. ization through the recurrent interaction of episodic memories: a model of the hippocampal system. Psy- Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., and chological review, 119(3):573. Wermter, S. (2019). Continual lifelong learning with neural networks: A review. Neural Networks. Lee, S.-W., Kim, J.-H., Jun, J., Ha, J.-W., and Zhang, B.-T. (2017). Overcoming catastrophic forgetting by Pihur, V., Korolova, A., Liu, F., Sankuratripati, incremental moment matching. In Neural informa- S., Yung, M., Huang, D., and Zeng, R. (2018). tion processing systems, pages 4652–4662. Differentially-private” draw and discard” machine learning. arXiv preprint arXiv:1807.04369. Li, A., Hu, H., Mirowski, P., and Farajtabar, M. (2019a). Cross-view policy learning for street navi- Ratcliff, R. (1990). Connectionist models of recog- gation. arXiv preprint arXiv:1906.05930. nition memory: constraints imposed by learning and forgetting functions. Psychological review, Li, C., Zhou, P., Xiong, L., Wang, Q., and Wang, 97(2):285. T. (2018). Differentially private distributed online learning. IEEE Transactions on Knowledge and Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, Data Engineering, 30(8):1440–1453. I., Tu, Y., and Tesauro, G. (2018). Learning
Orthogonal Gradient Descent for Continual Learning to learn without forgetting by maximizing trans- ical study of example forgetting during deep neural fer and minimizing interference. arXiv preprint network learning. arXiv preprint arXiv:1812.05159. arXiv:1810.11910. Vepakomma, P., Gupta, O., Swedish, T., and Raskar, Rios, A. and Itti, L. (2018). Closed-loop gan for con- R. (2018). Split learning for health: Distributed tinual learning. arXiv preprint arXiv:1811.01146. deep learning without sharing raw patient data. arXiv preprint arXiv:1812.00564. Ritter, H., Botev, A., and Barber, D. (2018). On- line structured laplace approximations for overcom- Vuorio, R., Cho, D.-Y., Kim, D., and Kim, J. ing catastrophic forgetting. In Advances in Neural (2018). Meta continual learning. arXiv preprint Information Processing Systems, pages 3738–3748. arXiv:1806.06928. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., Wen, J., Cao, Y., and Huang, R. (2018). Few-shot self and Wayne, G. (2018). Experience replay for con- reminder to overcome catastrophic forgetting. arXiv tinual learning. arXiv preprint arXiv:1811.11682. preprint arXiv:1812.00543. Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Wu, X., Li, F., Kumar, A., Chaudhuri, K., Jha, S., Gatta, C., and Bengio, Y. (2014). Fitnets: Hints for and Naughton, J. (2017). Bolt-on differential pri- thin deep nets. arXiv preprint arXiv:1412.6550. vacy for scalable stochastic gradient descent-based analytics. In Proceedings of the 2017 ACM Inter- Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, national Conference on Management of Data, pages H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., 1307–1322. ACM. and Hadsell, R. (2016). Progressive neural networks. arXiv preprint arXiv:1606.04671. Xiao, T., Zhang, J., Yang, K., Peng, Y., and Zhang, Z. (2014). Error-driven incremental learning in deep Schwarz, J., Czarnecki, W., Luketina, J., Grabska- random forest for large-scale image Barwinska, A., Teh, Y. W., Pascanu, R., and Had- classification. In Proceedings of the 22nd ACM inter- sell, R. (2018). Progress & compress: A scalable national conference on Multimedia, pages 177–186. framework for continual learning. In International ACM. Conference on Machine Learning, pages 4535–4544. Yoon, J., Yang, E., Lee, J., and Hwang, S. J. (2018). Shin, H., Lee, J. K., Kim, J., and Kim, J. (2017). Con- Lifelong learning with dynamically expandable net- tinual learning with deep generative replay. In Ad- works. In Sixth International Conference on Learn- vances in Neural Information Processing Systems, ing Representations. ICLR. pages 2990–2999. Zeng, G., Chen, Y., Cui, B., and Yu, S. (2018). Con- Simonyan, K. and Zisserman, A. (2014). Very deep tinuous learning of context-dependent processing in convolutional networks for large-scale image recog- neural networks. arXiv preprint arXiv:1810.01256. nition. arXiv preprint arXiv:1409.1556. Zenke, F., Poole, B., and Ganguli, S. (2017). Continual Smith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, learning through synaptic intelligence. In Proceed- A. S. (2017). Federated multi-task learning. In Ad- ings of the 34th International Conference on Ma- vances in Neural Information Processing Systems, chine Learning-Volume 70, pages 3987–3995. JMLR. pages 4424–4434. Zhang, M., Wang, T., Lim, J. H., and Feng, J. (2019). Sorokin, A. Y. and Burtsev, M. S. (2019). Continual Prototype reminding for continual learning. arXiv and multi-task supervised learning with shared preprint arXiv:1905.09447. episodic memory. arXiv preprint arXiv:1905.02662. Srivastava, R. K., Masci, J., Kazerounian, S., Gomez, F., and Schmidhuber, J. (2013). Compete to com- pute. In Advances in neural information processing systems, pages 2310–2318. Thrun, S. and Mitchell, T. M. (1995). Lifelong robot learning. Robotics and autonomous systems, 15(1- 2):25–46. Titsias, M. K., Schwarz, J., Matthews, A. G. d. G., Pascanu, R., and Teh, Y. W. (2019). Functional regularisation for continual learning using gaussian processes. arXiv preprint arXiv:1901.11356. Toneva, M., Sordoni, A., Combes, R. T. d., Trischler, A., Bengio, Y., and Gordon, G. J. (2018). An empir-
Orthogonal Gradient Descent for Continual Learning A Additional Experiment Information A.1 Variants of Orthogonal Gradient Descent As described in Section 3 we test OGD with three settings for the which gradients to store, and 3 settings for how many gradients to store. OGD-ALL stores the gradients with respect to all logits of the model. OGD-AVG stores the gradients with respect to the average of all logits. OGD-GTL stores the gradient with respect to the ground truth logit. We run tests storing 20, 200, and 2000 gradients. Table A.1 summarizes the results of this experiment. We observe that increasing the number of gradients improves performance across the board (which is expected). We observe the OGD-GTL and OGD-ALL have similar performance in most cases, with a bit of an edge to OGD-GTL. OGD-AVG performs worse in most cases. rotated F1-score ± Std (%) Mnist 20 Grads 200 Grads 2000 Grads all 75.7 ± 2.6 79.9 ± 1.4 86.6 ± 1.0 average 75.3 ± 2.4 75.5 ± 1.4 77.7 ± 1.6 ground truth 76.4 ± 2.2 82.9 ± 1.6 87.1 ± 1.1 permuted F1-score ± Std (%) Mnist 20 Grads 200 Grads 2000 Grads all 87.3 ± 2.8 89.7 ± 1.5 90.5 ± 0.9 average 86.8 ± 1.4 86.9 ± 1.4 89.4 ± 1.7 ground truth 86.5 ± 1.5 89.4 ± 1.0 91.4 ± 1.7 Table 4: The performance of various OGD gradient methods as a function of number of gradients stored on rotated Mnist (top) and permuted Mnist (bottom). Numbers are the F1-score on task 1 after fully training on task 2. A.2 Increased Training Epochs We study the effect that increasing the number of training epochs has on the performance of the different training methods on permuted Mnist. For the Mnist experiments in the Section 4, we train for 5 epochs per task, which is enough to achieve 93% F1-score on vanilla Mnist classification and is in the regime short enough to avoid over-fitting. In order to determine whether increased training time has an effect on the performance in the multi-task setting, we train a classifier on 2-task permuted Mnist running each task training for 20, 40, 80, and 120 epochs and report the classification F1-score on task 1 after task 2 has finished. The results are shown in Figure 5. Note that A-GEM and OGD have maintained competitive performance with increasing number of epochs while in the case of SGD and EWC the performance first increases and then drops. 100 90 80 70 60 50 mtl ogd gem ewc sgd a ycaruccA epochs = 20 epochs = 40 epochs = 80 epochs = 120 mtl ogd gem ewc sgd mtl ogd gem ewc sgd mtl ogd gem ewc sgd a a a Figure 5: The performance of OGD versus others as a function of the number of training epochs for each task on permuted Mnist.
Orthogonal Gradient Descent for Continual Learning A.3 Split Mnist We present the results of the split Mnist study described in Section 4.3 on 3 other instances of the split Mnist task. These instances differ by the way the Mnist classes are split into tasks and the order in which the tasks are presented. Tables 5, 6, and 7 show the results on these tests. We can see that the ordering of the task methods is preserved in all tests: MTL and OGD are very close in performance, with a gap before A-GEM, and finally EWC and SGD. F1-score ± Std. (%) Task 1 Task 2 Task 3 Task 4 Task 5 mtl 99.6 ± 0.2 98.5 ± 0.4 97.7 ± 0.4 96.8 ± 1.0 98.7 ± 0.3 ogd 99.6 ± 0.4 97.7 ± 0.1 97.3 ± 0.5 98.0 ± 0.9 99.3 ± 0.1 agem 99.2 ± 0.6 91.4 ± 3.7 91.4 ± 0.9 87.1 ± 3.9 98.9 ± 0.3 ewc 97.0 ± 3.2 92.7 ± 3.8 91.9 ± 5.7 94.3 ± 2.2 99.2 ± 0.6 sgd 97.4 ± 2.4 92.2 ± 3.5 89.2 ± 8.6 94.5 ± 1.4 99.1 ± 0.3 Table 5: The F1-score of models trained by different methods on split Mnist. The reported values are the F1-score of the model for test examples from the indicated class after the model has been trained on all tasks in sequence. This table contains the same settings as Table 3, but with a different order of Mnist classes assigned to the tasks. F1-score ± Std. (%) Task 1 Task 2 Task 3 Task 4 Task 5 mtl 99.4 ± 0.2 99.2 ± 0.3 98.6 ± 0.4 99.7 ± 0.3 98.6 ± 0.5 ogd 99.0 ± 0.4 98.6 ± 0.1 98.0 ± 0.2 99.6 ± 0.3 99.6 ± 0.2 agem 94.1 ± 2.9 93.8 ± 5.5 90.6 ± 2.2 99.4 ± 0.3 99.4 ± 0.3 ewc 94.8 ± 2.9 95.3 ± 3.1 95.5 ± 0.6 99.3 ± 0.2 99.3 ± 0.2 sgd 94.6 ± 2.1 96.3 ± 1.2 95.0 ± 1.6 99.3 ± 0.4 99.3 ± 0.2 Table 6: The F1-score of models trained by different methods on split Mnist. The reported values are the F1-score of the model for test examples from the indicated class after the model has been trained on all tasks in sequence. This table contains the same settings as Table 3, but with a different order of Mnist classes assigned to the tasks. F1-score ± Std. (%) Task 1 Task 2 Task 3 Task 4 Task 5 mtl 98.4 ± 0.2 100.0 ± 0.0 98.6 ± 0.3 99.5 ± 0.2 98.9 ± 0.5 ogd 98.1 ± 0.8 99.9 ± 0.1 97.8 ± 0.6 99.4 ± 0.3 99.5 ± 0.3 agem 92.1 ± 2.7 93.8 ± 8.2 93.0 ± 3.5 98.6 ± 0.5 99.5 ± 0.3 ewc 92.5 ± 2.2 98.1 ± 3.0 94.0 ± 0.9 99.4 ± 0.2 99.5 ± 0.3 sgd 89.6 ± 4.4 98.9 ± 1.0 89.1 ± 7.9 98.9 ± 0.7 99.5 ± 0.3 Table 7: The F1-score of models trained by different methods on split Mnist. The reported values are the F1-score of the model for test examples from the indicated class after the model has been trained on all tasks in sequence. This table contains the same settings as Table 3, but with a different order of Mnist classes assigned to the tasks.
