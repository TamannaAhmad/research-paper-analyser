2019-10-22 An Alternative Surrogate Loss for PGD-based Adversarial Testing Sven Gowal1, Jonathan Uesato1, Chongli Qin1, Po-Sen Huang1, Timothy Mann1 and Pushmeet Kohli1 1DeepMind Adversarial testing methods based on Projected Gradient Descent (PGD) are widely used for searching norm-bounded perturbations that cause the inputs of neural networks to be misclassified. This paper takes a deeper look at these methods and explains the effect of different hyperparameters (i.e., optimizer, step size and surrogate loss). We introduce the concept of MultiTargeted testing, which makes clever use of alternative surrogate losses, and explain when and how MultiTargeted is guaranteed to find op- timal perturbations. Finally, we demonstrate that MultiTargeted outperforms more sophisticated meth- ods and often requires less iterative steps than other variants of PGD found in the literature. Notably, MultiTargeted ranks first on MadryLab’s white-box Mnist and Cifar-10 leaderboards, reducing the F1-score of their Mnist model to 88.36% (with (cid:96) perturbations of ϵ = 0.3) and the F1-score of their ∞ Cifar-10 model to 44.03% (at ϵ = 8/255). MultiTargeted also ranks first on the TRADES leaderboard reducing the F1-score of their Cifar-10 model to 53.07% (with (cid:96) perturbations of ϵ = 0.031). ∞ 1. Introduction Despite the successes of deep learning (Goodfellow et al., 2016), it is well-known that neural networks are not intrinsically robust. In particular, it has been shown that the addition of small but carefully chosen deviations to the input, called adversarial perturbations, can cause the neural network to make incorrect predictions with high confidence (Carlini and Wagner, 2017a,b; Goodfellow et al., 2014; Kurakin et al., 2016; Szegedy et al., 2013). The sensitivity of neural network models necessitates the development of methods that can either systematically find such failure cases, or declare that no such failure case exists with high-confidence. Starting with Szegedy et al. (2013), there has been a lot of work on understanding and generating adversarial perturbations (Athalye and Sutskever, 2017; Carlini and Wagner, 2017b), and on building models that are robust to such perturbations (Goodfellow et al., 2014; Kannan et al., 2018; Madry et al., 2017; Papernot et al., 2015; Xie et al., 2018). Unfortunately, many strategies proposed in the literature target failure cases found through specific adversaries, and as such they are easily broken under different adversaries (Athalye et al., 2018; Uesato et al., 2018). Again, this phenomenon highlights the importance of understanding the limitations of different adversarial techniques when it comes to finding when models fail. Instead of resorting to methods that directly focus on improving robustness to specific attacks, Madry et al. (2017) formulate a saddle point problem whose goal is to find model parameters θ that minimize the adversarial risk: (cid:20) (cid:21) (cid:69) max L(f (ξ ), y) (1) (x,y)∼D θ ξ ∈S(x) where D is a data distribution over pairs of examples x and corresponding labels y, f is a model parametrized θ by θ , L is a suitable loss function (such as the 0 − 1 loss in the context of classification tasks), and S(x) defines the set of allowed perturbations (i.e., the adversarial input set or threat model). The aim of rigorous testing is to accurately estimate this adversarial risk. As such, finding the worst-case input (or optimal adversarial example) ξ (cid:63) ∈ S(x) is a key consideration for both training and testing models. Finding sub-optimal inputs ξˆ with L(f (ξˆ ), y) ≤ L(f (ξ (cid:63)), y) results in computing a lower bound on the true adversarial risk (or in the context of a θ θ classification task, an upper bound on the true robust F1-score), which may give a false sense of security. Hence, it is of practical importance to find the solution of the inner maximization problem in Equation (1) as efficiently and as accurately as possible. Corresponding author(s): sgowal@google.com 9102 tcO 12 ]GL.sc[ 1v83390.0191:viXra
An Alternative Surrogate Loss for PGD-based Adversarial Testing 1.1. Methods for Adversarial Testing Several methods (also known as “attacks”) have been proposed to find adversarial examples (and effectively solve the inner maximization problem in Equation (1)). Goodfellow et al. (2014) proposed one of the earliest method specific to (cid:96) -bounded perturbations. It is known as the Fast Gradient Sign Method (FGSM): it replaces ∞ the impractical 0 − 1 loss L with the cross-entropy loss Lˆ and computes an adversarial example ξˆ as (cid:16) (cid:17) x + ϵsign ∇ Lˆ(f (x), y) . (2) x θ A more powerful adversary is the multi-step variant FGSMK , which essentially performs K projected gradient steps on the surrogate loss Lˆ (Kurakin et al., 2016) to find ξˆ = ξ (K) with (cid:16) (cid:16) (cid:17) (cid:17) ξ (k+1) ← Proj ξ (k) + α sign ∇ Lˆ(f (ξ (k)), y) (3) S(x) ξ (k) θ where ξ (0) is typically chosen at random within S(x), and Proj is a projection operator, for example: Proj S(x)(ξ ) = argmin (cid:107)ξ (cid:48) − ξ (cid:107) 2. (4) ξ (cid:48)∈S(x) Another popular method stems from Carlini and Wagner (2017b) who extend the formulation of Szegedy et al. (2013). They investigate how different optimization methods and different losses Lˆ affect the quality of the adversarial examples. Crucially, and similarly to Uesato et al. (2018), they also propose to use the Adam (Kingma and Ba, 2014) optimizer rather than regular gradient descent. Finally, since gradient-based method are sensitive to initial conditions, it is not uncommon to repeat the optimization procedure many times with different initializations ξ (0) (i.e., multiple restarts). All above methods are approximate and one generally hopes to find an adversarial example ξˆ such that L(f (ξˆ , y) = L(f (ξ (cid:63), y). A “stronger” attack finds such adversarial examples more often than θ θ a “weaker” attack for (x, y) ∼ D. More recently, Zheng et al. (2018) developed the Distributionally Adversarial Attack (DAA) which performs optimization on the space of potential data distributions and ranked first place on MadryLab’s white-box leader- boards (both on Mnist1 and Cifar-102). This attack was overtaken by the Interval Attack (Wang et al., 2019) and the Fast Adaptive Boundary Attack (FAB, Croce and Hein, 2019) on the Mnist and Cifar-10 leaderboards, respectively. Brendel et al. (2019) extended their decision-based boundary attack (Brendel et al., 2017) with an emphasis on efficiency (which is not the focus on this work). Note that there exists many more attacks (Chen et al., 2018; Moosavi-Dezfooli et al., 2016; Papernot et al., 2016) that are either weaker or optimize norms other than (cid:96) . ∞ 1.2. Revisiting PGD The increasing complexity and sophistication of methods for adversarial testing makes them difficult to adapt to different models, architectures and application areas. This motivated us to revisit Projected Gradient Descent (PGD) as a means to solve the maximization problem in Equation (1). We focus on robustness against (cid:96) -bounded ∞ attacks (i.e., S(x) = {ξ : (cid:107)ξ − x (cid:107) ≤ ϵ } where ϵ is an a-priori defined constant). We expect the analysis ∞ performed here to translate well to (cid:96)1 and (cid:96)2-bounded attacks and reserve the evaluation of these threat models for future work. The contributions of this paper are as follows: • We create a guide to PGD, explaining the different variants and highlighting their use in practice. We provide a set of hyperparameters (i.e., optimizer, step size and surrogate loss) with which PGD does well across a wide range of models. We demonstrate that regular PGD (as opposed to the MultiTargeted version described below) can be tuned such that it would rank first on MadryLab’s white-box Mnist leaderboard, reducing the F1-score of their model to 88.21% (with (cid:96) perturbations of ϵ = 0.3). ∞ • We introduce the concept of a MultiTargeted attack and demonstrate its effectiveness by comparing it with other regular PGD-based attacks under the same computational budget. MultiTargeted is a PGD-based untargeted attack with an alternative surrogate loss. 1https://github.com/MadryLab/mnist_challenge 2https://github.com/MadryLab/cifar10_challenge 2
An Alternative Surrogate Loss for PGD-based Adversarial Testing • We compare MultiTargeted with other state-of-the-art methods. We show that MultiTargeted ranks first on MadryLab’s white-box Mnist leaderboard, reducing the F1-score of their model to 88.36% (with (cid:96) ∞ perturbations of ϵ = 0.3). We also found MultiTargeted to be particularly effective on models trained on Cifar-10 and MNIST. It ranks first on MadryLab’s white-box Cifar-10 leaderboard, reducing the F1-score of their model to 44.03% (with (cid:96) ∞ perturbations of ϵ = 8/255). MultiTargeted also ranks first on the TRADES leaderboard reducing the F1-score of their Cifar-10 model to 53.07% (with (cid:96) perturbations ∞ of ϵ = 0.031). 2. PGD-based rigorous testing As explained in Section 1.1, there exists many attack variants. In this paper, we focus on variants of PGD. A PGD-based attack typically executes the following procedure: Algorithm 1 White-box PGD-based attack Input: A nominal input x and label y, a model f θ and an adversarial input set S(x). α (k) is the step-size at iteration k and Opt is an optimizer (e.g., Adam). Output: Possible attack ξˆ ∈ S(x) 1: ξˆ ← x 2: for r ∈ {1, . . . , Nr } do (cid:46) Repeat the attack process Nr times 3: Initialize surrogate loss Lˆ(r) (cid:46) The surrogate loss may be different across restarts (see Sec. 3) 4: Initialize optimizer Opt (cid:46) The optimizer ingests gradients and computes an update 5: ξ (0) ← SampleFrom(x, S) (cid:46) Samples a random input that respects the threat model 6: for k ∈ {1, . . . , K } do (cid:46) K corresponds to the number of optimization steps (cid:16) (cid:16) (cid:17)(cid:17) 7: ξ (k) ← ProjS(x) ξ (k−1) + α (k)Opt ∇ ξ (k−1) Lˆ(r)(f θ (ξ (k−1)), y) 8: if L(f (ξ (k)), y) > L(f (ξˆ , y) then (cid:46) L is the loss of interest (e.g., 0 − 1 loss) θ θ 9: ξˆ ← ξ (k) (cid:46) If L is the 0 − 1 loss, the procedure can terminate early 10: end if 11: end for 12: end for Algorithm 1 highlights a few design choices. Beyond the number of restarts Nr and the number of PGD steps K3, one must decide which optimizer Opt, which sub-differentiable surrogate loss Lˆ, and which step size schedule (or learning rate) α (k) to use. Apart from sometimes varying the number of steps K, recent works often use a single set of fixed parameters to provide baseline results (irrespective of the network architecture). While none of these choices are necessarily trivial, there exists reasonable options (some of which are explained in details by Carlini et al., 2019). Here is a non-exhaustive list. Optimizer. Since the work from Kurakin et al. (2016) on (cid:96) -norm bounded perturbations, the sign function ∞ has been most commonly used. More recently, Carlini and Wagner (2017b) and subsequently Dong et al. (2018) proposed to use the Adam (Kingma and Ba, 2014) and Momentum (Polyak, 1964) optimizers, respectively. Most works limit themselves to zeroth-order and first-order methods for efficiency, but in principle higher order methods are applicable (especially when the dimensionality of x is small). In general, we found that the Adam optimizer produced the most consistent results (similar conclusions were reached by Uesato et al., 2018) and was less sensitive to the step size. There were, however, cases where the sign function was more efficient (e.g., when models used activations that saturate such as the sigmoid function). For clarity in the algorithm description, we categorize the sign function as being part of the optimizer. As such, it also worth mentioning that any other normalization of the gradient is possible (e.g., (cid:96)2-normalization when dividing by (cid:107)∇ x(k−1)Lˆ(r )(f θ (x(k−1)), y)(cid:107) 2). Surrogate loss. The most common (and closely related) surrogate losses used in the literature are (cid:32) (cid:33) C Lˆ(z, y) = −z + log (cid:213) ezi cross-entropy loss (5) y i=1 Lˆ(z, y) = −z + max z margin loss (6) y i i(cid:44)y 3We are actually performing projected gradient ascent steps, but will keep using the PGD acronym. 3
An Alternative Surrogate Loss for PGD-based Adversarial Testing where z = f (x) ∈ (cid:82)C are logits (and C is the number of classes). For most models, both losses tend to perform θ equally well (which is expected given their close relationship). In some cases, we found the margin loss to be slightly better (as reported by Carlini and Wagner, 2017b). Carlini and Wagner (2017b) also propose a list of seven losses that operate in both probability and logit spaces (loss 1 is the cross-entropy loss, while loss 6 is closely related to the margin loss). Step size. The step size (or learning rate) can have a significant influence on the success rate of the attack. It is often easier to tune the step size when using normalized gradients, and most works limit themselves to well established values. We found that using a schedule for the step size can help alleviate some the weakness induced by gradient obfuscation (Carlini and Wagner, 2017a; Uesato et al., 2018) while maintaining a small number of PGD steps. The right schedule depends on the choice of optimizer, model and dataset studied. Trade-off between number of restarts and number of PGD steps. When compute is not limited, it is always preferable to increase the number of restarts Nr and the number of PGD steps K. However, with limited compute, the relationship between Nr and reduce K is unclear. As a rule of thumb, we suggest tuning the optimizer, surrogate loss and step size with one restart and then increase the number of restarts as much as possible for the available compute. For classification tasks, a popular choice is FGSMK which can be implemented by setting the optimizer to the sign function, by using cross-entropy as surrogate loss, and by fixing α (k) to a fixed constant (typically around ϵ/10). While FGSMK often gives reasonable results, we found that these parameters were always sub-optimal across all the datasets (i.e., Mnist, Cifar-10, Svhn, MNIST) and models (e.g., WideResNet, ResNet and other standard convolutional networks) we tested over the past year. Our usual starting point for tuning PGD-based attacks is to set the optimizer to Adam, set the step size to 0.1 (with 10× decay at K/2 and 3K/4 steps) and use the margin loss. Again, it is really important to understand that, while these values tend to work well in practice for commonly seen models and datasets, they need to be tuned for each case specially. 3. Method MultiTargeted is an instantiation of Algorithm 1 where we set the surrogate loss to Lˆ(r )(z, y) = −z + z with T = {1, . . . , C} \ {y} (7) y T r mod |T| In other words, at each restart, we pick a new target class t ∈ T where the set T contains all classes other than y. Rewriting lines 2-3 of Algorithm 1 more programmatically, we obtain Algorithm 2. Algorithm 2 MultiTargeted attack 2a: Ni ← (cid:98)Nr/| T |(cid:99) (cid:46) Keep the number of iterations consistent 2b: for i ∈ {1, . . . , Ni } do 2c: for j ∈ {1, . . . , | T | } do 3: Use surrogate loss Lˆ(i|T|+j+1)(z, y) = −zy + zt with t = T j ... 11: end for 12: end for MultiTargeted uses |T | different surrogate losses (one for each class except the true class). To keep the number of restarts comparable with other PGD-based attacks that use a single fixed surrogate loss, Algorithm 2 uses less restarts per target class t (i.e., (cid:98)Nr/T (cid:99)). To limit runtime complexity, it is possible to restrict the number of target classes by targeting only the top-T classes: T = {t | T > |{i such that i (cid:44) y and z > z }|} (8) i t In Section 4, we will see that MultiTargeted outperforms regular PGD-based attacks (any of the variants explained in Section 2). As a results, it is often possible to lower the total number of PGD steps K, the number of inner restarts Ni or the size of the target set T to have comparable (or even lower) runtime complexity while still outperforming regular PGD-based attacks. 4
An Alternative Surrogate Loss for PGD-based Adversarial Testing 0 x † x x + † − Perturbed input ssoL misclassified 0 correct max(z1, z2) − z0 z1 − z0 z2 − z0 x † x x + † − Perturbed input (a) ssoL misclassified correct max(z1, z2) − z0 z1 − z0 z2 − z0 (b) Figure 1 | Panel a shows an example motivating why MultiTargeted can outperform other untargeted attacks. In this particular example a regular PGD-based attack with 2 restarts achieves 75% success rate (as opposed to 100% for MultiTargeted). Panel b shows a more extreme example. Here, a regular PGD-based attack with 2 restarts achieves 25% success rate only. For both panels, the areas shaded in gray are outside the adversarial input set. 3.1. Analysis 3.1.1. Toy example To motivate the effectiveness of the MultiTargeted attack, we use the example depicted in Figure 1a. This example considers a one dimensional 3-way classification task with a linear classifier, hence, there are three output logits z0, z1 and z2. Without perturbation, a single dimensional input x with true class 0 is correctly classified (i.e., z0 > z1 and z0 > z2). The y-axis represents the values taken by the margin loss max(z1, z2) − z0 (one of the surrogate losses used by regular PGD-based attacks), as well as the logit differences z1 − z0 and z2 − z0 (which are the surrogate losses used by MultiTargeted) as a function of the perturbed input x + δ . The adversarial input set is S(x) = [x − ϵ, x + ϵ]. The perturbed input x + δ is correctly classified if and only if the logit differences are both negative. Note how z1 − z0 cannot be positive (i.e., z1 > z0) for any |δ | < ϵ, to the contrary of z2 − z0 (i.e., z2 > z0). As such, within the adversarial input set, x + δ can only be confused for class 2. The success of a regular PGD-based attack that uses the margin loss depends on the initialization ξ (0). When ξ (0) is initialized between x and x + ϵ (this happens with 50% chance if ξ (0) is uniformly sampled in S(x), the attack will converge to ξˆ = x + ϵ, which is correctly classified. When ξ (0) is sampled between x − ϵ and ϵ, the attack will converge to ξˆ = x − ϵ, which is misclassified. Hence, with two restarts, when ξ (0) is chosen uniformly at random, the attack will succeed with probability 1 − (1 − 0.5)2 = 75%. However MultiTargeted, which explicitly uses the logit differences as surrogate losses independently, will have a 100% success rate (when using two restarts). Figure 1b shows a more extreme example where the logit difference z2 − z0 overlaps minimally with the margin loss. In this example, with two restarts, a regular PGD-based attack achieves 25% success rate only. 3.1.2. Linear models We can repeat the experiment illustrated by these examples. We set the nominal input x to zero and use a maximum perturbation radius ϵ of one. Assuming a linear classifier f (x) = W x + b, we uniformly at random sample weights W and biases b within the [−1, 1] interval. We assign the true class y to be index of the highest logit at x = 0 (i.e., argmax b ). We consider an input x to be attackable if there exists a perturbation such that a logit that i i does not belong to the true class is the highest (i.e., ∃δ ∈ [−ϵ, ϵ], i (cid:44) y such that f (x + δ ) > f (x + δ ) ). Given i y the linearity of the classifier and, as established above, if an input is attackable, MultiTargeted will succeed in finding a misclassified input 100% of the time (if the number of restarts is at least equal to the number of classes minus one). Repeating the experiment a million times, a regular PGD-based attack succeed only 96.16% of the time when the number of classes is three and the number of restarts is two. Slicing this result, we also notice that when the number of confusing classes (classes different from the true class for which there exists a perturbed input that is classified as such, i.e., |{i |∃δ with f (x + δ ) > f (x + δ ) }|) is two, it succeeds 100% of the time (as i y expected). When the number of confusing classes is one, it succeeds 92.39% of the time. Figure 2 shows how 5
An Alternative Surrogate Loss for PGD-based Adversarial Testing 100% 95% 90% 1 2 1 2 3 1 2 3 4 Number of confusing classes kcatta DGP raluger fo etar sseccuS 3-way classification (C = 3) 4-way classification (C = 4) 5-way classification (C = 5) MultiTargeted has 100% success rate Figure 2 | Success rate of a regular PGD-based attack with C − 1 restarts against a random linear classifier depending on the num- ber of confusing classes. For all possible numbers of confusing classes, MultiTargeted has a 100% success rate. these results are affects by additional classes. In particular, we observe that regular PGD becomes less efficient as the number of confusing classes decreases (which is the hallmark of good classifiers). This analysis demonstrates that Figure 1 generalizes to any linear model, multiple input dimensions and any convex adversarial input set (in the sense that S(x) is a convex set), and leads to the following theorem. Theorem 3.1 Given a globally linear model f with C output logits, for any input x, MultiTargeted is stronger θ than regular PGD attacks that use the margin loss (or cross-entropy loss) when the number of restart is greater or equal to C − 1 (i.e., N ≥ C − 1) and the adversarial input set S(x) is convex. In fact, MultiTargeted always finds r an optimal attack ξ (cid:63) in this case (i.e., (cid:154)ξ ∈ S(x) s.t. Lˆ(f (ξ , y) > Lˆ(f (ξ (cid:63), y)). θ θ Proof. Since the margin and cross-entropy surrogate losses Lˆ(W ξ + b, y) used by regular PGD-based attacks are convex with respect to ξ , finding their global maximum using gradient ascent depends solely on the initial guess ξ (0). If ξ (0) is randomly sampled, a regular PGD-based attack has no guarantee that it will find the global maximum as demonstrated in Figure 1 (increasing the number of restarts increases the probability that the global maximum is found). On the other hand, MultiTargeted, which inspects all logit differences, is guaranteed to find the global maximum (since the global maximum is contained within one the hyper-planes defined by one of the logit differences). In the same manner, we can also prove that MultiTargeted always finds the optimal attack for any model f where logit differences are strictly monotonic. θ 3.1.3. Deeper models For deeper models, and since the last layer is typically linear, we can reason about the efficiency of MultiTargeted by analyzing the shape of the propagated adversarial input set Z(x) = { f (ξ )|ξ ∈ S(x)}. If Z(x) is convex, θ we fallback onto the analysis made in the previous paragraph, for which we know that MultiTargeted is more effective than regular PGD. When Z(x) is non-convex, the efficiency of MultiTargeted cannot be proven. In fact, we can artificially build examples for which regular PGD is better, and vice-versa (see Appendix B). However, we intuitively expect MultiTargeted to do well when the behavior f is close to linear within S(x), since convexity θ is preserved under affine transformations. Previous work (Moosavi-Dezfooli et al., 2018; Qin et al., 2019) observed that robust classifiers (at least those trained via adversarial training, Madry et al., 2017) tend to behave linearly in the neighborhood of training examples. Our experimental results confirm these findings: for complex datasets beyond Mnist, for which robust models need to enforce smoothness, MultiTargeted is a stronger attack than regular PGD (Appendix C gives more details). Finally, we point out that this analysis generalizes to any specification beyond robustness to (cid:96) -bounded perturbations. We can summarize our findings in the following theorem. ∞ Theorem 3.2 For any input x and a convex adversarial input set S(x), if f has C outputs and f is locally linear θ θ around x (i.e., ∃W , b s.t. ∀ξinS(x), f (x) = W x + b), MultiTargeted always finds an optimal attack ξ (cid:63) within θ N = C − 1 restarts (i.e., (cid:154)ξ ∈ S(x) s.t. Lˆ(f (ξ , y) > Lˆ(f (ξ (cid:63), y)). r θ θ Proof. The proof follows directly from Theorem 3.1 and the fact that, under a linear transformation, the propagated adversarial input set Z(x) = { f (ξ )|ξ ∈ S(x)} remains convex. θ 6
An Alternative Surrogate Loss for PGD-based Adversarial Testing 65.0% 60.0% 55.0% 50.0% 45.0% 1 2 3 4 5 6 7 8 all Number of target classes (MultiTargeted with 20 restarts) kcatta rednu ycaruccA Number of restarts (Regular PGD) 46.2% 20 40 60 80 100 120 140 160 180 MultiTargeted (Madry et al.) Regular PGD 46.0% MultiTargeted (Zhang et al.) Regular PGD MultiTargeted (Uesato et al.) Regular PGD 45.8% MultiTargeted (Ours) Regular PGD 45.6% 45.4% 45.2% 45.0% 44.8% 44.6% 44.4% 10 20 50 100 200 500 1000 Number of PGD steps (a) kcatta rednu ycaruccA Madry et al. (1 restarts) Madry et al. (5 restarts) Madry et al. (10 restarts) Madry et al. (20 restarts) (b) Figure 3 | Panel a shows the F1-score under attacks of size ϵ = 8/255 for four different Cifar-10 models. The solid lines are the accuracies obtained by MultiTargeted for different values of T , while the dashed lines of the corresponding color are accuracies obtained by regular PGD (for the same computational budget). Panel b shows the F1-score under a MultiTargeted attack of size ϵ = 8/255 for Madry et al.’s model as a function of the number of PGD steps K and the number of restarts Ni. 4. Experimental Analysis In this section, we run MultiTargeted on widely available models and compare it against regular well-tuned untargeted PGD-based attacks. To avoid confusion, we denote by PGDK1×Nr the regular PGD-based attack with Nr restarts and K1 PGD steps that reaches the highest success rate among the different variants explained in Section 2 (that excludes MultiTargeted attacks, but includes FGSMK ). We also denote by MTK2×Ni×T the MultiTargeted attack that targets the top-T classes, uses Ni restarts per target and does K2 PGD steps. When we omit T , we target all classes other than the true class. To keep the number of compute equivalent, we compare attacks such that K1 × Nr = K2 × Ni × T (when T is omitted, for Mnist and Cifar-10, it is equal to 9). Unlike of regular PGDK1×Nr (which we tune using grid-search) – and because we did not find MultiTargeted to be as sensitive to hyperparameters as other PGD-based attacks – we fix MTK2×Ni×T to use Adam as its optimizer, set the initial step size at 0.1 and reduce it to 0.01 and 0.001 after K2/2 and 3K2/4 steps, respectively (this schedule is also included in the grid-search performed on regular PGD). Cifar-10. We focus our analysis on four models. All models are WideResNet variants. The first is adversarially trained by Madry et al. (trained against ϵ = 8/255) and is available from MadryLab’s white-box Cifar-10 leaderboard.4 The second model is trained using TRADES (Zhang et al., 2019) and is available on the TRADES leaderboard.5 It is empirically more robust than the first network. The third model is trained using Unsupervised Adversarial Training (UAT, Uesato et al., 2019) and is empirically more robust than the other two models but is three times deeper (with 106 layers). The last model is adversarially trained by us using the learning rate schedule proposed by Zhang et al. (2019). This model is as robust as the Zhang et al.’s model under regular PGD-based attacks. Figure 3a shows the F1-score reached by MultiTargeted as a function of the number of target classes T at ϵ = 8/255. It compares MT200×20×T with PGD200×(20·T ). For all models, MultiTargeted is the strongest attack. In fact, even when targeting only the top-2 classes, MultiTargeted significantly outperforms regular PGD on all models (except Madry et al.’s model). We also observe that the performance of regular PGD stagnates and that adding more restarts is ineffective, which appears to confirm that for some examples in the test set there is a low probability of finding an initial guess ξ (0) that leads to misclassified perturbation (as examplified in Figure 1b). For completeness, Figure 3b shows how the F1-score under an MTK×Ni attack evolves as a function of the number of PGD steps K and the number of restarts Ni. We note that, while this figure shows specifically what happens on Madry et al.’s model, the behavior is identical on the other three Cifar-10 models. As expected, the strength of 4https://github.com/MadryLab/cifar10_challenge; the model used here is the adv_trained model. 5https://github.com/yaodongyu/TRADES 7
An Alternative Surrogate Loss for PGD-based Adversarial Testing 13.0% 12.0% 11.0% 10.0% 9.0% 8.0% 7.0% 6.0% 5.0% 1 2 4 8 16 32 64 Number of target classes (MultiTargeted with 20 restarts) kcatta rednu ycaruccA Number of restarts (Regular PGD) 20 40 80 160 320 640 1280 MultiTargeted (Xie et al.) Regular PGD MultiTargeted (Denoise by Xie et al.) Regular PGD MultiTargeted (Ours) Regular PGD Figure 4 | F1-score under attacks of size ϵ = 16/255 for three different MNIST models. The solid lines are the accuracies obtained by MultiTargeted for different values of T , while the dashed lines of the corresponding color are accuracies obtained by regular PGD (for the same computational budget). the attack saturates with more restarts and steps. We also observe that MultiTargeted with a single restart is more effective than a regular PGD-based attack with hundreds of restarts. Finally, Table 1, at the end of this section, summarizes the performance of each attack on models that have an online leaderboard (i.e., Madry et al.’s and Zhang et al.’s models). For both models, MultiTargeted ranks first. MNIST. We focus our analysis on three models. All models are adversarially trained against random targeted attacks at ϵ = 16/255 (as is usual on MNIST). However, they all exhibit non-trivial empirical robustness to untargeted attacks; and we evaluate them in that context (as MultiTargeted is an untargeted attack). The first model is a standard ResNet-152: it is available from Xie et al.’s GitHub page.6 The second model is a variant of ResNet-152 that uses additional “denoise” blocks: it is also trained by Xie et al. (2018). The third model is trained by ourselves: we put emphasis on robustness under attack rather than F1-score on clean examples. As a result (on the first 1000 images of the test set) the last model has a clean F1-score of 51.4% compared to 66.8% for the “denoise” model and 64.1% for the regular model by Xie et al.. Figure 4 shows the F1-score reached by MultiTargeted as a function of the number of target classes T at ϵ = 16/255. It compare MT200×20×T with PGD200×(20·T ). For all models MultiTargeted is a better attack (yielding lower F1-score) starting with T = 2. Another interesting observation is the fact that under the MultiTargeted attack, the standard and “denoise” variants trained by Xie et al. are on par (the regular PGD-based attack would indicate otherwise).7 This confirms the observation made in Section 3.1. Indeed, Xie et al. (2018) demonstrate that under random targeted attacks the “denoise” model is stronger. As such, there are less confusing classes, which can explain why MultiTargeted is performing well in comparison to regular PGD on the “denoise” model. Leaderboards on Mnist and Cifar-10. In this section, we introduce a combined PGD+MTK×Ni attack that alternates between the logit differences shown in Equation (7) and the margin loss in Equation (6). Thus, for Mnist and Cifar-10, where the number of classes is ten, the total number of surrogate losses is ten and thusthe total number of gradient evaluation is K × Ni × 10 (which results in using about 11% more compute that MultiTargeted). Algorithm 3 in the supplementary material shows an implementation of this attack. For Mnist, we analyze the model adversarially trained by Madry et al. (trained against ϵ = 0.3). It is available from MadryLab’s white-box Mnist leaderboard.8 On the leaderboard, it states that this model achieves 89.62% F1-score under PGD100×50. With proper tuning, our implementation of PGD100×50 reaches 89.03% F1-score, which is closer to the state-of-art results obtained by Wang et al. (2019) with 88.42%. For Cifar-10, we re-analyze two WideResNet models. The first model from Madry et al. is available on MadryLab’s leaderboard,9 where it achieves 45.21% F1-score under PGD20×10 (at ϵ = 8/255). Our tuned PGD20×10 variant reaches a similar F1-score 6https://github.com/facebookresearch/MNIST-Adversarial-Training 7This analysis is performed using an untargeted attack and these conclusions are not necessarily at odds with the results shown by Xie et al. (2018), which looks at random targeted attacks. 8https://github.com/MadryLab/mnist_challenge; the model used here is the secret model. 9https://github.com/MadryLab/cifar10_challenge; the model used here is the secret model. 8
An Alternative Surrogate Loss for PGD-based Adversarial Testing F1-score under attack Mnist ϵ PGD1000×1800 MT1000×200 PGD+MT1000×200 Aggregated Wang et al. Madry et al. 0.3 88.21% 88.43% 88.36% 88.18% 88.42% Cifar-10 ϵ PGD1000×180 MT1000×20 PGD+MT1000×20 Aggregated Croce and Hein Madry et al. 8/255 44.51% 44.05% 44.03% 44.03% 44.51% Zhang et al. 0.031 53.70% 53.07% 53.07% 53.05% 53.44% Table 1 | Comparison with state-of-the-art attacks. On Mnist (top-half of the table), the best known attack is the “Interval Attack” by Wang et al. (2019). On Cifar-10 (bottom-half of the table), the best known attack is the “Fast Adaptive Boundary Attack” by Croce and Hein (2019). For both dataset, we compare regular PGD with MultiTargeted. The penultimate column is the F1-score obtained when combining all attacks into one (i.e., picking the worst-case adversarial example from any of the attacks). of 45.18% and the state-of-the-art is obtained by Croce and Hein (2019) with 44.51%. The second model of Zhang et al. is from the TRADES leaderboard, where it states that FGSM1000 reaches 56.53% in F1-score (at ϵ = 0.031). Our tuned PGD50×20 variant reaches 54.05% F1-score. The state-of-the-art obtained by Croce and Hein is 53.44%. Table 1 summarizes the results. Overall, this table illustrates well the dichotomy and complementary of regular PGD and MultiTargeted (see Section 3). On Mnist, for the same number of gradient evaluations, regular PGD tends to do better than MultiTargeted – while on Cifar-10, it is the opposite. These results do seem to indicate that, for Madry et al.’s Mnist model, the number of restarts is much more important than the choice of surrogate loss. As explained in Appendix C, this model does not behave linearly and is not smooth in the neighborhood of datapoints. As such, the propagated adversarial input set Z(x) can be highly non-convex and, thus, more restarts are required to avoid local minima. In fact, combining both regular PGD and MultiTargeted into a PGD+MT attack does not provide a significant advantage (beyond adding a few additional restarts). On both Mnist and Cifar-10, MT itself achieves state-of-the-art numbers. For both datasets, the best attack misses only a handful of adversarial examples (as visible by the small gap in F1-score with the “aggregated” column, which aggregates results across the three attack variants). 5. Conclusion This paper provides an overview of projected gradient descent and its use for rigorous testing. We introduce the concept of MultiTargeted attacks, which is a variant of PGD, and demonstrate that we can reach state-of-art results on three separate leaderboards. We highlight under which conditions MultiTargeted is more effective. We hope that this paper can serve as an inspiration for tuning PGD-based attacks and motivates why we should not necessarily rely on existing hyper-parameter values. References A. Athalye and I. Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017. A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. W. Brendel, J. Rauber, and M. Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017. W. Brendel, J. Rauber, M. Kümmerer, I. Ustyuzhaninov, and M. Bethge. Accurate, reliable and fast robustness evaluation. arXiv preprint arXiv:1907.01003, 2019. N. Carlini and D. Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 3–14. ACM, 2017a. N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy, pages 39–57. IEEE, 2017b. N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber, D. Tsipras, I. Goodfellow, and A. Madry. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705, 2019. 9
An Alternative Surrogate Loss for PGD-based Adversarial Testing P.-Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C.-J. Hsieh. EAD: elastic-net attacks to deep neural networks via adversarial examples. In Thirty-second AAAI conference on artificial intelligence, 2018. F. Croce and M. Hein. Minimally distorted adversarial examples with a fast adaptive boundary attack. arXiv preprint arXiv:1907.02044, 2019. Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li. Boosting adversarial attacks with momentum. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9185–9193, 2018. I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. URL http://www.deeplearningbook.org. I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. H. Kannan, A. Kurakin, and I. Goodfellow. Adversarial Logit Pairing. arXiv preprint arXiv:1803.06373, 2018. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016. A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2574–2582, 2016. S.-M. Moosavi-Dezfooli, A. Fawzi, J. Uesato, and P. Frossard. Robustness via curvature regularization, and vice versa. arXiv preprint arXiv:1811.09716, 2018. URL https://arxiv.org/pdf/1811.09716.pdf. N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. arXiv preprint arXiv:1511.04508, 2015. N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P), pages 372–387. IEEE, 2016. B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964. C. Qin, J. Martens, S. Gowal, D. Krishnan, A. Fawzi, S. De, R. Stanforth, P. Kohli, et al. Adversarial robustness through local linearization. arXiv preprint arXiv:1907.02610, 2019. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. J. Uesato, B. O’Donoghue, A. v. d. Oord, and P. Kohli. Adversarial Risk and the Dangers of Evaluating Against Weak Attacks. arXiv preprint arXiv:1802.05666, 2018. J. Uesato, J.-B. Alayrac, P.-S. Huang, R. Stanforth, A. Fawzi, and P. Kohli. Are labels required for improving adversarial robustness? arXiv preprint arXiv:1905.13725, 2019. S. Wang, Y. Chen, A. Abdou, and S. Jana. Enhancing gradient-based attacks with symbolic intervals. arXiv preprint arXiv:1906.02282, 2019. C. Xie, Y. Wu, L. van der Maaten, A. Yuille, and K. He. Feature denoising for improving adversarial robustness. arXiv preprint arXiv:1812.03411, 2018. URL https://arxiv.org/pdf/1812.03411. H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan. Theoretically principled trade-off between robustness and F1-score. arXiv preprint arXiv:1901.08573, 2019. T. Zheng, C. Chen, and K. Ren. Distributionally adversarial attack. arXiv preprint arXiv:1808.05537, 2018. 10
An Alternative Surrogate Loss for PGD-based Adversarial Testing A. Combining regular PGD and MultiTargeted As shown in Section 3, there exists cases where the margin loss is more efficient than using each logit difference (as proposed by MultiTargeted). Algorithm 3 below combines both approaches into a single PGD+MTK×Ni attack. Algorithm 3 PGD+MT attack Input: A nominal input x and label y, a model f (x) and a threat model S(x) θ Output: Possible attack ξˆ ∈ S(x) 1: T ← {1, . . . , C} \ {y} (cid:46) C is the number of classes. 2: for i ∈ {1, . . . , Ni} do 3: for j ∈ {1, . . . , C} do 4: if j = C then 5: Use surrogate loss Lˆ(iC+j+1)(z, y) = −z y + max t ∈T z t (cid:46) margin loss 6: else 7: Use surrogate loss Lˆ(iC+j+1)(z, y) = −z + z with t = T (cid:46) logit difference losses y t j 8: end if 9: Initialize optimizer Opt 10: ξ (0) ← SampleFrom(x, S) 11: for k ∈ {1, . . . , K } do (cid:16) (cid:16) (cid:17) (cid:17) 12: ξ (k) ← Proj S(x) ξ (k−1) + α k Opt ∇ ξ (k−1)Lˆ(r )(f θ (ξ (k−1)), y) 13: if L(f (ξ (k)), y) > L(f (ξˆ , y) then θ θ 14: ξˆ ← ξ (k) 15: end if 16: end for 17: end for 18: end for 11
An Alternative Surrogate Loss for PGD-based Adversarial Testing B. Non-convex adversarial input sets This section shows why, under non-convex adversarial input sets, MultiTargeted is not necessarily more efficient than regular PGD-based attacks – even for linear models (for the same compute budget). Let us consider a 2 dimensional input x and a 3-way classification task. Figure 5 shows the margin loss for two slightly different settings. For all panels, the adversarial input set is highlighted in blue and the region in purple is where misclassification occurs. When ξ (0) is sampled within the region highlighted in red, the attack for which that panel corresponds to is successful. At the top, in Figure 5a and b, we illustrate an example for which regular PGD using the margin loss is more likely to find the worst-case attack (located in the top-right corner). Indeed, the region in red is larger in panel b than in panel a. On the other hand, in Figure 5c and d, we illustrate the opposite. A slight shift of the input makes regular PGD less likely to find the optimal perturbation. 4 2 0 2 4 4 2 0 2 4 x 1 x 2 9 6 3 0 3 6 9 12 15 z ) z, z(xam ssol nigram 0 2 1 − 4 2 0 2 4 4 2 0 2 4 x 1 (a) Example 1 for MultiTargeted x 2 9 6 3 0 3 6 9 12 15 z ) z, z(xam ssol nigram 0 2 1 − (b) Example 1 for regular PGD 4 2 0 2 4 4 2 0 2 4 x 1 x 2 9 6 3 0 3 6 9 z ) z, z(xam ssol nigram 0 2 1 − 4 2 0 2 4 4 2 0 2 4 x 1 (c) Example 2 for MultiTargeted x 2 9 6 3 0 3 6 9 z ) z, z(xam ssol nigram 0 2 1 − (d) Example 2 for regular PGD Figure 5 | Examples with non-convex adversarial input sets. For all panels, the adversarial input set is highlighted in blue and the region in purple is where misclassification occurs. The solid line in black defines the boundary where the input is classified as class 2. The dashed line is where the two plane z1 − z0 and z2 − z0 intersect. When ξ (0) is sampled within the region highlighted in red, the attack for which that panel corresponds to is successful. At the top, we have an example for which regular PGD is more successful. At the bottom, we have an example for which MultiTargeted is more successful. 12
An Alternative Surrogate Loss for PGD-based Adversarial Testing C. Effect of linearity As discussed in Section 3, the effectiveness of MultiTargeted is dependent on the how the adversarial input set propagates through the network f . In particular, if the propagated set Z(x) = { f (ξ )|ξ ∈ S(x)} is convex, θ θ MultiTargeted is at least as effective as a regular PGD-based attack using the margin or cross-entropy loss (i.e., it always succeeds in finding the optimal attack given enough restarts, see Theorem 3.2). When the adversarial input set S(x) is convex, it is important to characterize the local behavior of f around the nominal input x. Indeed, if θ f behaves linearly then Z(x) is convex. Our hypothesis is that when the f is “close” to linear MultiTargeted θ θ is more effective. To analyze the behavior of a given model, we gather the gradients of all logits with respect to hundred randomly selected inputs around examples from the test set. When the singular values of the matrix that gathers these gradient are mostly concentrated around a few direction, the model should behave more linearly (in the extreme case where a single singular value is non-zero, the model is linear). Figure 6a shows the singular values of three Mnist models. The first two models, denoted “Madry et al.” and “Ours” are adversarially trained model of the same size (i.e., two convolutional layers followed by two fully connected layers). The last model, denoted by “Ours (small)” is a much smaller model (also adversarially trained). We observe that the smaller model tends to behave more linearly. This is also qualitatively shown in Figure 7 that shows how the logits change as a function of the input. Table 2 confirms that under the same compute budget MultiTargeted works better on the smaller, more locally linear, model. For less linear models, it is important to use more restarts and, as such, it is preferential to use a single loss (e.g., margin or cross-entropy loss) and increase the number of restarts. We highlight that, when given more restarts per target class, MultiTargeted can reach the same performance as regular PGD. In comparison, Figure 6b shows the singular values of three Cifar-10 models. We noticed in Section 4 that MultiTargeted is significantly better than regular PGD on robustly trained Cifar-10 models. This plot confirms that these models behave almost linearly (in the local neighborhood of datapoints). This is also qualitatively shown in Figure 8 that shows how the logits change as a function of the input for these models. 1.0 0.8 0.6 0.4 0.2 0.0 seulav ralugnis dezilamroN 1.0 Madry et al. Ours Ours (small) 0.8 0.6 0.4 0.2 0.0 (a) seulav ralugnis dezilamroN Madry et al. Zhang et al. Ours (b) Figure 6 | Ordered singular values of a matrix gathering the gradients of each logit with respect to hundred randomly selected inputs within the adversarial input set around ten test datapoints (the thicker line represents the average over logits and datapoints). All values are normalized by the largest singular value. As the behavior of the model becomes more linear, we expect the curve to become steeper. Panel a shows the singular values of three Mnist models. We observe that the smallest model behaves more linearly than the other two models. Panel b shows the singular values of three Cifar-10 models. We observe that Cifar-10 models tend to behave more linearly than Mnist models. F1-score under attack Mnist ϵ PGD1000×180 MT1000×20 MT1000×180 Madry et al. 0.3 88.49% 88.89% 88.45% Ours 0.3 91.98% 92.96% 92.05% Ours (small) 0.3 74.13% 73.56% 73.11% Table 2 | Comparison of MultiTargeted and regular PGD on three Mnist models. MultiTargeted is more effective on the smaller model. 13
An Alternative Surrogate Loss for PGD-based Adversarial Testing D. Reproducibility The code relevant to this publication is available as part of the Interval Bound Propagation library at https: //github.com/deepmind/interval-bound-propagation under src/attacks.py. 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 u v 12 9 6 3 0 3 6 9 12 eulav tigol 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 u (a) z7 on Madry et al. v 0.0 1.2 2.4 3.6 4.8 6.0 7.2 eulav tigol 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 u (b) z0 on Madry et al. v 4.0 5.5 7.0 8.5 10.0 11.5 13.0 eulav tigol (c) z1 on Madry et al. 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 u v 12 9 6 3 0 3 6 9 eulav tigol 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 u (d) z7 on our model v 1.2 2.0 2.8 3.6 4.4 5.2 6.0 6.8 7.6 eulav tigol 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 u (e) z0 on our model v 3.5 4.5 5.5 6.5 7.5 8.5 9.5 10.5 11.5 12.5 eulav tigol (f) z1 on our model 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 u v 9.6 8.0 6.4 4.8 3.2 1.6 0.0 1.6 3.2 eulav tigol 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 u (g) z7 on our small model v 1.6 4.0 6.4 8.8 11.2 13.6 16.0 18.4 eulav tigol 0.4 0.2 0.0 0.2 0.4 0.4 0.2 0.0 0.2 0.4 u (h) z0 on our small model v 3.6 5.4 7.2 9.0 10.8 12.6 14.4 eulav tigol (i) z1 on our small model Figure 7 | Logit landscapes around the nominal image of a “seven” (first image from the Mnist test set). It is generated by varying the input to the model, starting from the original input image toward either the worst attack found using PGD (u direction) or a random Rademacher direction (v direction). Panels show the contour plots of different logits: from left to right, we have logits z7, z0 and z1; from top to bottom, we have the Madry et al.’s model, our own adversarially trained model of similar size and our smaller adversarially trained model. The diamond-shape represents the projected (cid:96)∞ ball of size ϵ = 0.3 around the nominal image. We observe that within this adversarial input set, the smaller model tends to behave more linearly than the other two models. 14
An Alternative Surrogate Loss for PGD-based Adversarial Testing 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.04 0.02 0.00 0.02 0.04 u v 9.6 8.0 6.4 4.8 3.2 1.6 0.0 1.6 3.2 eulav tigol 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.04 0.02 0.00 0.02 0.04 u (a) z9 on Madry et al. v 9.0 7.2 5.4 3.6 1.8 0.0 1.8 eulav tigol 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.04 0.02 0.00 0.02 0.04 u (b) z0 on Madry et al. v 0.75 1.05 1.35 1.65 1.95 2.25 2.55 2.85 3.15 eulav tigol (c) z1 on Madry et al. 0.04 0.02 0.00 0.02 0.04 0.04 0.02 0.00 0.02 0.04 u v 5.25 4.75 4.25 3.75 3.25 2.75 2.25 1.75 1.25 0.75 eulav tigol 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.04 0.02 0.00 0.02 0.04 u (d) z9 on Zhang et al. v 4.40 4.24 4.08 3.92 3.76 3.60 3.44 3.28 3.12 eulav tigol 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.04 0.02 0.00 0.02 0.04 u (e) z0 on Zhang et al. v 1.2 0.9 0.6 0.3 0.0 0.3 0.6 0.9 1.2 eulav tigol (f) z1 on Zhang et al. 0.04 0.02 0.00 0.02 0.04 0.04 0.02 0.00 0.02 0.04 u v 7.2 6.0 4.8 3.6 2.4 1.2 0.0 eulav tigol 0.04 0.02 0.00 0.02 0.04 0.04 0.02 0.00 0.02 0.04 u (g) z9 on our model v 4.8 4.4 4.0 3.6 3.2 2.8 2.4 2.0 1.6 eulav tigol 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.04 0.02 0.00 0.02 0.04 u (h) z0 on our model v 0.90 0.45 0.00 0.45 0.90 1.35 1.80 eulav tigol (i) z1 on our model Figure 8 | Logit landscapes around the nominal image of a “truck” (first image from the Cifar-10 test set). It is generated by varying the input to the model, starting from the original input image toward either the worst attack found using PGD (u direction) or a random Rademacher direction (v direction). Panels show the contour plots of different logits: from left to right, we have logits z9, z0 and z1; from top to bottom, we have the Madry et al.’s model, Zhang et al.’s model and our own adversarially trained model of similar size. The diamond-shape represents the projected (cid:96)∞ ball of size ϵ = 8/255 around the nominal image. We observe that within this adversarial input set, all models tend to behave linearly. 15
