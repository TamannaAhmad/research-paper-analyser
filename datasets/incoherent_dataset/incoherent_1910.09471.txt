Modelling Generalized Forces with supervised learning for Sim-to-Real Transfer Rae Jeong, Jackie Kay, Francesco Romano, Thomas Lampe, Tom Rothorl, Abbas Abdolmaleki, Tom Erez, Yuval Tassa, Francesco Nori Abstract— Learning robotic control policies in the real world gives rise to challenges in data efficiency, safety, and controlling the initial condition of the system. On the other hand, simula- tions are a useful alternative as they provide an abundant source of data without the restrictions of the real world. Unfortunately, simulations often fail to accurately model complex real-world phenomena. Traditional system identification techniques are limited in expressiveness by the analytical model parameters, and usually are not sufficient to capture such phenomena. In this paper we propose a general framework for improving the analytical model by optimizing state dependent general- ized forces. State dependent generalized forces are expressive enough to model constraints in the equations of motion, while maintaining a clear physical meaning and intuition. We use supervised learning to efficiently optimize the mapping from states to generalized forces over a discounted infinite horizon. We show that using only minutes of real world data improves the sim-to-real control policy transfer. We demonstrate the feasibility of our approach by validating it on a nonprehensile Fig. 1: Manipulation task setup, on the real robot and in manipulation task on the Sawyer robot. simulation. Top row: 3D position matching task (green dot I. INTRODUCTION is the target position). Bottom row: 6D pose matching task (translucent target is the target 6D pose). supervised learning (RL) [1] algorithms have demon- strated potential in many simulated robotics domains such as locomotion [2], manipulation [3] and navigation [4]. Yet these algorithms still require large amounts of data, of observations, rather than direct adaptation the system dy- making learning from scratch on real robots challenging [5], namics [14]. Classical system identification methods provide [6], [7]. Recently, there have been many successes using a sample efficient way to improve the model, but are limited simulation to speed up robot learning [8], [9], [10], [11]. in expressiveness by the model parameters exposed from the Recent works have demonstrated that with enough domain analytical models or simulators [15]. randomization [8], zero-shot transfer of sim-to-real policies Nonprehensile robotic manipulation has been studied pre- are feasible, even in contact-rich, hybrid discrete-continuous viously in the form of planar pushing with dynamics ran- dynamical systems [8], [10]. domization and zero-shot sim-to-real transfer [8]. Policies A successful sim-to-real transfer method balances the trained with dynamics randomization has been shown to expensive real-world data and inexpensive simulated data to produce robust policies which can successfully perform real- yield sample-efficient performance in the real world. Dynam- world nonprehensile robotic manipulation tasks. However, ics randomization [8] can yield successful policy transfer, zero-shot sim-to-real transfer using dynamics randomization but relies on manual selection of simulation parameters does not use any real world data to improve the simulator to randomize and does not incorporate real-world data in and requires manual tuning of the choices and ranges of the the learning process. Model-based RL methods learn the physical parameters to vary in simulation. The chosen param- dynamics model from data and tend to be more sample eters are often very conservative, resulting in a policy that efficient than model-free RL methods, but learning a model trades robustness to large variation of the model parameters still requires high number of real world samples [12], [13], at the expense of overall performance. and may be prohibitive for complex dynamics. Adversarial Alternatively, the simulated model can be improved to domain adaptation methods have the potential to improve the resemble the real world more closely. System identification simulation, but have only been demonstrated for adaptation techniques have been applied to nonprehensile manipulation through joint system identification and state estimation pro- Authors are with DeepMind London, UK. {raejeong, kayj, fraromano, cedure that accurately models the contact dynamics [15]. thomaslampe, tcr, aabdolmaleki, etom, tassa, fnori}@google.com. Qualita- However, system identification methods require the true tive results can be found in our supplementary video: https://youtu. be/2diszIMOn6A system dynamics to be in the realm of expressiveness of 9102 tcO 12 ]OR.sc[ 1v17490.0191:viXra
the analytical model. To get around this, others modelled where q and ν are the system configuration and velocity the complex dynamics of planar pushing by combining both respectively. M is the mass matrix, c represents the bias the learned and analytical models [16]. forces, including Coriolis, centrifugal, joint friction and grav- Modelling complex dynamics is important for nonpre- itational forces. τ denotes the internal, actuation, torques, hensile manipulation but is also critical for locomotion. and 0 is a vector of size m of zeros denoting the unactuated m Recent work performed sim-to-real transfer of locomotion variables. External 6D forces f are mapped to generalized k tasks by improving the simulation fidelity through system forces by the corresponding contact Jacobian J . k identification, modelling the actuators and latency of the Note that Eq. (1) is quite general. Depending on the system, which was then used in conjunction with dynamics definition of the state variables q and ν, it can be used to randomization to train a robust policy for better sim-to-real represent an articulated system, e.g. a robot arm, a generic transfer [10]. The idea of using both the learned and the free floating object, or both systems. If the systems are inter- analytical model has also been applied to locomotion where acting with each other or with the environment, Eq. (1) needs the complex actuator dynamics were learned for a quadruped to be complemented with additional equations describing robot [17]. the contact constraints. Without making any assumption on Ground Action Transformation (GAT) [18] is a more the kind of contacts, we can express these constraints as a general method which modifies the actions from the pol- generic function of the state of the involved systems: icy trained in simulation. The modified actions are chosen h (q, ν) = 0. (2) such that, when applied in simulation, the resulting next c state matches the next state observed on the real robot. Note that, in the case of rigid contacts, (2) is greatly Our proposed approach can be seen as a generalization of simplified, but in the following we will consider its most the Ground Action Transformation method. We introduce general form. additive generalized forces to the environment instead of modifying the agent’s actions. The optimization of GAT B. supervised learning and our method is also different, in that GAT learns an We consider the supervised learning setting with a inverse dynamics model in both simulation and the real discounted infinite horizon Markov Decision Process (MDP). robot to compute the action transformation, while our method An MDP consists of the tuple (S, A, r, P, µ ): the set of valid 0 optimizes the objective directly over a discounted infinite states, set of valid actions, the reward function, transition horizon through the use of supervised learning. dynamics and the initial state distribution. Let r(s , a ) t t This paper focuses on sim-to-real transfer of robotic denote the reward obtained in state s of an MDP when t manipulation by efficiently utilizing a small amount of real executing action a . The supervised learning objective is t world data to improve the simulation. Our contribution is to find the optimal policy such that: twofold. First, we provide a novel sim-to-real method which ∞ (cid:104) (cid:88) (cid:105) learns a state dependent generalized force model using neural π∗ = argmax E γtr(s , a )|s = s¯ , (3) π∈Π π,P t t 0 networks as expressive function approximators to model t=0 the constraint forces of a dynamical system. Second, we where γ is the discount factor and Π is the set of all possible efficiently optimize the state-dependent generalized force policies and the expectation with respect to π is ∀t : a ∼ t model over a discounted infinite horizon through the use of π(·|s ), and the states follow the system dynamics of the t RL. We validate the proposed approach on a nonprehensile MDP, i.e. we have s ∼ P (·|s , a ) where P (s |s , a ) t+1 t t t+1 t t robotic manipulation task on the Saywer robot. denotes the transition probability. This paper is organized as follows. Section II introduces With this notation in place, we can define the action-value the mathematical background on mechanical systems and function, which evaluates the expected sum of rewards for a supervised learning problems. Section III describes how policy π with a one-step Bellman equation of Qπ(s, a): modelling the generalized forces can be used to improve (cid:104) (cid:105) sim-to-real transfer and how these forces can be learned Qπ(s , a ) = r(s , a ) + γE Qπ(s , a ) . (4) t t t t π,P t+1 t+1 efficiently using supervised learning. Finally, Section IV discusses the experimental validation of the proposed ap- The action-value function is learned for off-policy re- proach. Conclusions and perspectives conclude the paper. inforcement learning during the policy evaluation step to evaluate the policy. The policy itself is improved using the II. BACKGROUND result of policy evaluation in the policy improvement step. Iteratively applying these two steps is referred to as policy A. Mechanical Systems and Equations of Motion iteration [1]. In classical mechanics, any rigid body system subject to external forces can be described with the following equations III. MODELLING GENERALIZED FORCES WITH RL of motion [19, Ch. 13.5]: Our method uses a hybrid approach: the analytical model of a mechanical system is supplemented with a paramet- (cid:20) (cid:21) K M (q)v˙ + c(q, ν) = 0 m + (cid:88) J (cid:62)(q)f , (1) ric function representing additional generalized forces that τ k k are learned using real world data. Consider the dynamics k
Fig. 2: Description of the proposed method steps. 1) Train an agent in simulation with the original model parameters. 2) Use the agent trained in step 1 to collect real world data. 3) Learn the generalized force model (GFM): initialize the simulation to the initial state of a real world trajectory and simulate forward, choosing the same actions taken on the real robot. The GFM injects generalized forces into the simulation with the objective of minimizing the difference between the real world and simulation states. 4) Retrain the agent for solving the task of interest using the updated hybrid model. equation in Eq. (1). In addition to the actuation and external Define z = {x , a , x , a , x , · · · } a trajectory consist- i 0 0 1 1 2 forces, we add a state-dependent generalized force term, i.e. ing of a sequence of states and actions collected on the real system, and X = {z }N the corresponding dataset (cid:20) (cid:21) K i i=0 0 (cid:88) of all the acquired real world trajectories. Let us define the M v˙ + c = m + J (cid:62)f + F (q, ν). (5) τ k k φ following reward function: k r (x , s ) = e−||∆(xt,st)||2 , (7) We represent the contact-aware system dynamics in Eqs. f t t (5) and (2) as a hybrid discrete dynamics model s t+1 = with ∆(x t, s t) a suitable distance function for measuring the f θ(s t, a t), parameterized by vector θ, which takes as input distance between x t and s t. At the beginning of each training the current state and action and outputs the prediction of the episode, we randomly sample an initial state x ∈ X, and we 0 next state s t+1. We want to find the parameter vector θ∗ that initialize the internal dynamics model to x 0. The sum of the minimizes the dynamics gap along a horizon T , that is: rewards in Equation 7 is maximized with the RL objective in Equation 3, resulting in the following objective: T (cid:88) θ∗ = argmin ||x − f (s , a )||2 (6) ∞ t+1 θ t t (cid:104) (cid:88) (cid:105) F ∗ = argmax E γtr (x , s )|s = x , (8) t=0 φ φ Fφ,PS,X f t t 0 0 t=0 where x represents the resulting next state in the real t+1 where P is the transition probability distribution of the system. In contrast to other hybrid modelling approaches that S unmodified dynamic model given in Eqs. (1),(2). learn the residual state difference, we choose to model the The resulting generalized force model (GFM), F , is gap with generalized forces. This choice has two benefits: φ then used as the transition model P for the hybrid model i) generalized forces are applied directly to the dynamics f (5) and (2). We then train the policy for a control task of equation, that is, the next state resulting from the application interest using the hybrid model with the transition probability of Eqs. (5), (2) is a physically consistent state; ii) it is easier distribution P , resulting in the following reinforcement to impose constraints on the learned generalized forces as f learning objective: they have a physical correspondence. It is worth noting that the resulting hybrid model is non- (cid:104) (cid:88)∞ (cid:105) differentiable, as Eq. (2) is in general non-differentiable. By π∗ = argmax π∈Π E π,Pf γtr(s t, a t)|s 0 = s¯ . (9) consequence, the function f is not differentiable with re- t=0 θ spect to its arguments and the optimal generalized forces F ∗ To optimize both Equation 8 and 9, we chose the policy φ are not trivial to obtain using classical methods. However, iteration algorithm called Maximum a Posteriori Policy Op- the formulation of optimizing a state-dependent model inter- timization (MPO) [20], which uses an expectation maximiza- acting with a non-differentiable component over a horizon tion style policy improvement with an approximate off-policy strongly resembles the supervised learning objective in policy evaluation algorithm Retrace [21] which estimates the Equation 3. We therefore reformulate the original problem action-value function given in Equation 4. of optimizing the objective in Equation 6 as a reinforcement Fig. 2 shows the proposed method, highlighting the dif- learning problem. ferent steps from the first policy training in the unmodified
Fig. 3: Joint position control experiment. Pictured are the position setpoint reference trajectory for the base joint of the arm, the position of the real robot after following Fig. 4: Top row: object and plate end effector for the 3D the command, the prediction from the model with default position matching task. Bottom row: object and ball end parameters, after system identification and from the hybrid effector for the 6D pose matching task. Objects 6D poses model. are tracked using the AR tags on the sides of the object. A. Experimental Setup simulator (P , step 1) to the final policy trained on the S updated simulator model (P , step 4). Fig. 1 shows the real and simulated setup used in our f experiments. The platform is a 7 degrees of freedom (DoF) IV. EXPERIMENTAL RESULTS AND DISCUSSION Rethink Robotics Sawyer robotic arm. A basket, where the objects lie and move, is placed in front of the robot. In this section we present the results of the experiments Depending on the specific experiment, the object and the end we performed to validate the proposed sim-to-real transfer effector of the robot can change. The full hardware setup, method. The experiments have been devised to answer to the i.e. robot, basket and objects, are modelled in simulation following questions. i) Does the use of the GFM improve the using the MuJoCo simulator [22] where we represented the modelling capacity with respect to an analytical model with orientation component by using unitary quaternions, (cid:63) is the parameters obtained from system identification? ii) How does quaternion product and x(i)∗ denotes the complex conjugate t the training of GFMs scale with an increase in real world of the quaternion. We define the following limits for the data? iii) What is the impact of GFMs on policy training in learned generalized forces: simulation? iv) Does the use of GFMs improve sim-to-real • 5Nm for the forces acting on the robot joints, transfer for nonprehensile robotic manipulation tasks? • 0.03N for the forces acting on the translational part of Referring to Eq. (5), the configuration of our experimental the object and system is q ∈ R7 × R3 × SO(3) with the velocity ν ∈ R7+6. The actuated torques are τ ∈ R7 with the learned generalized • 0.03Nm for the forces acting on the rotational part of the object. forces F ∈ R7+6. φ While learning the GFMs, we also provide as observation The position and velocity of the robot joints are obtained to the RL algorithm the 5 previous states of the dynamics by using the local robot sensors, i.e. the joint encoders. The simulation. object is tracked by using a fiduciary marker system based on Both the GFM and the control policy are feed-forward AR tags, see Fig. 4. We track the object independently with neural networks with two layers, each with a size of 200. The three cameras and then merge the information into a single neural network outputs a Gaussian distribution with the mean 6D estimation value. To obtain the velocity, we perform a µ and diagonal Cholesky factors A, such that Σ = AAT . The finite differentiation of the estimated position. The robot is diagonal factor A has positive diagonal elements enforced controlled at 20Hz while the simulation is integrated every by the softplus transform A ← log(1 + exp(A )) to ensure 2ms. ii ii positive definiteness of the diagonal covariance matrix. With reference to the algorithm outlined in Sect. III, we define the distance function ∆(x t, s t) as follows: B. Joint Position Control (cid:40) To examine the GFM’s modelling capabilities, we collect x(i) − s(i) if x(i) ∈ Rl ∆(x(i), s(i)) = t t t (10) trajectories of the robot controlled in position mode, follow- t t s( ti) (cid:63) x( ti)∗ if x( ti) ∈ SO(3), ing a pre-defined waypoint-based trajectory. No interactions with the object or the basket took place in this experiment.
Fig. 6: Average return when learning the position matching Fig. 5: The 10 target positions for the position matching task task with and without the use of curriculum and GFM. are shown as the green triangles. The use of curriculum results in a non-monotonic learning curves, because throughout the course of training the control authority of the policy is reduced. MuJoCo by default exposes two parameters for modelling the actuator, namely the actuator gain and damping. We identify these parameters by using SciPy [23] Sequential environment, and, ii) lower velocity for the object whilst Least Squares Programming. We then train our proposed under interaction with the robot allows better pose estimation hybrid model using the approach described Sect. III. from the AR tags system. We report the comparison of the trajectories resulting from We evaluate the agent’s performance on the task with a the different models in Fig. 3. We can notice that both the binary success-failure criterion for 5 trials. An episode is a GFM and the parameters-identified model improve upon the success if the error between the object’s position and the default simulation model. However, the GFM hybrid model target is less than 2.5cm for 10 consecutive time steps, i.e. is able to capture the complexity of the real data better 0.5s. Each trial consists of 10 target positions, making the than the simulator’s simplified actuator model alone. This total number of target positions attempted for each policy to also demonstrates the generality of our GFM hybrid model 50. The initial pose of the object is reset to the center of the approach, as no specific knowledge about actuator dynamics basket at the beginning of every attempt. The target locations are needed for training the GFM. However, it is worth noting are fixed and are radially positioned around the center of the that the GFM hybrid model is a local approximation around basket – See Figure 5. the real data used for training. D. Ensemble of Hybrid Models C. Position Matching Task We first examine how the training of GFMs scales with To validate and assess quality of the proposed method, we the increase in real world data. use a position matching task for the free-moving object. The We train a policy for the position matching task using the goal of this task is to move the object to a specific target default simulator model as shown in step 1 of Fig. 2, and location. The robotic arm is equipped with an acrylic plate we use this policy to collect a dataset of 10 trajectories on as its end effector as can be seen in Fig. 4. the real robot. We provide the state of the robot, the pose of the object, We then train two GFMs using different amounts of real and the target position of the brick as observations to the world trajectories. The first one uses 5 trajectories, while agent, as described in Sect. IV-A. We provide a history of the second one uses all 10 trajectories. The second GFM observations by providing to the agent the last 10, including faces a more challenging modelling problem as it has to the current, observations. Each episode is composed of a learn to fit more data points from the real world. In our GFM maximum of 400 transitions (corresponding to 20s). The training, using 5 trajectories on average resulted in around episode is terminated, and considered a failure, if the end- 10% higher average return when compared to the GFM that effector of the robot leaves its workspace. used 10 trajectories. The agent’s actions consist of joint velocities, representing In practice, given that we would like to use more than reference setpoints for the robot’s low-level joint velocity 5 real world trajectories to improve the dynamic model, we controller. We decided to limit the maximum joints velocity decide to employ an ensemble of GFMs instead of increasing to 5 percent of the allowed range of velocities. Two reasons the number of trajectories used during the GFM learning. All motivated this choice: i) lower velocity implies safer action the following experiments are trained with the fixed number from the robot while interacting with the object and the of 5 trajectories per model.
TABLE I: Position Matching Task Evaluation TABLE II: Pose Matching Task Evaluation Models Task Success Std. Dev. Models Task Success Std. Dev. Original model 38% 3.4 Original model 18% 2.7 Original model with Curriculum 62% 3.4 Hybrid Model (3 models in ensemble) 44% 3.5 Random Force Perturbations 70% 3.2 Hybrid Model (1 model in ensemble) 58% 3.5 Hybrid Model (3 models in ensemble) 84% 2.6 Hybrid Model (5 models in ensemble) 74% 3.1 trajectories, it is plausible that the hybrid model is over- fitting to those particular trajectories. The use of 3 GFMs significantly improves the sim-to-real transfer and results in 84% success. Using 5 GFMs still outperforms all of the E. Training Policies on Hybrid Models baselines but results in a degradation of performance when The additional complexity introduced by the hybrid model compared to using only 3 GFMs. Indeed, while using more has an impact in the learning performance of the final policy. GFMs allows us to utilize more real world data, it also makes Fig. 6 shows that the additional F term significantly slows φ the task more challenging, even compared to the real world. down training when compared to the original unmodified This results in a more conservative controller which is not model. This is expected, as GFM essentially injects force as optimal as when we have the right balance of GFMs to perturbations which makes the task harder to solve. To reli- model the real world. ably obtain the optimal policy when training with the hybrid model, we employ a simple curriculum learning strategy. G. 6D Pose Matching Task At the beginning of the training, we allow higher control Finally, we provide the results for sim-to-real policy trans- authority to facilitate exploration and task completion, while fer for a much more challenging nonprehensile manipulation progressively lowering it towards the end. We implemented task. The task is similar to the one described in Sect. IV-C, the curriculum by introducing an additional multiplicative but this time we consider also the orientation of the object. gain to the controlled joint velocities. We started with a gain The task and learning setup is exactly the same as the one of 6, progressively reducing it to 1 by unitary steps every described in Sect. IV-C, except for the following changes. 1000 episodes. Fig. 6 shows the training curves for policy The episode duration is 600 transitions, i.e. 30s. During the learning on both the unmodified dynamic model and the evaluation procedure we consider an attempt as successful if hybrid model when using the curriculum strategy. We can the position error is less than 5cm and the orientation error clearly see that the curriculum improves the speed and the around the vertical axis is less than 20 degrees. final performance of the policy. Table II presents the results of the sim-to-real transfer for the 6D pose matching task. As for the previous experiment, F. Position Matching Task: Experimental Results our method is able to significantly improve over the baseline This section reports the results of the sim-to-real policy of 18% to 44% success. The success rate is lower when transfer for the nonprehensile position matching task when compared to the previous experiment, but this applies to both using both the model ensemble and curriculum learning the policy learned on the original model and on the hybrid strategy discussed in the previous sections. model, showing the complexity of the task. Table I summarizes the results, reporting the average success percentage and the standard deviation. We compare V. CONCLUSIONS our method to 3 baseline methods which are i) policy In this work we presented a novel sim-to-real method that trained with the original dynamic model, ii) policy trained aims to close the sim-to-real gap by modelling and learning with the original model and using curriculum learning and, the state dependent generalized forces that capture this dis- iii) policy trained with random perturbations of generalized crepancy. We validated the proposed approach by performing forces and using curriculum learning. As expected, the policy sim-to-real transfer for nonprehensile robotic manipulation trained with the original model performs poorly due to the tasks. We showed that our method improves the performance dynamics gap resulting in 38% success. However, the default of sim-to-real transfer for 3D position matching and 6D pose simulation trained with our curriculum learning performs matching task of different objects. While the results reported significantly better with 62% success. Given that the curricu- in Sect. IV-G on 6D pose matching showed that our approach lum learning induces time-varying actuator gains, this has a yields a considerable improvement with respect to the default similar effect to applying domain randomization. Lastly, the model, there are still rooms for improvements. We thus want addition of random generalized force perturbations on top of to analyze and improve on this task further. our curriculum learning results in 70% success. When controlling interaction forces, it is common to The lower part of Table I reports the performance of choose torque control as the low level controller. Despite transferred policies trained with the hybrid model for a this classic choice, the use of torque control is not common different number of model ensembles (in parentheses). While in supervised learning. Future work could investigate if the use of one GFM leads to an improvement over the the use of our proposed generalized force model helps in original model, it does not perform better than the other two learning an interaction task while controlling the robot by baselines. Given that a single GFM only uses 5 real world using torque control.
REFERENCES [1] R. S. Sutton and A. G. Barto, supervised learning: An introduction. MIT press, 2018. [2] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, S. M. A. Eslami, M. A. Riedmiller, and D. Silver, “Emergence of locomotion behaviours in rich environments,” CoRR, vol. abs/1707.02286, 2017. [3] A. Rajeswaran*, V. Kumar*, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine, “Learning Complex Dexterous Manip- ulation with Deep supervised learning and Demonstrations,” in Proceedings of Robotics: Science and Systems (RSS), 2018. [4] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu, “IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures,” CoRR, vol. abs/1802.01561, 2018. [5] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of deep visuomotor policies,” CoRR, vol. abs/1504.00702, 2015. [6] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen, “Learning hand- eye coordination for robotic grasping with deep learning and large- scale data collection,” CoRR, vol. abs/1603.02199, 2016. [7] M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, V. Mnih, N. Heess, and J. T. Springenberg, “Learning by playing solving sparse reward tasks from scratch,” in International Conference on Machine Learning, 2018, pp. 4341–4350. [8] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Sim-to- real transfer of robotic control with dynamics randomization,” CoRR, vol. abs/1710.06537, 2017. [9] F. Sadeghi and S. Levine, “Cad2rl: Real single-image flight without a single real image,” CoRR, vol. abs/1611.04201, 2016. [10] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bo- hez, and V. Vanhoucke, “Sim-to-real: Learning agile locomotion for quadruped robots,” CoRR, vol. abs/1804.10332, 2018. [11] Y. Zhu, Z. Wang, J. Merel, A. A. Rusu, T. Erez, S. Cabi, S. Tun- yasuvunakool, J. Krama´r, R. Hadsell, N. de Freitas, and N. Heess, “Reinforcement and imitation learning for diverse visuomotor skills,” CoRR, vol. abs/1802.09564, 2018. [12] I. Clavera, J. Rothfuss, J. Schulman, Y. Fujita, T. Asfour, and P. Abbeel, “Model-based supervised learning via meta-policy op- timization,” 2018. [13] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine, “Neural network dynamics for model-based deep supervised learning with model- free fine-tuning,” 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 7559–7566, 2018. [14] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakr- ishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, and V. Vanhoucke, “Using simulation and domain adaptation to improve efficiency of deep robotic grasping,” CoRR, vol. abs/1709.07857, 2017. [15] K. Lowrey, S. Kolev, J. Dao, A. Rajeswaran, and E. Todorov, “Re- inforcement learning for non-prehensile manipulation: Transfer from simulation to physical system,” CoRR, vol. abs/1803.10371, 2018. [16] N. F. M. B. L. P. K. J. B. T. Anurag Ajay, Jiajun Wu and A. Rodriguez, “Augmenting physical simulators with stochastic neural networks: Case study of planar pushing and bouncing,” in International Con- ference on Intelligent Robots and Systems (IROS), 2018. [17] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter, “Learning agile and dynamic motor skills for legged robots,” Science Robotics, vol. 4, no. 26, 2019. [18] J. P. Hanna and P. Stone, “Grounded action transformation for robot learning in simulation,” in AAAI, 2017. [19] J. E. Marsden and T. S. Ratiu, Introduction to Mechanics and Symme- try: A Basic Exposition of Classical Mechanical Systems. Springer Publishing Company, Incorporated, 2010. [20] A. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess, and M. A. Riedmiller, “Maximum a posteriori policy optimisation,” CoRR, vol. abs/1806.06920, 2018. [21] R. Munos, T. Stepleton, A. Harutyunyan, and M. G. Bellemare, “Safe and efficient off-policy supervised learning,” CoRR, vol. abs/1606.02647, 2016. [22] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for model-based control,” in Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE, 2012, pp. 5026–5033. [23] E. Jones, T. Oliphant, P. Peterson, et al., “SciPy: Open source scientific tools for Python,” 2001.
