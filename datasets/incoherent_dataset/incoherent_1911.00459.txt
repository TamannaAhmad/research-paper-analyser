POSITIVE-UNLABELED REWARD LEARNING Danfei Xu ∗ Misha Denil Stanford University Deepmind danfei@cs.stanford.edu mdenil@google.com ABSTRACT Learning reward functions from data is a promising path towards achieving scal- able supervised learning (RL) for robotics. However, a major challenge in training agents from learned reward models is that the agent can learn to ex- ploit errors in the reward model to achieve high reward behaviors that do not correspond to the intended task. These reward delusions can lead to unintended and even dangerous behaviors. On the other hand, adversarial imitation learning frameworks (Ho & Ermon, 2016) tend to suffer the opposite problem, where the discriminator learns to trivially distinguish agent and expert behavior, resulting in reward models that produce low reward signal regardless of the input state. In this paper, we connect these two classes of reward learning methods to positive- unlabeled (PU) learning, and we show that by applying a large-scale PU learning algorithm to the reward learning problem, we can address both the reward under- and over-estimation problems simultaneously. Our approach drastically improves both GAIL and supervised reward learning, without any additional assumptions. 1 INTRODUCTION While supervised learning (RL) has shown itself to be a powerful tool for automating con- trol and decision making, hand-specifying reward functions requires significant engineering effort, especially in real-world settings. Recent works have made promising progress in learning reward functions directly from human supervision, such as ratings (Cabi et al., 2019) and behavior prefer- ences (Wilson et al., 2012; Ibarz et al., 2018). However, in practice, these supervisions are expensive to curate and thus often only cover a fraction of the state space. As a result, the learned reward functions may have large errors in the unlabeled states, and policy learning algorithms tend to ex- ploit these errors to achieve extremely high pseudo-reward via unintended behaviors (Amodei et al., 2016). Practical solutions often require a human to provide supervision in the policy training loop iteratively (Christiano et al., 2017; Ibarz et al., 2018), resulting in a even more laborious process. On the other hand, works in Inverse supervised learning (IRL) propose to infer reward func- tions directly from expert behaviors (Ng et al., 2000; Ziebart et al., 2008), but scaling these methods to high-dimensional state space remains a challenge. Recently, Ho & Ermon (2016) introduced Generative Adversarial Imitation Learning (GAIL), which directly recovers expert behaviors via a proxy reward function1 derived from a discriminator that is trained to distinguish between expert demonstrations and the behaviors of an imitating policy. Ho & Ermon (2016), and many follow-up works show that GAIL can learn complex behaviors even in high-dimensional spaces. However, a major limitation of GAIL-like methods is that the learned reward functions may overfit to triv- ially distinguish between the expert and the agent via features that are irrelevant to the intended behaviors (Peng et al., 2018; Blonde´ & Kalousis, 2018; Reed et al., 2018; Z˙ ołna et al., 2019a). In this paper, we develop a unified reward learning algorithm that addresses both 1. The reward delusion problem that arises with supervised reward learning. 2. The overfitting problem encountered with GAIL-like methods. ∗This work was done during an internship at Deepmind. 1The sole purpose of the proxy reward function is for imitation learning, not to recover the environment reward. See Ho & Ermon (2016) and Fu et al. (2017) for more discussions. 1 9102 voN 1 ]GL.sc[ 1v95400.1191:viXra
GAIL Positive-Negative GAIL Positive-Unlabeled GAIL True Reward (requires priviliged information) (ours) Supervised Semi-supervised Reward supervision Expert Demonstration Policy Rollout Success Failure Figure 1: Left: Illustration of the reward delusion problem in supervised reward learning, along with our solution. The x-axis represents the state space of the task, and the y-axis represents reward. The black dots show annotated states used for training the reward model, and the red dashed line shows reward model predictions extended over the entire state space. There is a large region of the state space where the reward model vastly overestimates the true reward (shown in blue). Our solution in this case is to learn the function D, shown in green, that identifies regions of the state space where the reward model is a reliable estimate of the true reward. Right: Illustration of the overfitting problem of GAIL, along with our solution. Early in training (t ) agents will generally fail 0 to complete the intended task, and the discriminator can easily separate agent and expert experience. Later in training, when the agent is competent (t ), the agent will often successfully complete the T task, and the GAIL discriminator will overfit by focusing on irrelevant features. If we could detect success and failure for policy rollouts then we could train the discriminator to distinguish between them directly (middle column), but this information is generally not available. Our solution uses PU learning to train a success vs. failure classifier using positive (expert) and unlabeled (agent) data, without requiring success and failure annotations for policy rollouts. To develop such an algorithm, we begin by illustrating how a binary (positive-negative) classification problem arises in both settings. This classification problem corresponds to support set estimation for supervised reward learning, and the standard discriminator learning formulation for GAIL. We then point out a fundamental issue of such a formulation, which we argue is largely responsible for the overfitting problem of GAIL (Section 4.1). We argue that in both cases agent experience should not be treated as negative data, but rather as an unlabeled mixture of positive and negative data. In GAIL this changes the discriminator objective from distinguishing between expert and agent using a positive dataset of expert trajectories and a negative dataset of agent experience, to distinguishing between success and failure using a positive dataset of successes (the expert trajectories) and an unlabeled dataset containing a mixture of successes and failures (the agent experience). In supervised reward learning, we are faced with delusions in the reward function, which occur when the agent experience strays far from the annotated data. Here the appropriate classification problem is to train an indicator for the support set of reliable reward predictions. We can obtain a training set for this discriminator in the form of a positive dataset where the reliability of the reward model can be guaranteed (the training set for the reward model), and an unlabeled dataset containing a mixture reliable and unreliable states (the agent experience). In both cases, training the discriminator is posed as a positive-unlabeled (PU) learning problem (De- nis, 1998; Elkan & Noto, 2008; du Plessis et al., 2014), and we are able to train a reliable discrim- inator with the desired semantics by applying a recent large scale PU learning algorithm (Kiryo et al., 2017). We call the resulting reward learning framework Positive-Unlabeled Reward Learning (PURL). We empirically evaluate PURL on a standard benchmark task and two challenging robotic manip- ulation tasks with pixel inputs and continuous control space. We show that PURL is able to (1) learn robust reward functions with limited reward supervisions and (2) outperform a robust GAIL 2
baseline by a large margin. In addition, we test the robustness of PURL by comparing against GAIL and supervised reward learning when there is a domain gap between training and testing. 2 RELATED WORKS In this paper, we focus on two classes of reward learning methods: learning from expert demon- stration by adversarial imitation learning (Ho & Ermon, 2016) and learning from supervised reward signals (Cabi et al., 2019). Generative Adversarial Imitation Learning (GAIL; Ho & Ermon, 2016), has accrued a wide range of variants including Li et al. (2017), Fu et al. (2017), and Baram et al. (2017). The central idea is to train a discriminator model to assign higher reward values to expert demonstrations than the imitating policy through a binary classifcation objective. However, a key limitation of GAIL is that the discriminator may overfit to features of the observations that are irrelevant to the intended behaviors (Z˙ ołna et al., 2019a). This issue is especially pronounced in the cases of high-dimensional observations, where artifacts such as lighting can be used to trivially distinguish the data sources. A few works have attempted to address the overfitting problem of GAIL by regularizing the discrim- inator (Peng et al., 2018; Blonde´ & Kalousis, 2018; Reed et al., 2018). Peng et al. (2018) introduce a variational information bottlebeck to hide information from the discriminator. Blonde´ & Kalousis (2018) and Reed et al. (2018) propose to limit the capacity of the discriminator model. Z˙ ołna et al. (2019a) regularize the discriminator through data augmentation. In this paper, we point out a more fundamental issue to the discriminator objective, namely that agent behavior should not be treated as negative data but rather unlabeled data, and we show that this change in semantics largely eliminates the overfitting problem in GAIL. We also consider the case of learning a reward function from explicit supervision. There is a large body of works on learning reward functions from supervisions such as human ratings or prefer- ences (Cabi et al., 2019; Ibarz et al., 2018; Akrour et al., 2011). However, because the reward supervision often only covers a small part of the state space, RL algorithms may exploit the errors in the reward models to achieve high pseudo-reward from unintended behaviors (Everitt & Hutter, 2016). We directly address this problem by training a discriminator to identify the reliable support set of the reward function. Our approach can be applied to deep network-based reward models that take raw pixels as inputs. Our work builds on the recent progress of positive-unlabeled (PU) learning (du Plessis et al., 2014; 2015b; Kiryo et al., 2017), where the task is to train a classifier from positively-labeled data and un- labeled data. Most existing works in PU learning require certain loss functions, linear models, and/or special optimizers (du Plessis et al., 2014; 2015b; Patrini et al., 2016). Recently, Kiryo et al. (2017) proposed a large-scale PU learning algorithm that can be applied to complex decision functions such as deep networks and can be optimized using common parallel stochastic optimizers (Kingma & Ba, 2014). We frame reward learning problems discussed above as positive-unlabeled classifica- tion problems, and adapt the empirical risk estimator introduced in Kiryo et al. (2017) to train reward functions from either expert demonstrations or reward supervisions, combined with unlabeled agent experiences. 3 BACKGROUND Generative Adversarial Imitation Learning (GAIL). Ho & Ermon (2016) pose the Inverse Re- inforcement Learning (IRL) problem as a two-player zero-sum game within the Generative Ad- versarial Networks (GAN) framework (Goodfellow et al., 2014): A discriminator D is trained to θ distinguish between the behaviors of an agent policy π, and an expert policy π , while the agent e policy is trained to generate behaviors that maximally resemble the expert. The game reaches equi- librium when the discriminator cannot distinguish the agent behaviors from the expert behaviors. We take a state-only formulation of GAIL that is known to improve the robustness of the algorithm (Fu et al., 2017; Z˙ ołna et al., 2019b) and also does not require expert action information. The training objective for the discriminator is given by L = E [log(D (s))] + E [log(1 − D (s))] , (1) D π θ πe θ 3
where D : S → (0, 1). GAIL alternates between training the discriminator D using Equation 1, θ θ and the policy π using the learned reward function r (s) = − log(1 − D (s)). θ θ Semi-supervised reward learning. In addition to learning reward functions from expert demon- strations, we also consider the settings where we have access to a pool of experiences annotated with reward signals. Possible forms of annotations include human rating or ranking of states or trajecto- ries (Ibarz et al., 2018; Vecerik et al., 2019; Cabi et al., 2019). Without loss of generality, we assume we have access to a set of reward-annotated environment states D = {(s , r )}Ns . The goal of s i i i=1 supervised reward learning is to find a reward function r that minimizes a reward loss function l φ r on D : s (cid:88) arg min l (r (s), r) . (2) r φ φ (s,r)∈Ds However, as will be discussed in Section 4, and shown empirically in Section 5, a major challenge in using r for RL is that the policy learning algorithm tend to exploit the errors in r to achieve φ φ high pseudo-reward (Reed et al., 2018). In this paper, we address this challenge by making use of unlabeled states D = {(s )}Nu . An ideal source of such unlabeled states is the replay buffer filled u i i=1 with agent experiences. We refer to the new setup as the semi-supervised reward learning problem. Positive-unlabeled learning. Our reward learning algorithm builds on a long line of works in positive-unlabeled (PU) learning, which tackles the problem of learning classifiers from positive data D , and unlabeled data D . Following Kiryo et al. (2017), let (X, Y ) be the input and output of a p u binary classification problem, where X ∈ Rd and Y ∈ {0, 1}. We consider the two-sample problem setting, where the D ∼ P (X, Y = 1) data and the D ∼ P (X) data are drawn independently. Let p u g : Rd → R be the decision function, and l : R × {0, 1} → R be the loss function. Using the labeled risk operator Ry(D) = E [l(g(x), y)] , (3) g D and the corresponding empirical operator, Rˆy(D) = Eˆ [l(g(x), y)] = 1 (cid:88) l(g(x), y) , (4) g D |D| x∈D we can write the risk for a binary classification problem with positive data D and negative data D p n as the sum of the true positive and true negative risk Rpn(D , D ) = ηR1(D ) + (1 − η)R0(D ) . (5) g p n g p g n where η = P (Y = 1) is the positive class prior. The key insight of PU learning is that for an appropriately chosen loss function, the true negative risk R0(D ) can be expressed in terms of positive and unlabeled data as (du Plessis et al., 2014; g n 2015b) (1 − η)R0(D ) = R0(D ) − ηR0(D ) . (6) g n g u g p The PU risk of decision function g given only positive and unlabeled data is thus Rpu(D , D ) = ηR1(D ) − ηR0(D ) + R0(D ) . (7) g p u g p g p g u The empirical PU risk, Rˆpu(D , D ) = ηRˆ1(D ) − ηRˆ0(D ) + Rˆ0(D ) , (8) g p u g p g p g u is both an unbiased and consistent estimator of the true PU risk (du Plessis et al., 2014), and this estimator also enjoys other desirable properties such as upper-bounded risk (Niu et al., 2016). How- ever, these guarantees are lost when the decision function g becomes too complex, e.g. if g is a deep neural network, leading to issues with overfitting (Kiryo et al., 2017). Kiryo et al. (2017) subsequently propose a non-negative empirical estimator for Equation 7, for which the estimation error can be bounded even when g is complex, by enforcing the constraint 4
Rˆ0(D ) − ηRˆ0(D ) ≥ 0. In practice, this non-negativity constraint is relaxed with a slack variable g u g p β ≥ 0 to account for mini-batch stochastic optimization. The non-negative PU risk estimator of Kiryo et al. (2017) is given by R˜pu(D , D ) = ηRˆ1(D ) + max(−β, Rˆ0(D ) − ηRˆ0(D )) . (9) g p u g p g u g p In Section 4, we reduce both adversarial imitation learning and semi-supervised reward learning to PU learning problems, and show that we can use this non-negative risk estimator to improve both reward learning methods. 4 POSITIVE-UNLABELED REWARD LEARNING Our goal is to learn reward functions entirely from data for RL. We consider two problem settings. In the first, we learn reward functions from expert demonstrations in an adversarial imitation frame- work (Ho & Ermon, 2016). The challenge is that the discriminator may trivially distinguish between agent and expert behavior and that the resulting reward function produces low reward regardless of the input. The second setting is to learn reward functions from a fixed dataset of reward-annotated experiences and a pool of unlabeled agent experiences. The challenge here is that the RL algorithms tend to exploit errors in the reward functions to achieve high pseudo-reward. In this section, we reduce both problems to positive-unlabeled (PU) learning problems and propose an unified reward learning algorithm based on large-scale PU learning. 4.1 ADVERSARIAL IMITATION LEARNING AS PU LEARNING As explained in Section 3, the objective of the discriminator model in GAIL is to distinguish be- tween policy rollout and expert demonstrations. Here, we argue that this objective contributes to the discriminator overfitting problem, and we provide a new discriminator formulation that addresses the problem. To see how the discriminator objective leads to overfitting, we first observe that the GAIL objective (Equation 1) is the logistic loss (assuming D ∈ (0, 1)) of the binary classification between the states induced by the imitating policy π and the expert policy π . In other words, it is a positive- e negative (PN) learning problem, where the positive data is generated by π and the negative data is e generated by π. This objective may be effective at the beginning of the learning process, where π behaves randomly and the resulting state visitations can be considered as negative data relative to the expert demonstrations. However, as π improves, it generates more data that closely resembles the expert demonstrations. To keep distinguishing the two data sources, the discriminator may turn to focus on features that are not pertinent to the behaviors but rather environment artifacts. As aforementioned, recent works have attempted to address the resulting overfitting problem by proposing various techniques to regularize the discriminator (Peng et al., 2018; Reed et al., 2018; Blonde´ & Kalousis, 2018). In this work, we introduce an orthogonal perspective to the problem that leads to a new discriminator objective. First, we note that in most imitation learning problem settings, the ultimate goal is to learn to complete the demonstrated task successfully rather than just imitating the expert behaviors. Hence we may consider the expert demonstrations as examples of success in task completions. Accordingly, the discriminator objective ought to be to distinguish successful trajectories from that in failed ones. In other words, the positive and negative data in the binary classification problem ought to be examples of successes and failures, respectively. The key implication of this view is that the policy rollout data can no longer be considered as negative data, because as the policy improves, it will begin to complete the task successfully and some of the policy rollouts will become positive data. Rather, the policy rollouts are a mixture of positive and negative data. And because we have no ground-truth knowledge to distinguish these two types of data, the rollouts should be treated as unlabeled data. Therefore, we pose the problem of training a discriminator as a positive-unlabeled (PU) learning problem instead of a positive-negative (PN) learning problem. Formally, denote by D the expert demonstrations and D the running stream of agent behavior πe π data (i.e., the replay buffer). Based on Equation 7, we have the empirical risk estimator for the discriminator D under the non-negative PU learning setup as θ R˜pu (D , D ) = ηRˆ1 (D ) + max(−β, Rˆ0 (D ) − ηRˆ0 (D )) . (10) Dθ πe π Dθ πe Dθ π Dθ πe 5
Assuming the standard logistic loss for the the discriminator, and switching to a more familiar nota- tion, the nn-PUGAIL objective becomes L˜pu = ηEˆ [log(1 − D (s))] + max(−β, Eˆ [log(D (s)] − ηEˆ [log(D (s))]) , (11) Dθ πe θ πe θ π θ where Eˆ denotes an emprical expectation. This should be compared with the ordinary GAIL objec- tive in Equation 1. The complete adversarial imitation learning algorithm with the non-negative PU learning formulation is detailed in Section 4.3. 4.2 SEMI-SUPERVISED REWARD LEARNING AS PU LEARNING We turn to the semi-supervised reward learning setup, where the problem is to learn reward functions from a set of reward-annotated experiences D = {(s , r )}Ns , and unlabeled agent experiences s i i i=1 D = {s }Nu . As discussed previously, the main challenge in this setting is that policy learning u i i=1 tends to exploit errors in the learned reward model to achieve high pseudo-reward. To address the challenge, we consider modeling whether a reward prediction r = r (s) is reliable by training a φ discriminator D : S × R → {0, 1}, and suppressing the predicted reward if it is deemed unreliable. θ We define discriminator D on the input space of the supervised reward function. In other words, θ D is a discriminator of which states result in reliable reward prediction, i.e., D : S → {0, 1}, and θ θ if r (s) ≥ 0 we can write the discriminator-augmented reward function as φ rˆ (s) = [D (s) > 0.5]r (s) . (12) φ,θ θ φ We can formulate training the discriminator as a PU learning problem, with positive data drawn from D and unlabeled data drawn from the agent’s own experience D . This leads us to an objective for s π D (s) that corresponds exactly to Equation 10, except that the expert data D is replaced with the θ πe annotated data D , s R˜pu (D , D ) = ηRˆ1 (D ) + max(−β, Rˆ0 (D ) − ηRˆ0 (D )) . (13) Dθ s π Dθ s Dθ π Dθ s We separately train the reward function r (s) using the supervised reward learning objective in φ Equation 2, and the discriminator D using Equation 13, and combine them using Equation 12. θ In this formulation, the role of the discriminator is to identify regions of state space where the reward function r (s) is accurate. We argue that states in D can be used as positive data directly, because φ s r (s) is trained on D , and we can verify that r (s) is accurate on D by monitoring the training φ s φ s error. Using D as the unlabeled data is justified following the same argument as for nn-PUGAIL. π Moreover, if we were instead to use D as negative data we would be in the same setting as GAIL, π and would expect to face the same problems of overfitting to the annotated trajectories. Alternatively, this problem could be treated generatively. Following the assumption that D is a s representative sample from the support of r (s), we could train a generative model of the states φ in D and use the likelihood of states under this model to make decisions about reliability of s r (s). There are many existing techniques for modeling data distributions even for high-dimensional φ data (van den Oord et al., 2016; Brock et al., 2019); however, this remains an extremely hard prob- lem. Because we only need a binary classifier instead of a full-fledged generative model, it is simpler to train the discriminator directly, which corresponds to learning a level set of the posterior. 4.3 A UNIFIED REWARD LEARNING ALGORITHM Having reduced both the adversarial imitation learning problem and the semi-supervised reward learning problem to PU learning, we present a unified algorithm for Positive Unlabeled Reward Learning (PURL) in Algorithm 1 (see also Appendix A.2). For policy training we use D4PG (Barth-Maron et al., 2018), which has been shown by Reed et al. (2018) to be an effective policy learning method for GAIL. Several authors have noted that the data efficiency of GAIL is improved by using off-policy actor-critic learners (Sasaki et al., 2018; Blonde´ & Kalousis, 2018). Using GAIL with an off-policy learning method strictly speaking requires im- portance sampling correction (Kostrikov et al., 2018), but we did not find this to be necessary. The reward learning in Algorithm 1 is agnostic to the policy learning component, and could even be combined with on policy RL learning; however, it is convenient to combine PURL with off policy 6
Algorithm 1 POLICY LEARNING WITH PURL Inputs: Replay buffer D , either expert demonstrations D or a reward model r and its corre- π πe φ sponding training supervision D . Hyperparameters β ≥ 0, η ∈ [0, 1]. s Initialize policy π, decision function D . θ for i = 1, 2, 3, ... do, Sample minibatch sπ, aπ, sπ ∼ D t t t+1 π sp ← s ∼ D if D is given else s ∼ D πe πe s if Rˆ0 (sπ) − ηRˆ0 (sp) ≥ −β then Dθ t Dθ Update D by the stochastic gradient ∇ R˜pu (sp, sπ) θ θ Dθ t else Update D by the stochastic gradient ∇ (ηRˆ0 (sp) − Rˆ0 (sπ)) θ θ Dθ Dθ t end if if r is given then φ r ← [D (s ) > 0.5]r (s ) t θ t+1 φ t+1 else r ← − log(1 − D (s )) t θ t+1 end if Update the policy π with sπ, aπ, r , sπ using D4PG (Barth-Maron et al., 2018). t t t t+1 end for RL, since PURL requires a replay buffer of agent experience which can be shared with the RL learning component. Remark on η. We note that unlike the standard PU learning setup, the positive class prior η in our setting changes as policy learning progresses and the distribution of states in the replay buffer evolves. Specifically, η should increase as the policy improves. There have been many works that study the data distribution shift in the PU learning community (Hsieh et al., 2019; Sakai & Shimizu, 2019). However, further developing and adapting these techniques to a large-scale PU learning algorithm embedded within an inverse supervised learning loop is beyond the scope of this work, and we consider it as an exciting future direction. We treat η as a fixed hyperparameter in this paper. 5 EXPERIMENTS In this section, we aim to empirically verify the following hypotheses: 1. The GAIL discriminator may overfit and trivially distinguish expert demonstrations from policy rollouts. 2. RL algorithms can exploit errors in supervised reward models and achieve high pseudo- reward (reward hacking). 3. Our PU learning formulation addresses the above issues and improves the performance of both GAIL and supervised reward learning over their PN learning counterparts. In addition, we conduct an ablation study on the effect of the positive class prior η on the dis- criminator function in both reward learning settings. Finally, we deliberately create domain gaps between the training data (reward supervisions and expert demonstrations) and the policy learning environment, and demonstrate the robustness of our methods. 5.1 SETUP Tasks We conduct experiments in two different task domains: A standard benchmark 2D Walker (walker) task in the DM Control Suite (Tassa et al., 2018) and two challenging robotic manipulation tasks (lifting and stacking) in a simulated table-top environment. The walker task is to control a bipedal agent to move forward, and rewards are given for forward motion as well as maintaining the upper body elevated from the ground. The table-top manipulation environment consists of a Kinova Jaco arm, and four objects randomly initialized in a tray located in front of the arm, as shown in Figure 6. The lifting task is to control the arm to grasp and lift up the cyan banana-shaped object, 7
Walker Lifting Stacking Expert Figure 2: Results of the adversarial imitation learning methods on the three evaluation tasks. Each curve is the mean of 5 trials with confidence interval of 95%. and the ground truth shaped reward is defined on both the distance between the end-effector and the object and the height of the object above the tray. The stacking task is to pick up the red cube and stack it on top of the blue cube, and the ground truth shaped reward is defined on the relative position between the two cubes. Our reward models take raw pixels as input in all three tasks. Because the focus of this work is reward learning, we allow the policy learner to take low-dimensional state space inputs to isolate the effects of the reward learning methods. Expert demonstrations We obtain expert demonstrations by training an expert policy on the ground truth reward and hide the ground truth reward during imitation learning. We use D4PG (Barth-Maron et al., 2018) for training the expert policy. We collect N = 50 expert tra- jectories for walker and lifting, and N = 200 for stacking. Reward supervision We obtain reward supervisions from ground truth reward in all three tasks. We collect training data by first training an expert policy and then collect trajectories from check- points of the policy at different stages of learning: from the initial random exploration to the final convergence. Similar to the imitation learning setting, we collect N = 50 expert trajectories for walker and lifting, and N = 200 for stacking to reflect the difficulties of the tasks. Data augmentation We apply standard data augmentation to the inputs of all the discriminator models evaluated in the experiments. We find that data augmentation is in general effective in regularizing the discriminators. As shown below, data augmentation is necessary for the baseline GAIL agent to work on the more challenging manipulation tasks. Following Z˙ ołna et al. (2019a), we distort the image inputs by randomly changing brightness, contrast and saturation; random cropping and horizontal flipping; and adding Gaussian noise. We also add dropout layers at the end of the networks. 5.2 EVALUATION ON ADVERSARIAL IMITATION LEARNING To show the advantage of our PU and non-negative PU formulation of GAIL, we compare the fol- lowing methods in an adversarial imitation learning setup: • GAIL (no-reg): The original GAIL implementation (Ho & Ermon, 2016), which is to con- trast our strong GAIL baseline with heavy regularization and data augmentation. • GAIL: We apply extensive data augmentation techniques such as image flipping and random cropping to the discriminator input. We show that heavy regularization is crucial for GAIL to work on the challenging manipulation domain. • PUGAIL: A PU formulation of GAIL introduced in Section 4.1 without the non-negative constraint. • nn-PUGAIL: Our final non-negative PU formulation of GAIL introduced in Section 4.1. As shown in Figure 2, all methods achieve near-expert performance in the standard benchmark task walker. In the more challenging manipulation task lifting, we note that the vanilla GAIL implemen- tation GAIL (no-reg) experiences severe overfitting from the beginning, which prevents the policy 8
GAIL nn-PUGAIL (a) (b) (c) Figure 3: Ablation study of adversarial imitation learning on the lifting task: Effect of choosing dif- ferent positive class prior η values on the (a) discriminator performance and (b) policy performance for nn-PUGAIL. (c) compares the discriminator behavior of GAIL and nn-PUGAIL over the course of the policy learning. Each curve is the mean of 5 trials with confidence interval of 95%. from learning. Our nn-PUGAIL not only outperforms GAIL at convergence and achieve near-expert performance but also learns much master than GAIL. We observe similar trends in the most chal- lenging stacking task, where the discriminator of GAIL starts to overfit at 10 million environment steps. Although our nn-PUGAIL does not achieve the optimal performance, it outperforms GAIL by a wide margin. Effect of the positive class prior η. As aforementioned, the positive class prior η is determined via hyperparameter search. Here we study how different η affect the discriminator behavior of nn- PUGAIL and the resulting policies in the lifting task. We analyze the discriminator performance with respect to two fixed datasets with known ground truth classes: (1) a set of holdout expert demonstrations (expert) and (2) a set of holdout failure trajectories (failure). Specifically, we report the average sigmoid (logistic) values predicted by the discriminator for the respective class of the two data sources. An ideal discriminator should classify both datasets correctly, i.e., the sigmoid values should be greater than 0.5 for both datasets. Because the discriminator behavior evolves with the progress of the policy learning, the results are reported at the policy convergence. Figure 3(a) shows the results. We observe that low positive class prior η causes the discriminator to bias towards predicting all trajectories to be failure. Conversely, high η results in optimistic discrim- inators where all trajectories are predicted to be successes. At η = 0.5, the discriminator achieves optimal performance where both datasets are classified correctly. This is reflected in Figure 3(b), where the best policy performance is achieved at η = 0.5. We note that the optimal η value de- pends on both the number of expert demonstrations and the task. We defer a principled method for automatically selecting η to future works. Overfitting discriminator. To better understand the overfitting discriminator problem and the ad- vantage of the PU formulation over PN, we compare the discriminator performance of GAIL and nnPU-GAIL with a fixed η = 0.5 over the course of learning a lifting task. Specifically, we analyze the probability of states being classified as success (PoS) of four data sources: (1) the training expert demonstrations, (2) a set of holdout expert demonstrations, (3) a set of holdout failure trajectories, and (4) policy rollouts sampled from the replay buffer. Note that all data sources but (4) are fixed throughout the training. As shown in Figure 3(c), both methods classify the holdout failure trajectories correctly (near-0% PoS). The discriminator of GAIL classifies the training demonstrations correctly as success through- out the training, while the PoS of holdout expert demonstrations decreases as the learning progresses. This indicates that the discriminator of GAIL overfits to the training expert demonstrations. In con- trast, the discriminator of nnPU-GAIL maintains a 100% PoS for both the training and the holdout expert demonstrations. In addition, nnPU-GAIL classifies increasingly more policy rollout as suc- cess as the policy improves, and the PoS correctly approximates the positive class prior at the policy convergence. 9
Walker Lifting Stacking Figure 4: Results of supervised reward learning setting on the three evaluation tasks. Each curve is the mean of 5 trials with confidence interval of 95%. 5.3 EVALUATION ON SUPERVISED REWARD LEARNING We compare the following supervised reward learning methods: • PRL: A vanilla supervised reward learning baseline. • PRL-ensemble: A strong supervised reward learning baseline, where the reward prediction is taken as the min of the predictions from a set of supervised reward models trained from different initialization. This method is shown to effectively alleviate the reward hacking problem by Vecerik et al. (2019). • PNRL: A supervised reward model combined with a positive-negative discriminator. • PURL: Supervised reward model with a positive-unlabeled discriminator. • nn-PURL: Our final model, where the discriminator is trained with a non-negative PU for- mulation (Section 4.2). Figure 4 shows the results in all three tasks. First, we observe that in the walker task, all dis- criminator augmented reward learning methods are able to achieve competitive performance In contrast, PRL and PRL-ensemble both suf- fer from reward delusions. The performance of PRL plateaus soon after the beginning of the training. While PRL-ensemble outperforms PRL initially thanks to the ensemble strategy, the policy soon deteriorates because none of the (a) (b) ensemble models can make reliable predictions. In the more challenging lifting tasks, all base- Figure 5: Effect of positive class prior η values line methods but our nn-PURL have similar per- on the (a) reward prediction errors and (b) policy formances, whereas policies trained with our performance in nn-PURL. Results of PNRL and nn-PURL are able to achieve comparable per- PRL are included for reference. formance to policies trained with ground truth reward. We observe that the performance of PNRL gradually decreases after the initial improvement. This is due to that the discriminator with a positive-negative objective overfits to the training positive samples. This effect is more pronounced in the most challenging stacking task, where both PNRL and nn- PURL have similar progress at the beginning of the learning, but soon PNRL plateaus and starts to deteriorate. In contrast, our nn-PURL is able to converge to optimal performance. Effect of the positive class prior η. Here, we study how η affects the performance of nn-PURL in the lifting task. Figure 5 shows both the episodic reward prediction and the final policy performance. We observe that while PNRL and PRL have similar performances, the PNRL suffers high negative reward error while PRL has high positive reward error. This agrees with our hypothesis that (1) learning with only positive data results in reward delusions (high pseudo-reward) and (2) unregular- 10
Figure 6: Illustration of reward hacking. Left: Visualization of policies trained with a supervised reward model (PRL). Right: The corresponding reward values predicted by PRL and our nn-PURL method. ized semi-supervised reward learning leads to overfitting (low pseudo-reward). On the other hand, our nn-PURL mitigates the two extremes through the positive class prior η and enables the policy to achieve both the lowest reward prediction errors and the best performance at η = 0.5. Again, we note that the optimal choice of η is task and data-dependent, and a principled selection method is deferred to future works. Visualize reward hacking. We provide some qualitative examples of reward delusions and illustrate that our nn-PURL addresses the problem. In Figure 6, we visualize the behaviors of agents trained with the baseline supervised reward learning method (PRL) and compare the reward predictions from PRL and nn-PURL. In walker, PRL incorrectly predicts high reward when the agent poses as walking but is in fact not moving, whereas nn-PURL predicts zero reward after the agent has stopped moving (Frame 3). In lifting, the agent learns to exploit the error where PRL predicts high reward even after the agent has dropped the object (Frame 3) because the gripper is out of the camera view. nn-PURL learns to suppress reward as soon as the gripper moves out of the camera view (Frame 2). In stacking, the agent exploits the reward model error by occluding all objects from the camera. Our nn-PURL is able to detect the out-of-distribution state starting from Frame 2. 5.4 LEARNING WITH DOMAIN GAP Finally, we test the limit of our method’s ability to mitigate the discriminator overfitting problem by introducing domain gaps between the environment for generating expert demonstrations and reward supervision and the environment for policy learning. As shown on the left side of Figure 7, we vary the shape of the distractor objects in a lifting task such that the discriminator should be able to trivially distinguish the training data from the policy rollout data. The results are shown on the right side of Figure 7. We find that our nnPU-based methods outperform their PN-learning counterparts in both the supervised reward learning and the adversarial reward learning settings. In particular, nn-PURL is able to maintain the near-optimal performance despite the domain gap. 6 IDEAS FOR FUTURE WORK In this section we outline several ideas that we think could be interesting directions for future work. Some of these ideas are quite concrete and ought to be straightforward to execute, whereas others 11
supervision / demonstration policy learning Figure 7: Learning with domain gaps in the lifting task. Left: The domain gap is created by varying the shapes of distractor objects between the environment for collecting expert demonstrations and reward supervisions and the environment for policy learning. Right: Results on adversarial imitation learning and supervised reward learning with domain gaps. are more open-ended questions that may end up being more difficult to answer or even unproductive to pursue. PU learning for image GANs. We have shown that PU learning improves the performance of GAIL by changing the semantics of the discriminator to allow for the behavior policy to produce successful trajectories. This same approach could be applied to the discriminator of an ordinary image GAN to allow the generator to produce “real” images. Would applying PU learning in this setting lead to improvements in the quality of generated images as well? A careful treatment of η. Because we draw the pool of unlabeled data from the agent replay buffer during training the proportion of successes and failures in the unlabeled data does not remain constant over time. In this work we treat η as a hyperparameter, and show that this works well empirically but, following on the remark in Section 4.3, this is not a strictly correct thing to do. Is there a more sound approach to handling η in the reward learning setting? Perhaps a carefully chosen schedule of η’s could lead to better performance. There is work on empirical estimation of class priors from PU data (du Plessis et al., 2015a), perhaps a running estimate could be maintained. PNU learning. Equation 6 shows how to write the true negative risk of a binary classifier in terms of risk measurements on positive and unlabeled data. An entirely symmetric derivation shows that we can also write ηR1(D ) = R1(D ) − (1 − η)R1(D ) (14) g p g u g n to express the true positive risk in terms of risk measurements on the negative and unlabeled data. By substituting Equations 6 and 14 into Equation 5, we obtain an expression for the risk that makes use of positive, negative, and unlabeled data Rpnu(D , D , D ) = R1(D ) + R0(D ) − ηR0(D ) − (1 − η)R1(D ) . (15) g p n u g u g u g p g n Perhaps an empirical version of this expression (or a suitably modified non-negative estimator) could be used to further improve upon (nn-)PURL or (nn-)PUGAIL in the case where some examples of definite failures can be collected. Third person imitation. In Section 5.4 we showed that nn-PURL performs well even when there is a domain gap between the demonstrations and the agent environment. Perhaps this robustness could also extend to third-person imitation. Could PU learning replace the gradient flipping objective of Stadie et al. (2017)? Finding the right inductive bias. The PU learning objective can be thought of as a regularizer that encourages some proportion of the unlabeled data to be labeled as positive, but which examples are to be labeled as positive is left up to the inductive bias of the classifier. Can we design architectures that have an inductive bias that encourages better generalization with (nn-)PURL? 12
7 CONCLUSION In this paper, we presented PURL, a framework for reward learning that allows us to cast GAIL discriminator training and learning the support set estimation in supervised reward learning as a positive-unlabeled learning problem. In GAIL in particular, this formulation changes the semantics of the discriminator from distinguishing between expert and agent to distinguishing between success and failure of the demonstrated task. By applying a recent large scale PU learning algorithm to the PURL objective for discriminator training, we obtained large improvements in performance of agents, without introducing any additional assumptions. Through a series of experiments we demonstrated how PURL addresses both the overfitting problem of GAIL, and the underfitting problem of supervised reward learning. We also showed how PURL allows us to train reward models when there is a domain gap between reward model and policy learning. Several ablations were presented that demonstrate the importance both of framing the reward learning problem as PU learning, and also of the specific large scale PU learning algorithm we chose. Combining PU learning with RL and GANs is a promising direction, offering many directions for possible future work beyond the scope of this paper. We have outlined several such possible direc- tions in Section 6, in the hopes that these ideas will spark the creativity of the research community in this area. ACKNOWLEDGEMENTS We would like to thank the scientific python community for developing the core set of tools that en- abled this work, including Tensorflow (Abadi et al., 2016), Numpy (Oliphant, 2006), Pandas (McK- inney et al., 2010), and Matplotlib (Hunter, 2007). REFERENCES Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, San- jay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In 12th USENIX Sympo- sium on Operating Systems Design and Implementation (OSDI 16), pp. 265–283, 2016. Riad Akrour, Marc Schoenauer, and Michele Sebag. Preference-based policy learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 12–27. Springer, 2011. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane´. Concrete prob- lems in ai safety. arXiv preprint arXiv:1606.06565, 2016. Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to-end differentiable adversarial imitation learn- ing. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 390–399. JMLR. org, 2017. Gabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed Distributional Deterministic Policy Gradients. arXiv:1804.08617 [cs, stat], 2018. Marc G Bellemare, Will Dabney, and Re´mi Munos. A distributional perspective on supervised learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 449–458. JMLR. org, 2017. Lionel Blonde´ and Alexandros Kalousis. Sample-efficient imitation learning via generative adversarial nets. arXiv preprint arXiv:1809.02064, 2018. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large Scale GAN Training for High Fidelity Natural Image Synthesis. In Proceedings of the International Conference on Learning Representations, 2019. Serkan Cabi, Sergio Go´mez Colmenarejo, Alexander Novikov, Ksenia Konyushkova, Scott Reed, Rae Jeong, Konrad Z˙ ołna, Yusuf Aytar, David Budden, Mel Vecerik, Oleg Sushkov, David Barker, Jonathan Scholz, Misha Denil, Nando de Freitas, and Ziyu Wang. A Framework for Data-Driven Robotics. arXiv preprint arXiv:1909.12200, 2019. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep supervised learning from human preferences. In Advances in Neural Information Processing Systems, pp. 4299–4307, 2017. 13
Franois Denis. PAC Learning from Positive Statistical Queries. In Algorithmic Learning Theory, Berlin, Heidelberg, 1998. Springer Berlin Heidelberg. doi: 10.1007/3-540-49730-7 9. Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Analysis of Learning from Positive and Unlabeled Data. In Neural Information Processing Systems, 2014. Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Class-prior estimation for learning from positive and unlabeled data. In Proceedings of the Asian Conference on Machine Learning, 2015a. Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Convex Formulation for Learning from Positive and Unlabeled Data. In Proceedings of the International Conference on Machine Learning, 2015b. Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD 08, Las Vegas, Nevada, USA, 2008. ACM Press. doi: 10.1145/1401890.1401920. Tom Everitt and Marcus Hutter. Avoiding wireheading with value supervised learning. In International Conference on Artificial General Intelligence, pp. 12–22. Springer, 2016. Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse supervised learning. arXiv preprint arXiv:1710.11248, 2017. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Process- ing Systems, pp. 2672–2680, 2014. Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565–4573, 2016. Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt, and David Silver. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933, 2018. Yu-Guan Hsieh, Gang Niu, and Masashi Sugiyama. Classification from positive, unlabeled and biased negative data. In International Conference on Machine Learning, pp. 2820–2829, 2019. John D Hunter. Matplotlib: A 2d graphics environment. Computing in science & engineering, 9(3):90, 2007. Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning from human preferences and demonstrations in Atari. In Advances in Neural Information Processing Systems, pp. 8011–8023, 2018. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Ryuichi Kiryo, Gang Niu, Marthinus C du Plessis, and Masashi Sugiyama. Positive-unlabeled learning with non-negative risk estimator. In Advances in Neural Information Processing Systems, pp. 1675–1685, 2017. Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson. Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning. arXiv:1809.02925 [cs, stat], 2018. Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual demon- strations. In Advances in Neural Information Processing Systems, pp. 3812–3822, 2017. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep supervised learning. arXiv preprint arXiv:1509.02971, 2015. Wes McKinney et al. Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference, volume 445, pp. 51–56, 2010. Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse supervised learning. In Proceedings of the International Conference on Machine Learning, volume 1, pp. 2, 2000. Gang Niu, Marthinus Christoffel du Plessis, Tomoya Sakai, Yao Ma, and Masashi Sugiyama. Theoretical comparisons of positive-unlabeled learning against positive-negative learning. In Advances in Neural Infor- mation Processing Systems, pp. 1199–1207, 2016. Travis Oliphant. Guide to NumPy. USA: Trelgol Publishing, 2006. Giorgio Patrini, Frank Nielsen, Richard Nock, and Marcello Carioni. Loss factorization, weakly supervised learning and label noise robustness. In International conference on machine learning, pp. 708–717, 2016. Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational discriminator bottleneck: Improving imitation learning, inverse RL, and GANs by constraining information flow. arXiv preprint arXiv:1810.00821, 2018. Scott Reed, Yusuf Aytar, Ziyu Wang, Tom Paine, Aa¨ron van den Oord, Tobias Pfaff, Sergio Gomez, Alexander Novikov, David Budden, and Oriol Vinyals. Visual imitation with a minimal adversary. Technical report, Deepmind, 2018. 14
Tomoya Sakai and Nobuyuki Shimizu. Covariate shift adaptation on learning from positive and unlabeled data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 4838–4845, 07 2019. doi: 10.1609/aaai.v33i01.33014838. Fumihiro Sasaki, Tetsuya Yohira, and Atsuo Kawaguchi. Sample Efficient Imitation Learning for Continuous Control. In Proceedings of the International Conference on Learning Representations, 2018. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International Conference on Machine Learning, 2014. Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-Person Imitation Learning. arXiv:1703.01703 [cs], 2017. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller. DeepMind Control Suite. arXiv:1801.00690 [cs], 2018. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033. IEEE, 2012. Aa¨ron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional Image Generation with PixelCNN Decoders. arXiv:1606.05328 [cs], 2016. Mel Vecerik, Oleg Sushkov, David Barker, Thomas Rotho¨rl, Todd Hester, and Jon Scholz. A practical ap- proach to insertion with variable socket position using deep supervised learning. In 2019 International Conference on Robotics and Automation (ICRA), pp. 754–760. IEEE, 2019. Aaron Wilson, Alan Fern, and Prasad Tadepalli. A bayesian approach for policy learning from trajectory preference queries. In Advances in Neural Information Processing Systems, pp. 1133–1141, 2012. Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum Entropy Inverse Reinforce- ment Learning. In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, 2008. Konrad Z˙ ołna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarej, David Budden, Serkan Cabi, Misha Denil, Nando de Freitas, and Ziyu Wang. Task-Relevant Adversarial Imitation Learning. arXiv:1910.01077 [cs, stat], 2019a. Konrad Z˙ ołna, Negar Rostamzadeh, Yoshua Bengio, Sungjin Ahn, and Pedro O Pinheiro. Reinforced imitation in heterogeneous action space. arXiv preprint arXiv:1904.03438, 2019b. 15
Figure 8: Camera view of the Walker environment and the Jaco environment. A APPENDIX A.1 ENVIRONMENT DETAILS AND TRAINING DATA The evaluation environments are physically simulated using the Mujoco simulator (Todorov et al., 2012). Walker The walker domain is part of the DeepMind Control Suite (Tassa et al., 2018), where a bipedal agent is tasked to either stand up, walk forward, or run forward. The bipedal agent can only move in a horizontal 2D space (no lateral motion). We used the walker walk task in the environment, where the agent is rewarded for moving forward (to the right relative to the camera viewpoint). The task horizon for both the training and evaluation is 1000 time steps. Jaco The Jaco environment consists of a Kinova Jaco arm, a tray set on top of a table, and four objects randomly initialized in the tray. The objects are three 5cm3 colored blocks and a banana. The lifting task is to lift the banana. The stacking task is to stack the red block on top of the blue block. Both tasks share the same set of objects, where non-target objects act as distractors. Both tasks have shaped rewards ranging [0, 1] for reward supervision in the supervised reward learning setting. We use joint velocity control (9DOF) where we control all 6 joints of arm and all 3 joints of the hand. The simulation is run with a numerical time step of 10 milliseconds, integrating 5 steps, to get a control frequency of 20HZ. Both training and evaluation episodes are of 400 time steps long. We use two cameras to capture the scene: a front left camera and a front right camera. The viewpoints are visualized in Figure 8. Both cameras are rendered to RGB images of size 64×64. As described in the main paper, all reward models and discriminator models take only images as input, and the policies take low-dimensional environment state as inputs. For a full list of observations and environment states, see Table 1. Domain gap Section 5.4 evaluates various methods under visual domain gaps between the en- vironment for generating the reward learning training data (i.e., expert demonstrations and reward supervision) and the environment for policy training in a lifting task. The domain gaps are created by changing the shapes of the distractor objects from 5cm3 cuboids in the data generation environment to 4 × 4 × 8cm cuboids in the policy learning environment, as shown in Figure 7. A.2 REWARD LEARNING DETAILS Non-negative PU Reward Learning Algorithm. We introduced the unified PU Reward Learning algorithm in Section 4.3. We base the sub-routine for enforcing non-negative constraint on the algorithm first introduced in Kiryo et al. (2017). Below we present the hyperparameters used in each of the evaluation tasks (Table 2). 16
Table 1: Observation and environment state dimensions in the Jaco environment. Observation / state name Dimensions front left camera 64 × 64 × 3 front right camera 64 × 64 × 3 base force and torque sensors 6 arm joints position 6 arm joints velocity 6 wrist force and torque sensors 6 hand finger joints position 3 hand finger joints velocity 3 hand fingertip sensors 3 grip site position 3 pinch site position 3 Reward learning model architecture Both the supervised reward models and the discriminator models share the same architecture. Table 3 lists model architecture details. Data augmentation We observed in Section 5 that data augmentation is crucial for regularizing the discriminators. In addition, we empirically found that applying data augmentation to the input of the supervised reward models also improves their performance. We apply the same set of data augmentation operations on the input images for both the discriminator model and the supervised reward model. Table 3 lists the augmentation parameters. Table 2: Hyperparameters and dataset size for reward learning. Task Method Hyperparameters Training data size nn-PUGAIL β = 0.0, η = 0.25 50 Walker nn-PURL β = 0.0, η = 0.5 50 nn-PUGAIL β = 0.0, η = 0.5 50 Lifting nn-PURL β = 0.0, η = 0.5 50 nn-PUGAIL β = 0.0, η = 0.7 200 Stacking nn-PURL β = 0.7, η = 0.7 200 Table 3: Data augmentation specifications and the model architectures of the supervised reward models and the discriminators. Discriminator / Reward Network Specifications Residual Conv Blocks [2, 2, 2] Conv Channels [16, 32, 32] Conv Kernels Sizes [(3, 3), (3, 3), (3, 3)] Pooling MaxPooling = [(2, 2), (2, 2), (2, 2)] Activation ReLU Supervised Reward Loss Mean-Squared Error Optimizer Adam (Kingma & Ba, 2014) Learning Rate (Supervised) 0.0001 Learning Rate (Discriminator) 0.00001 Data Augmentation Specifications Dropout ProbKeep=0.5 Random flip Horizontal Random crop ratio 0.8 Random satuation range [0.5, 2.0] Random hue max delta=0.05 Random contrast range [0.5, 2.0] Clipped pixel noise [-16, 16] 17
A.3 D4PG DETAILS We use the Distributed Distributional Deterministic Policy Gradients (D4PG) (Barth-Maron et al., 2018) as our policy learning algorithm framework. D4PG is a distributed off-policy supervised learning algorithm designed specifically for continuous control problems. In short, D4PG extends the Q-learning formulation of the Deterministic Policy Gradients (Silver et al., 2014; Lillicrap et al., 2015) to distributional value function (Bellemare et al., 2017). Other features of D4PG include target network for training stability, distributed training (Horgan et al., 2018), and multi-step returns. Table 4 lists the parameters for D4PG used in our experiments and the network architecture. Again, we note that to focus on comparing the reward learning methods and isolate the effect of the policy learning algorithm, we allow the policy to take environment state as input to achieve faster and more stable policy training. All experiments share the same policy learning setup. Table 4: Details of the D4PG algorithm. D4PG Parameters Values V 0 min V 100 max V 51 bins N step (return) 5 Actor learning rate 0.0001 Critic learning rate 0.0001 Optimizer Adam (Kingma & Ba, 2014) Batch size 256 Discount factor 0.99 Number of actors 16 (walker), 128 (lifting, stacking) Network Specifications Values Actor network MLP=[300, 200] Critic network MLP=[400, 300] Activation function ReLU 18
