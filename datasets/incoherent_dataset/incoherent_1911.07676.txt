0202 beF 91 ]LM.tats[ 2v67670.1191:viXra Learning with Good Feature Representations in Bandits and in RL with a Generative Model Tor Lattimore 1 Csaba Szepesv´ari 2 3 Gell´ert Weisz 1 Abstract and suggest a negative answer. Efficiency here means learning a good policy with a small number of inter- The construction by Du et al. (2019) implies actions either with the environment (on-line learning), that even if a learner is given linear features or with a simulator (planning). A linear feature repre- in Rd that approximate the rewards in a sentation is called “good” if it approximates the value bandit with a uniform error of ε, then functions of all policies with a small uniform error. searching for an action that is optimal The same question can also be asked for learning in up to O(ε) requires examining essentially bandits. The ideas by Du et al. (2019) suggest that all actions. We use the Kiefer–Wolfowitz the answer is also negative in finite-armed bandits with theorem to prove a positive result that by a misspecified linear model. All is not lost, however. checking only a few actions, a learner can By relaxing the objective, we will show that one can always find an action that is suboptimal obtain positive results showing that efficient learning with an error of at most O(ε√d). Thus, is possible in interactive settings with good feature rep- features are useful when the approximation resentations. error is small relative to the dimensionality of the features. The idea is applied to The rest of this article is organised as follows. First stochastic bandits and supervised learning we introduce the problem of learning to identify with a generative model where the learner a near-optimal action with side information about has access to d-dimensional linear features the possible reward (Section 2). We then adapt that approximate the action-value functions the argument of Du et al. (2019) to show that no for all policies to an F1-score of ε. For linear algorithm can find an O(ε)-optimal action without bandits, we prove a bound on the regret of examining nearly all the actions, even when the order dn log(k) + εn√d log(n) with k the rewards lie within an ε-vicinity of a subspace spanned number of actions and n the horizon. For RL by some features available to the algorithm (Section 3). p we show that approximate policy iteration The negative result is complemented by a positive can learn a policy that is optimal up to an result showing that there exists an algorithm such that additive error of order ε√d/(1 γ)2 and using for any feature map of dimension d, the algorithm − d/(ε2(1 γ)4) samples from a generative is able to find an action with suboptimality gap of model. − These bounds are independent of at most O(ε√d) where ε is the maximum distance the finer details of the features. We also between the reward and the subspace spanned by the investigate how the structure of the feature features in the max-norm. The algorithm only needs set impacts the tradeoff between sample to investigate the reward at O(d log log d) well-chosen complexity and estimation error. actions. The main idea is to use the Kiefer-Wolfowitz theorem with a least-squares estimator of the reward function. Finally, we apply the idea to stochastic bandits (Section 5) and supervised learning with 1. Introduction a generative model (Section 6). Du et al. (2019) ask whether “good feature represen- tation” is sufficient for efficient supervised learning Related work Despite its importance, the problem of identifying near-optimal actions when rewards 1DeepMind, London 2DeepMind, Edmonton follow a misspecified linear model has only recently 3University of Alberta. Correspondence to: Tor Lat- timore <lattimore@google.com>. received attention. Of course, there is the recent paper by Du et al. (2019), whose negative result inspired this work and is summarised for our setting in Section 3.
Learning with Good Feature Representations in Bandits and in RL with a Generative Model A contemporaneous work also addressing the issues randomise. Two objectives are considered. The first raised by Du et al. (2019) is by Van Roy and Dong only measures the quality of the outputted action aˆ, (2019), who make a connection to the Eluder while the second depends on µˆ. dimension (Russo and Van Roy, 2013) and prove a Definition 2.1. A learner is called sound for ( , δ) if variation on our Proposition 4.2. The setting studied H µˆ µ < δ almost surely for all µ . It is called here in Section 3 is closely related to the query k − k∞ ∈ H max-sound for ( , δ) if µ > max µ δ almost surely aˆ a a complexity of exactly maximising a function in a H − for all µ . given function class, which was studied by Amin et al. ∈ H (2011). They introduced the haystack dimension Denote by q (A , µ) the expected number of queries δ as a hardness measure for exact maximisation. learner A executes when interacting with the Unfortunately, their results for infinite classes are environment specified by µ and let generally not tight and no results for misspecified l pi un re ear exm po lod re als tiow ner ie n p br ao nv did ite sd , . whA in chot whe ar s r pe ola pt ue ld ara isr ee da ii ns cm δ ax( H) = A :A is ( H,i δn )f -max-sound µs ∈u Hp q δ(A , µ) the machine learning community by Even-Dar et al. ce δst( H) = A :A is (in ,f δ)-sound µsup q δ(A , µ) , (2006); Audibert and Bubeck (2010). The standard H ∈H problem is to identify a (near)-optimal action in an which are the minimax query complexities for max- unstructured bandit. Soare et al. (2014) study pure sound/sound learners respectively when interacting exploration in linear bandits, but do not address with reward vectors in and with error tolerance the case where the model is misspecified. More δ. Both complexity meaH sures are increasing as the general structured settings have also been considered hypothesis class is larger in the sense that if U V , by Degenne et al. (2019) and others. The algorithms then cmax(U ) cmax(V ), and similarly for cest. ⊂ If a in these works begin by playing every action once, learnerδ is soun≤ d foδ r ( , δ) and aˆ = arg maxδ µˆ, then which is inconsequential in an asymptotic sense. Our clearly it is also max-H sound for ( , 2δ), which shows focus, however, is on the finite-time regime where the that H number of actions is very large, which makes these algorithms unusable. We discuss the related work on cmax( ) cest( ) . (1) 2δ H ≤ δ H linear bandits and RL in the relevant sections later. Our primary interest is to understand cmax( ). Upper δ H bounds, however, will be proven using Eq. (1) and Notation Given a matrix A Rn m, the set of rows is denoted by rows(A) and it∈ s ran× ge is range(A) = by controlling ce δst( H). Furthermore, in Section 4 we provide a simple characterisation of cest( ), while Aθ : θ Rm . When A is positive semi-definite, δ H w{ e define ∈ x 2 } = x Ax. The Minkowski sum of sets cm δ ax( H) is apparently more subtle. Later we need U, V Rdk iskA U + V⊤ = u + v : u U, v V . The the following intuitive result, which says that the ⊂ { ∈ ∈ } complexity of finding a near-optimal action when the standard basis vectors are e , . . . , e . There will never 1 d hypothesis set consists of the unit vectors is linear in be ambiguity about deducing the dimension. the number of actions. The proof is given in Section A. Lemma 2.2. cmax( e , . . . , e ) = (k + 1)/2. 2. Problem setup 1 { 1 k } It follows from the aforementioned monotonicity that We start with an abstract problem that is reminiscent if e , . . . , e , then cmax( ) (k + 1)/2. of pure exploration in bandits, but without noise. Fix { 1 k } ⊆ H 1 H ≥ δ > 0 and consider the problem of identifying a δ- 3. Negative result optimal action out of k actions with the additional information that the unknown reward vector µ Rk belongs to a known hypothesis set Rk. An a∈ ction Let Φ ∈ Rk ×d. The rows of Φ should be thought of H ⊂ as feature vectors assigned to each of the k actions; j [k] = 1, . . . , k is δ-optimal for µ = (µ )k if ∈ { } i i=1 accordingly we call Φ a feature matrix. Furthermore, µ > max µ δ. The learner sequentially queries j i i − when µ Rk and a rows(Φ), we abuse notation by pac ot ii no tns thi e∈ le[k ar] na en rd shob ouse ldrve ss tot phe ar ne dwa or ud tpµ ui. t A bot thsom ane writing µ∈ a for the val∈ ue of vector µ at the index of row a in Φ. Our interest lies in the regime where k is much estimated optimal action aˆ [k] and an estimation vector µˆ Rk. There is no no∈ ise, so the learner has no larger than d and where exp(d) is large. ∈ reason to query the same action twice. Of course, if We consider hypothesis classes where the true reward the learner queries all the actions, then it knows both lies within an ε-vicinity of range(Φ) as measured in µ and the optimal action. The learner is permitted to max-norm. Let ε = range(Φ) + B (ε), where HΦ ∞
Learning with Good Feature Representations in Bandits and in RL with a Generative Model B (ε) = [ ε, ε]k is a k-dimensional hypercube. How 4. Positive result la∞ rge is cma− x( ε ) for different regimes of δ and ε and δ HΦ The negative result of the previous section is feature matrices? As we shall see, for δ = Ω(ε√d) complemented with a positive result showing that the one can keep the complexity small, while for smaller δ query complexity can be bounded independently of k there exist feature matrices for which the complexity whenever δ = Ω(ε√d). For the remainder of the article can be as high as the large dimension, k. we make the following assumption: The latter result follows from the core argument of Assumption 4.1. Φ Rk d has unique rows and the × the recent paper by Du et al. (2019). The next span of rows(Φ) is all ∈ of Rd. lemma is the key tool, and is a consequence of the Johnson–Lindenstrauss lemma. It shows that there We discuss the relationship between this result and exist matrices Φ Rk d with k much larger than d Proposition 3.2 at the end of the section. × ∈ where rows have unit length and all non-equal rows Proposition 4.2. Let Φ Rk d and δ > 2ε(1 + √2d). × are almost orthogonal. Then, cmax( ε ) 4d log l∈ og d + 16. δ HΦ ≤ Lemma 3.1. For any ε > 0 and d [k] such that d 8 log(k)/ε2 , there exists a feature∈ matrix Φ Rk ≥ d The proof relies on the Kiefer–Wolfowitz theorem, ⌈ ⌉ ∈ × which we now recall. Given a probability distribution with unique rows such that for all a, b ∈ rows(Φ) with ρ : rows(Φ) [0, 1], let G(ρ) Rd ×d and g(ρ) R be a = b, a = 1 and a b ε. → ∈ ∈ 6 k k2 | ⊤ | ≤ given by rL ee sm ulm t:as 2.2 and 3.1 together imply the promised G(ρ) = ρ(a)aa⊤ , g(ρ) = a m rowa sx (Φ) ka k2 G(ρ)−1 . a ∈rXows(Φ) ∈ Proposition 3.2. For any ε > 0 and d [k] with d Theorem 4.3 (Kiefer and Wolfowitz 1960). The 8 log(k)/ε2 , there exists a feature mat∈ rix Φ Rk ×≥ d following are equivalent: ⌈ ⌉ ∈ such that cmax( ε ) (k + 1)/2. 1 HΦ ≥ 1. ρ is a minimiser of g. ∗ 2. ρ is a maximiser of f (ρ) = log det G(ρ). ∗ Proof. Let Φ be the matrix from Lemma 3.1 with 3. g(ρ ∗) = d. rows(Φ) = (a )k . We want to show that e ε i i=1 i ∈ HΦ Furthermore, there exists a minimiser ρ ∗ of g such that for all i [k] and then apply Lemma 2.2. If θ = a , i the support of ρ has cardinality at most supp(ρ) ∈ ∗ then Φθ = (a a , . . . , a a , . . . , a a ) . By the choice | | ≤ ⊤1 i ⊤i i ⊤k i ⊤ d(d + 1)/2. of Φ the ith component is one and the others are all less than ε in absolute value. Hence, Φθ e ε, The distribution ρ is called an (optimal) experimental i ∗ k − k∞ ≤ which completes the proof. design and the elements of its support are called its core set. Intuitively, when covariates are sampled from ρ, then g(ρ) is proportional to the maximum The proposition has a worst-case flavour. Not all variance of the corresponding least-squares estimator feature matrices have a high query complexity. For over all directions in rows(Φ). Hence, minimising g a silly example, the query complexity of the zero corresponds to minimising the worst-case variance of matrix Φ = 0 satisfies cmax( ε ) = 0 provided the resulting least-squares estimator. A geometric 1 HΦ ε < 1. That said, the matrix witnessing the interpretation is that the core set lies on the boundary claims in Lemma 3.1 can be found with non-negligible of the central ellipsoid of minimum volume that probability by sampling the rows of Φ uniformly from contains rows(Φ). The next theorem shows that there the surface of the (d 1)-dimensional sphere. There is exists a near-optimal design with a small core set. The − another way of writing this result, emphasising the role proof follows immediately from part (ii) of lemma 3.9 of the dimension rather than the number of actions. in the book by Todd (2016), which also provides an algorithm for computing such a distribution in roughly Corollary 3.3. For all δ > ε, there exists a feature matrix Φ Rk d with suitably large k such that order kd2 computation steps. × ∈ Theorem 4.4. There exists a probability distribution 1 d 1 ε 2 ρ such that g(ρ) 2d and the core set of ρ has size at cmax( ε ) exp − . ≤ δ HΦ ≥ 2 8 δ most 4d log log(d) + 16. (cid:18) (cid:16) (cid:17) (cid:19) The proof of Proposition 4.2 is a corollary of the The proof follows by rescaling the features in following more general result about least-squares Proposition 3.2 and is given in Appendix B. estimators over near-optimal designs.
Learning with Good Feature Representations in Bandits and in RL with a Generative Model Proposition 4.5. Let µ ε and η [ β, β]k. arg max µ be the optimal action. Then by Suppose that ρ is a pro∈ babH iliΦ ty distrib∈ utio− n over Proposita io∈nrow 4s .( 5Φ w) ita h η = 0, rows(Φ) satisfying the conclusions of Theorem 4.4. Then Φθˆ µ ε + (ε + β)√2d, where µ aˆ aˆ, θˆ ε 1 + √2d a∗, θˆ ε 1 + √2d k − k∞ ≤ ≥ h i − ≥ h i − (cid:16) (cid:17) (cid:16) (cid:17) θˆ = G(ρ) −1 ρ(a)(µ a + η a)a . ≥ µ a∗ − 2ε 1 + √2d > µ a∗ − δ . a ∈rXows(Φ) (cid:16) (cid:17) Discussion Corollary 3.3 shows that the query The problem can be reduced to the case where η = 0 complexity is exponential in d when δ is not much by noting that µ + η ε+β The only disadvantage larger than ε, but is benign when δ = Ω(ε√d). The ∈ HΦ is that this leads to an additional additive dependence positive result shows that in the latter regime the on β. complexity is more or less linear in d. Precisely, min δ : cmax( ε ) 4d log log(d) + 16 = O(ε√d) . Proof. Let µ = Φθ + ∆ where ∆ ε. The { δ HΦ ≤ } difference between θˆ and θ can be k wrik tt∞ en ≤ as The message is that there is a sharp tradeoff between θˆ θ = G(ρ) −1 ρ(a)a a ⊤θ + ∆ a + η a θ query complexity and error. The learner pays dearly − − a ∈rXows(Φ) (cid:0) (cid:1) in terms of query complexity if they demand an = G(ρ) 1 ρ(a)(∆ + η )a . estimation error that is close to the approximation − a a error. By sacrificing a factor of √d in estimation error, a ∈rXows(Φ) the query complexity is practically just linear in d. Next, for any b rows(Φ), ∈ Comparison to supervised learning As noted by Du et al. (2019), the negative result does not hold in b, θˆ θ = b, G(ρ) 1 ρ(a)(∆ + η )a − a a supervised learning, where the learner is judged on h − i * + a ∈rXows(Φ) its average prediction error with respect to the data = ρ(a)(∆ a + η a) b, G(ρ) −1a generating distribution. Suppose that a, a 1, . . . , a n are h i sampled i.i.d. from some distribution P on rows(Φ) a ∈rXows(Φ) and the learner observes (a )n and (µ )n . (ε + β) ρ(a) b, G(ρ) 1a t t=1 at t=1 − ≤ |h i| a ∈r Xows(Φ) θˆ = n a a −1 n a µ . ≤ (ε + β) ρ(a) hb, G(ρ) −1a i2 Xt=1 t ⊤t ! Xt=1 t at sa ∈rXows(Φ) Then, by making reasonable boundedness and span = (ε + β) ρ(a)b G(ρ) 1aa G(ρ) 1b assumptions on rows(Φ), and by combining the results ⊤ − ⊤ − sa rows(Φ) in chapters 13 and 14 of (Wainwright, 2019), with high ∈ X probability, = (ε + β) b 2 (ε + β) g(ρ) (ε + β)√2d , k kG(ρ)−1 ≤ ≤ d q p E (a θˆ µ )2 θˆ = O + ε2 . where the first inequality follows from Ho¨lder’s ⊤ − a n inequality and the fact that ∆ ε, the second h (cid:12) i (cid:18) (cid:19) (cid:12) by Jensen’s inequality and thek lak s∞ t tw≤ o by our choice Notice, there is no d (cid:12)multiplying the dependence of ρ and Theorem 4.4. Therefore on the approximation error. The fundamental difference is that a is sampled from P . The quantity hb, θˆ i ≤ hb, θ i + (ε + β)√2d ≤ µ b + ε + (ε + β)√2d . m tha ex la o∈wro ew rs b(Φ o) u( na d⊤θˆ sh− owµ sa .)2 behaves quite differently, as A symmetrical argument completes the proof. Feature-dependent bounds The negative result Proof of Proposition 4.2. Let ρ be a probability dis- in Section 3 shows that there exist feature matrices tribution over rows(Φ) satisfying the conclusions of for which the learner must query exponentially many Theorem 4.4. Consider the algorithm that evaluates actions or suffer an estimation error that expands µ on each point of the support of ρ and computes the approximation error by a factor of √d. On the least-squares estimator defined in Proposition 4.5 the other hand, Proposition 4.2 shows that for any and predicts aˆ = arg max a, θˆ . Let a = feature matrix, there exists a learner that queries a ∈rows(Φ)h i ∗
Learning with Good Feature Representations in Bandits and in RL with a Generative Model O(d log log(d)) actions for an estimation error of ε√d, Proof. Let µ = Φθ + ∆ with ∆ ε, which exists roughly matching the lower bound. One might wonder by the assumption that µ k ε . k W∞ e≤ only analyse the ∈ HΦ whether or not there exists a feature-dependent behaviour of the algorithm within an episode, showing measure that characterises the blowup in estimation that the least-squares estimator is guaranteed to have error in terms of the feature matrix and query budget. sufficient F1-score so that (a) arms that are sufficiently One such measure is given here. Given a set C [k] suboptimal are eliminated and (b) some near-optimal with C = q, let Φ Rq d be the matrix obta⊆ ined arms are retained. Fix any b . Using the notation C × | | ∈ ∈ A from Φ by restricting to those rows indexed by C. in Algorithm 1, Define u u λ q(Φ) = C [m k],in C =q v m Rdax 0 k ΦΦ Cv vk∞ . hb, θˆ − θ i = (cid:12) (cid:12)b ⊤G −1 Xs=1 ∆ XsX s + b ⊤G −1 Xs=1 X sη s (cid:12) (cid:12) ⊂ | | ∈ \{ } k k∞ (cid:12) u (cid:12) (cid:12) (cid:12) Proposition 4.6. Let 1 ≤ q < k and δ 1 = ε(1+λ q(Φ)) ≤ (cid:12)b ⊤G −1 (cid:12) u(a)a∆ a (cid:12) + (cid:12)b ⊤G −1 X sη s (cid:12) . ((cid:12)2) and δ 2 > ε(1 + 2λ q(Φ)). Then, (cid:12) (cid:12) a X∈A (cid:12) (cid:12) (cid:12) (cid:12) Xs=1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) cest( ε ) > q cest( ε ) . The fir(cid:12)st term is bounded u(cid:12) sin(cid:12)g Jensen’s inequa(cid:12) lity as δ1 HΦ ≥ δ2 HΦ before: The proof is supplied in Appendix C. By (1), it also h coo rld res spth oa nt dc inm 2δ ga 2x lo( H weΦε r) b≤ ouq n. dC , u hr or wen et vl ey r.we do not have a (cid:12) (cid:12) (cid:12)b ⊤G −1 a X∈A u(a)∆ aa (cid:12) (cid:12) (cid:12) ≤ ε a X∈A u(a) (cid:12)b ⊤G −1a (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ε u(a) b u(a)G 1aa G 1b 5. Misspecified linear bandits ≤ v u a ! ⊤ a − ⊤ − u X∈A X∈A Here we consider the classic stochastic bandit where t 2du the mean rewards are nearly a linear function of = ε u(a) kb k2 G−1 ≤ ε m ≤ 2ε√d , their associated features. We assume for simplicity sa r X∈A that no two actions have the same features. In where the first inequality follows form Ho¨lder’s inequal- case this does not hold, a representative action can ity, the second is Jensen’s inequality and the last fol- be chosen for each feature without changing the main theorem. Let Φ Rk d and µ ε . In lows from the exploration distribution that guaran- ∈ × ∈ HΦ tees b 2 2d/m. The second term in Eq. (2) is rounds t ∈ [n], the learner chooses actions (X t)n t=1 bounk dek dG− u1 sin≤ g standard concentration bounds. Pre- with X rows(Φ) and the reward is Y = µ + t ∈ t Xt ciesly, by eq. (20.2) of (Lattimore and Szepesv´ari, η where (η )n is a sequence of independent 1- t t t=1 2019), with probability at least 1 2α, subgaussian random variables. The optimal action has − expected reward µ = max µ and the expected ∗ a a u tr oegr ue st e eis sseR nn tial= ly tE h[ e sn t a= m1 eµ ∗ e∈ l− iA miµ nX at t] i. on aT lgh oe ritid he ma ais s (cid:12)b ⊤G −1 X sη s (cid:12) ≤ kb kG−1 s2 log α1 P (cid:12) Xs=1 (cid:12) (cid:18) (cid:19) (Lattimore and Szepesv´ari, 2019, chapter 22), which (cid:12) (cid:12) (cid:12) (cid:12) 4d 1 is summarised in Algorithm 1. In each episode, (cid:12) (cid:12) log the algorithm computes a near-optimal design over a ≤ s m (cid:18) α (cid:19) subset of the actions that are plausibly optimal. It then chooses each action in proportion to the optimal and b, θˆ θ 2ε√d + 4d log 1 . Continuing |h − i| ≤ m α design and eliminates arms that appear sufficiently with standard calculations, qprovided in Appendix D, (cid:0) (cid:1) suboptimal. one gets that the expected regret satisfies Proposition 5.1. When α = 1/(kn) and C is a suitably large universal constant and max a µ a − R n ≤ C dn log(nk) + εn√d log(n) min a µ a 1, Algorithm 1 satisfies hp i ≤ where C > 0 is a suitably large universal constant. The R C dn log(nk) + εn√d log(n) . logarithmic factor in the second term is due to the fact n ≤ that in each of the logarithmically many episodes the hp i algorithm may eliminate the best remaining arm, but In Appendix F, we show that the bound in keep an arm that is at most O(ε√d) worse than the Proposition 5.1 is tight up to logarithmic factors in best remaining arm. the interesting regime where k is comparable to n.
Learning with Good Feature Representations in Bandits and in RL with a Generative Model Algorithm 1 phased elimination result to ours is to use the Eluder dimension input Φ Rk d and confidence level α (0, 1) (Russo and Van Roy, 2013), which should first be × (1) Set m∈ = 4d log log d + 16 and =∈ rows(Φ) generalised a little to accommodate the need to use an (2) Find desi⌈ gn ρ : ⌉ [0, 1] witA h g(ρ) 2d and F1-score threshhold that does not decrease with the supp(ρ) 4d logA lo→ g(d) + 16 ≤ horizon. Then the Eluder dimension can be controlled | | ≤ using either our techniques or the alternative argument (3) Compute u(a) = mρ(a) and u = u(a) ⌈ ⌉ by Van Roy and Dong (2019). a (4) Take each action a exactlX y∈Au(a) times ∈ A with corresponding features (X )u and rewards (Y )u s s=1 Contextual linear bandits Algorithms based on s s=1 (5) Calculate the vector θˆ: phased elimination are not easily adapted to the contextual case, which is usually addressed using u optimistic methods. You might wonder whether or not θˆ = G −1 X sY s with G = u(a)aa ⊤ LinUCB (Abbasi-Yadkori et al., 2011) serendipitously s=1 a adapts to misspecified models in the contextual case. X X∈A Gopalan et al. (2016) have shown that LinUCB is (6) Update active set: robust in the non-contextual case when ε is very small. a : max θˆ, b a 2 4d log 1 . Their conditions, however, depend the structure of the A ← ( ∈ A b ∈A h − i ≤ s m (cid:18) α (cid:19)) problem, and in particular on having good control (7) m 2m and goto (1) of the 2-norm of ∆, which may scale like Ω(ε√k) ← and is too big for large action sets. We provide a negative result in the supplementary material, as well Remark 5.2. When the active set contains fewer than as a modification that corrects the algorithm, but d actions, then the conditions of Kiefer-Wolfowitz are requires knowledge of the approximation error. The not satisfied because cannot span Rd. Rest assured, modification is a data-dependent refinement of the however, since in theA se cases one can simply work in bonus used by Jin et al. (2019). An open question the smaller space spanned by and the analysis goes is to find an algorithm for contextual linear bandits through without further changA es. for which the regret similar to Proposition 5.1 and where the algorithm does not need to know the Known approximation error The logarithmic approximation error. factor in the second term in the regret bound can be removed when ε is known by modifying the elimination 6. supervised learning criteria so that with high probability the optimal We now consider discounted supervised learning action is never eliminated, as explained in Remark D.1. with a generative model, which means the learner can sample next-states and rewards for any state-action Infinite action sets The logarithmic dependence pair of their choice. The notation is largely borrowed on k follows from the choice of α, which is needed from (Szepesv´ari, 2010). Fix an MDP with state to guarantee the concentration holds for all actions. space [S], action space [A], transition kernel P , reward When k = Ω(exp(d)), the union bound can be function r : [S] [A] [0, 1] and discount factor improved by a covering argument or using the × → γ (0, 1). The finiteness of the state space is assumed argument in the next section. This leads to a bound ∈ only for simplicity. As usual, V π and Qπ refer to of O(d n log(n) + εn√d log(n)), which is independent the value and action-value functions for policy π (e.g., of the number of arms. p V π(s) is the total expected discounted reward incurred while following policy π in the MDP) and V and Q Other approaches We are not the first to consider ∗ ∗ the same for the optimal policy. The learner is given a misspecified linear bandits. Ghosh et al. (2017) feature matrix Φ RSA d such that Qπ ε for all consider the same setting and show that in the ∈ × ∈ HΦ policies π and where Qπ is vectorised in the obvious favourable case when one can cheaply test linearity, way. The notation Φ(s, a) Rd denotes the feature there exist algorithms for which the regret has order ∈ associated with state-action pair (s, a). min(d, √k)√n up to logarithmic factors. While such results a certainly welcome, our focus is on The main idea is the observation that if Q were ∗ the case where k has the same order of magnitude known with reasonable F1-score on the support of an as n and hence the dependence of the regret on approximately optimal design ρ on the set of vectors ε is paramount. Another way to obtain a similar (Φ(s, a) : s, a [S] [A]), then least-squares in ∈ ×
Learning with Good Feature Representations in Bandits and in RL with a Generative Model combination with our earlier arguments would provide Significantly, the choice of parameters ensures that the a good estimation of the optimal state-action value total number of samples from the generative model is function. Approximating Q on the core set = independent of S and A. ∗ C supp(ρ) [S] [A] is possible using approximate ⊂ × Theorem 6.1. Suppose that approximate policy policy iteration. For the remainder of this section let iteration is run with ρ be a design with g(ρ) 2d and with support and ≤ C G(ρ) = (s,a) ρ(s, a)Φ(s, a)Φ(s, a) ⊤. log 1 log 2k |C| log 1 ∈C k = ε√d m = α n = ε(1 −γ) . P 1(cid:16) γ (cid:17) 2ε2((cid:16)1 γ)(cid:17)2 (cid:16)1 γ (cid:17) Related work The idea to extrapolate a value − − − function by sampling from a few anchor state/action Then there exists a universal constant C such that with pairs has been used before in a few works. The recent probability at least 1 α, the policy π satisfies k+1 work by Zanette et al. (2019) consider approximate − value iteration in the episodic setting and do not max (V ∗(s) V πk+1 (s)) Cε√d/(1 γ)2 , make a connection to optimal design. The challenge s [S] − ≤ − ∈ in the finite-horizon setting is that one must learn one parameter vector for each layer and, at least When ρ is chosen using Theorem 4.4 so that naively, errors propagate multiplicatively. For this 4d log log(d) + 16, then the number of samples|C fr| om≤ reason using the anchor pairs from the support of an the generative model is kmn , which is experimental design would not make the algorithm |C| proposed by the aforementioned paper practical. log 1 log 2k |C| log 1 d log log(d) Yang and Wang (2019) assume the transition matrix O ε(1 −γ) α ε√d .  (cid:16) (cid:17) (cid:16)ε2(1 (cid:17) γ)4(cid:16) (cid:17)  has a linear structure and also use least-squares − with data from a pre-selected collection of anchor   state/action pairs. Their assumption is that the Before the proof we need two lemmas. The first features of all state-action pairs can be written as a controls the propagation of errors in policy iteration convex combination of the anchoring features, which when using Q k rather than Qπk . For policy π, let means the number of anchors is the number of corners P π : R[S] ×[A] R[S] ×[A] defined by (P πQ)(s, a) = → of the polytope spanned by rows(Φ) and may be much s′ P (s ′ |s, a)Q(s ′, π(s ′)). larger than d. One special feature of their paper is PLemma 6.2. Let δ i = Q i Qπi and E i = P πi+1(I that the dependency on the horizon of the sample γP πi+1) −1(I γP πi) P− π∗ . Then, Q ∗ Qπk − c itom isp qle ux ai rt ty ici .s Ecu ab ri lc ierin , L1 a/ k(1 sh− mγ in), aw rah yi ale nain n o eu t r alt .h (e 2o 0r 1em 8) (γP π∗ )k(Q ∗ −− Qπ0) + γ− ik =− 01(γP π∗ )k −i −1E− iδ i. ≤ described how anchor states (with some lag allowed) P Proof. This is stated as Eq. (7) in the proof of can be used to reduce the number of constraints part (b) of Theorem 3 of (Farahmand et al., 2010) and in the approximate linear programming approach to ultimately follows from Lemma 4 of Munos (2003). approximate planning in MDPs, while maintaining error bounds. The second lemma controls the value of the greedy policy with respect to a Q function in terms of the Approximate policy iteration Let π be an 1 quality of the Q function. arbitrary policy and define a sequence of policies (π ) inductively using the following procedure. Lemma 6.3 (Singh and Yee (1994), corollary 2). Let k ∞k=1 From each state-action pair (s, a) take m roll-outs π be greedy with respect to an action-value function Q. of length n following policy π k and∈ lC et Qˆ k(s, a) be the Then for any state s ∈ [S], V π(s) ≥ V ∗(s) − 1 2 γ kQ − empirical average, which is only defined on the core Q . − ∗ set . The estimation of Qπk is then extended to all k∞ C state-action pairs using the features and least-squares Proof of Theorem 6.1. Hoeffding’s bound and the definition of the roll-out length shows that for any θˆ k = G(ρ)−1 ρ(s, a)Φ(s, a)Qˆ k(s, a) Q k = Φθˆ k . (s, a) , with probability at least 1 α, (sX,a) ∈C ∈ C − Then π is chosen to be the greedy policy with 1 1 2 k+1 Qˆ (s, a) Qπi(s, a) log + ε = 2ε . respect to Q k and the process is repeated. The i − ≤ 1 γ s 2m α following theorem shows that for suitable choices of (cid:12) (cid:12) − (cid:18) (cid:19) (cid:12) (cid:12) roll-out length n, roll-out number m and iterations k, A(cid:12) t the end we anal(cid:12)yse the failure probability of the the policy π is nearly optimal with high probability. algorithm, but for now assume the above inequality k+1
Learning with Good Feature Representations in Bandits and in RL with a Generative Model holds for all i k and (s, a) . Let θ = algorithm needs to examine all features. Second, the i ≤ ∈ C arg min Qπi Φθ . Then, by Proposition 4.5 with argument in Section 6 heavily relies on the uniform θ k − k∞ β = 2ε, contraction property of the various operators involved. It remains to be seen whether similar arguments hold . Q i Qπi = Φθˆ i Qπi 3ε√2d + ε = δ . for other settings, such as the finite horizon setting k − k∞ k − k∞ ≤ or the average cost setting. Another interesting open Since the rewards belong to the unit interval, taking question is whether a similar result holds for the online the maximum norm of both sides in Lemma 6.2 shows setting when the learner needs to control its regret. that Q Qπk 2δ/(1 γ) + γk/(1 γ). Then, ∗ k − k∞ ≤ − − by the triangle inequality, 8. Bibliography Q Q Q Qπk + Q Qπk k k − ∗ k∞ ≤ k k − k∞ k ∗ − k∞ Y. Abbasi-Yadkori, D. Pa´l, and Cs. Szepesv´ari. 3δ γk Improved algorithms for linear stochastic bandits. + . ≤ 1 γ 1 γ In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, − − F. Pereira, and K. Q. Weinberger, editors, Advances Next, by Lemma 6.3, for any state s [S], in Neural Information Processing Systems 24, NIPS, ∈ pages 2312–2320. Curran Associates, Inc., 2011. 2 V πk+1(s) V ∗(s) Q k Q ∗ ≥ − 1 γ k − k∞ K. Amin, M. Kearns, and U. Syed. Bandits, − V (s) 2 3δ + γk . query learning, and the haystack dimension. In ∗ ≥ − (1 γ)2 Proceedings of the 24th Annual Conference on − (cid:0) (cid:1) Learning Theory, pages 87–106, 2011. All that remains is bounding the failure probability, which follows immediately from a union bound over all J.-Y. Audibert and S. Bubeck. Best arm identification iterations i k and state-action pairs (s, a) . in multi-armed bandits. In Proceedings of ≤ ∈ C Conference on Learning Theory (COLT), 2010. 7. Conclusions R. Degenne, W. M. Koolen, and P. M´enard. Non- Are good representations sufficient for efficient learn- asymptotic pure exploration by solving games. In ing in bandits or in RL with a generative model? The Advances in Neural Information Processing Systems, answer depends on whether one accepts a blowup of pages 14465–14474, 2019. the approximation error by a factor of √d, and is pos- itive if and only if this blowup is acceptable. The im- S. S. Du, S. M. Kakade, R. Wang, and L. F. Yang. Is plication is that the role of bias/prior information is a good representation sufficient for sample efficient more pronounced than in supervised learning where supervised learning?, 2019. the blowup does not appear. One may wonder whether E. Even-Dar, S. Mannor, and Y. Mansour. Action the usual changes to the learning problem, such as elimination and stopping conditions for the multi- considering sparse approximations, could reduce the armed bandit and supervised learning problems. blowup. Since sparsity is of little help even in the real- Journal of Machine Learning Research, 7(Jun):1079– isable setting (Lattimore and Szepesv´ari, 2019, chap- 1105, 2006. ter 23), we are only modestly optimistic in this regard. Note also that in supervised learning, the blowup A-m. Farahmand, R. Munos, and Cs. Szepesv´ari. is even harsher: in the discounted case we see that a Error propagation for approximate policy and value factor of 1/(1 γ)2 also appears, which we believe is − iteration (extended version). In NIPS, 2010. not improvable. The analysis in both the bandit and reinforcement V. V. Fedorov. Theory of optimal experiments. learning settings can be decoupled into two compo- Academic Press, New York, 1972. nents. The first is to control the query complexity of A. Ghosh, S. R. Chowdhury, and A. Gopalan. identifying a near-optimal action and the second is es- Misspecified linear bandits. In Thirty-First AAAI timating the value of an action/policy using roll-outs. Conference on Artificial Intelligence, 2017. This view may be prove fruitful when analysing (more) non-linear classes of reward function. Aditya Gopalan, Odalric-Ambrym Maillard, and There are many open questions. First, in order Mohammadi Zaki. Low-rank bandits with latent to compute an approximate optimal design, the mixtures. arXiv preprint arXiv:1609.01508, 2016.
Learning with Good Feature Representations in Bandits and in RL with a Generative Model C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably A. Zanette, A. Lazaric, M. J. Kochenderfer, and efficient supervised learning with linear function E. Brunskill. Limiting extrapolation in linear approximation. arXiv preprint arXiv:1907.05388, approximate value iteration. In H. Wallach, 2019. H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural J. Kiefer and J. Wolfowitz. The equivalence of Information Processing Systems 32, pages 5616– two extremum problems. Canadian Journal of 5625. Curran Associates, Inc., 2019. Mathematics, 12(5):363–365, 1960. C. Lakshminarayanan, S. Bhatnagar, and Cs. Szepesv´ari. A linearly relaxed approximate lin- A. Proof of Lemma 2.2 ear program for Markov decision processes. IEEE Transactions on Automatic Control, 63(4):1185– Recall that Lemma 2.2 states that 1191, 2018. T. Lattimore and Cs. Szepesv´ari. Bandit Algorithms. cmax( e , . . . , e ) = (k + 1)/2 . Cambridge University Press, 2019. draft. 1 { 1 k } R. Munos. Error bounds for approximate policy For the upper bound consider, an algorithm that iteration. 19th International Conference on Machine queries each coordinate in a random order, stopping Learning, pages 560–567, 2003. as soon as it receives a nonzero reward, at which point the algorithm can return µˆ = µ and aˆ = arg max µ. D. Russo and B. Van Roy. Eluder dimension and Clearly, this algorithm is sound. Let S Perm([k]) the sample complexity of optimistic exploration. ∈ be a (uniform) random permutation of [k], which In C. J. C. Burges, L. Bottou, M. Welling, represents the order of the algorithms queries. Let Z. Ghahramani, and K. Q. Weinberger, editors, T = min t 1 : S = i be the number of queries Advances in Neural Information Processing Systems i t { ≥ } if r = e . Note that (T ) is a random permutation of 26, NIPS, pages 2256–2264. Curran Associates, Inc., i i i [k]. Note also that by symmetry E[T ] = = E[T ]. 2013. 1 i Hence, max E[T ] = 1 E[ T ] = (k + 1)/2· .· · i i k i i S. P. Singh and R. C. Yee. An upper bound on the loss The proof of the lower Pbound is based on a similar from approximate optimal-value functions. Machine calculation. First, note that by Yao’s principle it Learning, 16(3):227–233, 1994. suffices to show that there exist a distribution P over M. Soare, A. Lazaric, and R. Munos. Best-arm the problem instances such that any deterministic identification in linear bandits. In Z. Ghahramani, algorithm needs to ask at least (k + 1)/2 queries on M. Welling, C. Cortes, N. D. Lawrence, and expectation when the instance that the algorithm runs K. Q. Weinberger, editors, Advances in Neural on is chosen randomly from the distribution. Let P be Information Processing Systems 27, NIPS, pages the uniform distribution. Note that any deterministic 828–836. Curran Associates, Inc., 2014. algorithm can be identified with a fixed sequence A s , s , [k] of queries. Call dominated if some 1 2 Cs. Szepesv´ari. Algorithms for Reinforcement · · · ∈ A other algorithm achieves better query complexity ′ Learning. Morgan & Claypool, 2010. A on any input instance while the query complexity M. J. Todd. Minimum-volume ellipsoids: Theory and of ′ is never worse than that of on any other A A algorithms, volume 23. SIAM, 2016. instance. Clearly, it suffices to consider nondominated algorithms. Hence, s , s , . . . , s cannot have repeated 1 2 k B. Van Roy and S. Dong. Comments on the Du- elements, i.e., (s , s , . . . , s ) is a permutation of [k]. 1 2 k Kakade-Wang-Yang lower bounds. In preparation, Further, must stop as soon as it receives a nonzero A 2019. answer or it would be dominated. This implies that the number of queries issued by when run on e is M. J. Wainwright. High-dimensional statistics: A A i t(i) = min t 1 : s = i . (Since is sound and non-asymptotic viewpoint, volume 48. Cambridge { ≥ t } A δ 1, cannot stop before time t(i) when running University Press, 2019. ≤ A on e and if stopped later, it would be dominated by i L. Yang and M. Wang. Sample-optimal parametric the algorithm that stops at time t(i)). Since (s i) i is a Q-learning using linearly additive features. In permutation of [k], so is (t(i)) i [k]. Then, the expected International Conference on Machine Learning, number of queries issued by ∈ is E[t(I)] = 1 t(i) = A k i pages 6995–7004, 2019. 1 i = (k + 1)/2. k i P P
Learning with Good Feature Representations in Bandits and in RL with a Generative Model B. Proof of Corollary 3.3 of plausible rewards consistent with the observation. By the assumption that the algorithm is sound with By the proof of Lemma 2.2, it should be clear that if respect to ( ε , δ ), it must be that δe , . . . , δe , then cmax( ) (k + 1)/2. Let HΦ 1 1 k ∈ H δ H ≥ 2 max ν max ν ξ 2δ , 1 d 1 ε 2 ν k k∞ ≤ ν,ξ k − k∞ ≤ k = exp − . ∈R ∈R 8 δ where the first inequality is true since for all ν we (cid:22) (cid:18) (cid:16) (cid:17) (cid:19)(cid:23) ∈ R have ν . Then, Then, by Lemma 3.1, there exists a feature matrix − ∈ R Φ ′ Rk ×d such that for all a = b rows(Φ ′), a = 1 max ν and∈ a, b ε/δ. Let rows(Φ 6 ) = ∈ a , . . . , a k . Tk hen, ν k k∞ ′ 1 k ∈R kδe i h − Φ ′i (δ≤ a i) k ≤ ε and hence δe i { ∈ HΦε for a} ll i ∈ [k]. = max kΦθ + ∆ k∞ : kΦ C θ k∞ ≤ ε, θ ∈ Rd, ∆ ∈ B ∞(ε) Thus, cm δ ax( HΦε ) ≥ (k + 1)/2. The result follows from = ε + m(cid:8)ax Φθ : Φ Cθ ε, θ Rd (cid:9) the definition of k. k k∞ k k∞ ≤ ∈ ε + ελ (Φ) q (cid:8) (cid:9) ≥ δ , C. Proof of Proposition 4.6 ≥ 1 which contradicts soundness and hence C > q. | | Algorithm 2 Optimal estimation algorithm input: Φ Rk d and q [k] and ε 0 D. Details for proof of Proposition 5.1 × ∈ ∈ ≥ (1) Find C [k] with C = q minimising ⊂ | | The proof is completed in two steps. First we max Φu : u Rd, Φ u 1 summarise what has already been established about C k k∞ ∈ k k ≤ the within-episode behaviour of the algorithm. Then, (cid:8) (cid:9) (2) Probe µ on C in the second step, the regret is summed over the (3) Find θˆ Rd such that Φ θˆ µ ε episodes. C C (4) Return∈ µˆ = Φθˆ k − k∞ ≤ In-episode behaviour Let F be the σ-algebra generated by the history up to the start of a given Upper bound The upper bound is realised by episode and µˆ be the least-squares estimate of µ Algorithm 2. Since µ ε , there exists a θ Rd and computed by the algorithm in that episode. Similarly, ∈ HΦ ∈ ∆ B (ε) such that µ = Φθ + ∆. By the definition let m, u and be the quantities defined in the given ∈ ∞ A of the algorithm, kµˆ C − µ C k∞ ≤ ε, which implies that e ap ni dso aˆd =e o af rgth me aa xlgori µt ˆhm . N. oL we ,t foa r∗ b = ar ,g lem t ax a b∈eAtµ ha e Φ C (θ θˆ) = µ C ∆ C µˆ C event a ∈A a ∈ A Eb k − k∞ k − − k∞ ε + µ µˆ C C ≤ k − k∞ 4d 1 2ε . = b, θˆ θ 2ε√d + log . ≤ Eb ( (cid:12)h − i (cid:12) ≤ s m (cid:18) α (cid:19)) Next, using the definition of λ , (cid:12) (cid:12) q We have sh(cid:12)own that(cid:12) P( F ) 1 α, which by a b E | ≥ − µ µˆ = Φ(θ θˆ) + ∆ union bound implies that k − k∞ k − k∞ Φ(θ θˆ) + ε P( F ) 1 kα . b b ≤ k − k∞ ∪ ∈AE | ≥ − Φv ≤ kΦ C(θ − θˆ) k∞ v m Rdax 0 k Φ C vk∞ + ε B noy t t eh lie md ie nfi an teit dio an t o tf heth ee ndalg oo f r ti hth em ep, ia sn oda ect ifion a ∈ A is ∈ \{ } k k∞ 2ελ (Φ) + ε q ≤ 4d 1 < δ 2 . θˆ, aˆ a 2 log , h − i ≤ s m δ (cid:18) (cid:19) Lower bound Suppose an algorithm is sound with which implies that respect to ( ε , δ ). It suffices to show that there exists HΦ 1 a µ ε such that whenever the algorithm halts with 4d 1 ∈ HΦ 2 log θˆ, aˆ a non-zero probability, it has made more than q queries. s m δ ≥ h − i (cid:18) (cid:19) Let µ = 0 and suppose the algorithm halts having queried µ on C [k] with non-zero probability and 4d 1 |C | ≤ q. Let R⊂ = {µ ∈ HΦε : µ C = 0 } be the set ≥ hθ, a ∗ − a i − 2 s m log (cid:18) α (cid:19) − 4ε√d .
Learning with Good Feature Representations in Bandits and in RL with a Generative Model Hence, if a is not eliminated, then 1 ∈ A C dn log + εn√d log(n) . µ a a, θ ε ≤ "s (cid:18) α (cid:19) # ≥ h i − 4d 1 The result follows because the regret due to failing a ∗, θ ε 4 log 4ε√d confidence intervals is at most αknL L log (n), ≥ h i − − s m α − ≤ ≤ 2 (cid:18) (cid:19) which is negligible relative to the above term. µ a∗ ε 4 4d log 1 4ε√d . Remark D.1. When ε is known, the elimination ≥ − − s m α − condition can be changed to (cid:18) (cid:19) Because the condition for eliminating arms does not θˆ, b a d 1 depend on ε, it is not possible to prove that a a : max h − i log + ε√d . is not eliminated with high probability. What w∗ e A ← ( ∈ A b ∈A 4 ≤ s m (cid:18) δ (cid:19) ) can show is that at least one near-optimal action is Repeating the analysis now shows that the optimal retained. Suppose that a ∗ is eliminated, then, using action is never eliminated with high probability, which the definition of the algorithm, eliminates the logarithmic dependence in the second term of Proposition 5.1. 4d 1 2 log < θˆ, aˆ a ∗ s m (cid:18) α (cid:19) h − i E. Linear contextual bandits 4d 1 θ, aˆ a∗ + 2 log + 4ε√d . In the contextual version of the misspecified linear ≤ h − i s m α bandit problem, the feature matrix changes from (cid:18) (cid:19) round to round. Let (k )n be a sequence of natural Rearranging shows that t t=1 numbers. At the start of round t the learner observes µ aˆ ≥ hθ, aˆ i − ε a matrix Φ t ∈ Rkt ×d, chooses an action X t ∈ rows(Φ t) and receives a reward Y = X , θ + η + ∆(X ) > hθ, a ∗ i − ε − 4ε√d where ∆ : Rd R satisfit es ∆h t i ε. Et liminatiot n µ a∗ 2ε(1 + 2√d) . (3) algorithms are → not suitable fork suk c∞ h p≤ roblems. Here we ≥ − show that if ε is known, then a simple modification of Of course, aˆ is not eliminated, which means that either LinUCB (Abbasi-Yadkori et al., 2011) can be effective. a is retained, or an action with nearly the same ∗ You might wonder whether or not this algorithm reward is. What we have shown is that arms are works well without modification. The answer is sadly eliminated if they are much worse than a and that ∗ negative. some arm with mean close to a is retained. We now ∗ combine these results. Let G t = I + t s=1 X sX s⊤ and define the regurlarised least-squares estimator based on data from the first t P Combining the episodes Let L be the number of rounds by episodes and δ be the suboptimality of the best arm in ℓ t the active set at the start of episode ℓ. By the previous θˆ t = G−t 1 X sY s . part, with probability at least 1 kαL, the good events − Xs=1 occur in all episodes. Suppose for a moment that this Assume for all a n rows(Φ ) that a 1 and happens. Then, by Eq. (3), ∈ ∪t=1 t k k2 ≤ a, θ 1. The standard version of LinUCB chooses |h i| ≤ δ 2ε(ℓ 1)(1 + 2√d) . ℓ ≤ − X t+1 = arg max ha, θˆ t i + ka kG− t 1β t , (4) Then, letting m ℓ = 2ℓ −1 4d log log(d) + 16 , the a ∈rows(Φt+1) ⌈ ⌉ regret is bounded by n with β = 1 + 2 log (n) + d log 1 + . (5) t d L r (cid:16) (cid:17) R n = u ℓ(a)(µ ∗ µ a) The modification chooses − Xℓ=2 a X∈Aℓ t ≤ m 1 + Xℓ=L 2 m ℓ "δ ℓ + 2 s m4 ℓd −1 log (cid:18) α1 (cid:19) + 4ε√d # X t+1 = a ∈a rr og wsm (Φa t+x 1)ha, θˆ t i + ka kG− t 1β t + ε Xs=1 |a ⊤G−t (61 )X s | . C dm log 1 + εm log(m )√d This modification is reminiscent of the algorithm ≤ "s L (cid:18) α (cid:19) L L # by Jin et al. (2019), who use a bonus of Ω(t1/2)
Learning with Good Feature Representations in Bandits and in RL with a Generative Model when using upper confidence bounds for least-squares Hence estimators for linear dynamics in supervised learning. d log(n) Theorem E.1. The regret of the algorithm defined by hθˆ t, (2, 1) i + k(2, 1) kG− t 1β t = −1 + O r nε2 ! Eq. (6) satisfies and this for every suitably large n the algorithm will R = O d√n log(n) + nε d log(n) . choose (0, 0) for all rounds t n/2 and suffer regret n ≥ at least n/2. Thus, if R (ε) is the regret on the above (cid:16) p (cid:17) n Proof sketch. The main point is that the additional problem, bonus term ensures optimism. Then, the standard R (ε) regret calculation shows that sup n = , n,ε>0 εn log(n) ∞ n t 1 R n = O d√n log(n) + εE " − |X t⊤G−t −1 1X s |#! . while for the modified ap lgorithm, t=1 s=1 X X The latter term is bounded by R n(ε) sup < + . n,ε>0 εn log(n) ∞ n t 1 n t 1 − − t=1 s=1 |X t⊤G−t −1 1X s | ≤ n v ut=1 s=1(X t⊤G−t −1 1X s)2 F. Lower boundsp for linear bandits X X uX X t n The upper bound in Section 5 cannot be improved in ≤ n v ut=1 kX t k2 G− t−1 1 t sh he owm so : st interesting regimes, as the following theorem uX t = O n d log(n) . Theorem F.1. There exists a feature matrix Φ Rk d such that for any algorithm there is a mea∈ n (cid:16) p (cid:17) × Hence the regret of this algorithm satisfies reward vector µ ε for which ∈ HΦ R = O d√n log(n) + εn d log(n) . n d 1 Remark E.2. (cid:16) As far as we p know, th(cid:17) ere is no R n ≥ ε min(n, (k − 1)/2) s 8 lo− g(k) . algorithm obtaining a similar bound when ε is unknown. Proof. By the negative result, we may choose Φ Rk d such that ∈ Failure of unmodified algorithm That the × algorithm defined by Eq. (5) is not good for contextual a, a = 1 for all a rows(Φ) bandits follows from the following example. Let η = 0 h i ∈ t for all rounds t and 8 log(k) a, b for all a, b rows(Φ) with a = b . h i ≤ d 1 ∈ 6 2 1 r − Φ = ε 0 Φ = 0 ε Φ = . odd even large 0 0 Next, let a rows(Φ) and (cid:18) (cid:19) ∗ (cid:0) (cid:1) (cid:0) (cid:1) ∈ Now suppose for odd rounds t n/2 the feature ≤ matrix is Φ = Φ in odd rounds and Φ = Φ d 1 t odd t even θ = δa ∗ with δ = − , in even rounds. For rounds t > n/2 the feature matrix s 8 log(k) is Φ . Then let θ = (1/2, 1/2) and ∆((ε, 0)) = ε large − − and ∆((0, ε)) = ε. Hence for t = n/2, which is chosen so that µ ε , where ∈ HΦ 1 + nε2/4 0 G t = 0 1 + nε2/4 µ = δ if a = a ∗ (cid:18) nε2/8 nε2/8 (cid:19) a (0 otherwise . θˆ = , . t (cid:18)− 1 + nε2/4 1 + nε2/4 (cid:19) Let τ = max t n : A s = a ∗ s t . Then E[R n] Therefore (2, 1) 2 20/(nε2) and δE[τ ]. Since{ th≤ e law of 6 the ∀ rew≤ ard} s is independen≥ t k kG− t 1 ≤ of a for t τ , it follows from the randomisation ∗ nε2/8 4 hammer that≤E[τ ] min(n, (k 1)/2) and the result θˆ , (2, 1) = 1 . ≥ − h t i − 1 + nε2/4 ≤ nε2 − follows.
Learning with Good Feature Representations in Bandits and in RL with a Generative Model G. Computation complexity We briefly discuss the computation complexity of our algorithms here. Both the bandit and RL algorithms rely on computing a near-optimal design, which is addressed first. Computing a near-optimal design The standard method for computing a near-optimal design is Frank– Wolfe, which in this setting is often attributed to Fedorov (1972). With this algorithm and an appropri- ate initialisation constant factor approximation of the optimal design can be computed in O(kd2 log log(d)) computations. For more details we recommend chap- ter 3 of the book by Todd (2016), which also describes a number of improvements, heuristics and practical guidance. Bandit algorithm computations Algorithm 1 has at most O(log(n)) episodes. In each episode it needs to (a) compute a near-optimal design and (b) collect data and find the least-squares estimator and (c) perform action elimination. The computation is dominated by finding the near optimal design and computing the covariance matrix G, which leads to a total computation of O(kd2 log log(d) log(n) + nd2). RL computations The algorithm described in Section 6 operates in episodes over k episodes. In each episode it computes an approximate design and performs m roll-outs of length n from each action in the core set. Assuming sampling from the generative model is O(1), the total computation, ignoring logarithmic factors, is dA SAd2 O˜ + . ε2(1 γ)4 1 γ (cid:18) − − (cid:19) Dishearteningly, the size of the state space appears in the computation of the optimal design. Hence, while the sample complexity of our algorithm is independent of the state space, the computation complexity is not.
