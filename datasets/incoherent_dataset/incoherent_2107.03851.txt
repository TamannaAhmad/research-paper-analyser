Imitation by Predicting Observations Andrew Jaegle 1 Yury Sulsky 1 Arun Ahuja 1 Jake Bruce 1 Rob Fergus 1 Greg Wayne 1 Abstract 2009; Huber et al., 2009). While most algorithms for imita- tion learning assume that demonstrations contain the actions Imitation learning enables agents to reuse and the expert executed, animals must imitate without directly adapt the hard-won expertise of others, offering observing what actions the expert took (i.e. without know- a solution to several key challenges in learning ing exactly what commands were issued to produce the behavior. Although it is easy to observe behav- observable changes). In the context of machine learning, ior in the real-world, the underlying actions may solving the problem of imitation from observation is a key not be accessible. We present a new method for step towards the tantalizing possibility of learning behavior imitation solely from observations that achieves from unlabeled and easy to collect data, such as raw video comparable performance to experts on challeng- footage of human activity. Many recent algorithms for imi- ing continuous control tasks while also exhibiting tation have focused on addressing the problem of imitation robustness in the presence of observations unre- in very small data regimes, but the challenge in imitating lated to the task. Our method, which we call from these abundant sources of data is not primarily one of FORM (for “Future Observation Reward Model”) quantity. The challenge is rather how to learn models for is derived from an inverse RL objective and im- imitation that are general enough to learn and generalize itates using a model of expert behavior learned from data that depicts a rich (and unknown) reward structure. by generative modelling of the expert’s observa- In this work, we show how predictive generative models can tions, without needing ground truth actions. We be used to learn a general reward model from observations show that FORM performs comparably to a strong alone. baseline IRL method (GAIL) on the DeepMind Control Suite benchmark, while outperforming Current state-of-the-art approaches to imitation (including GAIL in the presence of task-irrelevant features. from observation) pose learning as an adversarial game: a classifier estimates the probability that a state is visited by the expert or imitator, and the policy seeks to maximize 1. Introduction the classifier error (Merel et al., 2017; Torabi et al., 2019a). Because these methods are based on matching the expert’s The goal of imitation is to learn to produce behavior that occupancy using a fixed dataset of demonstrations, they tend matches that of an expert on unseen data, given demonstra- to be very sensitive to the precise details of the demonstra- tions of the expert’s behavior (Abbeel & Ng, 2004; Osa et al., tions and to the representation used. This property makes 2018). The field of imitation learning offers tools for learn- learning with adversarial methods difficult when using raw, ing behavior when programmed rewards cannot be provided, noisy observations without extensive tuning and careful or when rewards can only be partially or sparsely specified. use of strong forms of regularization (Peng et al., 2019), Imitation learning has been at the heart of several break- domain or task knowledge (Zolna et al., 2020), or a com- throughs in building AI agents (Pomerleau, 1989; Abbeel bination of behavioral cloning and careful representation et al., 2010; Silver et al., 2016; Vinyals et al., 2019; OpenAI design (Abramson et al., 2020). et al., 2019), allowing agents to learn even when faced with hard exploration problems (Gulcehre et al., 2020). In this work, we introduce the future observation reward model (FORM) (see Figure 1), which address the problem There is widespread evidence that imitation (among other of imitation from observation while exhibiting both (1) gen- forms of social learning) is a core mechanism by which hu- erality and expressiveness by coupling predictive generative mans and other animals learn to acquire a sophisticated be- models with inverse RL (IRL) and (2) improved robustness havioral repertoire (Tomasello, 1996; Laland, 2008; Byrne, by foregoing an adversarial formulation. In FORM, the imi- 1DeepMind. Correspondence to: Andrew Jaegle <drewjae- tator tries to match the probability of observation sequences gle@deepmind.com>. in the expert data. It does so using a learned generative model of expert observation sequences and a learned genera- Proceedings of the 38 th International Conference on Machine tive model of its own observation sequences. In other words, Learning, PMLR 139, 2021. Copyright 2021 by the author(s). 1202 luJ 8 ]GL.sc[ 1v15830.7012:viXra
Imitation by Predicting Observations Figure 1. FORM learns to imitate expert behavior using sequences of internal state observations, without access to the expert’s actions. Visualizations of agent behavior (top) and reward curves for a single episode (bottom) are shown after 0 (left), 50k (middle) and 5M update steps. FORM imitates using two learned models: both the demonstrator model (trained offline) and the imitator model (trained online) log-likelihoods track the unseen task reward as the imitation agent learns. Agent behavior is shown as images, but we use lower-dimensional internal state observations in this work. FORM casts the problem of learning from demonstrations erative models of expert and imitator behavior with as a sequence prediction problem, using a generative model standard policy optimization techniques. of expert sequences to guide RL. Because FORM builds sep- 2. We develop a practical algorithm for imitation learning arate models of expert and imitator sequences, rather than using the FORM reward and demonstrate that it per- using a single classifier to discriminate expert and imitator forms competitively with a well-tuned GAIfO model states, it is less prone to focus on irrelevant differences be- on the DeepMind Control Suite benchmark. tween the expert and imitator demonstrations. The structure of the FORM objective makes it theoretically straightfor- 3. We show that FORM is more robust than GAIfO in the ward to optimize using standard policy optimization tools presence of extraneous, task-irrelevant features, which and, as we show, empirically competitive on the DeepMind simulate domain shift between expert and imitator set- Control Suite continuous control benchmark domain. tings. This stands in contrast to adversarial methods, such as Gen- erative Adversarial Imitation Learning (GAIL), whose ob- 2. Background and related work jectives are known to be ill-posed (without additional regu- RL, IRL, and imitation supervised learning is con- larization) and challenging to optimize both in theory and cerned with learning a policy that maximizes the expected in practice (Arjovsky et al., 2017; Gulrajani et al., 2017; return, which is given as the expected sum of all future Mescheder et al., 2017). This property makes it difficult to discounted rewards (Sutton & Barto, 2018), which are typi- apply adversarial techniques to imitation in settings with cally observed. In imitation learning, on the other hand, we even small differences between expert and imitator settings are not given a reward function, but we do have access to (Zolna et al., 2020). Robustness to distractors is an im- demonstrations produced by a demonstrator (or expert) pol- portant part of behavior learning, as recently illustrated by icy, which maximizes some (unobserved) expected return. Stone et al. 2021 in the context of RL with image back- ground distractors. These situations are common in practice: IRL has the related goal of recovering the unobserved re- the lab environment where expert data is collected for a ward function from expert behavior. IRL offers a general for- robot will be quite different to where it might be deployed. mula for imitation: estimate the reward function underlying While it may be possible to collect a large number of demon- the demonstration data (a “reward model”) and maximize strations, it is impossible to exhaustively sample all possible this reward by RL (Ng & Russell, 2000), possibly iterating sources of differences between the two domains (such as multiple times until convergence. Alternative approaches the surface texture, robot physical parameters, or environ- to imitation, such as behavioral cloning (BC) (Pomerleau, ment appearance). These differences confound the signal 1989) or BC from observations (BCO) (Torabi et al., 2018), that must be imitated, leading to the risk of spurious depen- typically have difficulty producing reliable behavior away dencies between the two being learned. As we will show, from configurations seen in the expert demonstrations. This FORM exhibits greater robustness than a well-tuned adver- is because small errors in predicting actions or mimicking sarial imitation method, GAIL from Observation, or GAIfO short-term agent behavior accumulates over long behavioral (Torabi et al., 2019a) in presence of task-independent fea- timescales.1 IRL methods like FORM avoid this problem: tures. because they perform RL on a learned reward, they can learn through experience to recover from mistakes by focusing on We make the following technical contributions in this work: long-term consequences of each action. 1. We derive the FORM reward from an objective for 1The standard solution to this problem for BC assumes access inverse supervised learning from observations. We to an expert policy that can be repeatedly queried (Ross et al., show that this reward can be maximized using gen- 2011), which is not always feasible.
(cid:76)(cid:200)(cid:161)(cid:225)(cid:121)(cid:210)(cid:143)(cid:303)(cid:344)(cid:303)(cid:14)(cid:186)(cid:181)(cid:242)(cid:139)(cid:143)(cid:181)(cid:210)(cid:161)(cid:121)(cid:174) (cid:60)(cid:186)(cid:210)(cid:143)(cid:303)(cid:210)(cid:160)(cid:121)(cid:210)(cid:303)(cid:210)(cid:160)(cid:143)(cid:303)(cid:121)(cid:174)(cid:161)(cid:156)(cid:181)(cid:180)(cid:143)(cid:181)(cid:210)(cid:204)(cid:303)(cid:121)(cid:200)(cid:143)(cid:303) (cid:139)(cid:161)(cid:155)(cid:155)(cid:143)(cid:200)(cid:143)(cid:181)(cid:210)(cid:303)(cid:161)(cid:181)(cid:303)(cid:143)(cid:231)(cid:197)(cid:186)(cid:200)(cid:210)(cid:143)(cid:139)(cid:303)(cid:76)(cid:19)(cid:34)(cid:270)(cid:270)(cid:270) Imitation by Predicting Observations GAIL and occupancy-based IRL Most contemporary (cid:91) (cid:83) IRL-based approaches to imitation – as exemplified by (cid:87) (cid:55) GAIL – use a strategy of state-action occupancy match- ing, typically by casting imitation as an adversarial game (cid:68) and learning a classifier to discriminate states and actions sampled uniformly from the expert demonstrations from (cid:91) (cid:652) those encountered by the imitator (Ho & Ermon, 2016; (cid:87)(cid:16)(cid:20) Torabi et al., 2019a; Fu et al., 2018; Kostrikov et al., 2019; Ghasemipour et al., 2019). In contrast, rather than classi- Figure 2. FORM’s demonstrator and imitator effect models are fying states as belonging to the expert or imitator, FORM effect models, generative models p(x t|x t−1) of the change in ob- learns to imitate using separate generative models of expert servation (observed) produced by a policy π in an environment and imitator behavior. This means that FORM is built on pre- with transition dynamics p T (unobserved). The models used in model-based RL are usually of the form p(x |x , a ) and aim dictive models of the form p(x |x ), where xs are obser- t t−1 t−1 t t−1 to model transition dynamics rather than the full distribution of vations, rather than a single model of the form p(expert|x) outcomes given a policy. that tries to classify observations as generated by the expert or not. FORM’s objective is similar in spirit to classical that the structure of observation sequences can be exploited feature-matching and maximum-entropy formulations of to generate behavior, whether in the context of language imitation (Ng & Russell, 2000; Abbeel & Ng, 2004; Ziebart modeling (Brown et al., 2020), 3D navigation (Dosovitskiy et al., 2008), while also providing a fully probabilistic in- & Koltun, 2017), few-shot planning (Rybkin et al., 2019), or terpretation and making minimal assumptions about the value-based RL (Edwards et al., 2020). FORM uses genera- environment (the FORM objective does not require an MDP tive models of future observations to exploit this property of or deterministic transitions). observation transitions and connect it to inverse reinforce- ment learning to produce a practical algorithm for imitation. Other related methods for imitation Other recent work has used generative models in the context of imitation learn- ing: this work typically retains GAIL’s occupancy-based 3. Approach perspective (Baram et al., 2016; Jarrett et al., 2020; Liu 3.1. Inverse supervised learning from observations et al., 2021) or introduces a generative model to provide a heuristic reward (Yu et al., 2020). Unlike FORM, which Our goal is to learn a policy that produces behavior like uses effect models (see Figure 2) that are suitable for imi- an expert (or demonstrator) by IRL, using only observation. tation from observations, this work models quantities that Historically, the IRL procedure has been framed as matching are useful primarily in conjunction with actions (modeling the expected distribution over states and actions (or their state-action densities and/or dynamics models for GAIL features) along the imitator and demonstrator paths (Ng & augmentation). Other recently proposed methods learn re- Russell, 2000; Abbeel & Ng, 2004; Ziebart et al., 2008). As ward models either purely or partially offline (Kostrikov also noted in (Arenz & Neumann, 2020), we can express et al., 2020; Jarrett et al., 2020; Arenz & Neumann, 2020). this as a divergence minimization problem: This approach leans on the presence of actions in the demon- strator data. Although FORM’s demonstrator effect model min D [pI (τ )||pD(τ )], (1) KL θ is learned offline, FORM’s online phase is essential to the θ process of distilling an effect model (which doesn’t use where τ = {x , a , x , a , . . . , a , x } is a trajectory 0 0 1 1 T −2 T −1 actions) into a policy (which does). consisting of actions are actions A = {a , . . . a } and 0 T −1 Learning to act from observations Many methods have states X = {x 0, . . . , x T −1}. We use x rather than o (for been proposed for imitation from observation (Torabi et al., observation) or s (for state) because FORM does not assume 2019b), but most methods that do so using IRL are based that its inputs are Markovian – FORM applies to generic around GAIL (Wang et al., 2017; Torabi et al., 2019a; Sun observations – but speaking in terms of states simplifies et al., 2019). Recent work has obtained interesting results the comparison to other methods (like GAIL) that assume using solutions based around tracking or matching trajec- Markovian states are given or inferred. pD(τ ) is the distribu- tories in learned feature spaces (Peng et al., 2018; Merel tion over trajectories induced by the demonstrator’s policy et al., 2019), by matching imitator actions to learned models and the environment dynamics, while pI (τ ) is the corre- θ of expert trajectories (interpreted as inverse models of the sponding distribution induced by an imitator with learnable expert action) (Schmeckpeper et al., 2020; Zhu et al., 2020; parameters θ. Edwards et al., 2018; Pathak et al., 2018), and by learning to In imitation learning from observation, the imitator must rea- match features in learned invariant spaces (Sermanet et al., son about the demonstrator’s behavior without supervised 2017). Finally, we note that much recent work has observed access to the expert’s actions (its control signals). Accord-
Imitation by Predicting Observations ingly, we focus on distributions over observation sequences, pD(x |x ) and (2) the imitator pI (x |x ). We refer t t−1 θ t t−1 which amounts to integrating out the imitator’s actions: to these as effect models to differentiate them from how (cid:90) (cid:90) “model” is used elsewhere in the RL literature to refer to pI (X) = pI (τ ) = pI (A, X) models of transition dynamics (Figure 2). Unlike transi- θ θ θ A A tion models, which are typically action-conditional and are (cid:90) = (cid:89) p(x |x , a )πI (a |x , a ). assumed to model policy-independent transition dynamics, t <t <t θ t−1 <t <t−1 A t≥0 effect models are not conditioned on actions and attempt to capture the effects of policy and environment dynamics. (2) A similar class of models was used to model an expert’s This density reflects both the environment transition dy- behavior in recent work (Rhinehart et al., 2020). namics p(x |x , a ) and the imitator policy π (a |x ), t t−1 t−1 θ t t whose parameters θ we seek to learn. Similarly, we can Algorithm 1 Imitation learning with FORM write the probability of a demonstrator trajectory in terms Input: A fixed dataset D of expert state transitions, a replay of the unobserved expert policy as buffer to fill with imitator data, an environment. (cid:89) Init: Randomly initialize demonstrator effect model pD(X) = pD(x |x ) t <t pD(x | x ), imitator effect model pI (x | x ), ω t t−1 φ t t−1 t≥0 and imitator policy πI (a |x ). = (cid:90) (cid:89) p(x |x , a )πD(a |x , a ). while pD not converged doθ t t t <t <t t−1 <t <t−1 ω A t≥0 # Train demonstrator effect model (3) Sample batch of trajectories from the expert dataset D. Update pD by taking a gradient step (e.g. with Adam) ω Our objective is to minimize the KL-divergence2 between on: these two densities: (cid:20) (cid:21) (cid:88) max E log pD(x | x ) . min D KL[pI θ(X)||pD(X)] ω D ω t t−1 θ t≥0 (cid:20) (cid:21) = min E pI(X) log pI θ(X) − log pD(X) . (4) end θ θ while πI (a |x ) not converged do θ t t # Train imitator effect model and policy Minimizing the divergence corresponds to maximizing the Sample trajectories from the environment using πI and following expression in expectation: θ add them to the replay buffer. ρ = log pD(X) − log pI (X). (5) Sample batch of trajectories from the replay buffer. FORM θ Label the reward of each sampled transition In this work, we propose to imitate by treating ρ as a (x , a , x ) using pD(x | x ) and pI (x | FORM t−1 t−1 t ω t t−1 φ t return and maximizing it using RL. x ): t−1 3.2. Optimizing FORM with effect models r t = log pD ω (x t | x t−1) − log pI φ(x t | x t−1). To see how we will capture this expression, first note that Update πI (a |x ) with a step of a policy improvement θ t t each term of ρ FORM is a log-density over the states encoun- algorithm (e.g. with MPO) using returns computed tered in an episode log p(X), which we can rewrite as from the reward-labeled trajectories (e.g. with Re- (cid:80) log p(x 0) + t>0 log p(x t|x <t) using the chain rule for trace). probability. As the initial state is independent of the policy, Update pI (x | x ) by taking a gradient step (e.g. φ t t−1 we can simplify the expression used in each reward term with Adam) on: (cid:80) to log p(x |x ). This means the return can be ex- t≥0 t t−1 (cid:88) pressed solely in terms of next-step conditional densities. To max E [ log pI (x | x )]. I φ t t−1 simplify the discussion, we present all results from here for- φ t≥0 ward in terms of one-step predictive models log p(x |x ), t t−1 but the FORM derivation and algorithm applies equally well end to generic sequence models log p(x |x ). t <t We propose to learn a reward model by introducing models We wish to maximize this return using standard tools for of the state transition densities under (1) the demonstrator policy optimization. We can do this without introducing 2We use the reverse KL because the policy learns on its own bias only if the policy gradients do not depend on gradients trajectories, as in RL (Levine, 2018). of any term in the reward (which aren’t accounted for by
Imitation by Predicting Observations standard policy optimizers). As we derive in Sec. A of the 3.3. GAIL, occupancy-based imitation, and robustness appendix, this assumption holds, and we can write the policy GAIL and its variants are justified in terms of matching gradient as: the state-action occupancy of an expert – GAIL attempts to (cid:88) ∇ J (πI ) = E [ρ ∇ log πI (a |x )]. (6) unconditionally match the rates at which states and actions θ FORM θ τ∼πI FORM θ θ t t θ are visited – rather than directly matching a policy or its t≥0 effects. In contrast, FORM’s reward is derived directly from Intuitively, the policy gradient does not involve gradients an objective that matches a policy’s effects on a sequence of either pI or pD because neither of these densities are θ (eq. 5). This has consequences for their robustness, as we conditioned on the actions sampled from the policy (in ef- will explain. fect, the contribution of the density to the policy gradient is integrated out). Because the demonstrator effect model is First, note that a policy is a local concept (it describes how to independent of the imitator, we can train it offline on expert map states or observations to actions), while an occupancy demonstrations using a maximum likelihood objective: is a global concept (it describes the rates at which an agent (cid:88) visits states and actions in expectation). To see why the max E [ log pD(x | x )]. (7) pD(X) ω t t−1 occupancy is global, note that the occupancy (Ho & Ermon, ω t≥0 2016; Torabi et al., 2019a) of a state x by a policy π is given i The model of the imitator density log pI θ(X), on the other by ρ π(x i) = (cid:80)∞ t=0 γtp(x t = x i|π), where in general: hand, needs to capture the transition density under the cur- rent policy (it acts as a self-model). Accordingly, we train it (cid:90) by taking stochastic gradient descent steps on the following p(x = x |π) = p(x = x |x , a )p(x , a |π) t i t i <t <t <t <t objective at the same time as the imitator policy is training: {x,a}<t max E [(cid:88) log pI (x | x )]. (8) (10) pI(X) φ t t−1 φ θ t≥0 In other words, to reason about the occupancy of a state is By incorporating both models, we obtain the full FORM to reason about every possible way the policy might arrive policy objective: there. In practice, for GAIL, the discriminator computes max E (cid:20) (cid:88) log pD(x | x ) − log pI (x | x )(cid:21) . a state’s reward by comparing the frequency of x t to the πI(X) ω t t−1 φ t t−1 frequency of all other states that are seen in the data, what- θ θ t≥0 ever the conditions under which that state was produced. (9) Because FORM relies on conditional probabilities and does not depend on long-horizon visitation in its derivation, the Despite the inclusion of two terms with opposite signs, the only relevant states are those that appear under similar con- FORM policy objective is not an adversarial loss: FORM ditions. Essentially, FORM’s reward involves comparisons is based on a KL-minimization objective, rather than an to fewer observations because it takes a state’s context – adversarial minimax objective, and is not formulated as a namely, the transition that produced it – into account. zero-sum game. The second term in the objective can be viewed as an entropy-like expression, similar to the one that We expect this property to mitigate GAIL’s sensitivity to arises in maximum-entropy RL (Levine, 2018). noise. It’s easiest to see why this should happen by compar- ing GAIfO and FORM for two-state inputs. Here, FORM This objective includes both an expectation with respect to maximizes log pD(x |x ) − log pI (x |x ) (each term t t−1 t t−1 the current imitator policy and a term that reflects the current estimated separately by maximum likelihood), while GAIfO imitator effect model. This suggests that this objective is maximizes log pD(xt,xt−1) = log pD(xt|xt−1)pD(xt−1) (the easiest to optimize in an on-policy setting. Nonetheless, we pI(xt,xt−1) pI(xt|xt−1)pI(xt−1) entire log ratio is estimated in one go by a discriminator). find that it can be stably optimized in a moderately off-policy If a feature is present in the imitator data but was never in setting. In all experiments here, we sample transitions from the demonstrator data, then pD(x ) will be close to 0 on a replay buffer, computing rewards as they are consumed. t−1 this data, driving the log ratio to −∞ regardless of the prob- We compute returns using the Retrace algorithm on the raw ability of the transition that follows. The presence of noise rewards (Munos et al., 2016) (which corrects for mildly makes spurious features like this inevitable. This makes it off-policy actions using importance sampling). We optimize difficult for GAIL to focus on the meaningful controllable the policy using the MPO algorithm (Abdolmaleki et al., differences in the data, namely in the transition probabilities 2018). We choose MPO because it is known to perform well p(x |x ). By estimating each term separately (avoiding a in mildly off-policy settings: FORM itself does not make t t−1 discriminator) and including only transition-related terms any MPO-specific assumptions, and we expect it to perform (using a conditional density), FORM reduces the suscepti- well with many other policy optimizers. We describe our bility to sensitivity of this kind. full procedure in Algorithm 1.
Imitation by Predicting Observations Expert BC BCO GAIfO GAIfO+GP VAIfO VAIfO+GP FORM Reacher Easy 974.6 970.3 ± 12.2 966.6 ± 7.9 869.9 ± 48.6 915.9 ± 37.8 861.6 ± 61.4 901.3 ± 30.4 950.2 ± 14.9 Reacher Hard 981.3 892.4 ± 19.1 940.1 ± 3.9 818.7 ± 11.3 783.7 ± 119.7 604.4 ± 426.3 891.0 ± 73.9 957.3 ± 6.1 Cheetah Run 930.5 227.5 ± 37.4 75.7 ± 4.2 607.6 ± 429.6 921.3 ± 6.9 820.0 ± 98.8 918.3 ± 6.4 827.9 ± 31.9 Quadruped Walk 972.4 752.1 ± 37.3 191.9 ± 33.6 672.6 ± 409.8 963.6 ± 4.8 927.8 ± 5.0 945.8 ± 15.5 963.6 ± 2.5 Quadruped Run 962.9 719.2 ± 14.0 271.4 ± 48.4 952.5 ± 7.5 952.3 ± 2.1 926.6 ± 38.3 950.0 ± 2.7 948.5 ± 1.5 Hopper Stand 965.8 534.1 ± 13.2 91.4 ± 8.5 400.0 ± 164.3 748.5 ± 224.1 835.8 ± 103.0 891.2 ± 42.1 815.7 ± 9.2 Hopper Hop 711.5 98.4 ± 4.8 9.1 ± 7.2 689.2 ± 10.0 694.4 ± 0.3 610.5 ± 74.8 683.6 ± 22.3 636.2 ± 38.9 Walker Stand 993.6 731.7 ± 29.7 385.9 ± 27.6 989.4 ± 1.5 985.4 ± 1.6 989.4 ± 0.5 986.0 ± 1.9 985.1 ± 2.6 Walker Walk 983.2 719.5 ± 50.0 61.9 ± 20.7 976.5 ± 2.8 981.6 ± 1.4 971.2 ± 5.6 975.2 ± 1.3 977.8 ± 1.0 Walker Run 952.1 108.5 ± 33.2 39.0 ± 7.8 949.5 ± 5.6 947.6 ± 5.5 949.0 ± 2.6 948.5 ± 2.1 942.0 ± 4.5 Humanoid Stand 905.9 780.7 ± 30.5 9.99 ± 2.51 4.9 ± 1.0 856.2 ± 15.5 257.5 ± 12.4 863.5 ± 7.7 704.6 ± 12.1 Humanoid Walk 809.5 293.9 ± 16.2 9.61 ± 5.73 1.2 ± 0.4 798.4 ± 1.0 658.2 ± 123.6 795.5 ± 3.4 783.0 ± 3.3 Humanoid Run 736.6 54.2 ± 5.1 1.04 ± 0.24 0.6 ± 0.0 683.4 ± 6.9 676.6 ± 25.8 691.6 ± 24.0 691.1 ± 7.8 Table 1. Asymptotic performance on 13 tasks from six DCS domains (mean ± standard deviation across three seeds) of our method (FORM) and baselines Behavioral Cloning from Observations (BCO) (Torabi et al., 2018), GAIL from Observations (GAIfO) (Torabi et al., 2019a), and regularized variants with a tuned gradient penalty (Gulrajani et al., 2017) (GAIfO+GP), a variational discriminator bottleneck (Peng et al., 2019) (VAIfO), or both forms of regularization (VAIfO+GP). Because BC (Pomerleau, 1989) uses expert actions it is not comparable to the other methods, but nevertheless performs poorly on many tasks, even with 1000 demonstrations. FORM performs competitively with well-regularized forms of GAIfO, while generally outperforming BCO and GAIfO. For each task, we highlight the method with best and second best mean performance. Finally, we note that GAIL is typically justified by the obser- Mujoco state representations: these are smaller than e.g. vation that recovering an expert’s occupancy is equivalent image observations, and vary in size from 6- (reacher) to 67- to recovering its policy, but this is only true in Markov De- (humanoid) and 78-dimensional (quadruped). As observed cision Processes (MDPs) (Syed et al., 2008; Ho & Ermon, by (Zolna et al., 2020), GAIL struggles to imitate in the pres- 2016) and not in general. In practice, imitation must often ence of a small number of differences between expert and be done using noisy or high-dimensional observations rather imitator domains. We conduct a similar experiment to char- than ground-truth MDP states, and matching occupancy in acterize the robustness of FORM and GAIfO to irrelevant, these spaces is problematic. In settings like this, relying on but undersampled, factors of variation in the demonstrator the global occupancy induced by a policy rather than on the data. Because the focus of our evaluation concerns robust- immediate effects of a policy may lead to misleading results. ness to distractors, rather than the minimum number of For example, GAIL will attempt to match the occupancy of demonstrations needed for successful imitation, we conduct all noise dimensions, and this is usually possible. In practice, all experiments using 1000 demonstrations, sufficient to this means that the GAIL objective needs to be carefully reg- ensure mostly satisfactory performance in the absence of ularized to avoid overfitting to irrelevant differences. These distractors. effects appear to be stronger when training an IRL agent Expert data. For all domains, we train an expert via RL from replay, as discussed in (Kostrikov et al., 2019), and on the ground truth task reward. Experts are trained to they may be further exacerbated when imitating without convergence using MPO, with the same policy and value actions. In our experiments, GAIfO fails completely on the architecture used for imitation (under all imitation condi- Humanoid tasks of the Control Suite when unregularized. tions). For imitation, we generate a fixed dataset of 1000 Even with strong regularization, GAIfO is very sensitive to demonstration trajectories from each policy, each of which the presence of irrelevant differences between demonstrator depicts a single episode 1000 timesteps in duration (i.e. 106 and imitator domains, as our experiments illustrate. steps total). All imitation methods are trained using the 4. Experiments same demonstrator data. Distractor data. To probe robustness to a domain shift We evaluate FORM against strong baselines on 13 tasks between the expert and imitation domains, we deliberately from six domains from the DeepMind Control Suite (DCS) introduce spurious signals, unrelated to the task or agent (Tassa et al., 2018), a set of benchmarks for continuous con- state, into the state observation vector. During the demon- trol domains, chosen to match those frequently used in the imitation learning literature3. All approaches use internal stration phase, these take the form of binary noise patterns drawn from a fixed set which are appended to the state vec- 3We note that many imitation learning methods are evaluated on tor and held constant for the duration of the episode. During the superficially similar OpenAI Gym Mujoco benchmark (Brock- man et al., 2016), but the Gym domains have essentially deter- suited for evaluating imitation learning methods (see Sec. D of the ministic initial states and other properties that make them poorly Appendix for a discussion).
Imitation by Predicting Observations Walker Stand Walker Walk Walker Run Quadruped Walk 1000 800 FORM, 8 distractors 600 FORM, 16 distractors GAIfO+GP, 8 distractors GAIfO+GP, 16 distractors 400 GAIfO, 8 distractors GAIfO, 16 distractors 200 0 Humanoid Stand Humanoid Walk Humanoid Run Quadruped Run 1000 800 600 400 200 0 103 102 101 100 103 102 101 100 103 102 101 100 103 102 101 100 Distractor pool size nruter rotatimI Figure 3. Performance of FORM, GAIfO+GP, and GAIfO in the presence of distractor features. The distractor pool size M (#unique points sampled in expert data) is varied from 103 down to 1 for both N = 8 and N = 16 distractor dimensions. FORM exhibits greater stability than GAIfO+GP in both settings, maintaining performance down to M = 10. Error bars indicate standard deviation across 3 seeds. For reasons of legibility and space, comparison to all other baselines are given in the appendix. imitation binary noise vectors are also appended, but any the expert data collection and imitation during deployment binary pattern is permitted (i.e. no longer need come from are performed correspond to two distinct distractor patterns the fixed set). The different patterns appended during the that are intermingled with task-relevant portions of the state. expert and imitation phases impose a domain shift between For IL to work in such settings the algorithm must be robust them. to changes in the background distractors. We can see how sensitive a model is to the presence of undersampled factors Formally, during imitation we append the state vector x t of variation by observing how stable its performance is as with a binary pattern b ∼ [0, 1]N to form an augmented ob- the pool size M decreases. servation x˜ = [x ; b], where N is the number of distractor t t dimensions. During demonstration, b ∼ {b , . . . , b }, b ∈ 1 M 4.1. Details of the FORM implementation [0, 1]N , while M controls the the number of distinct patterns, known as the pool size. M and N control the magnitude Architecture. We use simple feedforward architectures to of the domain shift: increasing N makes the task harder parameterize the density models (3 layer MLPs with 256 by reducing the fraction of state that contains signal, while units, and tanh and ELU (Clevert et al., 2016) nonlinear- increasing M makes the task easier by ensuring that all dis- ities). We model the density as a mixture of 4 Gaussian tractor features are present in both demonstrator and imitator components, with the network outputting GMM mixture data. coefficients and the means and standard deviations of each component. We use Gaussians with a diagonal covariance Due to the input normalization procedure (Sec. 4.1), the IL matrix. In all experiments, we clip the standard deviation to agent has no way of distinguishing noise dimensions from a minimum value of 0.0001. We use the same architecture ones carrying state information. Ideally however, it should and same hyperparameters for the imitator and demonstrator learn to ignore the extra dimensions since they are unrelated effect model in each setting. to the task, making it robust to changes in the distractor pattern. Our setup directly parallels situations encountered Effect model training. All demonstrator models were in practice involving undersampled factors of variation. For trained offline for 2 million steps. We standardized effect example, when performing IL using visual inputs with a model inputs using per-dimension means and variances es- robot, the background appearance of the rooms in which timated by by exponential moving average: we found that
Imitation by Predicting Observations this improved generative model training (it did not affect algorithms we evaluate for imitation from observation. GAIfO training). 4.3. Policy architecture Three forms of regularization were used with the demon- strator and imitator generative models: (i) (cid:96) 2 weight-decay, For both IRL methods (FORM and GAIfO), the underlying (ii) training on data generated by agent rollouts, i.e. us- policy is trained with MPO and experience replay. Both the ing the network output at a timestep as the input at the policy and critic networks encode a concatenation of the next during training (a common trick used in the recurrent environment’s observations that has been passed through neural network literature (Bengio et al., 2015)), (iii) pre- a tanh activation. Both encode the observations with inde- diction of observations at multiple future timesteps (Hafner pendent 3-layer MLPs using ELU activations. The policy et al., 2019). See the Appendix for more details. In all network then projects to parameterize the mean and scale experiments, we share the hyperparameter settings of all of a Gaussian action distribution. The critic concatenates regularizers between the demonstrator and imitator effect the sampled action, applies layernorm (Ba et al., 2016) and model (we do not tune them separately). We tuned (cid:96) 2 weight a tanh, and applies another 3-layer MLP to produce the (sweeping values of [0.0, 0.01, 0.1, and 1.0]) and the frac- Q-value. All hidden layers have a width of 256 units. tion of each batch generated by agent rollouts (sweeping values of [0.0, 0.01, 0.1, 1.0]) per domain, but otherwise use 4.4. Results identical hyperparameters for all FORM models. No distractors. In Table 1 we compare FORM to BCO and GAIfO, with various strong forms of regularization on the 4.2. Baselines: GAIfO and BCO DeepMind Control Suite in the absence of distractors. The results are shown alongside the reward obtained by the ex- To ensure DMCS experiments were fair and well-calibrated, pert RL agent. BCO succeeds only on the Reacher domain, we impelemented and tuned a strong GAIfO baseline. The performing poorly on the others. GAIfO in general performs GAIfO discriminator is conditioned on the current obser- well but fails completely on the Humanoid domain. The vation. We found that there was no benefit to conditioning addition of a tuned gradient penalty or the introduction of a on pairs of subsequent observations (see Table 5 in the Ap- variational discriminator bottleneck allows GAIfO to also pendix). This is likely because DCS observations include perform well on the Humanoid domain. FORM achieves velocity observations as well as static positions. The dis- competitive performance to strongly regularized forms of criminator network uses the same architecture as the FORM GAIfO (GAIfO+GP, VAIfO, VAIfO+GP). Despite access to effect models except for the mixture-of-Gaussians head, 1000 demonstrations, no method is able to match expert per- which is replaced by a (scalar) classifier head. We addi- formance on the Humanoid tasks, illustrating the challenge tionally found that there was no benefit in standardizing this domain poses due to the dimensionality of the state- the observations as we do for FORM. For GAIfO+GP, we space and highly variable initial conditions. This number apply a gradient penalty (Gulrajani et al., 2017) to the last of demonstrations (1000) may seem large when compared two layers of the discriminator. For VAIfO (VAIL from to the numbers used in work that uses the Gym benchmark observations), we introduce a variational bottleneck in the (Brockman et al., 2016). But please note that tasks from the discriminator architecture and add a KL-constraint term to Gym benchmark are easier to imitate, requiring almost no the loss, as in (Peng et al., 2019). Following (Kostrikov generalization between demonstrator and imitator due to its et al., 2019), we train both the policy and the discriminator essentially deterministic initialization. using data sampled from a replay buffer. With distractors. We now explore the robustness of the In BCO, an inverse model p(a |x , x ) is trained on imita- t t t+1 different approaches to settings where distractors are present tor trajectories and then used to label the actions on demon- in the observations. Figure 3 compares FORM with GAIfO strator trajectories (Torabi et al., 2018). We train the inverse and GAIfO+GP with N = 8 and N = 16 dimensions of dis- model using the same architecture as the FORM effector tractor features, as the distractor pool size M is varied from models and the GAIfO discriminator, replacing the output 103 down to 1. With N = 8 distractor dimensions, FORM head with a Gaussian distribution (the same class of distri- is consistently able to maintain performance as M = 10, butions used by the RL agent to produce the actions). The by which point the performance of GAIfO+GP has dropped BCO agent is then trained in a supervised fashion on expert significantly. For N = 16 distractor dimensions the degra- trajectories labeled by the inverse model. The BC agent is dation for GAIfO+GP is more severe, even at the easier trained directly on expert trajectories with expert actions. setting of M = 100. In contrast, FORM is still able to Because BC is trained using expert actions, while the other perform well on most tasks (Humanoid Stand being the ex- imitation algorithms we evaluate are not, it is not strictly ception). We compare FORM to all other baselines on this comparable. We include it to calibrate readers to the dif- setting in Figs. 5 and 6 of the appendix, and this trend holds ficulty of these tasks and the relative performance of the generally. We find that GAIfO with a variational bottleneck
Imitation by Predicting Observations Distractor 1 Distractor 2 Figure 4. Left, Middle — subset of observation dimensions (3 internal states (top) and 3 distractor features (bottom)) from a FORM imitation agent on the Walker Run task. Distractor features appear as horizontal lines as they do not vary with time in an episode. True observations, with demonstrator and imitator predictions overlaid. Distractor 1 shows the agent learning with a pattern previously seen in the expert demonstrations. Distractor 2 uses a novel pattern not seen in expert data. The FORM agent behavior and model predictions are qualitatively unchanged, showing robustness to the distractor pattern. Right — reward model traces for both FORM and GAIfO+GP, alongside the ground truth task reward. Top: the log-likelihoods of the demonstrator and imitator components of the FORM reward model, for distractor 1 (top) and distractor 2 (bottom). Bottom: the expert probability output by the GAIfO+GP discriminator for distractor 1 (top) and distractor 2 (bottom). Both FORM and GAIfO+GP agents were trained with with N = 8 distractor dimensions and a pool of M = 10 distractors in expert data. FORM is robust to distractor features in this setting even when its predictions are imperfect, and the imitation agent obtains good reward on the task. In contrast, the behavior of the GAIfO+GP agent depends significantly on the distractors and largely fails at the task. regularizer (VAIfO and VAIfO+GP) performs similarly to els that has been extensively studied and that can be scaled GAIfO, and still exhibits sensitivity to noise. BC and BCO to real-world, noisy data. These properties make FORM generally perform worse than FORM, but BC shows good a good candidate for the development of sophisticated ap- noise resilience on Humanoid Stand in particular. proaches to imitation that can handle high-dimensional data with domain shifts. Figure 4 visualizes the effect on imitation performance when the distractor pattern (M = 10, N = 8) is changed between FORM currently has several limitations. The demonstra- imitation training runs on the Walker Run task. Demonstra- tor model pD must be trained off-line before learning the tor and imitator model predictions from the FORM model imitator model pI and policy πI . This two-stage training show minimal change, with the agent achieving good re- is inefficient in wall clock terms relative to the monolithic ward for both patterns. In contrast, the GAIfO+GP model is training procedure of GAIL. This is compounded by the highly sensitive to the change in distractor pattern and fails difficulty in assessing the quality of pD using training likeli- at the task. Collectively, these results show the fragility of hood alone. In practice, we find that it is a poor predictor GAIL-based IL methods to task-irrelevant features, and also of subsequent imitator performance, necessitating both both illustrate the superior robustness of FORM in this setting. training stages to be performed in order to ascertain if pD was modeled effectively. A second issue is that we currently model proprioceptive state: moving to image pixel-based 5. Discussion inputs will require larger and more complex generative mod- In this work we introduce the Future Observation Reward els, which will likely lead to added difficulties. Model, or FORM, an approach to inverse supervised learning that can be used for imitation from observations Acknowledgements without actions. FORM makes few assumptions about the data being modeled, which makes it a promising approach We are grateful to Josh Abramson, Feryal Behbahani, Fed- for learning behavior from data collected under realistic con- erico Carnevale, Ashley Edwards, Tom Erez, Karol Gregor, ditions. In particular, we show that FORM is competitive Raia Hadsell, Leonard Hasenclever, Nicolas Heess, Alden with GAIL from observations while exhibiting improved Hung, Josh Merel, Nikolay Savinov, Yuval Tassa, Konrad stability in the face of spurious features. FORM imitates Zolna and others at DeepMind for insightful discussions using likelihood-based generative models, a family of mod- and suggestions.
Imitation by Predicting Observations References Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. OpenAI gym. Abbeel, P. and Ng, A. Y. Apprenticeship learning via inverse arXiv preprint arXiv:1606.01540, 2016. supervised learning. In Proceedings of International Conference on Machine Learning (ICML), 2004. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Abbeel, P., Coates, A., and Ng, A. Y. Autonomous Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., helicopter aerobatics through apprenticeship learning. Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., International Journal of Robotics Research, 29(13): Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., 1608–1639, 2010. Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, S., Radford, A., Sutskever, I., and Amodei, D. Language R., Heess, N., and Riedmiller, M. Maximum a posteri- models are few-shot learners. In Proceedings of Neural ori policy optimisation. In Proceedings of International Information Processing Systems (NeurIPS), 2020. Conference on Learning Representations (ICLR), 2018. Byrne, R. W. Animal imitation. Current Biology, 19(3):111 Abramson, J., Ahuja, A., Brussee, A., Carnevale, F., Cassin, – 114, 2009. M., Clark, S., Dudzik, A., Georgiev, P., Guy, A., Harley, Clevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast T., Hill, F., Hung, A., Kenton, Z., Landon, J., Lillicrap, and accurate deep network learning by exponential linear T., Mathewson, K., Muldal, A., Santoro, A., Savinov, units (ELUs). In Proceedings of International Conference N., Varma, V., Wayne, G., Wong, N., Yan, C., and Zhu, on Learning Representations (ICLR), 2016. R. Imitating interactive intelligence. arXiv preprint arXiv:2012.05672, 2020. Dosovitskiy, A. and Koltun, V. Learning to act by predicting the future. In Proceedings of International Conference Arenz, O. and Neumann, G. Non-adversarial imitation on Learning Representations (ICLR), 2017. learning and its connections to adversarial methods. arXiv preprint arXiv:2008.03525, 2020. Edwards, A. D., Sahni, H., Schroecker, Y., and Isbell, C. L. Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein Imitating latent policies from observation. In Proceed- GAN. In Proceedings of International Conference on ings of International Conference on Machine Learning Machine Learning (ICML), 2017. (ICML), 2018. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. Edwards, A. D., Sahni, H., Liu, R., Hung, J., Jain, A., arXiv preprint arXiv:1607.06450, 2016. Wang, R., Ecoffet, A., Miconi, T., Isbell, C., and Yosinski, J. Estimating Q(s,s’) with deep deterministic dynamics Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, gradients. In Proceedings of International Conference on J., Buchlovsky, P., Budden, D., Cai, T., Clark, A., Dani- Machine Learning (ICML), 2020. helka, I., Fantacci, C., Godwin, J., Jones, C., Hennigan, T., Hessel, M., Kapturowski, S., Keck, T., Kemaev, I., Fu, J., Luo, K., and Levine, S. Learning robust rewards with King, M., Martens, L., Mikulik, V., Norman, T., Quan, adversarial inverse supervised learning. In Proceed- J., Papamakarios, G., Ring, R., Ruiz, F., Sanchez, A., ings of International Conference on Learning Represen- Schneider, R., Sezener, E., Spencer, S., Srinivasan, S., tations (ICLR), 2018. Stokowiec, W., and Viola, F. The DeepMind JAX Ecosys- Ghasemipour, S. K. S., Zemel, R., and Gu, S. A divergence tem, 2020. URL http://github.com/deepmind. minimization perspective on imitation learning methods. Baram, N., Anschel, O., and Mannor, S. Model-based In Conference on Robotic Learning (CoRL), 2019. adversarial imitation learning. In Proceedings of Neural Gulcehre, C., Paine, T. L., Shahriari, B., Denil, M., Hoffman, Information Processing Systems (NeurIPS), 2016. M., Soyer, H., Tanburn, R., Kapturowski, S., Rabinowitz, Bengio, S., Vinyals, O., Jaitly, N., and Shazeer, N. M. Sched- N., Williams, D., Barth-Maron, G., Wang, Z., de Freitas, uled sampling for sequence prediction with recurrent neu- N., and Team, W. Making efficient use of demonstrations ral networks. In Proceedings of Neural Information Pro- to solve hard exploration problems. In Proceedings of cessing Systems (NeurIPS), 2015. International Conference on Learning Representations (ICLR), 2020. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Wanderman-Milne, S., and Zhang, Q. JAX: composable Courville, A. C. Improved training of wasserstein GANs. transformations of Python+NumPy programs, 2018. URL In Proceedings of Neural Information Processing Systems http://github.com/google/jax. (NeurIPS), 2017.
Imitation by Predicting Observations Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Merel, J., Hasenclever, L., Galashov, A., Ahuja, A., Pham, Lee, H., and Davidson, J. Learning latent dynamics for V., Wayne, G., Teh, Y. W., and Heess, N. Neural prob- planning from pixels. In Proceedings of International abilistic motor primitives for humanoid control. In Pro- Conference on Machine Learning (ICML), 2019. ceedings of International Conference on Learning Repre- sentations (ICLR), 2019. Ho, J. and Ermon, S. Generative adversarial imitation learn- ing. In Proceedings of Neural Information Processing Mescheder, L., Nowozin, S., and Geiger, A. The numerics of Systems (NeurIPS), 2016. GANs. In Proceedings of Neural Information Processing Systems (NeurIPS), 2017. Huber, L., Range, F., Voelkl, B., Szucsich, A., Virányi, Z., Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, and Miklosi, A. The evolution of imitation: what do M. G. Safe and efficient off-policy supervised learning. the capacities of non-human animals tell us about the In Proceedings of Neural Information Processing Systems mechanisms of imitation? Philosophical Transactions of (NeurIPS), 2016. the Royal Society of London B, 364(1528):2299 – 2309, 2009. Ng, A. and Russell, S. Algorithms for inverse supervised learning. In Proceedings of International Conference on Jarrett, D., Bica, I., and van der Schaar, M. Strictly batch Machine Learning (ICML), 2000. imitation learning by energy-based distribution matching. In Proceedings of Neural Information Processing Systems OpenAI, Berner, C., Brockman, G., Chan, B., Cheung, V., (NeurIPS), 2020. De˛biak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R., Gray, S., Olsson, C., Pa- Kingma, D. P. and Ba, J. Adam: A method for stochastic chocki, J., Petrov, M., de Oliveira Pinto, H. P., Raiman, optimization. In Proceedings of International Conference J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., on Learning Representations (ICLR), 2014. Sutskever, I., Tang, J., Wolski, F., and Zhang, S. Dota 2 with large scale deep supervised learning. arXiv Kostrikov, I., Agrawal, K. K., Dwibedi, D., Levine, S., and preprint arXiv:1910.07113, 2019. Tompson, J. Discriminator-actor-critic: Addressing sam- ple inefficiency and reward bias in adversarial imitation Osa, T., Pajarinen, J., Neumann, G., Bagnell, J. A., Abbeell, learning. In Proceedings of International Conference on P., and Peters, J. An algorithmic perspective on imitation Learning Representations (ICLR), 2019. learning. Foundations and Trends in Robotics, 7(1–2): 1–179, 2018. Kostrikov, I., Nachum, O., and Tompson, J. Imitation learn- Pathak, D., Mahmoudieh, P., Luo, G., Agrawal, P., Chen, ing via off-policy distribution matching. In Proceedings D., Shentu, Y., Shelhamer, E., Malik, J., Efros, A. A., and of International Conference on Learning Representations Darrell, T. Zero-shot visual imitation. In Proceedings of (ICLR), 2020. International Conference on Learning Representations Laland, K. Animal cultures. Current Biology, 18(9):366 – (ICLR), 2018. 370, 2008. Peng, X. B., Abbeel, P., Levine, S., and van de Panne, M. Deepmimic: Example-guided deep reinforcement learn- Levine, S. supervised learning and control as proba- ing of physics-based character skills. ACM Transactions bilistic inference: Tutorial and review. arXiv preprint on Graphics, 37(4):143:1–143:14, 2018. arXiv:1805.00909, 2018. Peng, X. B., Kanazawa, A., Toyer, S., Abbeel, P., and Liu, M., He, T., Xu, M., and Zhang, W. Energy-based Levine, S. Variational discriminator bottleneck: Im- imitation learning. In Proceedings of the International proving imitation learning, inverse RL, and GANs by Conference on Autonomous Agents and Multiagent Sys- constraining information flow. In Proceedings of Interna- tems (AAMAS), 2021. tional Conference on Learning Representations (ICLR), 2019. Mania, H., Guy, A., and Recht, B. Simple random search of static linear policies is competitive for reinforcement Pomerleau, D. A. ALVINN: An autonomous land vehicle in learning. In Proceedings of Neural Information Process- a neural network. In Proceedings of Neural Information ing Systems (NeurIPS), 2018. Processing Systems (NeurIPS), 1989. Merel, J., Tassa, Y., TB, D., Srinivasan, S., Lemmon, J., Rhinehart, N., McAllister, R., and Levine, S. Deep imitative Wang, Z., Wayne, G., and Heess, N. Learning human models for flexible inference, planning, and control. In behaviors from motion capture by adversarial imitation. Proceedings of International Conference on Learning arXiv preprint arXiv:1707.02201, 2017. Representations (ICLR), 2020.
Imitation by Predicting Observations Ross, S., Gordon, G. J., and Bagnell, J. A. A reduction of Torabi, F., Warnell, G., and Stone, P. Generative adversarial imitation learning and structured prediction to no-regret imitation from observation. In Imitation, Intent, and online learning. In International Conference on Artificial Interaction (I3) (ICML Workshop), 2019a. Intelligence and Statistics (AISTATS), 2011. Torabi, F., Warnell, G., and Stone, P. Recent advances in Rybkin, O., Pertsch, K., Derpanis, K. G., Daniilidis, K., imitation learning from observation. In Proceedings of and Jaegle, A. Learning what you can do before doing International Joint Conference on Artificial Intelligence, anything. In Proceedings of International Conference on 2019b. Learning Representations (ICLR), 2019. Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Schmeckpeper, K., Xie, A., Rybkin, O., Tian, S., Daniilidis, Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., K., Levine, S., and Finn, C. Learning predictive mod- Georgie, P., Oh, J., Horgan, D., Kroiss, M., Danihelka, I., els from observation and interaction. In Proceedings of Huang, A., Sifre, L., Cai, T., Agapiou, J. P., Jaderberg, M., European Conference on Computer Vision (ECCV), 2020. Vezhnevets, A. S., Leblond, R., Pohlen, T., Dalibard, V., Budden, D., Sulsky, Y., Molloy, J., Paine, T. L., Gulcehre, Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., C., Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, Schaal, S., and Levine, S. Time-contrastive networks: D., Wünsch, D., McKinney, K., Smith, O., Schaul, T., Self-supervised learning from video. In Proceedings of Lillicrap, T., Kavukcuoglu, K., Hassabis, D., Apps, C., IEEE International Conference on Robotics and Automa- and Silver, D. Grandmaster level in StarCraft II using tion, 2017. multi-agent supervised learning. Nature, 575(7782): Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, 350––354, 2019. L., Driessche, G. v. d., Schrittwieser, J., Antonoglou, I., Wang, Z., Merel, J., Reed, S., Wayne, G., de Freitas, N., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, and Heess, N. Robust imitation of diverse behaviors. In D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Proceedings of Neural Information Processing Systems Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, (NeurIPS), 2017. D. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484—-489, 2016. Yu, X., Lyu, Y., and Tsang, I. W. Intrinsic reward driven im- Stone, A., Ramirez, O., Konolige, K., and Jonschkowski, R. itation learning via generative model. In Proceedings of The distracting control suite – a challenging benchmark International Conference on Machine Learning (ICML), for supervised learning from pixels. arXiv preprint 2020. 2101.02722, 2021. Zhu, Z., Lin, K., Dai, B., and Zhou, J. Off-policy imitation Sun, W., Vemula, A., Boots, B., and Bagnell, J. A. Prov- learning from observations. In Proceedings of Neural ably efficient imitation learning from observation alone. Information Processing Systems (NeurIPS), 2020. In Proceedings of International Conference on Machine Ziebart, B. D. Modeling Purposeful Adaptive Behavior with Learning (ICML), 2019. the Principle of Maximum Causal Entropy. PhD thesis, Sutton, R. S. and Barto, A. G. supervised learning: An Carnegie Mellon University, 2010. Introduction. The MIT Press, second edition, 2018. Ziebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. Syed, U., Bowling, M., and Schapire, R. E. Apprenticeship Maximum entropy inverse supervised learning. In Pro- learning using linear programming. In Proceedings of ceedings of AAAI Conference on Artificial Intelligence, International Conference on Machine Learning (ICML), 2008. 2008. Zolna, K., Reed, S., Novikov, A., Colmenarejo, S. G., Bud- Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., den, D., Cabi, S., Denil, M., de Freitas, N., and Wang, Z. de Las Casas, D., Budden, D., Abdolmaleki, A., Merel, J., Task-relevant adversarial imitation learning. In Confer- Lefrancq, A., Lillicrap, T., and Riedmiller, M. DeepMind ence on Robotic Learning (CoRL), 2020. control suite. arXiv preprint arXiv:1801.00690, 2018. Tomasello, M. Do apes ape? In Heyes, C. M. and Jr., B. G. G. (eds.), Social Learning in Animals: The Roots of Culture, chapter 15, pp. 319–346. Academic Press, 1996. Torabi, F., Warnell, G., and Stone, P. Behavioral cloning from observation. In Proceedings of International Joint Conference on Artificial Intelligence, 2018.
Imitation by Predicting Observations Appendices A. Derivation of the FORM policy gradient In this section, we derive the following result (given in Sec. 3.2 of the main text): (cid:20) (cid:21) (cid:88) ∇ J (πI ) = E ρ ∇ log πI (a |x ) . (11) θ FORM θ τ∼πI FORM θ θ t t θ t≥0 This result is useful because it establishes that the gradient of the imitator policy’s parameters does not depend on gradients of either the imitator or demonstrator components of the FORM reward. Because of this, we can safely learn these models in parallel to policy optimization without introducing any bias: this is at the heart of the FORM algorithm. As in the main text, we present our result in terms of models of the form p(x |x , a ) and policies of the form π(a |x ), but the results t t−1 t−1 t t hold without loss of generality to models and policies that depend on states and actions arbitrarily far back into the past. First, note that for a trajectory τ = (x , a , x , a , ..., x , a ) produced by a policy with parameters θ: 0 0 1 1 T −1 T −1 (cid:88) ∇ p (τ ) = p (τ )∇ log πI (a |x ). (12) θ θ θ θ θ t t t≥0 This result can be shown by decomposing τ into causal conditional probabilities for state and action (see e.g. Ziebart 2010): (cid:20) (cid:21) (cid:89) ∇ p (τ ) = ∇ p(x |x , a )π (a |x ) θ θ θ t t−1 t−1 θ t t t≥0 (cid:18) (cid:19) (cid:18) (cid:19) (cid:89) (cid:89) (cid:89) (cid:89) = p(x |x , a )∇ π (a |x ) + π (a |x )∇ p(x |x , a ) t t−1 t−1 θ θ t t θ t t θ t t−1 t−1 t≥0 t≥0 t≥0 t≥0 (cid:18) (cid:19) (cid:89) (cid:89) = p(x |x , a )∇ π (a |x ) t t−1 t−1 θ θ t t t≥0 t≥0 (cid:18) (cid:19) = p θ(τ ) ∇ (cid:89) π (a |x ) (cid:81) π (a |x ) θ θ t t t≥0 θ t t t≥0 (cid:89) = p (τ )∇ log π (a |x ) θ θ θ t t t≥0 (cid:88) = p (τ )∇ log π (a |x ). θ θ θ t t t≥0 With this identity, we can now show the main result. First, we write the policy gradient in terms of the demonstrator and imitator components of the return: (cid:20) (cid:21) ∇ J (πI ) = ∇ E log pD(X) − log pI (X) (13) θ θ θ τ∼πI θ θ For the first component, the results follows directly from the identity in equation (12) and the log-derivative trick:
Imitation by Predicting Observations (cid:90) ∇ E log pD(X) = ∇ pI (A, X) log pD(X) θ τ∼πI θ θ θ X,A (cid:90) (cid:18) (cid:19) = ∇ pI (A, X) log pD(X) θ θ X,A (cid:90) = pI (A, X)∇ log pD(X) + log pD(X)∇ pI (A, X) θ θ θ θ X,A (cid:90) = log pD(X)∇ pI (A, X) θ θ X,A (cid:90) (cid:88) = log pD(X)pI (A, X)∇ log πI (a |x ) θ θ θ t t X,A t≥0 (cid:20) (cid:21) (cid:88) = E log pD(X)∇ log πI (a |x ) (14) τ∼πI θ θ t t θ t≥0 The derivation of the gradient for the second component follows a similar pattern: (cid:90) ∇ E log pI (X) = ∇ pI (A, X) log pI (X) θ τ∼πI θ θ θ θ θ X,A (cid:90) (cid:18) (cid:19) = ∇ pI (A, X) log pI (X) θ θ θ X,A (cid:90) (cid:18) (cid:19) (cid:18) (cid:19) = pI (A, X)∇ log pI (X) + log pI (X)∇ pI (A, X) θ θ θ θ θ θ X,A (cid:90) (cid:18) (cid:19) (cid:90) (cid:18) (cid:19) = pI (X)∇ log pI (X) + log pI (X)∇ pI (A, X) θ θ θ θ θ θ X,A X,A The first of these two integrals vanishes: (cid:90) (cid:18) (cid:19) (cid:90) (cid:18) (cid:19) pI (X)∇ log pI (X) = pI (X)∇ log pI (X) θ θ θ θ θ θ X,A X (cid:90) ∇ pI (X) = pI (X) θ θ θ pI (X) X θ (cid:90) = ∇ pI (X) θ θ X (cid:90) = ∇ pI (X) = ∇ 1 = 0, θ θ θ X leaving: (cid:90) (cid:18) (cid:19) (cid:20) (cid:21) (cid:88) log pI (X)∇ pI (A, X) = E log pI (X)∇ log πI (a |x ) , (15) θ θ θ τ∼πI θ θ t t X,A θ t≥0 by our identity. By stitching the expressions in equations (14) and (15) back into equation (13), we get the final result:
Imitation by Predicting Observations (cid:20) (cid:21) ∇ J (πI ) = ∇ E log pD(X) − log pI (X) θ θ θ τ∼πI θ θ (cid:20)(cid:18) (cid:19) (cid:21) (cid:88) = E log pD(X) − log pI (X) ∇ log πI (a |x ) τ∼πI θ θ θ t t θ t≥0 (cid:20) (cid:21) (cid:88) = E ρ ∇ log πI (a |x ) . τ∼πI FORM θ θ t t θ t≥0 B. Additional results B.1. Additional distractor baselines We include comparisons to all baseline methods on the distractor experiments. All experiments are conducted identically to the experiment described in Section 4.4 of the main paper. We include FORM in each plot for ease of comparison. Plots depict FORM vs. BC vs. BCO (Fig. 5) and FORM vs. VAIfO vs. VAIfO+GP (Fig. 6). B.2. Analysis of FORM regularization hyperparameters We conducted several additional experiments to estimate the effect of FORM’s regularization on the results presented in the main paper, including the effect of autoregressive noise weight on FORM performance (Tab. 2), the effect of maximum overshooting offset on FORM performance (Tab. 3), and the effect of L2 regularization (Tab. 4). Each of these contributes a small amount overall, but we found that moderate regularization was important to produce demonstrator models that could effectively guide imitation. We suspect moderate regularization is necessary to prevents the demonstrator log likelihood log pD(x |x ) from going to −∞ on transitions not present in the demonstrations. If that were to occur, imitation with a t t−1 learned model on new data would be very difficult. B.3. GAIfO: single vs. two timestep models All results presented for GAIfO in the main paper input a single timestep to the discriminator. We made this choice as GAIfO is more susceptible to overfitting when more timesteps are presented as input (for an intuition for why this may happen, see Section 3.3 in the main paper). The results of a comparison between GAIfO and GAIfO+GP in the single or two-frame setting are shown in Table 5. In early experiments, we observed even more dramatic overfitting when using contexts of length greater than 2. C. Experimental Details All experiments were conducted in JAX (Bradbury et al., 2018) using tools from the DeepMind JAX ecosystem (Babuschkin et al., 2020). Below we list details of our experimental setup not included in the main paper for reasons of space. C.1. Distributed Training We train all experiments in a distributed manner: 1 GPU learner updates its parameters from a batch of 64 rollouts pulled from experience replay. Each rollout is 100 timesteps, and the replay buffer stores a maximum of 10,000 rollouts at any time. 50 actors running on CPU execute the environment and push rollouts to the replay buffer. To simplify the implementation, we use the same setup to process offline trajectories (demonstrations) as well as online trajectories (imitator experience). For BC, we keep a single replay buffer and all actors sample a recorded episode and push rollouts to it. For GAIfO and BCO, we keep two replay buffers, one set for the live environment and one set for the demonstration trajectories, and use separate sets of actors to push rollouts to each replay buffer. C.2. Network Architectures The agents we describe below use a shared architecture to encode observations. The observation encoder: • flattens and concatenates its inputs,
Imitation by Predicting Observations Walker Stand Walker Walk Walker Run Quadruped Walk 1000 800 600 400 FORM, 8 distractors FORM, 16 distractors VAIfO+GP, 8 distractors 200 VAIfO+GP, 16 distractors VAIfO, 8 distractors VAIfO, 16 distractors 0 Humanoid Stand Humanoid Walk Humanoid Run Quadruped Run 1000 800 600 400 200 0 103 102 101 100 103 102 101 100 103 102 101 100 103 102 101 100 Distractor pool size nruter rotatimI Figure 5. Performance of FORM compared to VAIfO (GAIfO with a variational discriminator bottleneck) and VAIfO+GP in the presence of distractor features. • linearly projects this observation vector to 256 dimensions, • (optionally) applies layer norm, • activates using a tanh nonlinearity, and • (optionally) further encodes with some number of fully-connected hidden layers of size 256, • activates using an ELU nonlinearity. C.3. MPO We train all policies using MPO (Abdolmaleki et al., 2018). This holds for both expert (demonstrator) policies and for the online policy optimization components of both IRL methods (FORM and GAIfO). Our MPO agent uses independent policy and critic networks. Both policy and critic networks use an observation encoder with layer norm and one additional hidden layer. The policy linearly projects the encoded observation to parameterize the mean and scale of a Gaussian action distribution; we ensure that the scale doesn’t collapse by transforming it with softplus and adding a minimum value: the scale output is given by log(1 + exp σ) + 10−4. The critic concatenates the encoded observation with the sampled action (activated with tanh), linearly projects to 256, applies layer norm and tanh again, then further encodes this with a 3-layer MLP with 256-width hidden units ELU activations to produce a (scalar) value output. To improve stability, we use separate target networks for MPO’s policy and critic. We update target networks every 200 gradient updates. MPO uses samples from its Q-function to compare actions at a particular state: we use 20 samples to make each estimate. The critic is then trained using episodic returns computed using Retrace with λ = 1 and a discount factor of 0.99. MPO uses independent KL terms to constrain the mean and the scale of the policy: we use constraint weights (cid:15) = 0.005 and (cid:15) = 0.00001 for the two terms, and a shared temperature of (cid:15) = 0.1 mean scale temp We optimize both policy and critic with Adam and a fixed learning rate of 10−4; we update the temperature and mean/scale duals with Adam and a fixed learning rate of 10−3.
Imitation by Predicting Observations Walker Stand Walker Walk Walker Run Quadruped Walk 1000 800 600 400 FORM, 8 distractors FORM, 16 distractors BC, 8 distractors 200 BC, 16 distractors BCO, 8 distractors BCO, 16 distractors 0 Humanoid Stand Humanoid Walk Humanoid Run Quadruped Run 1000 800 600 400 200 0 103 102 101 100 103 102 101 100 103 102 101 100 103 102 101 100 Distractor pool size nruter rotatimI Figure 6. Performance of FORM compared to BC and BCO in the presence of distractor features. C.4. FORM effect model training We train two next-step effect models in the FORM agent: one offline on the demonstrations and one online on the live environment and current policy. We found that regularizing the generative models was important to produce good imitation. As described in the main text, we used three simple forms of regularization: (i) L2 regularization, (ii) training on data generated by agent rollouts, i.e. using the network output at a timestep as the input at the next during training , (iii) a form of observation overshooting, i.e. predicting the observations at multiple future timesteps. Rather than overshooting autoregressively, we pass an additional label to the output head, indicating which offset δ should be predicted. Given input at time t, the network is trained to predict the observation at time t + δ. We train the model to predict all offsets in [1, 5]. Overshooting is used only during training of the demonstrator and imitator generative models: the reward term reflects the log-likelihood of only the next step. We also use standardization to ensure the input observations are well-conditioned, as the range of observations varies considerably from task to task on the Control Suite. The use of standardization also prevents imitators from exploiting the structure of the input data to produce misleadingly good results. For example: the observation with largest magnitude on Cheetah Run is the forward velocity, which corresponds almost perfectly to the underlying task reward. An agent that mimics only the forward velocity (while ignoring all other signals) or that maximizes all signals (without imitating) will perform well on the task if the raw observations are used. Results from a strategy like this are misleading in the sense that they may perform well on Cheetah Run because of the design of the observations, not because a general imitation strategy has been learned. Standardization ensures that the mean and standard deviation of all signals are roughly constant, preventing agents from exploiting signals like this. We use standardization for both FORM and GAIL in all experiments in the main paper. We estimate the mean and standard deviation used for standardization by maintain an exponentially decaying running estimate of these quantities for each dimension in the observation (with a decay of 0.99 per batch of unrolls). Including training on model rollouts, overshooting, and standardization, the models maximize 1 (cid:80)δmax=5 log p(σ(x ) | 5 δ=1 t+δ σ(x )) + β log p(σ(x ) | σ(x˜ )), where σ is the standardization operator. The first term here is the maximum likelihood, t ar t+1 t and the second term is the autoregressive regularizer, which conditions on the model’s own output x˜ ∼ p(· | x ). β t t−1 ar was tuned per environment with a grid search over [0.01, 0.1, 1] (see Appendix Table 2 for the results of the sweep on two representative domains). We always used overshooting of 5 in the experiments in the paper, as this value generally produced
Imitation by Predicting Observations β = 0.01 β = 0.1 β = 1.0 ar ar ar Humanoid run 676.02 ± 8.8 668.4 ± 19.4 694.0 ± 10.8 Quadruped run 952.7 ± 3.6 945.4 ± 2.4 955.5 ± 0.4 Table 2. Effect of autoregressive noise weight β on imitator return. ar δ = 1 δ = 3 δ = 5 max max max Humanoid run 266.1 ± 190.5 645.9 ± 32.1 615.8 ± 17.4 Quadruped run 920.57 ± 11.6 920.15 ± 1.2 921.52 ± 9.8 Table 3. Effect of maximum overshooting offset (δ ) on imitator return. All experiments in the paper use δ = 5. max max good results in early experiments (see Appendix Table 3 for ablation results on two representative domains). We found in early experiments that regularization as a whole helped prevent model overfitting (as measured on held-out demonstrator data) and generally led to more stable imitation. At each timestep, observations are standardized and encoded as described above. To compute log p(σ(x ) | σ(x )) for a t+δ t given offset δ, we concatenate a one-hot encoding of δ to the encoding of σ(x ), then process this with an additional hidden t layer of width 256 and linearly project each predicted observation to parameterize a four-component diagonal Gaussian mixture. The scale term of each Gaussian is transformed with softplus to ensure it is non-negative and is added to a small bias term of 10−4 to avoid degeneracy. The models are trained with Adam and a fixed learning rate of 10−4, with an additional L regularizer whose coefficient 2 was tuned per-environment by a grid search over [0.01, 0.1, 1] (see Appendix Table 4 for the results of the sweep on two representative domains). C.5. GAIfO The discriminator network encodes the current observation (as described above), without layer norm or extra hidden layers, and then applies a two-layer MLP decoder with hidden a width of 256 units to produce the discriminator log odds. As in FORM, we standardize the input observations. We tried training GAIfO using single and two frame input: we generally found better performance using single frames, notably on the humanoid tasks (see Appendix Table 5). GAIfO required regularization to perform adequately on many tasks. We used a gradient penalty on the decoder MLP. The final objective that we maximized for the discriminator was log p(expert | τ ) + log p(imitator | τ ) + expert imitator β |∇decoder(interpolate(τ , τ ))| /256 where β is tuned per environment (typically 10) and interpolate is a gp expert imitator 2 gp function which mixes the encodings of the expert and imitator observations with random weights sampled each update. We generally observed worse performance when using two frames than one frame without the gradient penalty (Appendix Table 5). When using a variational bottleneck, we add an additional hard KL constraint loss term, using a learned weighting term α that is optimized via gradient ascent to keep the bound hard. The value of the constraint itself is set via a hyperparameter (cid:15): we swept the value of this constraint in [0, 0.01, 0.1, 1.0, 10.0] and generally obtained best results using a value of 1.0. We report results using (cid:15) = 1.0 throughout. The discriminator is trained with Adam and a fixed learning rate of 10−4. Its output is used as the intrinsic reward by the underlying RL agent at each timestep after applying a softplus transform: log(1 + p(expert | x )). t weight = 0.01 weight = 0.1 weight = 1.0 Humanoid run 682.9 ± 16.8 694.0 ± 10.8 602.5 ± 74.3 Quadruped run 945.4 ± 2.4 953.0 ± 2.8 918.4 ± 0.7 Table 4. Effect of L2 regularization weight on imitator return.
Imitation by Predicting Observations GAIfO (1 frame) GAIfO (2 frames) GAIfO+GP (1 frame) GAIfO+GP (2 frames) Reacher Easy 869.9 ± 48.6 916.1 ± 54.0 915.9 ± 37.8 922.9 ± 15.5 Reacher Hard 818.7 ± 11.3 779.0 ± 44.2 783.7 ± 119.7 837.4 ± 23.5 Cheetah Run 607.6 ± 429.6 5.5 ± 3.4 921.3 ± 6.9 920.4 ± 6.3 Quadruped Walk 672.6 ± 409.8 125.0 ± 69.0 963.6 ± 4.8 966.2 ± 4.2 Quadruped Run 952.5 ± 7.5 168.0 ± 25.9 952.3 ± 2.1 951.0 ± 4.6 Hopper Stand 400.0 ± 164.3 324.3 ± 42.7 748.5 ± 224.1 947.7 ± 7.5 Hopper Hop 689.2 ± 10.0 683.1 ± 18.6 694.4 ± 0.3 708.9 ± 7.2 Walker Stand 989.4 ± 1.5 990.3 ± 1.5 985.4 ± 1.6 989.1 ± 0.8 Walker Walk 976.5 ± 2.8 982.6 ± 0.9 981.6 ± 1.4 977.7 ± 1.0 Walker Run 949.5 ± 5.6 953.7 ± 1.3 947.6 ± 5.5 945.4 ± 6.2 Humanoid Stand 4.9 ± 1.0 5.2 ± 0.4 856.2 ± 15.5 697.4 ± 167.8 Humanoid Walk 1.2 ± 0.4 1.4 ± 0.4 798.4 ± 1.0 792.0 ± 9.4 Humanoid Run 0.6 ± 0.0 0.7 ± 0.1 683.4 ± 6.9 676.9 ± 17.5 Table 5. GAIfO results when conditioned on one or two frames. We report one frame results in the main table, as this setting was generally stabler and produced the best overall results on the humanoid tasks. C.6. BC and BCO In behavioral cloning, we train a Gaussian policy parameterized by a 3-layer MLP. This is the same architecture used for the policy of all other imitation methods. The policy is trained via maximum likelihood to predict the expert actions on trajectories sampled from the recorded demonstrations. For BCO, we additionally train an inverse model. The inverse model is trained on environment transitions from the learned the policy. The inverse model is then used to predict actions on the expert trajectories, and the policy is updated via the BC objective. Both the inverse model and policy are updated with Adam (Kingma & Ba, 2014) and a fixed learning rate of 10−4. D. Gym and Control Suite as Imitation Benchmark Domains Here, we evaluate methods on the DeepMind Control Suite, but many imitation learning methods are evaluated on the OpenAI Gym Mujoco benchmark (Brockman et al., 2016). The Control Suite sidesteps two limitations of evaluation on the Gym, which are not always acknowledged in the imitation learning literature, and which make it hard to interpret results. First, Gym tasks include early termination conditions. For example, Gym’s Humanoid task terminates when the agent’s head falls below a certain height. Early termination can be helpful for speeding up agent training, but in the context of IRL it introduces a confound: an agent may learn the task by modelling and maximizing the expert’s reward function or by making the episode last as long as possible. Any IRL algorithm that produces strictly positive rewards or is otherwise biased to produce longer episodes, can perform well on these tasks while ignoring the expert. GAIL and GAIfO with a softplus discriminator nonlinearity, a typical choice on continuous control domains, fall into this category (Kostrikov et al., 2019). In contrast, episodes on the Control Suite have a fixed duration of 1000 timesteps. Second, all Gym domains use very stereotyped initial state distributions: agents are initialized in a single, stable configuration plus a small amount of noise. This means that there is essentially no variation between the configurations seen in the expert demonstrations and on evaluation episodes, which means that even methods that are known to generalize poorly to configurations not seen in the expert data (such as BC (Ross et al., 2011)) can produce good results on apparently held-out data. In contrast, initial states in the Control Suite are sampled uniformly over the whole configuration space, resulting in a fairly large variation between episodes, especially towards the beginning of episodes. Similar limitations of the Gym control suite environment as a benchmark for control and supervised learning are discussed in (Mania et al., 2018).
