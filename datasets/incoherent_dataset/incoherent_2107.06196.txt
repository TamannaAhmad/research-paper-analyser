No Regrets for Learning the Prior in Bandits Soumya Basu Branislav Kveton Manzil Zaheer Csaba Szepesvári Google Google Research Google Research DeepMind / University of Alberta Abstract We propose AdaTS, a Thompson sampling algorithm that adapts sequentially to bandit tasks that it interacts with. The key idea in AdaTS is to adapt to an unknown task prior distribution by maintaining a distribution over its parameters. When solving a bandit task, that uncertainty is marginalized out and properly accounted for. AdaTS is a fully-Bayesian algorithm that can be implemented efficiently in several classes of bandit problems. We derive upper bounds on its Bayes regret that quantify the loss due to not knowing the task prior, and show that it is small. Our theory is supported by experiments, where AdaTS outperforms prior algorithms and works well even in challenging real-world problems. 1 Introduction We study the problem of maximizing the total reward, or minimizing the total regret, in a sequence of stochastic bandit instances [29, 4, 31]. We consider a Bayesian version of the problem, where the bandit instances are drawn from some distribution. More specifically, the learning agent interacts with m bandit instances in m tasks, with one instance per task. The interaction with each task is for n rounds and with K arms. The reward distribution of arm i ∈ [K] in task s ∈ [m] is p (·; θ ), where i s,∗ θ is a shared parameter of all arms in task s. When arm i is pulled in task s, the agent receives s,∗ a random reward from p (·; θ ). The parameters θ , . . . , θ are drawn independently of each i s,∗ 1,∗ m,∗ other from a task prior P (·; µ ). The task prior is parameterized by an unknown meta-parameter ∗ µ , which is drawn from a meta-prior Q. The agent does not know µ or θ , . . . , θ . However, it ∗ ∗ 1,∗ m,∗ knows Q and the parametric forms of all distributions, which help it to learn about µ . This is a form ∗ of meta-learning [39, 40, 7, 8], where the agent learns to act from interactions with bandit instances. A simple approach is to ignore the hierarchical structure of the problem and solve each bandit task independently with some bandit algorithm, such as Thompson sampling (TS) [38, 11, 2, 37]. This may be highly suboptimal. To illustrate this, imagine that arm 1 is optimal for any θ in the support of P (·; µ ). If µ was known, any reasonable algorithm would only pull arm 1 and have zero regret over ∗ ∗ any horizon. Likewise, a clever algorithm that learns µ should eventually pull arm 1 most of the ∗ time, and thus have diminishing regret as it interacts with a growing number of tasks. Two challenges arise when designing the clever algorithm. First, can it be computationally efficient? Second, what is the regret due to adapting to µ ? ∗ We make the following contributions. First, we propose a Thompson sampling algorithm for our problem, which we call AdaTS. AdaTS maintains a distribution over the meta-parameter µ , which ∗ concentrates over time and is marginalized out when interacting with individual bandit instances. Second, we propose computationally-efficient implementations of AdaTS for multi-armed bandits [29, 4], linear bandits [14, 1], and combinatorial semi-bandits [19, 13, 26]. These implementations are for specific reward distributions and conjugate task priors. Third, we bound the n-round Bayes regret of AdaTS in linear bandits and semi-bandits, and multi-armed bandits as a special case. The Bayes regret is defined by taking an expectation over all random quantities, including µ ∼ Q. Our ∗ bounds show that not knowing µ has a minimal impact on the regret as the number of tasks grows, √ ∗ √ of only O˜( mn). This is in a sharp contrast to prior work [28], where this is O˜( mn2). Finally, 2202 beF 52 ]GL.sc[ 2v69160.7012:viXra
our experiments show that AdaTS quickly adapts to the unknown meta-parameter µ , is robust to ∗ meta-prior misspecification, and performs well even in challenging classification problems. We present a general framework for learning to explore from similar past exploration problems. One potential application is cold-start personalization in recommender systems where users are tasks. The users have similar preferences, but neither the individual preferences nor their similarity is known in advance. Another application could be online regression with bandit feedback (Appendix E.2) where individual regression problems are tasks. Similar examples in the tasks have similar mean responses, which are unknown in advance. 2 Setting We first introduce our notation. The set {1, . . . , n} is denoted by [n]. The indicator 1{E} denotes that event E occurs. The i-th entry of vector v is v . If the vector or its index are already subindexed, i we write v(i). We use O˜ for the big-O notation up to polylogarithmic factors. A diagonal matrix with entries v is denoted diag (v). We use the terms “arm” and “action” interchangeably, depending on the context. Our setting was proposed in Kveton et al. [28] and is defined µ Σ Σ σ2 as follows. Each bandit problem instance has K arms. Each q q 0 arm i ∈ [K] is defined by distribution p (·; θ) with parameter i θ ∈ Θ. The parameter θ is shared among all arms. The mean of p (·; θ) is denoted by r(i; θ). The learning agent interacts i µ θ Y with m instances, one at each of m tasks. At the beginning ∗ s,∗ s,t of task s ∈ [m], an instance θ ∈ Θ is sampled i.i.d. from a s,∗ t = 1, ..., n task prior P (·; µ ), which is parameterized by µ . The agent ∗ ∗ s = 1, ..., m interacts with θ for n rounds. In round t ∈ [n], it pulls one s,∗ arm and observes a stochastic realization of its reward. We Figure 1: Graphical model of our en- denote the pulled arm in round t of task s by A ∈ [K], the vironment. s,t realized rewards of all arms in round t of task s by Y ∈ RK, s,t and the reward of arm i ∈ [K] by Y (i) ∼ p (·; θ ). We assume that the realized rewards Y are s,t i s,∗ s,t i.i.d. with respect to both s and t. A graphical model of our environment is drawn in Figure 1. We define the distribution-specific parameters µ , Σ , Σ , and σ2 when we instantiate our framework. q q 0 Our terminology is summarized in Appendix A. The n-round regret of an agent or algorithm over m tasks with task prior P (·; µ ) is defined as ∗ (cid:88)m (cid:34) (cid:88)n (cid:12) (cid:12) (cid:35) R(m, n; µ ) = E r(A ; θ ) − r(A ; θ ) (cid:12) µ , (1) ∗ s,∗ s,∗ s,t s,∗ (cid:12) ∗ (cid:12) s=1 t=1 where A = arg max r(i; θ ) is the optimal arm in the random problem instance θ in task s,∗ i∈[K] s,∗ s,∗ s ∈ [m]. The above expectation is over problem instances θ ∼ P (·; µ ), their realized rewards, s,∗ ∗ and also pulled arms. Note that µ is fixed. Russo and Van Roy [36] showed that the Bayes regret, ∗ which matches the definition in (1) in any task, of Thompson sampling in a K-armed bandit with n √ √ rounds is O˜( Kn). So, when TS is applied independently in each task, R(m, n; µ ) = O˜(m Kn). ∗ Our goal is to attain a comparable regret without knowing µ . We frame this problem in a Bayesian ∗ fashion, where µ ∼ Q before the learning agent interacts with the first task. The agent knows Q ∗ and we call it a meta-prior. Accordingly, we consider R(m, n) = E [R(m, n; µ )] as a metric and ∗ call it the Bayes regret. Our approach is motivated by hierarchical Bayesian models [20], where the uncertainty in prior parameters, such as µ , is represented by another distribution, such as Q. In these ∗ models, Q is called a hyper-prior and µ is called a hyper-parameter. We attempt to learn µ from ∗ ∗ sequential interactions with instances θ ∼ P (·; µ ), which are also unknown. The agent can only s,∗ ∗ observe their noisy realizations Y . s,t 3 Algorithm Our algorithm is presented in this section. To describe it, we need to introduce several notions of history, the past interactions of the agent. We denote by H = (A , Y (A ))n the history in s s,t s,t s,t t=1 2
Algorithm 1 AdaTS: Instance-adaptive exploration in Thompson sampling. 1: Initialize meta-prior Q 0 ← Q 2: for s = 1, . . . , m do 3: Compute meta-posterior Q s (Proposition 1) 4: Compute uncertainty-adjusted task prior P s (Proposition 1) 5: for t = 1, . . . , n do 6: Compute posterior of θ in task s, P s,t(θ) ∝ L s,t(θ)P s(θ) 7: Sample θ˜ s,t ∼ P s,t, pull arm A s,t ← arg max i∈[K] r(i; θ˜ s,t), and observe Y s,t(A s,t) task s and by H = H ⊕ · · · ⊕ H a concatenated vector of all histories in the first s tasks. The 1:s 1 s history up to round t in task s is H = (A , Y (A ))t−1 and all history up to round t in task s s,t s,(cid:96) s,(cid:96) s,(cid:96) (cid:96)=1 is H = H ⊕ H . We denote the conditional probability distribution given history H by 1:s,t 1:s−1 s,t 1:s,t P (·) = P (· | H ) and the corresponding conditional expectation by E [·] = E [· | H ]. s,t 1:s,t s,t 1:s,t Our algorithm is a form of Thompson sampling [38, 11, 2, 37]. TS pulls arms proportionally to being optimal with respect to the posterior. In particular, let L (θ) = (cid:81)t−1 p (Y (A ); θ) be the s,t (cid:96)=1 As,(cid:96) s,(cid:96) s,(cid:96) likelihood of observations in task s up to round t. If the prior P (·; µ ) was known, the posterior of ∗ instance θ in round t would be P TS (θ) ∝ L (θ) P (θ; µ ). TS would sample θ˜ ∼ P TS and pull arm s,t s,t ∗ t s,t A = arg max r(i; θ˜ ). t i∈[K] t We address the case of unknown µ . The key idea in our method is to maintain a posterior density of ∗ µ , which we call a meta-posterior. This density represents uncertainty in µ given history. In task s, ∗ ∗ we denote it by Q and define it such that P (µ ∈ B | H ) = (cid:82) Q (µ) dκ (µ) holds for any s ∗ 1:s−1 µ∈B s 1 set B, where κ is the reference measure for µ. We use this more general notation, as opposing to 1 dµ, because µ can be both continuous and discrete. When solving task s, Q is used to compute an s uncertainty-adjusted task prior P , which is a posterior density of θ given history. Formally, P is s s,∗ s a density such that P (θ ∈ B | H ) = (cid:82) P (θ) dκ (θ) holds for any set B, where κ is the s,∗ 1:s−1 θ∈B s 2 2 reference measure for θ. After computing P , we run TS with prior P to solve task s. To maintain s s Q and P , we find it useful expressing them using a recursive update rule below. s s Proposition 1. Let L (θ) = (cid:81)n p (Y (A ); θ) be the likelihood of observations in task s. s (cid:96)=1 As,(cid:96) s,(cid:96) s,(cid:96) Then for any task s ∈ [m], (cid:90) (cid:90) P (θ) = P (θ; µ) Q (µ) dκ (µ) , Q (µ) = L (θ) P (θ; µ) dκ (θ) Q (µ) . s s 1 s s−1 2 s−1 µ θ The claim is proved in Appendix A. The proof uses the Bayes rule, where we carefully account for the fact that the observations are collected adaptively, the pulled arm in round t of task s depends on history H . The pseudocode of our algorithm is in Algorithm 1. Since the algorithm adapts to the 1:s,t unknown task prior P (·; µ ), we call it AdaTS. AdaTS can be implemented efficiently when P is a ∗ s conjugate prior for rewards, or a mixture of conjugate priors. We discuss several exact and efficient implementations starting from Section 3.1. The design of AdaTS is motivated by MetaTS [28], which also maintains a meta-posterior Q . The s difference is that MetaTS samples µ˜ ∼ Q in task s to be optimistic with respect to the unknown s s µ . Then it runs TS with prior P (·; µ˜ ). While simple and intuitive, the sampling of µ˜ induces a ∗ s s high variance and leads to a conservative worst-case analysis. We improve MetaTS by avoiding the sampling step. This leads to tighter and more general regret bounds (Section 4), beyond multi-armed bandits; while the practical performance also improves significantly (Section 5). 3.1 Gaussian Bandit We start with a K-armed Gaussian bandit with mean arm rewards θ ∈ RK. The reward distribution of arm i is p (·; θ) = N (·; θ , σ2), where σ > 0 is reward noise and θ is the mean reward of arm i. A i i i natural conjugate prior for this problem class is P (·; µ) = N (·; µ, Σ ), where Σ = diag (cid:0) (σ2 )K (cid:1) 0 0 0,i i=1 is known and we learn µ ∈ RK. Because the prior is a multivariate Gaussian, AdaTS can be implemented efficiently with a Gaussian meta-prior Q(·) = N (·; µ , Σ ), where µ = (µ )K and Σ = diag (cid:0) (σ2 )K (cid:1) are known mean q q q q,i i=1 q q,i i=1 3
parameter vector and covariance matrix, respectively. In this case, the meta-posterior in task s is also a Gaussian Q (·) = N (·; µˆ , Σˆ ), where µˆ = (µˆ )K and Σˆ = diag (cid:0) (σˆ2 )K (cid:1) are defined as s s s s s,i i=1 s s,i i=1 (cid:32) s−1 (cid:33) s−1 µˆ = σˆ2 µ q,i + (cid:88) T (cid:96),i B (cid:96),i , σˆ−2 = σ−2 + (cid:88) T (cid:96),i . (2) s,i s,i σ2 T σ2 + σ2 T s,i q,i T σ2 + σ2 q,i (cid:96)=1 (cid:96),i 0,i (cid:96),i (cid:96)=1 (cid:96),i 0,i Here T = (cid:80)n 1{A = i} is the number of pulls of arm i in task (cid:96) and the total reward from (cid:96),i t=1 (cid:96),t these pulls is B = (cid:80)n 1{A = i} Y (i). The above formula has a very nice interpretation. (cid:96),i t=1 (cid:96),t (cid:96),t The posterior mean µˆ of the meta-parameter of arm i is a weighted sum of the noisy estimates s,i of the means of arm i from the past tasks B /T and the prior. In this sum, each bandit task is (cid:96),i (cid:96),i essentially a single observation. The weights are proportional to the number of pulls in a task, giving the task with more pulls a higher weight. They vary from (σ2 + σ2)−1, when the arm is pulled only 0,i once, up to σ−2. This is the minimum amount of uncertainty that cannot be reduced by more pulls. 0,i The update in (2) is by Lemma 7 in Appendix A, which we borrow from Kveton et al. [28]. From Proposition 1, we have that the uncertainty-adjusted prior for task s is P (·) = N (·; µˆ , Σˆ + Σ ). s s s 0 3.2 Linear Bandit with Gaussian Rewards Now we generalize Section 3.1 and consider a linear bandit [14, 1] with K arms and d dimensions. Let A ⊂ Rd be an action set such that |A| = K. We refer to each a ∈ A as an arm. Then, with a slight abuse of notation from Section 2, the reward distribution of arm a is p (·; θ) = N (·; a(cid:62)θ, σ2), a where θ ∈ Rd is shared by all arms and σ > 0 is reward noise. A conjugate prior for this problem class is P (·; µ) = N (·; µ, Σ ), where Σ ∈ Rd×d is known and we learn µ ∈ Rd. 0 0 As in Section 3.1, AdaTS can be implemented efficiently with a meta-prior Q(·) = N (·; µ , Σ ), q q where µ ∈ Rd is a known mean parameter vector and Σ ∈ Rd×d is a known covariance matrix. In q q this case, Q (·) = N (·; µˆ , Σˆ ), where s s s µˆ = Σˆ (cid:32) Σ−1µ + (cid:88)s−1 B (cid:96) − G (cid:96) (cid:18) Σ−1 + G (cid:96) (cid:19)−1 B (cid:96) (cid:33) , s s q q σ2 σ2 0 σ2 σ2 (cid:96)=1 Σˆ −1 = Σ−1 + (cid:88)s−1 G (cid:96) − G (cid:96) (cid:18) Σ−1 + G (cid:96) (cid:19)−1 G (cid:96) . s q σ2 σ2 0 σ2 σ2 (cid:96)=1 Here G = (cid:80)n A A(cid:62) is the outer product of the feature vectors of the pulled arms in task (cid:96) and (cid:96) t=1 (cid:96),t (cid:96),t B = (cid:80)n A Y (A ) is their sum weighted by their rewards. The above update follows from (cid:96) t=1 (cid:96),t (cid:96),t (cid:96),t Lemma 7 in Appendix A, which is due to Kveton et al. [28]. From Proposition 1, the uncertainty- adjusted prior for task s is P (·) = N (·; µˆ , Σˆ + Σ ). s s s 0 We note in passing that when K = d and A is the standard Euclidean basis of Rd, the linear bandit reduces to a K-armed bandit. Since the covariance matrices are unrestricted here, the formulation in this section also shows how to generalize Section 3.1 to arbitrary covariance matrices. 3.3 Semi-Bandit with Gaussian Rewards A stochastic combinatorial semi-bandit [19, 12, 25, 26, 42], or semi-bandit for short, is a K-armed bandit where at most L ≤ K arms are pulled in each round. After the arms are pulled, the agent observes their individual rewards and its reward is the sum of the individual rewards. Semi-bandits can be used to solve online combinatorial problems, such as learning to route. We consider a Gaussian reward distribution for each arm, as in Section 3.1. The difference in the semi-bandit formulation is that the action set is A ⊆ Π (K), where Π (K) is the set of all subsets L L of [K] of size at most L. In round t of task s, the agents pulls arms A ∈ A. The meta-posterior is s,t updated analogously to Section 3.1. The only difference is that 1{A = i} becomes 1{i ∈ A }. (cid:96),t (cid:96),t 3.4 Exponential-Family Bandit with Mixture Priors We consider a general K-armed bandit with mean arm rewards θ ∈ RK. The reward distribution of arm i is any one-dimensional exponential-family distribution parameterized by θ . In a Bernoulli i 4
bandit, this would be p (·; θ) = Ber(·; θ ). A natural prior for this reward model would be a product i i of per-arm conjugate priors, such as the product of betas for Bernoulli rewards. It is challenging to generalize our approach beyond Gaussian models because we require more than the standard notion of conjugacy. Specifically, to apply AdaTS to an exponentially-family prior, such as the product of betas, we need a computationally tractable prior for that prior. In this case, it does not exist. We circumvent this issue by discretization. More specifically, let {P (·; j)}L be a set of L j=1 potential conjugate priors, where each P (·; j) is a product of one-dimensional exponential-family priors. Then a suitable meta-prior is a vector of initial beliefs into each potential prior. In particular, it is Q(·) = Cat(·; w ), where w ∈ ∆ is the belief and ∆ is the L-simplex. q q L−1 L (cid:82) In this case, Q (j) = L (θ)P (θ; j) dκ (θ) Q (j) in Proposition 1 has a closed form, since it s θ s−1 2 s−1 is a standard conjugate posterior update for a distribution over θ followed by integrating out θ. In addition, P (θ) = (cid:80)L Q (j)P (θ; j) is a mixture of exponential-family priors over θ. This is an s j=1 s instance of latent bandits [22]. For these problems, Thompson sampling can be implemented exactly and efficiently. We do not analyze this setting because prior-dependent Bayes regret bounds for this problem class do not exist yet. 4 Regret Bounds We first introduce common notation used in our proofs. The action set A ⊆ Rd is fixed. Recall that a matrix X ∈ Rd×d is positive semi-definite (PSD) if it is symmetric and its smallest eigenvalue is non-negative. For such X, we define σ2 (X) = max a(cid:62)Xa. Although σ2 (X) depends on max a∈A max A, we suppress this dependence because A is fixed. We denote by λ (X) the maximum eigenvalue 1 of X and by λ (X) the minimum eigenvalue of X. d We also need basic quantities from information theory. For two probability measures P and Q over a common measurable space, we use D(P ||Q) to denote the relative entropy of P with respect to Q. It is defined as D(P ||Q) = (cid:82) log( dP ) dP , where dP/dQ is the Radon-Nikodym derivative of P with dQ respect to Q; and is infinite when P is not absolutely continuous with respect to Q. We slightly abuse our notation and let P (X) denote the probability distribution of random variable X, P (X ∈ ·). For jointly distributed random variables X and Y , we let P (X | Y ) be the conditional distribution of X given Y , P (X ∈ · | Y ), which is Y -measurable and depends on random Y . The mutual information between X and Y is I(X; Y ) = D(P (X, Y )||P (X)P (Y )), where P (X)P (Y ) is the distribution of the product of P (X) and P (Y ). Intuitively, I(X; Y ) measures the amount of information that either X or Y provides about the other variable. For jointly distributed X, Y , and Z, we also need the conditional mutual information between X and Y conditioned on Z. We define this quantity as I(X; Y | Z) = E[Iˆ(X; Y | Z)], where Iˆ(X; Y | Z) = D(P (X, Y | Z)||P (X | Z)P (Y | Z)) is the random conditional mutual information between X and Y given Z. Note that Iˆ(X; Y | Z) is a function of Z. By the chain rule for the random conditional mutual information, Iˆ(X; Y , Y | Z) = 1 2 E[Iˆ(X; Y | Y , Z) | Z] + Iˆ(X; Y | Z), where expectation is over Y | Z. We would get the usual 1 2 2 2 chain rule I(X; Y , Y ) = I(X; Y | Y ) + I(X; Y ) without Z. 1 2 1 2 2 4.1 Generic Regret Bound We start with a generic adaptation of the analysis of Lu and Van Roy [33] to our setting. In round t of task s, we denote the pulled arm by A , its observed reward by Y ∼ p (·; θ ), and the s,t s,t As,t s,∗ suboptimality gap by ∆ = r(A ; θ ) − r(A ; θ ). For random variables X and Y , we denote s,t s,∗ s,∗ s,t s,∗ by I (X; Y ) = Iˆ(X; Y | H ) the random mutual information between X and Y given history s,t 1:s,t H of all observations from the first s − 1 tasks and the first t − 1 rounds of task s. Similarly, for 1:s,t random variables X, Y , and Z, we denote by I (X; Y | Z) = E[Iˆ(X; Y | Z, H ) | H ] the s,t 1:s,t 1:s,t random mutual information between X and Y conditioned on Z, given history H . It is helpful to 1:s,t think of I as the conditional mutual information of X | H , Y | H , and Z | H . s,t 1:s,t 1:s,t 1:s,t Let Γ and (cid:15) be potentially history-dependent non-negative random variables such that s,t s,t (cid:113) E [∆ ] ≤ Γ I (θ ; A , Y ) + (cid:15) (3) s,t s,t s,t s,t s,∗ s,t s,t s,t 5
holds almost surely. We want to keep both Γ and (cid:15) “small”. The following lemma provides a s,t s,t bound on the total regret over n rounds in each of m tasks in terms of Γ and (cid:15) . s,t s,t Lemma 2. Suppose that (3) holds for all s ∈ [m] and t ∈ [n], for some Γ , (cid:15) ≥ 0. In addition, s,t s,t let (Γ ) and Γ be non-negative constants such that Γ ≤ Γ ≤ Γ holds for all s ∈ [m] and s s∈[m] s,t s t ∈ [n] almost surely. Then (cid:112) (cid:88)m (cid:113) (cid:88)m (cid:88)n R(m, n) ≤ Γ mnI(µ ; H ) + Γ nI(θ ; H | µ , H ) + E [(cid:15) ] . ∗ 1:m s s,∗ s ∗ 1:s−1 s,t s=1 s=1 t=1 The first term above is the price for learning µ , while the second is the price for learning all θ ∗ s,∗ when µ is known. Accordingly, the price for learning µ is negligible when the mutual information ∗ ∗ terms grow slowly with m and n. Specifically, we show shortly in linear bandits that Γ and (cid:15) s,t s,t can be set so that the last term of the bound is comparable to the rest, while Γ grows slowly with s,t m and n. At the same time, I(µ ; H ) and I(θ ; H | µ , H ) are only logarithmic in m and ∗ 1:m√ s,∗ s ∗ 1:s−1 √ n. Thus the price for learning µ is O˜( mn) while that for learning all θ is O˜(m n). Now we ∗ s,∗ are ready to prove Lemma 2. Proof. First, we use the chain rule of random conditional mutual information and derive I (θ ; A , Y ) ≤ I (θ , µ ; A , Y ) = I (µ ; A , Y ) + I (θ ; A , Y | µ ) . s,t s,∗ s,t s,t s,t s,∗ ∗ s,t s,t s,t ∗ s,t s,t s,t s,∗ s,t s,t ∗ √ √ √ Now we take the square root of both sides, apply a + b ≤ a + b to the right-hand side, and multiply both sides by Γ . This yields s,t (cid:113) (cid:113) (cid:113) Γ I (θ ; A , Y ) ≤ Γ I (µ ; A , Y ) + Γ I (θ ; A , Y | µ ) . (4) s,t s,t s,∗ s,t s,t s,t s,t ∗ s,t s,t s,t s,t s,∗ s,t s,t ∗ We start with the second term in (4). Fix task s. From Γ ≤ Γ , followed by the Cauchy-Schwarz s,t s and Jensen’s inequalities, we have (cid:118) (cid:34) (cid:88)n (cid:113) (cid:35) (cid:117) (cid:117) (cid:34) (cid:88)n (cid:35) E Γ I (θ ; A , Y | µ ) ≤ Γ (cid:116)nE I (θ ; A , Y | µ ) . s,t s,t s,∗ s,t s,t ∗ s s,t s,∗ s,t s,t ∗ t=1 t=1 Thanks to E [I (θ ; A , Y | µ )] = I(θ ; A , Y | µ , H ) and the chain rule of mutual s,t s,∗ s,t s,t ∗ s,∗ s,t s,t ∗ 1:s,t information, we have E [(cid:80)n I (θ ; A , Y | µ )] = I(θ ; H | µ , H ). t=1 s,t s,∗ s,t s,t ∗ s,∗ s ∗ 1:s−1 Now we consider the first term in (4). We bound Γ using Γ, then apply the Cauchy-Schwarz and s,t Jensen’s inequalities, and obtain E (cid:104) (cid:80)m (cid:80)n Γ (cid:112) I (µ ; A , Y )(cid:105) ≤ Γ(cid:112) mnI(µ ; H ); s=1 t=1 s,t s,t ∗ s,t s,t ∗ 1:m where we used the chain rule to get (cid:34) m n (cid:35) m n (cid:88) (cid:88) (cid:88) (cid:88) E I (µ ; A , Y ) = I(µ ; A , Y | H ) = I(µ ; H ) . s,t ∗ s,t s,t ∗ s,t s,t 1:s,t ∗ 1:m s=1 t=1 s=1 t=1 This completes the proof. 4.2 Linear Bandit with Gaussian Rewards Now we derive regret bounds for linear bandits (Section 3.2). Without loss of generality, we make an assumption that the action set is bounded. Assumption 1. The arms are vectors in a unit ball, max (cid:107)a(cid:107) ≤ 1. a∈A 2 Our analysis is for AdaTS with a small amount of forced exploration in each task. This guarantees that our estimate of µ improves uniformly in all directions after each task s. Therefore, we assume ∗ that the action set is diverse enough to explore in all directions. Assumption 2. There exist arms {a }d ⊆ A such that λ ((cid:80)d a a(cid:62)) ≥ η for some η > 0. i i=1 d i=1 i i This assumption is without loss of generality. In particular, if such a set does not exist, the action set A can be projected into a lower dimensional space where the assumption holds. AdaTS is modified as follows. In each task, we initially pulls the arms {a }d to explore all directions. i i=1 6
We start by showing that (3) holds for suitably “small” Γ and (cid:15) . In AdaTS, in round t of task s, s,t s,t the posterior distribution of θ is N (µˆ , Σˆ ), where s,∗ s,t s,t µˆ = Σˆ (cid:32) (Σ + Σˆ )−1µˆ + (cid:88)t−1 A s,(cid:96)Y s,(cid:96) (cid:33) , Σˆ −1 = (Σ + Σˆ )−1 + (cid:88)t−1 A s,(cid:96)A(cid:62) s,(cid:96) , s,t s,t 0 s s σ2 s,t 0 s σ2 (cid:96)=1 (cid:96)=1 and µˆ and Σˆ are defined in Section 3.2. Then, from the properties of Gaussian distributions and s s that AdaTS samples from the posterior, we get a bound on Γ and (cid:15) as a function of a tunable s,t s,t parameter δ ∈ (0, 1]. Lemma 3. For all tasks s ∈ [m], rounds t ∈ [n], and any δ ∈ (0, 1], (3) holds almost surely for (cid:115) σ2 (Σˆ ) (cid:113) Γ = 4 max s,t log(4|A|/δ) , (cid:15) = 2δσ2 (Σˆ ) + 2E E [(cid:107)θ (cid:107) ] , s,t log(1 + σ2 (Σˆ )/σ2) s,t max s,t s,t s,t s,∗ 2 max s,t where E is the indicator of forced exploration in round t of task s. Moreover, for each task s, the s,t following history-independent bound holds almost surely,  λ (Σ ) (cid:16) 1 + σ2 (cid:17)  σ m2 ax(Σˆ s,t) ≤ λ 1(Σ 0) 1 + λ (Σ1 )q + σ2/η η +λ1 s( λΣ0 () Σ )  . (5) 1 0 1 q (cid:112) Lemma 3 is proved in Appendix C.3. By using the bound in (5), we get that Γ = O( log(1/δ)) √ s,t and (cid:15) = O( δ). Lemma 3 differs from Lu and Van Roy [33] in two aspects. First, it considers s,t uncertainty in the estimate of µ along with θ . Second, it does not require that the rewards are ∗ s,∗ bounded. Our next lemma bounds the mutual information terms in Lemma 2, by exploiting the hierarchical structure of our linear bandit model (Figure 1). Lemma 4. For any H -adapted action sequence and any s ∈ [m], we have 1:s,t (cid:16) (cid:17) (cid:16) (cid:17) I(θ ; H | µ , H ) ≤ d log 1 + λ1(Σ0)n , I(µ ; H ) ≤ d log 1 + λ1(Σq)m . s,∗ s ∗ 1:s−1 2 σ2 ∗ 1:m 2 λd(Σ0)+σ2/n Now we are ready to prove our regret bound for the linear bandit. We take the mutual-information bounds from Lemma 4, and the bounds on Γ and (cid:15) from Lemma 3, and plug them into Lemma 2. s,t s,t Specifically, σ2 (Σˆ ) ≤ λ (Σ ) + λ (Σ ) holds for any s and t by Lemma 3, which yields Γ in max s,t 1 q 1 0 Lemma 2. On the other hand, Γ is bounded using the upper bound in (5), which relies on forced s exploration. Our regret bound is stated below. The terms c to c are at most polylogarithmic in d, m, 1 4 and n; and thus small. The term c arises due to summing up Γ over all tasks s. 2 s Theorem 5 (Linear bandit). The regret of AdaTS is bounded for any δ ∈ (0, 1] as √ R(m, n) ≤ c dmn + (m + c ) R (n; µ ) + c dm , 1 2 δ ∗ 3 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Learning of µ∗ Per-task regret Forced exploration where (cid:115) (cid:16) (cid:17) c = 8 λ1(Σq)+λ1(Σ0) log(4|A|/δ) log 1 + λ1(Σq)m , 1 log(cid:18) 1+ λ1(Σq)+λ1(Σ0) (cid:19) λd(Σ0)+σ2/n σ2 c = (cid:16) 1 + σ2 (cid:17) log m, and c = 2(cid:112) (cid:107)µ (cid:107)2 + tr(Σ + Σ ). The per-task regret is bounded as 2 ηλ1(Σ√0) 3 q 2 q 0 (cid:112) R (n; µ ) ≤ c dn + 2δλ (Σ )n, where δ ∗ 4 1 0 (cid:115) (cid:16) (cid:17) c = 8 λ1(Σ0) log(4|A|/δ) log 1 + λ1(Σ0)n . 4 log(cid:18) 1+ λ1(Σ0) (cid:19) σ2 σ2 The bound in Theorem 5 is sublinear in n for δ = 1/n2. It has three terms. The first term is the regret √ due to learning µ over all tasks; and it is O˜( dmn). The second term is the regret for acting in ∗ √ m tasks under the assumption that µ is known; and it is O˜(m dn). The last term is the regret for ∗ 7
√ forced exploration; and it is O˜(dm). Overall, the extra regret due to unknown µ is O˜( dmn + dm) √ ∗ and is much lower than O˜(m dn) when d (cid:28) n. Therefore, we call AdaTS a no-regret algorithm for linear bandits. Our bound also reflects the fact that the regret decreases as both priors become more informative, λ (Σ ) → 0 and λ (Σ ) → 0. 1 0 1 q √ A frequentist regret bound for linear TS with finitely-many arms is O˜(d n) [3]. When applied to √ √ m tasks, it would be O˜(dm n) and is worse by a factor of d than our regret bound. To show that our bound reflects the structure of our problem, we compare AdaTS to two variants of linear TS that are applied independently to each task. The first variant knows µ and thus has more information. ∗ Its regret can bounded by setting c = c = c = 0 in Theorem 5 and is lower than that of AdaTS. 1 2 3 The second variant knows that µ ∼ N (µ , Σ ) but does not model that the tasks share µ . This is ∗ q q ∗ analogous to assuming that θ ∼ N (µ , Σ + Σ ). The regret of this approach can be bounded by s,∗ q q 0 setting c = c = c = 0 in Theorem 5 and replacing λ (Σ ) in c by λ (Σ + Σ ). Since the task 1 2 3 1 0 4 1 q 0 regret increases linearly with m and λ (Σ + Σ ) > λ (Σ ), this approach would ultimately have a 1 q 0 1 0 higher regret than AdaTS as the number of tasks m increases. 4.3 Semi-Bandit with Gaussian Rewards In semi-bandits (Section 3.3), we use the independence of arms to decompose the per-round regret differently. Similarly to Section 4.2, we analyze AdaTS with forced exploration, where each arm is initially pulled at least once. This is always possible in at most K rounds, since there exists at least one a ∈ A that contains any given arm. Let Γ (k) and (cid:15) (k) be non-negative history-dependent constants, for each arm k ∈ [K], were we s,t s,t use (k) to refer to arm-specific quantities. Then an analogous bound to (3) is (cid:88) (cid:18) (cid:113) (cid:19) E [∆ ] ≤ P (k ∈ A ) Γ (k) I (θ (k); k, Y (k)) + (cid:15) (k) . s,t s,t s,t s,t s,t s,t s,∗ s,t s,t k∈[K] The term (k, Y (k)) is a tuple of a pulled arm k and its observation in round t of task s. For any k, s,t from the chain rule of mutual information, we have I (θ (k); k, Y (k)) ≤ I (µ (k); k, Y (k)) + I (θ (k); k, Y (k) | µ (k)) . s,t s,∗ s,t s,t ∗ s,t s,t s,∗ s,t ∗ Next we combine the mutual-information terms across all rounds and tasks, as in Lemma 2, and bound corresponding Γ (k) and (cid:15) (k) independently of m and n. Due to forced exploration, the s,t s,t estimate of µ (k) improves for all arms k as more tasks are completed, and Γ (k) decreases with s. ∗ s,t This leads to Theorem 6, which is proved in Appendix D. Theorem 6 (Semi-bandit). The regret of AdaTS is bounded for any δ ∈ (0, 1] as √ √ R(m, n) ≤ c KLmn + (m + c ) R (n; µ ) + c K3/2m + c σ 2δmn , 1 2 δ ∗ 3 4 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Learning of µ∗ Per-task regret Forced exploration where (cid:118) c = 4(cid:117) (cid:117) 1 (cid:88) σ q2 ,k+σ 02 ,k log(4K/δ) log (cid:16) 1 + σ q2 ,km (cid:17) , 1 (cid:116) K (cid:32) σ2 +σ2 (cid:33) σ2 +σ2/n k∈[K] log 1+ q,k σ2 0,k 0,k (cid:18) (cid:19) (cid:115) c = 1 + max σ2 log m , c = 2 (cid:88) (µ2 + σ2 + σ2 ) , 2 σ2 3 q,k q,k 0,k k∈[K]: σ0,k>0 0,k k∈[K] c = (cid:115) 1 (cid:88) log (cid:16) 1 + σ q2 ,km (cid:17) . 4 K σ2 k∈[K]: σ0,k=0 √ (cid:113) The per-task regret is bounded as R (n; µ ) ≤ c KLn + 2δ 1 (cid:80) σ2 n, where δ ∗ 5 K k∈[K] 0,k (cid:118) c = 4(cid:117) (cid:117) 1 (cid:88) σ 02 ,k log(4K/δ) log (cid:16) 1 + σ 02 ,kn (cid:17) . 5 (cid:116) K (cid:32) σ2 (cid:33) σ2 k∈[K]: σ0,k>0 log 1+ σ0 2,k The prior widths σ and σ are defined as in Section 3.1. q,k 0,k 8
200 150 100 50 0 0 5 10 15 20 Task s tergeR Gaussian (K = 2, ¾q = 0.500) Gaussian (K = 2, ¾q = 1.000) Linear (d = 2, ¾q = 1.000) 160 350 OracleTS 140 300 TS 120 250 MetaTS 100 200 AdaTS 80 150 60 40 100 20 50 0 0 0 5 10 15 20 0 5 10 15 20 Task s Task s Figure 2: Comparison of AdaTS to three baselines on three bandit problems. The bound in Theorem 6 is sublinear in n for δ = 1/n2. Its form resembles Theorem 5. Specifically, √ the regret for learning µ is O˜( KLmn) and for forced exploration is O˜(K3/2m). Both of these ∗ √ are much lower than the regret for learning to act in m tasks when µ is known, O˜(m KLn), for ∗ K (cid:28) Ln. Therefore, AdaTS is also a no-regret algorithm for semi-bandits. Theorem 6 improves upon a naive application of Theorem 5 to semi-bandits. This is because all prior width constants are averages, as opposing to the maximum over arms in Theorem 5. To the best of our knowledge, such per-arm prior dependence has not been captured in semi-bandits by any prior work. To illustrate the difference, consider a problem where σ > 0 for only K(cid:48) (cid:28) K arms. This 0,k √ means that only K(cid:48) arms are uncertain in the tasks. Then the bound in Theorem 6 is O˜(m K(cid:48)Ln), √ while the bound in Theorem 5 would be O˜(m KLn). For the arms k where σ = 0, the regret 0,k over all tasks is sublinear in m. 5 Experiments We experiment with two synthetic problems. In both problems, the number of tasks is m = 20 and each task has n = 200 rounds. The first problem is a Gaussian bandit (Section 3.1) with K = 2 arms. The meta-prior is N (0, Σ ) with Σ = σ2I , the prior covariance is Σ = σ2I , and the reward q q q K 0 0 K noise is σ = 1. We experiment with σ ≥ 0.5 and σ = 0.1. Since σ (cid:29) σ , the entries of θ are q 0 q 0 s,∗ likely to have the same order as in µ . Therefore, a clever algorithm that learns µ could have very ∗ ∗ low regret. The second problem is a linear bandit (Section 3.2) in d = 2 dimensions with K = 5d arms. The action set is sampled from a unit sphere. The meta-prior, prior, and noise are the same as in the Gaussian bandit. All results are averaged over 100 runs. AdaTS is compared to three baselines. The first is idealized TS with the true prior N (µ , Σ ) and we ∗ 0 call it OracleTS. OracleTS shows the minimum attainable regret. The second is agnostic TS, which ignores the structure of the problem. We call it TS and implement it with prior N (0, Σ + Σ ), since q 0 θ can be viewed as a sample from this prior when the structure is ignored (Section 4.2). The third s,∗ baseline is MetaTS of Kveton et al. [28]. All methods are evaluated by their cumulative regret up to task s, which we plot as it accumulates round-by-round within each task (Figure 2). The regret of the algorithms that do not learn µ (OracleTS and TS) is obviously linear in s, as they solve s similar ∗ tasks with the same policy (Section 2). A lower slope indicates a better policy. As no algorithm can outperform OracleTS, no regret can grow sublinearly in s. STelcarO ST STateM Task 1 STadA Our results are reported in Figure 2. We start with a Gaussian bandit with σ = 0.5. This setting is identical to Figure 1b of Kveton et al. q [28]. We observe that AdaTS outperforms TS, which does not learn µ , ∗ and is comparable to OracleTS, which knows µ . Its regret is about ∗ 30% lower than that of MetaTS. Now we increase the meta-prior width to σ = 1. In this setting, meta-parameter sampling in MetaTS leads q to high biases in earlier tasks. This leads to a major increase in regret, while AdaTS performs comparably to OracleTS. We end with a linear bandit with σ = 1. In this experiment, AdaTS outperforms MetaTS q again and has more than three times lower regret. Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8 Task 9 Task 10 Appendix E contains more experiments. In Appendix E.1, we exper- Figure 3: Meta-learning of iment with more values of K and d, and show the robustness of AdaTS a highly rewarding digit 1. to missspecified meta-prior Q. In Appendix E.2, we apply AdaTS to 9
bandit classification problems. In Figure 3, we show results for one of these problems, meta-learning a highly rewarding digit 1 in the bandit setting. For each method and task s, we show the average digit corresponding to the pulled arms in round 1 of task s. AdaTS learns a good meta-parameter µ ∗ almost instantly, since its average digit in task 2 already resembles digit 1. 6 Related Work Two closest related works are Bastani et al. [6] and Kveton et al. [28]. Bastani et al. [6] proposed Thompson sampling that learns the prior from a sequence of pricing experiments. The algorithm is tailored to pricing and learns through forced exploration using conservative TS. Therefore, it is conservative. Bastani et al. [6] also did not derive prior-dependent bounds. Our studied setting is identical to Kveton et al. [28]. However, the design of AdaTS is very different from MetaTS. MetaTS samples the meta-parameter µ at the beginning of each task s and uses it s to compute the posterior of the task parameter θ . Since µ is fixed within the task, MetaTS does s,∗ s not have a correct posterior of θ given the history. AdaTS marginalizes out the uncertainty in the s,∗ meta-parameter µ and thus has a correct posterior of θ within the task. This seemingly minor ∗ s,∗ difference leads to an approach that is more principled, comparably general, has a fully-Bayesian analysis beyond multi-armed bandits, and may have several-fold lower regret in practice. While it is possible that the analysis of MetaTS could be extended to linear bandits, the price for meta-learning √ would likely remain O˜( mn2). This cost arises due sampling the meta-parameter µ at the beginning √ s of each task s. The price of meta-learning in our work is mere O˜( mn), a huge improvement. AdaTS is a meta-learning algorithm [39, 40, 7, 8, 17, 18]. Meta-learning has a long history in multi- armed bandits. Some of the first works are Azar et al. [5] and Gentile et al. [21], who proposed UCB algorithms for multi-task learning. Deshmukh et al. [15] studied multi-task learning in contextual bandits. Cella et al. [10] proposed a UCB algorithm that meta-learns the mean parameter vector in a linear bandit, which is akin to learning µ in Section 3.2. Another recent work is Yang et al. [43], ∗ who studied regret minimization with multiple parallel bandit instances, with the goal of learning their shared subspace. All of these works are frequentist, analyze a stronger notion of regret, and often lead to conservative algorithm designs. In contrast, we leverage the fundamentals of Bayesian reasoning to design a general-purpose algorithm that performs well when run as analyzed. Several recent papers approached the problem of learning a bandit algorithm using policy gradients [16, 9, 27, 44, 35], including learning Thompson sampling [27, 35]. These works focus on offline optimization against a known bandit-instance distribution and have no convergence guarantees in general [9, 27]. Tuning of bandit algorithms is known to reduce regret [41, 34, 24, 23]. Typically it is ad-hoc and we believe that meta-learning is a proper way of framing this problem. 7 Conclusions We propose AdaTS, a fully-Bayesian algorithm for meta-learning in bandits that adapts to a sequence of bandit tasks that it interacts with. AdaTS attains low regret by adapting the uncertainty in both the meta and per-task parameters. We analyze the Bayes regret of AdaTS using information-theory tools that isolate the effect of learning the meta-parameter from that of learning the per-task parameters. For linear bandits and semi-bandits, we derive novel prior-dependent regret bounds that show that the price for learning the meta-parameter is low. Our experiments underscore the generality of AdaTS, good out-of-the-box performance, and robustness to meta-prior misspecification. We leave open several questions of interest. For instance, except for Section 3.4, our algorithms are for Gaussian rewards and priors, and so are their regret analyses. An extension beyond Gaussians would be of both practical and theoretical value. Our current work also relies heavily on a particular parameterization of tasks, where the mean θ is unknown but the covariance Σ is known. It is not s,∗ 0 immediately obvious if a computationally-efficient extension to unknown Σ exists. 0 10
References [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pages 2312–2320, 2011. [2] Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In Proceeding of the 25th Annual Conference on Learning Theory, pages 39.1–39.26, 2012. [3] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In Proceedings of the 30th International Conference on Machine Learning, pages 127–135, 2013. [4] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47:235–256, 2002. [5] Mohammad Gheshlaghi Azar, Alessandro Lazaric, and Emma Brunskill. Sequential transfer in multi-armed bandit with finite set of models. In Advances in Neural Information Processing Systems 26, pages 2220–2228, 2013. [6] Hamsa Bastani, David Simchi-Levi, and Ruihao Zhu. Meta dynamic pricing: Transfer learning across experiments. CoRR, abs/1902.10918, 2019. URL https://arxiv.org/abs/ 1902.10918. [7] Jonathan Baxter. Theoretical models of learning to learn. In Learning to Learn, pages 71–94. Springer, 1998. [8] Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:149–198, 2000. [9] Craig Boutilier, Chih-Wei Hsu, Branislav Kveton, Martin Mladenov, Csaba Szepesvari, and Manzil Zaheer. Differentiable meta-learning of bandit policies. In Advances in Neural Informa- tion Processing Systems 33, 2020. [10] Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear bandits. In Proceedings of the 37th International Conference on Machine Learning, 2020. [11] Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in Neural Information Processing Systems 24, pages 2249–2257, 2012. [12] Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework, results and applications. In Proceedings of the 30th International Conference on Machine Learning, pages 151–159, 2013. [13] Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit and its extension to probabilistically triggered arms. CoRR, abs/1407.8339, 2014. URL http://arxiv.org/ abs/1407.8339. [14] Varsha Dani, Thomas Hayes, and Sham Kakade. Stochastic linear optimization under bandit feedback. In Proceedings of the 21st Annual Conference on Learning Theory, pages 355–366, 2008. [15] Aniket Anand Deshmukh, Urun Dogan, and Clayton Scott. Multi-task learning for contextual bandits. In Advances in Neural Information Processing Systems 30, pages 4848–4856, 2017. [16] Yan Duan, John Schulman, Xi Chen, Peter Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast supervised learning via slow supervised learning. CoRR, abs/1611.02779, 2016. URL http://arxiv.org/abs/1611.02779. [17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, pages 1126–1135, 2017. 11
[18] Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Advances in Neural Information Processing Systems 31, pages 9537–9548, 2018. [19] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Transactions on Networking, 20(5):1466–1478, 2012. [20] Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. Bayesian Data Analysis. Chapman & Hall, 2013. [21] Claudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. In Proceedings of the 31st International Conference on Machine Learning, pages 757–765, 2014. [22] Joey Hong, Branislav Kveton, Manzil Zaheer, Yinlam Chow, Amr Ahmed, and Craig Boutilier. Latent bandits revisited. In Advances in Neural Information Processing Systems 33, 2020. [23] Chih-Wei Hsu, Branislav Kveton, Ofer Meshi, Martin Mladenov, and Csaba Szepesvari. Empir- ical Bayes regret minimization. CoRR, abs/1904.02664, 2019. URL http://arxiv.org/ abs/1904.02664. [24] Volodymyr Kuleshov and Doina Precup. Algorithms for multi-armed bandit problems. CoRR, abs/1402.6028, 2014. URL http://arxiv.org/abs/1402.6028. [25] Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson. Matroid bandits: Fast combinatorial optimization with learning. In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, pages 420–429, 2014. [26] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics, 2015. [27] Branislav Kveton, Martin Mladenov, Chih-Wei Hsu, Manzil Zaheer, Csaba Szepesvari, and Craig Boutilier. Differentiable meta-learning in contextual bandits. CoRR, abs/2006.05094, 2020. URL http://arxiv.org/abs/2006.05094. [28] Branislav Kveton, Mikhail Konobeev, Manzil Zaheer, Chih-Wei Hsu, Martin Mladenov, Craig Boutilier, and Csaba Szepesvari. Meta-Thompson sampling. In Proceedings of the 38th International Conference on Machine Learning, 2021. [29] T. L. Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4–22, 1985. [30] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015. [31] Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2019. [32] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. [33] Xiuyuan Lu and Benjamin Van Roy. Information-theoretic confidence bounds for supervised learning. In Advances in Neural Information Processing Systems, volume 32, 2019. [34] Francis Maes, Louis Wehenkel, and Damien Ernst. Meta-learning of exploration/exploitation strategies: The multi-armed bandit case. In Proceedings of the 4th International Conference on Agents and Artificial Intelligence, pages 100–115, 2012. [35] Seungki Min, Ciamac Moallemi, and Daniel Russo. Policy gradient optimization of Thompson sampling policies. CoRR, abs/2006.16507, 2020. URL http://arxiv.org/abs/2006. 16507. [36] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221–1243, 2014. 12
[37] Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on Thompson sampling. Foundations and Trends in Machine Learning, 11(1):1–96, 2018. [38] William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285–294, 1933. [39] Sebastian Thrun. Explanation-Based Neural Network Learning - A Lifelong Learning Approach. PhD thesis, University of Bonn, 1996. [40] Sebastian Thrun. Lifelong learning algorithms. In Learning to Learn, pages 181–209. Springer, 1998. [41] Joannes Vermorel and Mehryar Mohri. Multi-armed bandit algorithms and empirical evaluation. In Proceedings of the 16th European Conference on Machine Learning, pages 437–448, 2005. [42] Zheng Wen, Branislav Kveton, and Azin Ashkan. Efficient learning in large-scale combinatorial semi-bandits. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [43] Jiaqi Yang, Wei Hu, Jason Lee, and Simon Du. Provable benefits of representation learning in linear bandits. CoRR, abs/2010.06531, 2020. URL http://arxiv.org/abs/2010. 06531. [44] Kaige Yang and Laura Toni. Differentiable linear bandit algorithm. CoRR, abs/2006.03000, 2020. URL http://arxiv.org/abs/2006.03000. 13
A Algorithm Details Our terminology is summarized below: θ Bandit instance parameter in task s, generated as θ ∼ P (·; µ ) s,∗ s,∗ ∗ P (·; µ ) Task prior, a distribution over bandit instance parameter θ ∗ s,∗ µ Meta-parameter, a parameter of the task distribution ∗ Q Meta-prior, a distribution over the meta-parameter µ ∗ P Uncertainty-adjusted prior in task s, a distribution over θ conditioned on H s s,∗ 1:s−1 Q Meta-posterior in task s, a distribution over µ conditioned on H s ∗ 1:s−1 Y Stochastic rewards of all arms in round t of task s s,t A Pulled arm in round t of task s s,t We continue with two lemmas, which are used in the algorithmic part of the paper (Section 3). Proposition 1. Let L (θ) = (cid:81)n p (Y (A ); θ) be the likelihood of observations in task s. s (cid:96)=1 As,(cid:96) s,(cid:96) s,(cid:96) Then for any task s ∈ [m], (cid:90) (cid:90) P (θ) = P (θ; µ) Q (µ) dκ (µ) , Q (µ) = L (θ) P (θ; µ) dκ (θ) Q (µ) . s s 1 s s−1 2 s−1 µ θ Proof. To simplify presentation, our proof is under the assumption that θ and µ take on countably- s,∗ ∗ many values. A more general measure-theory treatment, where we would maintain measures over θ and µ , would follow the same line of reasoning; and essentially replace all probabilities with s,∗ ∗ densities. A good discussion of this topic is in Section 34 of Lattimore and Szepesvari [31]. The following convention is used in the proof. The values of random variables that we marginalize out, such as θ and µ , are explicitly assigned. For fixed variables, such as the history H , we s,∗ ∗ 1:s−1 also treat H as the actual value assigned to H . 1:s−1 1:s−1 We start with the posterior distribution of θ in task s, which can be expressed as s,∗ (cid:88) P (θ = θ | H ) = P (θ = θ, µ = µ | H ) s,∗ 1:s−1 s,∗ ∗ 1:s−1 µ (cid:88) = P (θ = θ | µ = µ) P (µ = µ | H ) . s,∗ ∗ ∗ 1:s−1 µ The second equality holds because θ is independent of history H given µ . Now note that s,∗ 1:s−1 ∗ P (µ = µ | H ) is the meta-posterior in task s. It can be rewritten as ∗ 1:s−1 P (µ = µ | H ) P (µ = µ | H ) = ∗ 1:s−1 P (µ = µ | H ) ∗ 1:s−1 P (µ = µ | H ) ∗ 1:s−2 ∗ 1:s−2 P (H | H , µ = µ) = s−1 1:s−2 ∗ P (µ = µ | H ) P (H | H ) ∗ 1:s−2 s−1 1:s−2 ∝ P (H | H , µ = µ) P (µ = µ | H ) , s−1 1:s−2 ∗ ∗ 1:s−2 (cid:124) (cid:123)(cid:122) (cid:125) f1(µ) where P (µ = µ | H ) is the meta-posterior in task s − 1. The last step follows from the fact that ∗ 1:s−2 P (H | H ) is constant in µ. Now we focus on f (µ) above and rewrite it as s−1 1:s−2 1 (cid:88) f (µ) = P (H , θ = θ | H , µ = µ) 1 s−1 s−1,∗ 1:s−2 ∗ θ (cid:88) = P (H | H , θ = θ, µ = µ) P (θ = θ | H , µ = µ) s−1 1:s−2 s−1,∗ ∗ s−1,∗ 1:s−2 ∗ θ (cid:88) = P (H | H , θ = θ) P (θ = θ | µ = µ) . s−1 1:s−2 s−1,∗ s−1,∗ ∗ (cid:124) (cid:123)(cid:122) (cid:125) θ f2(θ) In the last step, we use that the history H is independent of µ given H and θ , and that s−1 ∗ 1:s−2 s−1,∗ the task parameter θ is independent of H given µ . s−1,∗ 1:s−2 ∗ 14
Now we focus on f (θ) above. To simplify notation, it is useful to define Y = Y (A ) and 2 t s−1,t s−1,t A = A . Then we can rewrite f (θ) as t s−1,t 2 n (cid:89) f (θ) = P (A , Y | H , θ = θ) 2 t t 1:s−1,t s−1,∗ t=1 n (cid:89) = P (Y | A , H , θ = θ) P (A | H , θ = θ) t t 1:s−1,t s−1,∗ t 1:s−1,t s−1,∗ t=1 n (cid:89) = P (Y | A , θ = θ) P (A | H ) ∝ P (Y | A , θ = θ) . t t s−1,∗ t 1:s−1,t 1:n 1:n s−1,∗ t=1 In the third equality, we use that the reward Y is independent of history H given the pulled t 1:s−1,t arm A and task parameter θ , and that A is independent of θ given H . In the last t s−1,∗ t s−1,∗ 1:s−1,t step, we use that P (A | H ) is constant in θ. t 1:s−1,t Finally, we combine all above claims, note that P (θ = θ | µ = µ) = P (θ = θ | µ = µ) = P (θ; µ) , s,∗ ∗ s−1,∗ ∗ and get (cid:88) P (θ = θ | H ) = P (θ; µ) P (µ = µ | H ) , s,∗ 1:s−1 ∗ 1:s−1 µ (cid:88) P (µ = µ | H ) = P (Y | A , θ = θ) P (θ; µ) P (µ = µ | H ) . ∗ 1:s−1 1:n 1:n s−1,∗ ∗ 1:s−2 θ These are the claims that we wanted to prove, since P (θ) = P (θ = θ | H ) , s s,∗ 1:s−1 Q (µ) = P (µ = µ | H ) , s ∗ 1:s−1 L (θ) = P (Y | A , θ = θ) . s−1 1:n 1:n s−1,∗ This concludes the proof. Lemma 7. Fix integers s and n, features (x ) , and consider a generative process (cid:96),t (cid:96)∈[s],t∈[n] µ ∼ N (µ , Σ ) , ∗ q q ∀(cid:96) ∈ [s] : θ | µ ∼ N (µ , Σ ) , (cid:96),∗ ∗ ∗ 0 ∀(cid:96) ∈ [s], t ∈ [n] : Y | µ ∼ N (x(cid:62) θ , σ2) , (cid:96),t ∗ (cid:96),t (cid:96),∗ where all variables are drawn independently. Then µ | (Y ) ∼ N (µˆ, Σˆ ) for ∗ (cid:96),t (cid:96)∈[s], t∈[n] µˆ = Σˆ (cid:32) Σ−1µ + (cid:88)s B (cid:96) − G (cid:96) (cid:18) Σ−1 + G (cid:96) (cid:19)−1 B (cid:96) (cid:33) , q q σ2 σ2 0 σ2 σ2 (cid:96)=1 Σˆ −1 = Σ−1 + (cid:88)s G (cid:96) − G (cid:96) (cid:18) Σ−1 + G (cid:96) (cid:19)−1 G (cid:96) , q σ2 σ2 0 σ2 σ2 (cid:96)=1 where G = (cid:80)n x x(cid:62) is the outer product of the features in task (cid:96) and B = (cid:80)n x Y is (cid:96) t=1 (cid:96),t (cid:96),t (cid:96) t=1 (cid:96),t (cid:96),t their sum weighted by observations. Proof. The claim is proved in Appendix D of Kveton et al. [28]. We restate it for completeness. 15
B Proofs for Section 4.1: Generic Regret Bound B.1 Preliminaries and Omitted Definitions Notation for History: Let us recall that H = ((A , Y ), . . . , (A , Y )) denote the s,t s,1 s,1 s,t−1 s,t−1 events in task s upto and excluding round t for all t ≥ 1 (H = ∅). The events in task s is denoted s,1 as H = H and all the events upto and including stage s is denoted as H = ∪s H . s s,n+1 1:s s(cid:48)=1 s(cid:48) Let us also define history upto and excluding round t in task s as H = {H ∪ H }, with 1:s,t 1:s−1 s,t H = H . Given the history upto and excluding round t in task s, the conditional probability 1:s 1:s,n+1 is given as P (·) = P[· | H ], and the conditional expectation is given as E (·) = E[· | H ]. s,t 1:s,t s,t 1:s,t Note P[·] and E[·] denote the unconditional probability and expectation, respectively. History dependent Entropy and Mutual Information: We now define the entropy and mutual information terms as a function of history. The mutual information between the parameter θ , and the action (A ) and reward (Y ) at the s,∗ s,t s,t beginning of round t in task s, for any s ≤ m and t ≤ n, as a function of history is defined as (cid:20) (cid:18) P (θ , Y , A ) (cid:19)(cid:21) I (θ ; A , Y ) = E log s,t s,∗ s,t s,t s,t s,∗ s,t s,t s,t P (θ )P (Y , A ) s,t s,∗ s,t s,t s,t We also define the mutual information between the parameter µ , and the action (A ) and reward ∗ s,t (Y ) at the beginning of round t in task s, for any s ≤ m and t ≤ n as s,t (cid:20) (cid:18) P (µ , Y , A ) (cid:19)(cid:21) I (µ ; A , Y ) = E log s,t ∗ s,t s,t s,t ∗ s,t s,t s,t P (µ )P (Y , A ) s,t ∗ s,t s,t s,t Further, the history dependent conditional mutual information between (µ , θ ), and A and Y , ∗ s,∗ s,t s,t namely I (θ , µ ; A , Y ), is defined below. s,t s,∗ ∗ s,t s,t (cid:20) (cid:18) P (θ , µ , Y , A ) (cid:19)(cid:21) I (θ , µ ; A , Y ) = E log s,t s,∗ ∗ s,t s,t s,t s,∗ ∗ s,t s,t s,t P (θ , µ )P (Y , A ) s,t s,∗ ∗ s,t s,t s,t Finally, we define the history dependent conditional mutual information between θ , and A and s,∗ s,t Y given µ as I (θ ; A , Y | µ ). s,t ∗ s,t s,∗ s,t s,t ∗ (cid:20) (cid:18) P (θ , Y , A | µ ) (cid:19)(cid:21) I (θ ; A , Y | µ ) = E log s,t s,∗ s,t s,t ∗ s,t s,∗ s,t s,t ∗ s,t P (θ | µ )P (Y , A | µ ) s,t s,∗ ∗ s,t s,t s,t ∗ The conditional entropy terms are defined as follows: h (θ ) = E [− log (P (θ ))] , s,t s,∗ s,t s,t s,∗ h (µ ) = E [− log (P (µ ))] , s,t ∗ s,t s,t ∗ h (θ | µ ) = E [− log (P (θ | µ ))] . s,t s,∗ ∗ s,t s,t s,∗ ∗ Therefore, all the different mutual information terms I (·; A , Y ), and the entropy terms h (·) s,t s,t s,t s,t are random variables that depends on the history H . 1:s,t We next state some entropy and mutual information relationships which we will use later. Proposition 8. For all s, t, and any history H , the following hold 1:s,t I (θ , µ ; A , Y ) = I (µ ; A , Y ) + I (θ ; A , Y | µ ) , s,t s,∗ ∗ s,t s,t s,t ∗ s,t s,t s,t s,∗ s,t s,t ∗ I (θ ; A , Y ) = h (θ ) − h (θ ) . s,t s,∗ s,t s,t s,t s,∗ s,t+1 s,∗ History Independent Entropy and Mutual Information: The history independent conditional mutual information and entropy terms are then given by taking expectation over the possible histories I(·; A , Y | H ) = E[I (·; A , Y )], h(· | H ) = E[h (·)] s,t s,t 1:s,t s,t s,t s,t 1:s,t s,t I(·; A , Y | µ , H ) = E[I (·; A , Y | µ )], h(· | µ , H ) = E[h (· | µ )] s,t s,t ∗ 1:s,t s,t s,t s,t ∗ ∗ 1:s,t s,t ∗ 16
An important quantity that will play a pivotal role in our regret decomposition is the conditional mutual information of the meta-parameter given the entire history, which is expressed as m n m n (cid:88) (cid:88) (cid:88) (cid:88) I(µ ; H ) = I(µ ; A , Y | H ) = E I (µ ; A , Y ). ∗ 1:m ∗ s,t s,t 1:s,t s,t ∗ s,t s,t s=1 t=1 s=1 t=1 The first equality is due to chain rule of mutual information, where at each round the new history H = H ∪ (A , Y ). 1:s,t+1 1:s,t s,t s,t Similarly, in each stage s, the mutual information between parameter θ and the events in stage s, s,∗ i.e. H , conditioned on µ and history up to task (s − 1) is key in quantifying the local regret of task s ∗ s. Which is again expressed as n n (cid:88) (cid:88) I(θ ; H | µ , H ) = I(θ ; A , Y | µ , H ) = E I (θ ; A , Y | µ ). s,∗ s ∗ 1:s−1 s,∗ s,t s,t ∗ 1:s,t s,t s,∗ s,t s,t ∗ t=1 t=1 The first inequality again follows chain rule of mutual information with new history being the combination of old history, and the action and the observed reward in the current round. We further have the relation of mutual information and conditional entropy as I(θ ; H | µ , H ) = h(θ | µ , H ) − h(θ | µ , H ) , s,∗ s ∗ 1:s−1 s,∗ ∗ 1:s−1 s,∗ ∗ 1:s I(µ ; H ) = h(µ ) − h(µ | H ) . ∗ 1:m ∗ ∗ 1:m Weyl’s Inequalities: In this paper, the matrices under consideration are all Positive Semi-definite (PSD) and symmetric. Thus, the eignevalues are non-negative and admits a total order. We denote the eigenvalues of a PSD matrix A ∈ Rd, for any integer d ≥ 1, as λ (A) ≤ · · · ≤ λ (A); where λ (A) d 1 1 is the maximum eigenvalue, and λ (A) is the minimum eigenvalue of the PSD matrix A. d Weyl’s inequality states for two Hermitian matrices (PSD and Symmetric in reals) A and B, λ (A) + λ (B) ≤ λ (A + B) ≤ λ (A) + λ (B), ∀ j + k − d ≥ i ≥ r + s − 1. j k i r s The two important relations, derived from Weyl’s inequality, that we frequently use in the proofs are given next. For PSD and symmetric matrices {A } we have i (cid:88) (cid:88) (cid:88) (cid:88) λ ( A ) ≤ λ (A ), and λ ( A ) ≥ λ (A ). 1 i 1 i d i d i i i i i 17
Lemma 2. Suppose that (3) holds for all s ∈ [m] and t ∈ [n], for some Γ , (cid:15) ≥ 0. In addition, s,t s,t let (Γ ) and Γ be non-negative constants such that Γ ≤ Γ ≤ Γ holds for all s ∈ [m] and s s∈[m] s,t s t ∈ [n] almost surely. Then (cid:112) (cid:88)m (cid:113) (cid:88)m (cid:88)n R(m, n) ≤ Γ mnI(µ ; H ) + Γ nI(θ ; H | µ , H ) + E [(cid:15) ] . ∗ 1:m s s,∗ s ∗ 1:s−1 s,t s=1 s=1 t=1 Proof. The proof follows through the series of inequalities below (explanation added). (cid:88) R(m, n) = E [∆ ] s,t s,t (cid:88) (cid:113) (cid:88) [Eq. (3)] ≤ E Γ I (θ ; A , Y ) + E (cid:15) s,t s,t s,∗ s,t s,t s,t s,t s,t (cid:88) (cid:113) (cid:88) [I(X; Z) ≤ I(X, Y ; Z)] ≤ E Γ I (θ , µ ; A , Y ) + E (cid:15) s,t s,t s,∗ ∗ s,t s,t s,t s,t s,t (cid:88) (cid:113) (cid:88) [Chain Rule] = E Γ I (µ ; A , Y ) + I (θ ; A , Y | µ ) + E (cid:15) s,t s,t ∗ s,t s,t s,t s,∗ s,t s,t ∗ s,t s,t s,t √ √ √ (cid:88) (cid:113) (cid:88) (cid:113) [ a + b≤ a+ b] ≤ E Γ I (µ ; A , Y ) + E Γ I (θ ; A , Y | µ ) s,t s,t ∗ s,t s,t s,t s,t s,∗ s,t s,t ∗ s,t s,t (cid:88) + E (cid:15) s,t s,t (cid:34) (cid:35) (cid:88) (cid:113) (cid:88) (cid:88) (cid:113) [Γ ≤ Γ ≤ Γ, ∀s, t, w.p. 1] ≤ Γ E I (µ ; A , Y ) + Γ E I (θ ; A , Y | µ ) s,t s s,t ∗ s,t s,t s s,t s,∗ s,t s,t ∗ s,t s t (cid:88) + E (cid:15) s,t s,t (cid:88) (cid:113) (cid:88) (cid:88) (cid:113) [Jensen’s Inequality] ≤ Γ EI (µ ; A , Y ) + Γ EI (θ ; A , Y | µ ) s,t ∗ s,t s,t s s,t s,∗ s,t s,t ∗ s,t s t (cid:88) + E (cid:15) s,t s,t (cid:115) (cid:115) (cid:88) (cid:88) (cid:88) [Cauchy-Schwarz] ≤ Γ mn EI (µ ; A , Y ) + Γ n EI (θ ; A , Y | µ ) s,t ∗ s,t s,t s s,t s,∗ s,t s,t ∗ s,t s t (cid:88) + E (cid:15) s,t s,t (cid:112) (cid:88) (cid:113) (cid:88) [Chain Rule] = Γ mnI(µ ; H ) + Γ nI(θ ; H | µ , H ) + E (cid:15) ∗ 1:m s s,∗ s ∗ 1:s−1 s,t s s,t - The first inequality follows due to Eq. (3). - The second inequality uses the fact that I(X; Z) ≤ I(X, Y ; Z) for any random variables X, Y , and Z. Here X = θ , Y = µ , and Z = (A , Y ). s,∗ ∗ s,t s,t - The second equality uses the chain rule I(X, Y ; Z) = I(X; Z) + I(X; Z | Y ), as stated in Proposition 8, with the same random variables X, Y , and Z. √ - The Jensen’s inequality uses concavity of ·. 18
C Proofs for Section 4.2: Linear Bandit C.1 Marginalization of the Variables Notation in Marginalization: Let N (x; µ, Σ) denote a (possibly multivariate) Gaussian p.d.f. with mean µ and covariance matrix Σ for variable x. We now recall the notations of posterior distributions at different time of our algorithm P (θ; µ) = P (θ = θ | µ = µ) = N (θ; µ, Σ ), Q(µ) = P (µ = µ) = N (µ; µ , Σ ) s,∗ ∗ 0 ∗ 0 q (cid:90) P (θ) = P (θ = θ | H ) = P (θ; µ)Q (µ) dµ, s s,∗ 1:s−1 s µ P (θ) = P (θ = θ | H ) ∝ P (H | θ = θ) P (θ), s,t s,∗ 1:s,t s,t s,∗ s (cid:90) Q (µ) = P (µ = µ | H ) = P (H | θ = θ) P (θ; µ) dθQ (µ) s ∗ 1:s−1 s−1 s−1,∗ s−1 θ The marginalization is proved in an inductive manner due to the dependence of the action matrix A on the history. We recall the expression of the rewards, Y = AT θ + w s,t s,t s,∗ s,t In each round t and task s, given the parameter θ and the action A , the reward Y has the s,∗ s,t s,t p.d.f. P(Y | θ , A ) = N (Y ; AT θ , σ2). Let ∝ denote that the proportionality constant s,t s,∗ s,t s,t s,t s,∗ X is independent of X (possibly a set). We obtain the posterior probability of the true parameter in task s in round t, given the true parameter µ . Let us define for all s ≤ m, and t ≤ n. ∗ t−1 (cid:89) P (θ) = P(θ = θ | µ , H ) ∝ P(Y | θ = θ, A )P (θ, µ ) s,t,µ∗ s,∗ ∗ 1:s,t s,t(cid:48) s,∗ s,t(cid:48) ∗ t(cid:48)=1 ∝ t (cid:89)−1 exp (cid:32) − (Y s,t(cid:48) − AT s,t(cid:48)θ)2 (cid:33) N (θ; µ , Σ ) θ 2σ2 ∗ 0 t(cid:48)=1 (cid:32) t−1 (cid:33) ∝ exp − (cid:88) (θ − A Y )T A s,t(cid:48)AT s,t(cid:48) (θ − A Y ) N (θ; µ , Σ ) θ s,t(cid:48) s,t(cid:48) 2σ2 s,t(cid:48) s,t(cid:48) ∗ 0 t(cid:48)=1 (cid:32) t−1 (cid:33) (cid:34) t−1 t−1 (cid:35) ∝ exp − (cid:0) θ − θ¯(cid:1)T (cid:88) A s,t(cid:48)AT s,t(cid:48) (cid:0) θ − θ¯(cid:1) N (θ; µ , Σ ) θ¯ = ((cid:88) A s,t(cid:48)AT s,t(cid:48) )−1 (cid:88) A Y θ 2σ2 ∗ 0 σ2 s,t(cid:48) s,t(cid:48) t(cid:48)=1 t(cid:48)=1 t(cid:48)=1 (cid:32) (cid:32) t−1 (cid:33) (cid:33) (cid:34) t−1 (cid:35) ∝ N θ; Σˆ Σ−1µ + (cid:88) A Y , Σˆ Σˆ −1 = Σ−1 + (cid:88) A s,t(cid:48)AT s,t(cid:48) θ s,t,µ∗ 0 ∗ s,t(cid:48) s,t(cid:48) s,t,µ∗ s,t,µ∗ 0 σ2 t(cid:48)=1 t(cid:48)=1 We now obtain the posterior probability of the true parameter in task s in round t as by taking integral over the prior of the parameter µ . ∗ t−1 (cid:90) (cid:89) P (θ) = P(θ = θ | H ) ∝ P(Y | θ = θ, A ) P (θ, µ)Q (µ)dµ s,t s,∗ 1:s,t s,t(cid:48) s,∗ s,t(cid:48) s t(cid:48)=1 µ ∝ t (cid:89)−1 exp (cid:32) − (Y s,t(cid:48) − AT s,t(cid:48)θ)2 (cid:33) (cid:90) N (θ; µ, Σ )N (µ; µˆ , Σˆ )dµ θ 2σ2 0 s s t(cid:48)=1 µ (cid:32) t−1 (cid:33) ∝ exp − (cid:88) (θ − A Y )T A s,t(cid:48)AT s,t(cid:48) (θ − A Y ) N (θ; µˆ , Σ + Σˆ ) θ s,t(cid:48) s,t(cid:48) 2σ2 s,t(cid:48) s,t(cid:48) s 0 s t(cid:48)=1 (cid:32) t−1 (cid:33) ∝ exp − (cid:88) (θ − A Y )T A s,t(cid:48)AT s,t(cid:48) (θ − A Y ) N (θ; µˆ , Σ + Σˆ ) θ s,t(cid:48) s,t(cid:48) 2σ2 s,t(cid:48) s,t(cid:48) s 0 s t(cid:48)=1 19
(cid:32) t−1 (cid:33) (cid:34) t−1 t−1 (cid:35) ∝ exp − (cid:0) θ − θ¯(cid:1)T (cid:88) A s,t(cid:48)AT s,t(cid:48) (cid:0) θ − θ¯(cid:1) N (θ; µˆ , Σ + Σˆ ) θ¯ = ((cid:88) A s,t(cid:48)AT s,t(cid:48) )−1 (cid:88) A Y θ 2σ2 s 0 s σ2 s,t(cid:48) s,t(cid:48) t(cid:48)=1 t(cid:48)=1 t(cid:48)=1 (cid:32) t−1 t−1 t−1 (cid:33) ∝ N θ; ((cid:88) A s,t(cid:48)AT s,t(cid:48) )−1 (cid:88) A Y , ((cid:88) A s,t(cid:48)AT s,t(cid:48) )−1 N (θ; µˆ , Σ + Σˆ ) θ σ2 s,t(cid:48) s,t(cid:48) σ2 s 0 s t(cid:48)=1 t(cid:48)=1 t(cid:48)=1 (cid:32) (cid:32) t−1 (cid:33) (cid:33) (cid:34) t−1 (cid:35) ∝ N θ; Σˆ (Σ + Σˆ )−1µˆ + (cid:88) A Y , Σˆ Σˆ −1 = (Σ + Σˆ )−1 + (cid:88) A s,t(cid:48)AT s,t(cid:48) θ s,t 0 s s s,t(cid:48) s,t(cid:48) s,t s,t 0 s σ2 t(cid:48)=1 t(cid:48)=1 (cid:16) (cid:17) Thus, for µˆ = Σˆ (Σ + Σˆ )−1µˆ + (cid:80)t−1 A Y , the parameter conditioned on the s,t s,t 0 s s t(cid:48)=1 s,t(cid:48) s,t(cid:48) history is distributed as θ | H ∼ N (µˆ , Σˆ ). s,∗ 1:s,t s,t s,t We now compute the posterior of the meta-parameter µ in a similar way, but some of the computation ∗ can be avoided by using Lemma 7. (cid:90) Q (µ) = P(H | θ = θ)P (θ; µ)dθQ (µ) s+1 s s,∗ s θ (cid:90) n (cid:89) ∝ P(Y | θ = θ, A )P (θ, µ)dθQ (µ) θ,µ s,t s,∗ s,t s θ t=1 s (cid:90) n (cid:89) (cid:89) ∝ P(Y | θ = θ , A )P (θ , µ)dθ Q (µ) θ,µ (cid:96),t (cid:96),∗ (cid:96) (cid:96),t (cid:96) s 0 (cid:96)=1 θ(cid:96) t=1 = N (µˆ , Σˆ ) s+1 s+1 The second equality is obtained by expanding out the Q (µ) expressions iteratively, and using the s fact that Q (µ) is the prior distribution of µ at the beginning. The final equality follows from the 0 application of Lemma 7, by observing that the expression describes a setting identical to the setting therein, with actions x = A for all (cid:96) ∈ [s] and t ∈ [n]. The probability of playing the actions (cid:96),t (cid:96),t A (as oppossed to fixed x in Lemma 7) are absorbed by the proportionality constant. (cid:96),t (cid:96),t Recall that we have due to Lemma 7, for G = (cid:80)n A AT , ∀(cid:96) ∈ [m] and for any s ∈ [m], (cid:96) t=1 (cid:96),t (cid:96),t s−1 s−1 Σˆ −1 = Σ−1 + (cid:88) G(cid:96) − G(cid:96) (cid:0) Σ−1 + G(cid:96) (cid:1)−1 G(cid:96) = Σ−1 + (cid:88) G(cid:96) (cid:0) Σ−1 + G(cid:96) (cid:1)−1 Σ−1. s q σ2 σ2 0 σ2 σ2 q σ2 0 σ2 0 (cid:96)=1 (cid:96)=1 Further, if in task (cid:96) if forced exploration is used, then G is invertible, and using Woodbury matrix (cid:96) identity we have G σ2(cid:96) (cid:0) Σ− 0 1 + G σ2(cid:96) (cid:1)−1 Σ− 0 1 = (cid:0) Σ 0 + ( G σ2(cid:96) )−1(cid:1)−1 . 20
C.2 Proof of Lemma 4 Lemma 4. For any H -adapted action sequence and any s ∈ [m], we have 1:s,t (cid:16) (cid:17) (cid:16) (cid:17) I(θ ; H | µ , H ) ≤ d log 1 + λ1(Σ0)n , I(µ ; H ) ≤ d log 1 + λ1(Σq)m . s,∗ s ∗ 1:s−1 2 σ2 ∗ 1:m 2 λd(Σ0)+σ2/n Proof. We obtain the conditional mutual entropy of θ given the history upto (s − 1)-th task and θ s,∗ (similar to Lu et al.[33]) I(θ ; H | µ , H ) = h(θ | µ , H ) − h(θ | µ , H ) s,∗ s ∗ 1:s−1 s,∗ ∗ 1:s−1 s,∗ ∗ 1:s = E[h (θ | µ )] − E[h (θ | µ )] s−1,n+1 s,∗ ∗ s,n+1 s,∗ ∗ = 1 log(det(2πeΣ )) − E[ 1 log(det(2πeΣˆ ))] 2 0 2 s,n,µ∗ = 1 E[log(det(Σ ) det(Σˆ −1 ))] 2 0 s,n,µ∗ (cid:34) d (cid:35) = 1 E (cid:89) λ (Σ )λ (Σˆ −1 ))) 2 i 0 i s,n,µ∗ i=1 (cid:32) d (cid:18) (cid:19)(cid:33) (cid:89) 1 n ≤ 1 log λ (Σ ) + 2 i 0 λ (Σ ) σ2 i 0 i=1 (cid:18) (cid:19) λ (Σ ) ≤ d log 1 + n 1 0 2 σ2 The first inequality follows from the definition of conditional mutual information (here we have outer expectation). Using the relation between the history-independent and history-dependent entropy terms we obtain the second inequality. Note that h (θ | µ ) is independent of history, as s−1,n+1 s,∗ ∗ the θ given µ does not depend on old tasks. s,∗ ∗ For the first inequality, we derive the following history independent bound. (cid:32) n (cid:33) λ (Σˆ −1 ) = λ Σ−1 + 1 (cid:88) A AT i s,n,µ∗ i 0 σ2 s,t(cid:48) s,t(cid:48) t(cid:48)=1 ≤ λ (cid:0) Σ−1(cid:1) + λ (cid:32) (cid:88)n A s,t(cid:48)AT s,t(cid:48) (cid:33) i 0 1 σ2 t(cid:48)=1 ≤ 1 + tr (cid:32) (cid:88)n A s,t(cid:48)AT s,t(cid:48) (cid:33) λ (Σ ) σ2 i 0 t(cid:48)=1 1 n ≤ + λ (Σ ) σ2 i 0 The matrices 1 (cid:80)n A AT , and Σ−1 are Hermitian matrices, giving us the first inequality by σ2 t(cid:48)=1 s,t(cid:48) s,t(cid:48) 0 applicaiton of Weyl’s inequality. The last inequality first uses linearity of trace, and tr(A AT ) = s,t(cid:48) s,t(cid:48) tr(AT A ) ≤ 1, by Assumption 1. s,t(cid:48) s,t(cid:48) Similarly, we derive the mutual information of the meta-parameter of θ given the history as follows I(µ ; H ) = h(µ ) − h(µ | H ) ∗ 1:m ∗ ∗ 1:m = h(µ ) − E[h (µ )] ∗ m,n+1 ∗ = 1 log(det(2πeΣ )) − E[ 1 log(det(2πeΣˆ ))] 2 q 2 m+1 = 1 E[log(det(Σ ) det(Σˆ −1 ))] 2 q m+1 (cid:18) (cid:19) mnλ (Σ ) ≤ d log 1 + 1 q 2 nλ (Σ ) + σ2 d 0 For the final inequality above, we derive a history independent bounds in a similar manner.  m (cid:32) n (cid:33)−1 λ i(Σˆ − m1 +1) ≤ λ i(Σ− q 1) + λ 1  (cid:88) Σ 0 + ((cid:88) A s(cid:48),t σ(cid:48)A 2 T s(cid:48),t(cid:48) )−1  s(cid:48)=1 t(cid:48)=1 21
m (cid:32) n (cid:33)−1 ≤ λ i(Σ− q 1) + (cid:88) λ 1  Σ 0 + ((cid:88) A s(cid:48),t σ(cid:48)A 2 T s(cid:48),t(cid:48) )−1  s(cid:48)=1 t(cid:48)=1 m (cid:32) n (cid:33) ≤ 1 + (cid:88) λ−1 Σ + ((cid:88) A s(cid:48),t(cid:48)AT s(cid:48),t(cid:48) )−1 λ (Σ ) d 0 σ2 i q s(cid:48)=1 t(cid:48)=1 m (cid:32) (cid:32) n (cid:33)(cid:33)−1 ≤ 1 + (cid:88) λ (Σ ) + λ ((cid:88) A s(cid:48),t(cid:48)AT s(cid:48),t(cid:48) )−1 λ (Σ ) d 0 d σ2 i q s(cid:48)=1 t(cid:48)=1 m (cid:32) n (cid:33)−1 ≤ 1 + (cid:88) λ (Σ ) + λ−1((cid:88) A s(cid:48),t(cid:48)AT s(cid:48),t(cid:48) ) λ (Σ ) d 0 1 σ2 i q s(cid:48)=1 t(cid:48)=1 m ≤ 1 + (cid:88) (cid:16) λ (Σ ) + σ2 (cid:17)−1 = 1 + mn λ (Σ ) d 0 n λ (Σ ) nλ (Σ ) + σ2 i q i q d 0 s(cid:48)=1 C.3 Proof of Lemma 3 Lemma 3. For all tasks s ∈ [m], rounds t ∈ [n], and any δ ∈ (0, 1], (3) holds almost surely for (cid:115) σ2 (Σˆ ) (cid:113) Γ = 4 max s,t log(4|A|/δ) , (cid:15) = 2δσ2 (Σˆ ) + 2E E [(cid:107)θ (cid:107) ] , s,t log(1 + σ2 (Σˆ )/σ2) s,t max s,t s,t s,t s,∗ 2 max s,t where E is the indicator of forced exploration in round t of task s. Moreover, for each task s, the s,t following history-independent bound holds almost surely,  λ (Σ ) (cid:16) 1 + σ2 (cid:17)  σ m2 ax(Σˆ s,t) ≤ λ 1(Σ 0) 1 + λ (Σ1 )q + σ2/η η +λ1 s( λΣ0 () Σ )  . (5) 1 0 1 q Proof. We next derive the confidence interval bounds, similar to Lu et al. [33], for the reward Y s,t around it’s mean conditioned on the history H ∪ H . Let θˆ be the parameter sampled by s−1 s,t−1 s,t TS in task s and round t, when we do not have forced exploration. E [∆ ] = E [AT θ − AT θ ] = E [AT θˆ − AT θ ] s,t s,t s,t s,∗ s,∗ s,t s,∗ s,t s,t s,t s,t s,∗ d The last equality holds as for Thompson sampling (= denotes equal distribution) AT θ | H =d AT θˆ | H . s,∗ s,∗ 1:s,t s,t s,t 1:s,t When for task s and round t we have forced exploration the bound is given as E [∆ ] = E [AT θˆ − AT θ ] + E [AT θ − AT θˆ ] s,t s,t s,t s,t s,t s,t s,∗ s,t s,∗ s,∗ s,t s,t ≤ E [AT θˆ − AT θ ] + 2E [max |aT θ |] s,t s,t s,t s,t s,∗ s,t s,∗ a∈A ≤ E [AT θˆ − AT θ ] + 2E [(cid:107)θ (cid:107) ]. s,t s,t s,t s,t s,∗ s,t s,∗ 2 In the second last inequality we use the fact that θ | H =d θˆ | H . s,∗ 1:s,t s,t 1:s,t Recall Y (a) denote the reward obtained by taking action a in task s and round t. Also recall that s,t θˆ | H ∼ N (µˆ , Σˆ ). Let us consider the set s,t 1:s,t s,t s,t (cid:113) Θ = {θ :| aT θ − aT θˆ |≤ Γs,t I (θ ; a, Y (a)), ∀a ∈ A}. s,t s,t 2 s,t s,∗ s,t 22
The history dependent conditional mutual entropy of θ given the history H ∪ H (not µ ) s,∗ s−1 s,t ∗ (which will be useful in deriving concentration bounds) as I (θ ; A , Y ) = h (θ ) − h (θ ) s,t s,∗ s,t s,t s,t s,∗ s,t+1 s,∗ = 1 log(det(2πe(Σˆ ))) − 1 log(det(2πeΣˆ )) 2 s,t−1 2 s,t = 1 log(det(Σˆ Σˆ −1)) 2 s,t−1 s,t (cid:18) (cid:18) (cid:19)(cid:19) = 1 log det I + Σˆ As,tAT s,t 2 s,t−1 σ2 (cid:18) (cid:18) (cid:19)(cid:19) = 1 log det 1 + AT s,tΣˆ s,t−1As,t 2 σ2 The last step above uses Matrix determinant lemma.1 Recall that σ2 (Σˆ ) = max aT Σˆ a max s,t a∈A s,t for all s ≤ m and t ≤ n. For δ ∈ (0, 1], let (cid:115) σ2 (Σˆ ) Γ = 4 max s,t−1 log( 4|A| ). s,t log(1 + σ2 (Σˆ )/σ2) δ max s,t−1 Now it follows from Lu et al. [33] Lemma 5 that for the Γ defined as above we have s,t P (θˆ ∈ Θ ) ≥ 1 − δ/2. s,t s,t s,t We continue with the regret decomposition as E [∆ ] s,t s,t (cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105) = E 1(θˆ , θ ∈ Θ ) AT θˆ − AT θ + E 1c(θˆ , θ ∈ Θ ) AT θˆ − AT θ s,t s,t s,∗ s,t s,t s,t s,t s,∗ s,t s,t s,∗ s,t s,t s,t s,t s,∗ (cid:34) (cid:35) (cid:88) (cid:113) ≤ E 1(A = a)Γ I (θ ; a, Y (a)) s,t s,t s,t s,t s,∗ s,t a∈A (cid:115) (cid:20)(cid:16) (cid:17)2(cid:21) + P (θˆ or θ ∈/ Θ )E AT θˆ − AT θ s,t s,t s,∗ s,t s,t s,t s,t s,t s,∗ (cid:115) (cid:113) (cid:113) (cid:20)(cid:16) (cid:17)2(cid:21) ≤ Γ I (θ ; A Y ) + P (θˆ or θ ∈/ Θ ) max E aT θˆ − aT θ s,t s,t s,∗ s,t s,t s,t s,t s,∗ s,t s,t s,t s,∗ a∈A (cid:113) (cid:113) ≤ Γ I (θ ; A Y ) + 2δσ2 (Σˆ ) s,t s,t s,∗ s,t s,t max s,t−1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:15)s,t - The left side term in the first inequality uses the definition of Θ . The right side term s,t in the first inequality holds due to Cauchy–Schwarz. In particular, we use E[XY ] ≤ (cid:112)E[X2]E[Y 2] with X = 1c(θˆ , θ ∈ Θ ) and Y = (cid:16) AT θˆ − AT θ (cid:17) . s,t s,∗ s,t s,t s,t s,t s,∗ - The left side term in the second inequality follows steps similar to proof of Lemma 3 in Lu et al. [33]. The right side term in the second inequality maximizes over the possible actions (we can take the max out of the expectation as action A is a function of history upto task s,t s, and round t − 1). The last inequality follows from the following derivation (cid:20)(cid:16) (cid:17)2(cid:21) E aT θˆ − aT θ s,t s,t s,∗ (cid:20) (cid:16) (cid:17)2(cid:21) ≤ E aT (θˆ − µ ) − (θ − µ ) s,t s,t s,t−1 s,∗ s,t−1 ≤ aT (cid:16) E (cid:104) (θˆ − µ )(θˆ − µ )T (cid:105) + E (cid:2) (θ − µ )(θ − µ )T (cid:3)(cid:17) a s,t s,t s,t−1 s,t s,t−1 s,t s,∗ s,t−1 s,∗ s,t−1 ≤ 2aT Σˆ a ≤ 2σ2 (Σˆ ) s,t−1 max s,t−1 1Matrix determinant lemma states that for an invertible square matrix A, and vectors u and v det (cid:0) A + uvT (cid:1) = (cid:0) 1 + vT A−1u(cid:1) det (A) . We use A = I, u = Σˆ A , and v = A /σ2. s,t−1 s,t s,t 23
This conclude the proof of the first part. We first claim that σ2 (Σˆ ) ≤ λ (Σˆ ). Indeed, as (cid:107)a(cid:107) ≤ 1, we have max s,t 1 s,t 2 σ2 (Σˆ ) = max aT Σˆ a ≤ max aT λ (Σˆ )a ≤ λ (Σˆ ). max s,t s,t 1 s,t 1 s,t a∈A a∈A Furthermore, λ (Σˆ ) decreases with s and t (precisely with n(s − 1) + t). To show this we use 1 s,t λ (Σˆ ) = λ−1(Σˆ −1) 1 s,t d s,t (cid:32) t (cid:33) = λ−1 (Σ + Σˆ )−1 + (cid:88) A s,t(cid:48)AT s,t(cid:48) d 0 s σ2 t(cid:48)=1 (cid:32) t−1 (cid:33) ≤ λ−1 (Σ + Σˆ )−1 + (cid:88) A s,t(cid:48)AT s,t(cid:48) d 0 s σ2 t(cid:48)=1 = λ−1(Σˆ −1 ) = λ (Σˆ ) d s,t−1 1 s,t−1 The inequality holds due to Weyl’s inequality and As,tAT s,t being a PSD matrix. In particular, we σ2 have λ (A + B) ≥ λ (A) + λ (B), given A and B are Hermitian. Thus d d d λ−1(A + B) ≤ (λ (A) + λ (B))−1 ≤ λ−1(A). d d d d Recall in each task s, due to forced exploration, we have λ ((cid:80)n A s,t(cid:48)AT s,t(cid:48) ) ≥ η , where η is d t(cid:48)=1 σ2 σ2 the forced exploration constant. We now prove an upper bound for the term λ (Σˆ ) independent of 1 s action sequences. λ (Σ + Σˆ ) − λ (Σ ) ≤ λ (Σˆ ) = λ−1(Σˆ −1) 1 0 s 1 0 1 s d s  s−1 (cid:32) n (cid:33) (cid:32) n (cid:33)−1  = λ− d 1 Σ− q 1 + (cid:88) (cid:88) A s(cid:48),t σ(cid:48)A 2 T s(cid:48),t(cid:48) Σ− 0 1 + (cid:88) A s(cid:48),t σ(cid:48)A 2 T s(cid:48),t(cid:48) Σ− 0 1  s(cid:48)=1 t(cid:48)=1 t(cid:48)=1  s−1 (cid:32) n (cid:33)−1−1 ≤ λ d(Σ− q 1) + (cid:88) λ d  Σ 0 + ((cid:88) A s(cid:48),t σ(cid:48)A 2 T s(cid:48),t(cid:48) )−1  s(cid:48)=1 t(cid:48)=1  s−1 (cid:32) (cid:32) n (cid:33)(cid:33)−1−1 ≤ λ d(Σ− q 1) + (cid:88) λ 1(Σ 0) + λ 1 ((cid:88) A s(cid:48),t σ(cid:48)A 2 T s(cid:48),t(cid:48) )−1  s(cid:48)=1 t(cid:48)=1 ≤ (cid:0) λ−1(Σ ) + s(λ (Σ ) + σ2/η)−1(cid:1)−1 1 q 1 0 In the above derivation, we use the Weyl’s inequalities multiple times. Note the direction of inequality should be ≤ if there are even number of inverses, whereas it should be ≥ if there are an odd number (cid:80) (cid:80) of inverses associated. The first inequality uses the inequality λ ( A ) ≥ λ (A ) given all the d i i (cid:80) i d (cid:80)i matrices A -s are Hermitian. The second inequality similarly uses λ ( A ) ≤ λ (A ) given i 1 i i i 1 i all the matrices A -s are Hermitian. The final inequality uses the minimum eigenvalue bound when i forced exploration is used. This concludes the second part of the proof, in particular   σ2/η σ m2 ax(Σˆ s,t) ≤ λ 1(Σ 0 + Σˆ s) ≤ λ 1(Σ 0) 1 + λ1λ (Σ1( 0Σ )+q) σ(1 2+ /ηλ +1 s( λΣ 10 () Σ) q)  . C.4 Proof of Theorem 5 Theorem 5 (Linear bandit). The regret of AdaTS is bounded for any δ ∈ (0, 1] as √ R(m, n) ≤ c dmn + (m + c ) R (n; µ ) + c dm , 1 2 δ ∗ 3 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Learning of µ∗ Per-task regret Forced exploration 24
where (cid:115) (cid:16) (cid:17) c = 8 λ1(Σq)+λ1(Σ0) log(4|A|/δ) log 1 + λ1(Σq)m , 1 log(cid:18) 1+ λ1(Σq)+λ1(Σ0) (cid:19) λd(Σ0)+σ2/n σ2 c = (cid:16) 1 + σ2 (cid:17) log m, and c = 2(cid:112) (cid:107)µ (cid:107)2 + tr(Σ + Σ ). The per-task regret is bounded as 2 ηλ1(Σ√0) 3 q 2 q 0 (cid:112) R (n; µ ) ≤ c dn + 2δλ (Σ )n, where δ ∗ 4 1 0 (cid:115) (cid:16) (cid:17) c = 8 λ1(Σ0) log(4|A|/δ) log 1 + λ1(Σ0)n . 4 log(cid:18) 1+ λ1(Σ0) (cid:19) σ2 σ2 Proof. We note that, for each s, we can bound w.p. 1 (cid:118)   (cid:117) σ2/η (cid:117) (cid:117) (cid:117) (cid:117) λ 1(Σ 0) 1 + λ1λ (Σ1( 0Σ )+q) σ(1 2+ /ηλ +1 s( λΣ 10 () Σ) q)  Γ ≤ 4(cid:117) log(4|A|/δ). s,t (cid:117)    (cid:117) σ2/η (cid:117) (cid:116) log 1 + λ1(Σ0) 1 + λ1(Σq)(1+ λ1(Σ0) )  σ2 λ1(Σ0)+σ2/η+sλ1(Σq) This is true by using the upper bounds on σ2 (Σˆ ) in Lemma 3, and because the function max s,t (cid:112) x/ log(1 + ax) for a > 0 increases with x. Similarly, we have (cid:118) (cid:15) s,t ≤ (cid:117) (cid:117) (cid:117) (cid:116)δλ 1(Σ 0)  1 + λ1λ (Σ1( 0Σ )+q) σ(1 2+ /ηλ +σ 1 s2 ( λΣ/ 1η 0 () Σ) q)  . Therefore, we have the bounds Γ ≤ Γ w.p. 1 for all s and t by using appropriate s, and by setting s,t s s = 0 we obtain Γ. We are now at a position to provide the final regret bound. For any δ > 0 (cid:112) (cid:88) (cid:113) (cid:88) R(m, n) ≤ Γ mnI(µ ; H ) + E Γ nI(θ ; H | µ , H ) + E (cid:15) ∗ m s s,∗ s ∗ s−1 s,t s s,t (cid:115) (cid:115) (cid:18) (cid:19) λ (Σ ) + λ (Σ ) mnλ (Σ ) ≤ 4 1 q 1 0 log(4 | A | /δ) mn d log 1 + 1 q log(1 + (λ (Σ ) + λ (Σ ))/σ2) 2 nλ (Σ ) + σ2 1 q 1 0 d 0 (cid:124) (cid:123)(cid:122) (cid:125) regret for learning µ (cid:118)   (cid:117) σ2/η + (cid:88)m 4(cid:117) (cid:117) (cid:117) (cid:117) (cid:117) λ 1(Σ 0)1 + λ1λ (Σ1( 0Σ )+q) σ(1 2+ /ηλ +1 s( λΣ 10 () Σ) q)  log(4|A|/δ)(cid:115) n d log (cid:18) 1 + n λ 1(Σ 0) (cid:19) (cid:117) (cid:117)   σ2/η  2 σ2 s=1 (cid:117) (cid:116) log 1 + λ1(Σ0) 1 + λ1(Σq)(1+ λ1(Σ0) )  σ2 λ1(Σ0)+σ2/η+sλ1(Σq) (cid:118) + (cid:88)m n(cid:117) (cid:117) (cid:117) (cid:116)2δλ 1(Σ 0) 1 + λ1λ (Σ1( 0Σ )+q) σ(1 2+ /ηλ +σ 1 s2 ( λΣ/ 1η 0 () Σ) q)   s=1 (cid:32) m n (cid:33) (cid:88) (cid:88) + 2 E E[(cid:107)θ (cid:107) ] s,t s,∗ 2 s=1 t=1 (cid:115) (cid:115) (cid:18) (cid:19) λ (Σ ) + λ (Σ ) mnλ (Σ ) ≤ 4 1 q 1 0 log(4|A|/δ) mn d log 1 + 1 q log(1 + (λ (Σ ) + λ (Σ ))/σ2) 2 nλ (Σ ) + σ2 1 q 1 0 d 0 (cid:124) (cid:123)(cid:122) (cid:125) regret for learning µ 25
(cid:32) m (cid:33) + m + 1 (cid:88) λ1(Σq)(λ1(Σ0)+σ2/η) × 2λ1(Σ0) λ1(Σ0)+σ2/η+sλ1(Σq) s=1   (cid:118) (cid:115) 4(cid:117) (cid:117) λ 1(Σ 0) log(4|A|/δ) n d log (cid:18) 1 + n λ 1(Σ 0) (cid:19) + n(cid:112) 2λ (Σ )δ  (cid:116) log (cid:16) 1 + λ1(Σ0) (cid:17) 2 σ2 1 0  σ2 (cid:113) + 2md (cid:107)µ (cid:107)2 + tr(Σ + Σ ) q 2 q 0 (cid:115) (cid:115) (cid:18) (cid:19) λ (Σ ) + λ (Σ ) mnλ (Σ ) ≤ 4 1 q 1 0 log(4|A|/δ) mn d log 1 + 1 q log(1 + (λ (Σ ) + λ (Σ ))/σ2) 2 nλ (Σ ) + σ2 1 q 1 0 d 0 (cid:124) (cid:123)(cid:122) (cid:125) regret for learning µ (cid:16) (cid:17) + m + (1 + σ2/η ) log(m) × λ1(Σ0)   (cid:118) (cid:115) 4(cid:117) (cid:117) λ 1(Σ 0) log(4|A|/δ) n d log (cid:18) 1 + n λ 1(Σ 0) (cid:19) + n(cid:112) 2λ (Σ )δ  (cid:116) log (cid:16) 1 + λ1(Σ0) (cid:17) 2 σ2 1 0  σ2 (cid:113) + 2md (cid:107)µ (cid:107)2 + tr(Σ + Σ ) q 2 q 0 The first inequality follows by substituting the appropriate bounds. The second inequality first removes the part highlighted in blue (which is positive) inside the logarithm, and then uses the fact √ that 1 + x ≤ 1 + x/2 for all x ≥ 1. We also use E[(cid:107)θ (cid:107) ] = (cid:112) (cid:107)µ (cid:107)2 + tr(Σ + Σ ) and the s,∗ 2 q 2 q 0 fact that AdaTS explores for d rounds in each task. The final inequality replaces the summation by an integral over s and derives the closed form. 26
D Proofs for Section 4.3: Semi-Bandit In this section, we expand the linear bandit analysis to handle multiple inputs as is common in semi-bandit feedback in combinatorial optimizations. Furthermore, as the rewards for each base-arm are independent for each arm we can improve our analysis providing tighter prior dependent bounds. The center piece of the proof is again the mutual information separation between the meta-parameter and the parameter in each stage. However, in the regret decomposition we sum the confidence intervals of different arms separately. Notations: We recall the necessary notations for the proof of regret upper bound in the semi-bandit setting. For each arm k the meta-parameter µ ∼ N (µ , σ2 ). The mean reward at the beginning ∗,k q,k q,k for each task s, for an arm k is sampled from N (µ , σ2 ). The reward realization of arm k in ∗,k 0,k round t and task s is denoted by Y (k) = θ (k) + w (k) where be the reward of the arm k at s,t s,∗ s,t time t (arm k need not be played during time t). Then the reward obtained for the action a (a subset (cid:80) of [K] with size at most L) is given as Y (a) = Y (k). Let, for each task s and round t, the s,t k∈a s,t action (a subset of [K]) be A , and the observed reward vector be Y = (Y (k) : k ∈ A ). s,t s,t s,t s,t The linear bandits notations for history, conditional probability, and conditional expectation carry forward to semi-bandits. Additionally, let us denote the number of pulls for arm k, in phase s, upto and excluding round t as N (k). The total number of pulls for arm k in task s is denoted as s,t N (k) = N (k), and up to and including task s is denoted by N (t). s s,n+1 1:s Mutual Information in Semi-bandits: The history dependent and independent mutual information terms are defined analogously, but we are now interested in the terms for each arms separately. For any arm k ∈ [K] and action a ⊆ [K], the history dependent mutual information terms of interest are (cid:20) P (θ (k), A , Y | µ ) (cid:21) I (θ (k); A , Y | µ ) = E s,t s,∗ s,t s,t ∗,k s,t s,∗ s,t s,t ∗,k s,t P (θ (k) | µ )P (A , Y | µ ) s,t s,∗ ∗,k s,t s,t s,t ∗,k (cid:20) P (θ (k), Y (k) | µ , A = a) (cid:21) I (θ (k); a, Y (a) | µ ) = E s,t s,∗ s,t ∗,k s,t s,t s,∗ s,t ∗,k s,t P (θ (k) | µ , A = a)P (Y (a) | µ , A = a) s,t s,∗ ∗,k s,t s,t s,t ∗,k s,t (cid:20) P (µ , A , Y ) (cid:21) I (µ ; A , Y ) = E s,t ∗,k s,t s,t s,t ∗,k s,t s,t s,t P (µ )P (A , Y ) s,t ∗,k s,t s,t s,t (cid:20) P (µ , Y (k) | A = a) (cid:21) I (µ ; a, Y (a)) = E s,t ∗,k s,t s,t s,t ∗,k s,t s,t P (θ (k) | A =a)P (Y (a) | A =a) s,t s,∗ s,t s,t s,t s,t The history mutual information independent terms of interest are I(θ (k); A , Y | µ , H ) = E[I (θ (k); A , Y | µ )], s,∗ s,t s,t ∗,k 1:s,t s,t s,∗ s,t s,t ∗,k I(µ ; A , Y | H ) = E[I (µ ; A , Y )]. ∗,k s,t s,t 1:s,t s,t ∗,k s,t s,t We now derive the mutual information of θ (k) and events in task s, i.e. H , given µ , and history s,∗ s ∗,k upto and excluding task s, i.e. H . 1:s−1 Lemma 9. For any k ∈ [K], s ∈ [m], and H adapted sequence of actions ((A )n )m , the 1:s,t s,t t=1 s=1 following statements hold for a (K, L)-Semi-bandit (cid:88) I(θ (k); H | µ , H ) = E P (k ∈ A )I (θ (k); k, Y (k) | µ ) s,∗ s ∗,k 1:s−1 s,t s,t s,t s,∗ s,t ∗,k t (cid:88) (cid:88) I(µ | H ) = E P (k ∈ A )I (µ ; k, Y (k)). ∗,k 1:m s,t s,t s,t ∗,k s,t s t Proof. The proof follows by the application of the chain rule of mutual information, and noticing that the rounds when an arm k was not played the mutual information I (θ (k); k, Y (k) | µ ) s,t s,∗ s,t ∗,k and I (µ ; k, Y (k)) both are zero. This is true because no information is gained about the s,t ∗,k s,t parameters θ (k) and µ in those rounds. s,∗ ∗,k 27
I(θ (k); H | µ , H ) s,∗ s ∗,k 1:s−1 (cid:88) = E I(θ (k); A , Y | µ , H , H ) s,∗ s,t s,t ∗,k 1:s−1 s,t−1 t (cid:88) = E I (θ (k); A , Y | µ ) s,t s,∗ s,t s,t ∗,k t (cid:88) (cid:88) = E P (A = a)I (θ (k); a, Y (a) | µ ) s,t s,t s,t s,∗ s,t ∗,k t a∈A (cid:88) (cid:88) = E P (A = a)1(k ∈ a)I (θ (k); k, Y (k) | µ ) s,t s,t s,t s,∗ s,t ∗,k t a∈A (cid:88) (cid:88) + E P (A = a)I (θ (k); a \ k, Y (a \ k) | µ , (k, Y (k))) s,t s,t s,t s,∗ s,t ∗,k s,t t a∈A (cid:88) = E P (k ∈ A )I (θ (k); k, Y (k) | µ ) s,t s,t s,t s,∗ s,t ∗,k t Here, a \ k implies the action with arm k removed from subset a. Due to the independence of the reward of each arm, for any fixed action a, θ (k) ⊥ (a \ k, Y (a \ k)) conditioned on µ , s,∗ s,t ∗,k (k, Y (k)), and history H . Therefore, we have s,t 1:s,t I (θ (k); a \ k, Y (a \ k) | µ , (k, Y (k))) = 0. s,t s,∗ s,t ∗,k s,t A similar sequence of steps lead to (cid:88) (cid:88) I(µ | H ) = E P (k ∈ A )I (µ ; k, Y (k)). ∗,k 1:m s,t s,t s,t ∗,k s,t s t The above equalities develop the chain rules of mutual information for each of the arms separately, by leveraging the independence of the rewards per arms. Per Task Regret Bound: We derive the per task regret using the information theoretical confidence intervals while accounting for each arm separately. Let the posterior distribution of θ (k) at the s,∗ beginning of round t of task s be N (µˆ (k), σˆ2 (k)) for appropriate µˆ (k) and σˆ2 (k) that depends s,t s,t s,t s,t on the history H , for all k ∈ [K], s ∈ [m], and t ∈ [n]. We will derive these terms or bounds on 1:s,t these terms later. Lemma 10. For an H adapted sequence of actions ((A )n )m , and any δ ∈ (0, 1], the 1:s,t s,t t=1 s=1 expected regret in round t of stage s in a (K, L)-Semi-bandit is bounded as (cid:88) (cid:18) (cid:113) (cid:113) (cid:19) E [∆ ] = P (k ∈ A ) Γ (k) I (θ (k); k, Y (k)) + 2δ 1 σ2 (k) , (6) s,t s,t s,t s,t s,t s,t s,∗ s,t K s,t k∈[K] where (cid:115) σˆ2 (k) Γ (k) = 4 s,t−1 log( 4K ). s,t log(1 + σˆ2 (k)/σ2) δ s,t−1 Proof. Similar to linear bandits we have without forced exploration (cid:88) (cid:88) E [∆ ] = E [ θ (k) − θ (k)] s,t s,t s,t s,∗ s,∗ k∈As,∗ k∈As,t = E [ (cid:88) θˆ (k) − (cid:88) θ (k)] s,t s,t s,∗ k∈As,t k∈As,t = E [(cid:88) 1(A = a) (cid:88) (θˆ (k) − θ (k))] s,t s,t s,t s,∗ a∈A k∈a = E [ (cid:88) 1(k ∈ A )(θˆ (k) − θ (k))] s,t s,t s,t s,∗ k∈[K] = (cid:88) P (k ∈ A )E [θˆ (k) − θ (k)]. s,t s,t s,t s,t s,∗ k∈[K] 28
d The second equality is due to Thompson sampling (= denotes equal distribution) (cid:88) θ (k) | H =d (cid:88) θˆ (k) | H . s,∗ 1:s,t s,t 1:s,t k∈As,∗ k∈As,t When forced exploration is used in some task s and round t we have, θ (k) | H =d θˆ (k) | s,∗ 1:s,t s,t H 1:s,t (cid:88) (cid:88) E [∆ ] = E [ θ (k) − θ (k)] s,t s,t s,t s,∗ s,∗ k∈As,∗ k∈As,t = E [ (cid:88) θˆ (k) − (cid:88) θ (k)] + E [ (cid:88) θ (k) − (cid:88) θˆ (k)] s,t s,t s,∗ s,t s,∗ s,t k∈As,t k∈As,t k∈As,∗ k∈As,t = E [ (cid:88) θˆ (k) − (cid:88) θ (k)] + E [ (cid:88) θ (k) − (cid:88) θˆ (k)] s,t s,t s,∗ s,t s,∗ s,t k∈As,t k∈As,t k∈As,∗ k∈As,t √ (cid:115) ≤ (cid:88) P (k ∈ A )E [θˆ (k) − θ (k)] + 2 KE [ (cid:88) θ2 (k)] s,t s,t s,t s,t s,∗ s,t s,∗ k∈[K] k∈K For each k ∈ [K], for appropriate µˆ (k) and σˆ2 (k) we know that θˆ (k) | H ∼ s,t s,t s,t 1:s,t N (µˆ (k), σˆ2 (k)). We define the confidence set for each arm k at round t of task s, for some s,t s,t Γ (k), which can be a function of H , to be specified late, as s,t 1:s,t (cid:113) Θ (k) = {θ :| θ − µˆ (k) |≤ Γs,t(k) I (θ (k); k, Y (k))}. s,t s,t 2 s,t s,∗ s,t A derivation equivalent to linear bandits, gives us I (θ (k); k, Y (k)) = 1 log (cid:16) 1 + σˆ s2 ,t−1(k) (cid:17) . s,t s,∗ s,t 2 σ2 Because, we only consider arm k we obtain as a corollary of Lemma 5 in Lu et al. [33] that for any k, and any δ 1 > 0 for K (cid:115) σˆ2 (k) Γ (k) = 4 s,t−1 log( 4K ). s,t log(1 + σˆ2 (k)/σ2) δ s,t−1 we have P (θˆ (k) ∈ Θ (k)) ≥ 1 − δ/2K. s,t s,t s,t We proceed with the regret bound as E [θˆ (k) − θ (k)] s,t s,t s,∗ ≤ E [1(θˆ (k), θ (k) ∈ Θ (k))(θˆ (k) − θ (k))] s,t s,t s,∗ s,t s,t s,∗ + E [1c(θˆ (k), θ (k) ∈ Θ (k))(θˆ (k) − θ (k))] s,t s,t s,∗ s,t s,t s,∗ (cid:113) (cid:113) ≤ Γ (k) I (θ (k); k, Y (k)) + P(θˆ (k) or θ (k) ∈/ Θ (k))E [(θˆ (k) − θ (k))2] s,t s,t s,∗ s,t s,t s,∗ s,t s,t s,t s,∗ (cid:113) (cid:113) ≤ Γ (k) I (θ (k); k, Y (k)) + δ 1 E [(θˆ (k) − µˆ (k))2 + (θ (k) − µˆ (k))2] s,t s,t s,∗ s,t K s,t s,t s,t s,∗ s,t (cid:113) (cid:113) ≤ Γ (k) I (θ (k); k, Y (k)) + 2δ 1 σ2 (k) s,t s,t s,∗ s,t K s,t This concludes the proof. Regret Decomposition: We now develop the regret decomposition for the (K, L)-Semi-bandit based on the per step regret characterization in Lemma 10. Lemma 11. Let, for each k ∈ [K], (Γ (k)) and Γ(k) be non-negative constants such that s s∈[m] Γ (k) ≤ Γ (k) ≤ Γ(k) holds for all s ∈ [m] and t ∈ [n] almost surely. Then for any δ ∈ (0, 1] the s,t s 29
regret of AdaTS admits the upper bound √ (cid:115) (cid:88) (cid:115) (cid:88) (cid:16) (cid:17) R(m, n) ≤ mnKL 1 Γ2(k)I(µ ; H ) + 2 mK µ2(k) + σ2 + σ2 K ∗,k 1:m q 0,k q,k k∈[K] k∈K   m √ (cid:115) (cid:115) (cid:88) (cid:88) (cid:88) +  nKL K1 Γ2 s(k)I(θ s,∗(k); H s | µ ∗,k, H 1:s−1) + n 2δ K1 σˆ s2(k) . s=1 k∈[K] k∈[K] Proof. The regret decomposition is computed in the following steps. Recall that E is the indicator s,t if in round t of task s we use exploration. (cid:34) (cid:35) (cid:88) R(m, n) = E ∆ s,t s,t   (cid:88) (cid:88) (cid:18) (cid:113) (cid:19) ≤ E  P s,t(k ∈ A s,t) Γ s,t(k) I s,t(θ s,∗(k); k, Y s,t(k))  s,t k∈[K]     √ (cid:115) (cid:88) (cid:88) (cid:88) (cid:88) + E  P s,t(k ∈ A s,t)(cid:15) s,t(k) + 2E  E s,t KE s,t[ θ s2 ,∗(k)] s,t k∈[K] s,t k∈K   (cid:88) (cid:88) (cid:18) (cid:113) (cid:19) ≤ E  P s,t(k ∈ A s,t) Γ s,t(k) I s,t(θ s,∗(k), µ ∗,k; k, Y s,t(k))  s,t k∈[K]   (cid:88) (cid:88) (cid:113) (cid:115) (cid:88) + E  P s,t(k ∈ A s,t) 2δ K1 σ s2 ,t(k) + 2mK3/2E[ θ s2 ,∗(k)] s,t k∈[K] k∈K   (cid:88) (cid:88) (cid:18) (cid:113) (cid:19) = E  P s,t(k ∈ A s,t) Γ s,t(k) I s,t(θ s,∗(k); k, Y s,t(k) | µ ∗,k) + I s,t(µ ∗,k; k, Y s,t(k))  s,t k∈[K]   (cid:88) (cid:88) (cid:113) (cid:115) (cid:88) (cid:16) (cid:17) + E  P s,t(k ∈ A s,t) 2δ K1 σ s2 ,t(k) + 2mK3/2 µ2 q(k) + σ 02 ,k + σ q2 ,k s,t k∈[K] k∈K   (cid:88) (cid:88) (cid:18)(cid:113) (cid:113) (cid:19) ≤ E  P s,t(k ∈ A s,t)Γ s,t(k) I s,t(θ s,∗(k); k, Y s,t(k) | µ ∗,k) + I s,t(µ ∗,k; k, Y s,t(k))  s,t k∈[K]   (cid:88) (cid:88) (cid:113) (cid:115) (cid:88) (cid:16) (cid:17) + E  P s,t(k ∈ A s,t) 2δ K1 σ s2 ,t(k) + 2mK3/2 µ2 q(k) + σ 02 ,k + σ q2 ,k s,t k∈[K] k∈K (cid:34) (cid:35) (cid:88) (cid:88) (cid:88) (cid:113) ≤ Γ (k) E P (k ∈ A ) I (θ (k); k, Y (k) | µ ) s s,t s,t s,t s,∗ s,t ∗,k s k∈[K] t (cid:34) (cid:35) (cid:88) (cid:88) (cid:113) + Γ(k)E P (k ∈ A ) I (µ ; k, Y (k)) s,t s,t s,t ∗,k s,t k∈[K] s,t (cid:88) (cid:88) (cid:113) (cid:88) (cid:115) (cid:88) (cid:16) (cid:17) + 2δ 1 σˆ2(k)E[ P (k ∈ A )] + 2mK3/2 µ2(k) + σ2 + σ2 K s s,t s,t q 0,k q,k s k∈[K] t k∈K The first inequality follows from the expression for the reward gaps in Equation 6. The next two equations follow due to the chain rule of mutual information, similar to the linear bandit case. The only difference in this case we use the parameters for each arm (θ (k) and µ ) separately. Also, we use s,∗ ∗,k the fact that there are at most mK rounds where forced exploration is used for the m tasks. The next 30
√ √ √ inequality is due to a + b ≤ a + b. The final inequality follows as Γ (k) ≤ Γ (k) ≤ Γ(k), s,t s and σ (k) ≤ σ (k) w.p. 1 for all k ∈ [K], and s ≤ m and t ≤ n. s,t s We now derive the bounds for the sum of the mutual information terms for the per task parameters given the knowledge of the meta-parameter. (cid:34) (cid:35) (cid:88) (cid:88) (cid:88) (cid:113) Γ (k)E P (k ∈ A ) I (θ (k); k, Y (k) | µ ) s s,t s,t s,t s,∗ s,t ∗,k s k∈[K] t (cid:34) (cid:35) (cid:88) (cid:88) (cid:88) (cid:113) (cid:113) = Γ (k)E P (k ∈ A ) P (k ∈ A )I (θ (k); k, Y (k) | µ ) s s,t s,t s,t s,t s,t s,∗ s,t ∗,k s k∈[K] t   (cid:115) (cid:115) (cid:88) (cid:88) (cid:88) (cid:88) ≤ Γ s(k)E  P s,t(k ∈ A s,t) P s,t(k ∈ A s,t)I s,t(θ s,∗(k); k, Y s,t(k) | µ ∗,k) s k∈[K] t t (cid:115) (cid:115) (cid:88) (cid:88) (cid:88) (cid:88) ≤ Γ (k) E P (k ∈ A ) E P (k ∈ A )I (θ (k); k, Y (k) | µ ) s s,t s,t s,t s,t s,t s,∗ s,t ∗,k s k∈[K] t t (cid:88) (cid:88) (cid:112) (cid:113) = Γ (k) EN (k) I(θ (k); H | µ , H ) s s s,∗ s ∗,k 1:s−1 s k∈[K] (cid:115) (cid:115) (cid:88) (cid:88) (cid:88) ≤ K EN (k) 1 Γ2(k)I(θ (k); H | µ , H ) s K s s,∗ s ∗,k 1:s−1 s k∈[K] k∈[K] √ (cid:115) (cid:88) (cid:88) = nKL 1 Γ2(k)I(θ (k); H | µ , H ) K s s,∗ s ∗,k 1:s−1 s k∈[K] The first equality is easy to see. Next sequence of inequalities follow mainly by repeated application of Cauchy-Schwarz in different forms, and application of chain rule of mutual information. We now describe the other ones. - The second equation follow as (cid:80) a b ≤ (cid:112)(cid:80) a2 (cid:80) b2 for a , b ≥ 0, with a = i i i i i i i i i i (cid:112)P (k ∈ A ) and b = (cid:112)P (k ∈ A )I (θ (k); k, Y (k) | µ ). s,t s,t i s,t s,t s,t s,∗ s,t ∗,k - The third equation uses E[XY ] ≤ (cid:112)E[X2]E[Y 2] for X, Y > 0 w.p. 1 (positive random variables). - The next equality first uses the relation E (cid:80) P (k ∈ A ) = E[N (k)] where N (k) t s,t s,t s s is the number of time arm k is played in the task s. Then it also use the chain rule for I(θ (k); H | µ , H ). s,∗ s ∗,k 1:s−1 - For the next inequality, we apply Cauchy-Schwarz ((cid:80) a b ≤ (cid:112)(cid:80) a2 (cid:80) b2) again as i i i i i i i a = (cid:112)E[N (k)] and b = I(θ (k); H | µ , H ). Also note that K and 1 cancels i s i s,∗ s ∗,k 1:s−1 K out. - The final inequality is attained by noticing E[(cid:80) N (k)] ≤ nL, as at most L arms can be k s played in each round. The sum of the mutual information terms pertaining to the meta-parameter can be derived equivalently. (cid:34) (cid:35) (cid:88) (cid:88) (cid:113) Γ(k)E P (k ∈ A ) I (µ ; k, Y (k)) s,t s,t s,t ∗,k s,t k∈[K] s,t (cid:34) (cid:35) (cid:88) (cid:88) (cid:113) (cid:113) = Γ(k)E P (k ∈ A ) P (k ∈ A )I (µ ; k, Y (k)) s,t s,t s,t s,t s,t ∗,k s,t k∈[K] s,t   (cid:115) (cid:115) (cid:88) (cid:88) (cid:88) ≤ Γ(k)E  P s,t(k ∈ A s,t) P s,t(k ∈ A s,t)I s,t(µ ∗,k; k, Y s,t(k)) k∈[K] s,t s,t 31
(cid:115) (cid:115) (cid:88) (cid:88) (cid:88) ≤ Γ(k) E P (k ∈ A ) E P (k ∈ A )I (µ ; k, Y (k)) s,t s,t s,t s,t s,t ∗,k s,t k∈[K] s,t s,t (cid:88) (cid:112) (cid:113) = Γ(k) EN (k) I(µ ; H ) 1:m ∗,k 1:m k∈[K] (cid:115) (cid:115) (cid:88) (cid:88) ≤ K EN (k) 1 Γ2(k)I(µ ; H ) 1:m K ∗,k 1:m k∈[K] k∈[K] √ (cid:115) (cid:88) = mnKL 1 Γ2(k)I(µ ; H ) K ∗,k 1:m k∈[K] For the third term we have (cid:88) (cid:113) (cid:88) 2δ 1 σˆ2(k)E[ P (k ∈ A )] K s s,t s,t k∈[K] t (cid:88) (cid:113) ≤ 2δ 1 σˆ2(k)E[N (k)] K s s k∈[K] (cid:115) (cid:115) (cid:88) (cid:88) ≤ 2δ 1 σˆ2(k) (E[N (k)])2 K s s k∈[K] k∈[K] (cid:115) (cid:115) (cid:88) (cid:88) (cid:88) ≤ 2δ 1 σˆ2(k) E[N (k)] ≤ n 2δ 1 σˆ2(k) K s s K s k∈[K] k∈[K] k∈[K] This provides us with the bound stated in the lemma. (cid:114) (cid:113) (cid:16) (cid:17) Finally, we have E[ (cid:80) θ2 (k)] ≤ (cid:80) µ2(k) + σ2 + σ2 k∈K s,∗ k∈K q 0,k q,k Bounding Mutual Information: The derivation of the mutual information can be done similar to the linear bandits while using the diagonal nature of the covariance matrices. We present a different argument here. Lemma 12. For any H -adapted action-sequence and any s ∈ [m] and k ∈ [K], we have 1:s,t I(θ (k); H | µ , H ) ≤ 1 log (cid:16) 1 + n σ 02 ,k (cid:17) , s,∗ s ∗,k 1:s−1 2 σ2 I(µ ; H ) ≤ 1 log (cid:16) 1 + m σ q2 ,k (cid:17) . ∗,k 1:m 2 σ2 +σ2/n 0,k Proof. We have the following form for the conditional mutual information θ (k) with the events s,∗ H (k) conditioned on the meta-parameter µ , and the history of arm k pulls upto stage s (for each s ∗,k s) H , as a function of H , given as 1:s−1 s I(θ (k); H | µ , H ) = 1 E log (cid:16) 1 + N (k) σ 02 ,k (cid:17) ≤ 1 log (cid:16) 1 + n σ 02 ,k (cid:17) . s,∗ s ∗,k 1:s−1 2 s σ2 2 σ2 Another way to see this is, in each stage if µ was known then the variance of the estimate of ∗,k θ (k), or equivalently of (θ (k) − µ ), after N (k) samples and with an initial variance σ2 s,∗ s,∗ ∗,k s 0,k will be 1 . We note that only when N (k) ≥ 1 the mutual information is non-zero. Thus weσ 0− h2 a( vk e)+ thN es( mk) uσ l− ti2 p( lk ic) ation with P(N (k) ≥ 1). Ts he mutual information is then derived easily. s Similarly, the mutual information of θ (k) and the entire history of arm k pulls, i.e. H (k), is s,∗ 1:m stated as follows. I(µ ; H ) = 1 E log (cid:32) 1 + (cid:88)m σ q2 ,k (cid:33) ≤ 1 log (cid:32) 1 + m σ q2 ,k (cid:33) . ∗,k 1:m 2 σ2 + σ2/N (k) 2 σ2 + σ2/n s=1 0,k s 0,k 32
We claim (proven shortly) that at the end of task m the variance of estimate of µ is (σˆ−2(k) + ∗,k q (cid:80)m (σ2 + σ2/N (k))−1)−1. This gives the first equality. The final inequality holds by noting s(cid:48)=1 0,k s(cid:48) that minimizing the terms σ2/N (k) with N (k) = n, for all s, (as any arm can be pulled at most n s s times in any task) maximizes the mutual information. We now derive the variance of µ . Let the distribution of µ at the beginning of stage s is ∗,k ∗,k N (µˆ (k), σˆ2(k)). From the N (k) samples of arm k, we know θ (k) ∼ N (θˆ (k), σ2/N (k)) s s s s,∗ s s where θˆ (k) is the empirical mean of arm k in task s. Further, θ (k) − µ ∼ N (0, σ2 ) by our s s,∗ ∗,k 0,k reward model. Thus, we have from the two above relation (cid:32) (cid:33) σ2/N (k) µ ∼ N s θˆ (k), σ2 + σ2/N (k) . ∗,k σ2 + σ2/N (k) s 0,k s 0,k s However, we also know independently that µ ∼ N (µˆ (k), σˆ2(k)). Therefore, a similar combina- ∗,k s s tion gives us µ ∼ N (µˆ (k), σˆ2 (k)) where ∗,k s+1 s+1 (cid:16) (cid:17) µˆ (k) = σˆ−2 (k) µˆ (k)σˆ2(k) + θˆ (k)σ2/N (k) s+1 s+1 s s s s σˆ−2 (k) = σˆ−2(k) + (σ2 + σ2/N (k))−1 s+1 s 0,k s s (cid:88) σˆ−2 (k) = σ−2 + (σ2 + σ2/N (k))−1. s+1 q,k 0,k s(cid:48) s(cid:48)=1 The last equality follows from induction with the base case σˆ2(k) = σ2 . 0 q,k Bounding Γ (k): We finally provide the bound on the Γ (k) and Γ(k) terms used in the regret s s decomposition Lemma 11. Lemma 13. For all s ∈ [m], and (Γ (k)) and Γ(k) as defined in Lemma 11 admit the following s s∈[m] bounds, for any δ ∈ (0, 1], almost surely (cid:118) (cid:117) (cid:117) σ2 (cid:16) 1 + (1+σ2/σ 02 ,k)σ q2 ,k (cid:17) (cid:117) 0,k (σ2 +σ2)+sσ2 Γ (k) ≤ 4(cid:117) 0,k q,k log( 4K ), s (cid:116) 1 log (cid:16) 1 + σ 02 ,k (cid:16) 1 + (1+σ2/σ 02 ,k)σ q2 ,k (cid:17)(cid:17) δ 2 σ2 (σ2 +σ2)+sσ2 0,k q,k (cid:118) Γ(k) ≤ 4(cid:117) (cid:117) σ 02 ,k+σ q2 ,k log( 4K ). (cid:116) (cid:32) σ2 +σ2 (cid:33) δ log 1+ 0,k q,k σ2 Proof. At the beginning of task s we know that θ (k) ∼ N (µˆ (k), σ2 + σˆ2(k)). And as the s,∗ s 0,k s variance of θ (k) decreases during task s with new samples from arm k, we have the variance s,∗ σˆ2 (k) ≤ σ2 + σˆ2(k) ≤ σ2 + (σ 02 ,k+σ2)σ q2 ,k . s,t 0,k s 0,k (σ2 +σ2)+sσ2 0,k q,k The inequality holds by taking N (k) = 1 in the expression of σˆ2(k) for all tasks as arm k has been s(cid:48) s played using forced exploration. Therefore, we can bound Γ (k), for any s, as s (cid:118) (cid:117) (cid:117) σ2 (cid:16) 1 + (1+σ2/σ 02 ,k)σ q2 ,k (cid:17) (cid:117) 0,k (σ2 +σ2)+sσ2 Γ (k) ≤ 4(cid:117) 0,k q,k log( 4K ) s (cid:116) 1 log (cid:16) 1 + σ 02 ,k (cid:16) 1 + (1+σ2/σ 02 ,k)σ q2 ,k (cid:17)(cid:17) δ 2 σ2 (σ2 +σ2)+sσ2 0,k q,k (cid:118) This implies Γ(k) ≤ 4(cid:117) (cid:117) (cid:116) (cid:32) σ 02 , σk+ 2 σ +q2 , σk 2 (cid:33) log( 4 δK ) by setting s = 0. log 1+ 0,k q,k σ2 33
Deriving Final Regret Bound: We proceed with our final regret bound as Theorem 6 (Semi-bandit). The regret of AdaTS is bounded for any δ ∈ (0, 1] as √ √ R(m, n) ≤ c KLmn + (m + c ) R (n; µ ) + c K3/2m + c σ 2δmn , 1 2 δ ∗ 3 4 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Learning of µ∗ Per-task regret Forced exploration where (cid:118) c = 4(cid:117) (cid:117) 1 (cid:88) σ q2 ,k+σ 02 ,k log(4K/δ) log (cid:16) 1 + σ q2 ,km (cid:17) , 1 (cid:116) K (cid:32) σ2 +σ2 (cid:33) σ2 +σ2/n k∈[K] log 1+ q,k σ2 0,k 0,k (cid:18) (cid:19) (cid:115) c = 1 + max σ2 log m , c = 2 (cid:88) (µ2 + σ2 + σ2 ) , 2 σ2 3 q,k q,k 0,k k∈[K]: σ0,k>0 0,k k∈[K] c = (cid:115) 1 (cid:88) log (cid:16) 1 + σ q2 ,km (cid:17) . 4 K σ2 k∈[K]: σ0,k=0 √ (cid:113) The per-task regret is bounded as R (n; µ ) ≤ c KLn + 2δ 1 (cid:80) σ2 n, where δ ∗ 5 K k∈[K] 0,k (cid:118) c = 4(cid:117) (cid:117) 1 (cid:88) σ 02 ,k log(4K/δ) log (cid:16) 1 + σ 02 ,kn (cid:17) . 5 (cid:116) K (cid:32) σ2 (cid:33) σ2 k∈[K]: σ0,k>0 log 1+ σ0 2,k The prior widths σ and σ are defined as in Section 3.1. q,k 0,k Proof. We now use the regret decomposition in Lemma 11, the bounds on terms Γ (k) and Γ(k) in s Lemma 13, and the mutual information in Lemma 12 bounds derived earlier to obtain our final regret bound for the semi-bandits. The regret bound follows from the following chain of inequalities. R(m, n) √ (cid:115) (cid:88) (cid:115) (cid:88) (cid:16) (cid:17) ≤ mnKL 1 Γ2(k)I(µ ; H ) + 2 mK µ2(k) + σ2 + σ2 K ∗,k 1:m q 0,k q,k k∈[K] k∈K   √ (cid:115) (cid:115) (cid:88) (cid:88) (cid:88) +  nKL K1 Γ2 s(k)I(θ s,∗(k); H s | µ ∗,k, H 1:s−1) + n 2δ K1 σˆ s2(k) s k∈[K] k∈[K] (cid:118) ≤ 4(cid:117) (cid:117) (cid:117) 1 (cid:88) σ 02 ,k + σ q2 ,k log (cid:32) 1 + m σ q2 ,k (cid:33) log( 4K )√ mnKL (cid:116) K (cid:16) σ2 +σ2 ) (cid:17) σ2 + σ2/n δ k∈[K] log 1 + 0,k σ2 q,k 0,k (cid:115) (cid:88) (cid:16) (cid:17) + 2mK3/2 µ2(k) + σ2 + σ2 q 0,k q,k k∈K   (cid:88) √ (cid:115) (cid:88) (cid:115) (cid:88) +  nKL K1 Γ2 s(k)I(θ s,∗(k); H s | µ ∗,k, H 1:s−1) + n 2δ K1 σˆ s2(k) s k∈[K]:σ2 =0 k∈[K]:σ2 =0 0,k 0,k   (cid:88) √ (cid:115) (cid:88) (cid:115) (cid:88) +  nKL K1 Γ2 s(k)I(θ s,∗(k); H s | µ ∗,k, H 1:s−1) + n 2δ K1 σˆ s2(k) s k∈[K]:σ2 >0 k∈[K]:σ2 >0 0,k 0,k (cid:118) ≤ 4(cid:117) (cid:117) (cid:117) 1 (cid:88) σ 02 ,k + σ q2 ,k log (cid:32) 1 + m σ q2 ,k (cid:33) log( 4K )√ mnKL (cid:116) K (cid:16) σ2 +σ2 ) (cid:17) σ2 + σ2/n δ k∈[K] log 1 + 0,k σ2 q,k 0,k (cid:118) + 2mK3/2(cid:115) (cid:88) (cid:16) µ2(k) + σ2 + σ2 (cid:17) + n(cid:117) (cid:117)2mδ 1 (cid:88) (cid:88)m σ2σ q2 ,k q 0,k q,k (cid:116) K σ2+sσ2 q,k k∈K k∈[K]:σ2 =0 s=1 0,k 34
(cid:32) m (cid:33) + m + (cid:88) max 1 (σ 02 ,k+σ2)σ q2 ,k k∈[K]:σ2 >0 2σ 02 ,k (σ 02 ,k+σ2)+sσ q2 ,k s=1 0,k  (cid:118)  × 4(cid:117) (cid:117) (cid:117) 1 (cid:88) σ 02 ,k log (cid:32) 1 + n σ 02 ,k (cid:33) log( 4K )√ nKL + n(cid:115) 2δ 1 (cid:88) σ2   (cid:116) K (cid:16) σ2 (cid:17) σ2 δ K 0,k log 1 + 0,k k∈[K]:σ 02 ,k>0 σ2 k∈[K]:σ 02 ,k>0 (cid:118) ≤ 4(cid:117) (cid:117) (cid:117) 1 (cid:88) σ 02 ,k + σ q2 ,k log (cid:32) 1 + m σ q2 ,k (cid:33) log( 4K )√ mnKL (cid:116) K (cid:16) σ2 +σ2 ) (cid:17) σ2 + σ2/n δ k∈[K] log 1 + 0,k σ2 q,k 0,k (cid:115) (cid:118) + 2mK3/2 (cid:88) (cid:16) µ2(k) + σ2 + σ2 (cid:17) + n(cid:117) (cid:117)2δσ2m 1 (cid:88) log(1 + m σ q2 ,k ) q 0,k q,k (cid:116) K σ2 k∈K k∈[K]:σ2 =0 0,k (cid:32) (cid:32) (cid:33) (cid:33) + m + 1 + max σ2 log(m) k∈[K]:σ2 >0 σ 02 ,k 0,k  (cid:118)  × 4(cid:117) (cid:117) (cid:117) 1 (cid:88) σ 02 ,k log (cid:32) 1 + n σ 02 ,k (cid:33) log( 4K )√ nKL + n(cid:115) 2δ 1 (cid:88) σ2   (cid:116) K (cid:16) σ2 (cid:17) σ2 δ K 0,k log 1 + 0,k k∈[K]:σ 02 ,k>0 σ2 k∈[K]:σ 02 ,k>0 The derivation follows through steps similar to the corresponding derivations for the linear bandits. In the second inequality we differentiate the arms which has σ2 = 0 against the rest. Any arm k 0,k with σ2 = 0 has no mutual information once µ is known, i.e. I(θ (k); H | µ , H ) = 0 0,k ∗,k s,∗ s ∗,k 1:s−1 for all such k. 35
200 150 100 50 0 0 5 10 15 20 tergeR Gaussian (K = 2, ¾q = 0.500) Gaussian (K = 4, ¾q = 0.500) Gaussian (K = 8, ¾q = 0.500) 450 900 OracleTS AdaTS 400 800 TS AdaTS+ 350 700 MetaTS AdaTS− 300 600 250 500 200 400 150 300 100 200 50 100 0 0 0 5 10 15 20 0 5 10 15 20 180 160 140 120 100 80 60 40 20 0 0 5 10 15 20 Task s tergeR Gaussian (K = 2, ¾q = 1.000) Gaussian (K = 4, ¾q = 1.000) Gaussian (K = 8, ¾q = 1.000) 400 800 350 700 300 600 250 500 200 400 150 300 100 200 50 100 0 0 0 5 10 15 20 0 5 10 15 20 Task s Task s Figure 4: AdaTS in a K-armed Gaussian bandit. We vary both K and meta-prior width σ . q 400 350 300 250 200 150 100 50 0 0 5 10 15 20 tergeR Linear (d = 2, ¾q = 0.500) Linear (d = 4, ¾q = 0.500) Linear (d = 8, ¾q = 0.500) 800 1400 OracleTS AdaTS 700 1200 TS AdaTS+ 600 1000 MetaTS AdaTS− 500 800 400 600 300 200 400 100 200 0 0 0 5 10 15 20 0 5 10 15 20 350 300 250 200 150 100 50 0 0 5 10 15 20 Task s tergeR Linear (d = 2, ¾q = 1.000) Linear (d = 4, ¾q = 1.000) Linear (d = 8, ¾q = 1.000) 700 1200 600 1000 500 800 400 600 300 400 200 100 200 0 0 0 5 10 15 20 0 5 10 15 20 Task s Task s Figure 5: AdaTS in a d-dimensional linear bandit with K = 5d arms. We vary both d and meta-prior width σ . q E Supplementary Experiments We conduct two additional experiments. In Appendix E.1, we extend synthetic experiments from Section 5. In Appendix E.2, we experiment with two real-world classification problems: MNIST [32] and Omniglot [30]. E.1 Synthetic Experiments This section extends experiments in Section 5 in three aspects. First, we show the Gaussian bandit with K ∈ {2, 4, 8} arms. Second, we show the linear bandit with d ∈ {2, 4, 8} dimensions. Third, we implement AdaTS with a misspecified meta-prior. Our results are reported in Figures 4 and 5. The setup of this experiment is the same as in Figure 2, and it confirms all earlier findings. We also experiment with two variants of misspecified AdaTS. In AdaTS+, the meta-prior width is widened to 3σ . This represents an overoptimistic agent. In AdaTS−, q the meta-prior width is reduced to σ /3. This represents a conservative agent. We observe that this q misspecification has no major impact on the regret of AdaTS, which attests to its robustness. 36
1000 800 600 400 200 0 0 2 4 6 8 10 Task s tergeR MNIST OracleTS TS MetaTS AdaTS STelcarO ST STateM Task 1 STadA 1000 800 600 400 200 0 0 2 4 6 8 10 Task s Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8 Task 9 Task 10 tergeR MNIST OracleTS TS MetaTS AdaTS STelcarO ST STateM Task 1 STadA Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8 Task 9 Task 10 Figure 6: AdaTS in two meta-learning problems of digit classification from MNIST. On the top, we plot the cumulative regret as it accumulates over rounds within each task. Below we visualize the average digit, corresponding to the pulled arms in round 1 of the tasks. 200 150 100 50 0 0 2 4 6 8 10 Task s tergeR Omniglot OracleTS TS MetaTS AdaTS STelcarO ST STateM STadA 200 150 100 50 0 0 2 4 6 8 10 Task s Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8 Task 9 Task 10 tergeR Omniglot OracleTS TS MetaTS AdaTS STelcarO ST STateM STadA Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8 Task 9 Task 10 Figure 7: AdaTS in two meta-learning problems of character classification from Omniglot. On the top, we plot the cumulative regret as it accumulates over rounds within each task. Below we visualize the average character, corresponding to the pulled arms in round 1 of the tasks. E.2 Online One-Versus-All Classification Experiments We consider online classification on two real-world datasets, which are commonly used in meta- learning. The problem is cast as a multi-task linear bandit with Bernoulli rewards. Specifically, we have a sequence of image classification tasks where one class is selected randomly to be positive. In each task, at every round, K random images are selected as the arms and the goal is to pull the arm corresponding to an image from the positive class. The reward of an image from the positive class is Ber(0.9) and for all other classes is Ber(0.1). Dataset-specific settings are as follows: 1. MNIST [32]: The dataset contains 60 000 images of handwritten digits, which we split into equal-size training and test sets. We down-sample each image to d = 49 features and then use these as arm features. The training set is used to estimate µ and Σ for each digit. The 0 0 bandit algorithms are evaluated on the test set. In each simulation, we have m = 10 tasks with horizon n = 200 and K = 30 arms. 2. Omniglot [30]: The dataset contains 1 623 different handwritten characters from 50 different alphabets. This is an extremely challenging dataset because we have only 20 human-drawn images per character. Therefore, it is important to adapt quickly. We train a 4-layer CNN to extract d = 64 features using characters from 30 alphabets. The remaining 20 alphabets are 37
split into equal-size training and test sets, with 10 images per character in each. The training set is used to estimate µ and Σ for each character. The bandit algorithms are evaluated on 0 0 the test set. In each simulation, we have m = 10 tasks with horizon n = 10 and K = 10 arms. We guarantee that at least one character from the positive class is among the K arms. In all problems, the meta-prior is N (0, I ) and the reward noise is σ = 0.1. We compare AdaTS with d the same three baselines as in Section 5, repeat all experiments 20 times, and report the results in Figures 6 and 7. Along with the cumulative regret, we also visualize the average digit / character corresponding to the pulled arms in round 1 of each task. We observe that AdaTS learns a very good meta-parameter µ almost instantly, since its average digit / character in task 2 already resembles the ∗ unknown highly-rewarding digit / character. This happens even in Omniglot, where the horizon of each task is only n = 10 rounds. Note that the meta-prior was not selected in any dataset-specific way. The fact that AdaTS still works well attests to the robustness of our method. 38
