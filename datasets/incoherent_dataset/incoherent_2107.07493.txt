Algorithmic Concept-based Explainable Reasoning Dobrik Georgiev Pietro Barbiero Dmitry Kazhdan University of Cambridge University of Cambridge University of Cambridge dgg30@cam.ac.uk pb737@cam.ac.uk dk525@cam.ac.uk Petar Velicˇkovic´ Pietro Liò DeepMind University of Cambridge petarv@deepmind.com pl219@cam.ac.uk Abstract Recent research on graph neural network (GNN) models successfully applied GNNs to classical graph algorithms and combinatorial optimisation problems. This has numerous benefits, such as allowing applications of algorithms when preconditions are not satisfied, or reusing learned models when sufficient training data is not available or can’t be generated. Unfortunately, a key hindrance of these approaches is their lack of explainability, since GNNs are black-box models that cannot be interpreted directly. In this work, we address this limitation by applying existing work on concept-based explanations to GNN models. We introduce concept- bottleneck GNNs, which rely on a modification to the GNN readout mechanism. Using three case studies we demonstrate that: (i) our proposed model is capable of accurately learning concepts and extracting propositional formulas based on the learned concepts for each target class; (ii) our concept-based GNN models achieve comparative performance with state-of-the-art models; (iii) we can derive global graph concepts, without explicitly providing any supervision on graph-level concepts. 1 Introduction Graph neural networks (GNNs) have successfully been applied to problems involving data with irregular structure, such as quantum chemistry (Gilmer et al., 2017), drug discovery (Stokes et al., 2020), social networks (Pal et al., 2020) and physics simulations (Battaglia et al., 2016). One of the latest areas of GNN research focuses on using GNNs for emulation of classical algorithms (Cappart et al., 2021). In particular, this research explored applications of GNNs to iterative algorithms (Velicˇkovic´ et al., 2020b; Georgiev and Liò, 2020), pointer-based data structures (Velicˇkovic´ et al., 2020a; Strathmann et al., 2021), and even planning tasks (Deac et al., 2020). Importantly, these works demonstrate that GNNs are capable of strongly generalising to input graphs much larger than the ones seen during training. Unfortunately, in all of the aforementioned cases, these state-of-the-art GNN models are black-boxes, whose behaviour cannot be understood/intepreted directly. In practice, this can lead to a lack of trust in such models, making it challenging to apply and regulate these models in safety-critical applications, such as healthcare. Furthermore, this lack of interpretability also makes it difficult to extract the knowledge learned by such models, which prevents users from better understanding the corresponding tasks (Adadi and Berrada, 2018; Molnar, 2020; Doshi-Velez and Kim, 2017). Recent work on Explainable AI (XAI) introduced a novel type of random forest (CNN) explanation approach, referred to as concept-based explainability (Koh et al., 2020; Kazhdan et al., 2020b; Ghorbani et al., 2019; Kazhdan et al., 2021). Concept-based explanation approaches Preprint. Under review. 1202 luJ 51 ]GL.sc[ 1v39470.7012:viXra
BFS Parallel coloring Kruskal’s algorithm visited node 1 2 3 4 5 edge in MST S1 = {a, b, e, c} Colors: visited edge e S2 = {d} 5 1 f 100 a 4 b 2 a e 44 2 12 155 8 n 33 d c b c d 58 248 6 CBGNN CBGNN CBGNN NOT colored, has priority has NOT been vis. color 1, 2 and 3 seen lighter edges visited has visited neighbour color 4 NOT seen nodes in different sets CBGNN CBGNN CBGNN v = 1 c = 4 m = 1 c n cd Figure 1: An overview of our Concept Bottleneck Graph Neural Network (CBGNN) approach. Importantly, CBGNN models can be trained to extract concept information for a given task as well as algorithm rules. We give examples of 3 algorithms, showing how CBGNN extract concepts from the input data and then uses these to compute the output. provide model explanations in terms of human-understandable units, rather than individual features, pixels, or characters (e.g., the concepts of a wheel and a door are important for the detection of cars) (Kazhdan et al., 2020b). In particular, work on Concept Bottleneck Models (CBMs) relies on concepts and introduces a novel type of interpretable-by-design CNN, which perform input processing in two distinct steps: computing a set of concepts from an input, and then computing the output label from the concepts (Koh et al., 2020). In this paper, we apply the idea of CBMs to GNN models, by introducing Concept Bottleneck Graph Neural Networks (CBGNNs). In particular, we rely on the encode-process-decode paradigm (Hamrick et al., 2018), and apply concept bottleneck layers before the output of GNN models – see Figure 1. By doing this we are able to extract update/termination rules for the step updates of step-level combinatorial optimisation approaches (Velicˇkovic´ et al., 2020b,a; Deac et al., 2020; Strathmann et al., 2021). Importantly, we show that by relying on a suitable set of concepts and supervising on them, we are capable of deriving the rules of classical algorithms such as breadth-first search (Moore, 1959), and Kruskal’s algorithm (Kruskal, 1956), as well as more advanced heuristics such as parallel graph coloring (Jones and Plassmann, 1993). Furthermore, we present an approach to utilise node-level concepts for extracting graph-level rules. Our evaluation experiments demonstrate that all of our extracted rules strongly generalise to graphs of 5× larger size. To summarise, we make the following contributions in this work: • Present Concept Bottleneck Graph Neural Networks (CBGNN), a novel type of GNN relying on intermediate concept processing. To the best of our knowledge, this is the first work to apply concept bottleneck approaches to GNNs. • Quantitatively evaluate our approach using three different case-studies (BFS, graph colour- ing, and Kruskal’s), showing that our CBGNN approch is capable of achieving performance on-par with that of existing state-of-the-art • Qualitatively evaluate our approach, by demonstrating how the concepts utilised by CBGNN models can be used for providing rules summarising the heuristics the CBGNN has learned 2 Related work GNN Explainability Recent work began exploring applications of XAI techniques in the context of GNNs. For instance, work in Pope et al. (2019); Baldassarre and Azizpour (2019); Schnake et al. 2
(2020) adapt feature-importance gradient-based approaches used for CNN applications (such as Class Activation Mappings, or Layer-wise Relevance Propagation) to GNNs, in order to identify the most important nodes/subgraphs responsible for individual predictions. Alternatively, works in Ying et al. (2019); Vu and Thai (2020); Luo et al. (2020) focus on more complex approaches unique to GNN explainability, such as those based on mutual information maximisation, or Markov blanket conditional probabilities of feature explanations. Importantly, these works focus on GNN tasks and benchmarks involving social networks, chemistry, or drug discovery, instead of focusing on combinatorial optimisation tasks, which is the focus of this work. Furthermore, these works focus on explaining pre-trained GNNs in a post-hoc fashion, whereas we focus on building GNN models interpretable-by-design. Finally, these works focus on feature-importance-based explanation approaches (i.e. returing relative importance of input nodes/subgraphs), whereas we rely on concept- based explanation approaches instead. Concept-based Explainability A range of existing works have explored various concept-based explanations applied to CNN models. For instance, work in Ghorbani et al. (2019); Kazhdan et al. (2020b); Yeh et al. (2019) introduce approaches for extracting concepts from pre-trained CNNs in an unsupervised, or semi-supervised fashion. Work in Chen et al. (2020); Koh et al. (2020) rely on concepts for introducing CNN models interpretable-by-design, performing processing in two distinct steps: concept extraction, and label prediction. Other works on concepts include studying the connection between concepts and disentanglement learning (Kazhdan et al., 2021), as well as using concepts for data distribution shifts (Wijaya et al., 2021). Importantly, these works explore concepts exclusively in the context of CNNs, with Kazhdan et al. (2020a) being the only work exploring concepts in the context of RNN models. In this work, we focus on concept-based explainability for GNNs, where, similar to Koh et al. (2020), the concepts are human-specified. Combinatorial Optimisation for GNNs Following the hierarchy defined in Cappart et al. (2021), our work classifies as a step-level approach. We directly extend on Velicˇkovic´ et al. (2020a,b), therefore we use the models presented in these works as baselines. We do not compare our model to an algorithm-level combinatorial optimisation approaches (Xu et al., 2020; Tang et al., 2020; Joshi et al., 2020) or unit-level ones (Yan et al., 2020) for the following reasons: Algorithm-level approaches usually give one output per data sample (rather than one output per step), but rules/invariants of a given algorithm come from how the iteration proceeds making algorithm-level combinatorial optimisation less suitable for a concept bottleneck. Unit-level learning focuses on learning primitive units of computation, such as taking maximum or merging lists and then combining these manually – having explanations at this level would not be of great benefit. To the best of our knowledge, only Velicˇkovic´ et al. (2020a) attempted to explain GNN predictions, using GNNExplainer (Ying et al., 2019). However, their model (i) was not explainable by design and (ii) required further optimisation for a single sample to give a local explanation. All other previous works operated in a black-box fashion and did not consider explainability of the learnt models. 3 Methodology Encode-process-decode Following the “blueprint” for neural execution outlined in Velicˇkovic´ et al. (2020b), we model the algorithms by the encode-process-decode architecture (Hamrick et al., 2018). For each algorithm A, an encoder network f encodes the algorithm-specific node-level A inputs z(t) into the latent space. These node embeddings are then processed using the processor i network P , usually a GNN. The processor takes as input the encoded inputs Z(t) = {z(t)} and i i∈V graph edge index E to produce latent features H(t) = {h(t) ∈ R|L|} , where |L| is the size i i∈V of the latent dimension. In contrast with previous work, we calculate algorithm outputs by first passing the latent embeddings through a decoder network g(cid:48) , which produces concepts for each node A C(t) = {c(t) ∈ (0, 1)|C|}, where |C| is number of concepts. The concepts are then passed through a i concept decoder g to produce node-level outputs Y(t) = {y(t)}. A i Where applicable, we also utilise a termination network T for deciding when to stop. However, in A contrast with prior work, we observed that training is more stable if we calculate the termination probability based on potential next step embeddings (i.e. a belief about what is the state after an iteration has been executed). Additionally we found it insufficient to use the average node embeddings as input to T – averaging would obfuscate the signal if there is just a single node which should tell A 3
h y b y G c graph-level node-level output h a m bc output decoder decoder m ab h b h G c b m db m be h d h e concept PrediNet decoder h h(cid:48) f b Node embeddings Figure 2: A high level overview of our GNN architecture: Left: (Eqns. 1-4) To produce node-level outputs, messages from neighbouring nodes (m ) are combined with current node representation ij (h ), resulting in an updated representation (h(cid:48) ). Concepts (c ) are then extracted from the updated b b b representations, and node-level outputs (y ) are extracted from the concepts. Right: (Eqns. 5-8) b A graph-level embedding (h ) is obtained by passing the node embeddings through PrediNet. We G extract graph-level outputs y (in our case – termination probability) directly from the latent state G h – graph-level concepts are extracted through a full enumeration approach over the node concepts. G us whether to continue iterating or not. Instead, we opted to use the output of an adapted PrediNet (Shanahan et al., 2020) architecture with one attention head. PrediNet is designed to represent the conjunction of elementary propositions, therefore it can (theoretically) capture the logical bias of the termination rules. The whole process is summarised in Figure 2 as well as in the equations below: (cid:16) (cid:17) (cid:16) (cid:17) z(t) = f x(t), h(t−1) (1) z(cid:48)(t) = f y(t), h(t) (5) i A i i i A i i (cid:16) (cid:17) (cid:16) (cid:17) H(t) = P Z(t), E (2) H(cid:48)(t) = P Z(cid:48)(t) , E (6) (cid:16) (cid:16) (cid:17)(cid:17) c(t) = σ g(cid:48) z(t), h(t) (3) H(t) = PrediNet(H(cid:48)(t) ) (7) i A i i (cid:16) (cid:16) (cid:17)(cid:17) y(t) = g (cid:16) c(t)(cid:17) (4) τ (t) = σ T A H(t) (8) i A i where σ is a logistic sigmoid function. When using T , equations 1-8 are repeated if τ (t) > 0.5. A The combination of encoded inputs, together with the latent state of the given node, contains sufficient information not only about the output at a given step, but also: (i) a node’s current state and (ii) observations about other nodes’ states in its neighbourhood. If our concepts are engineered to capture some knowledge of either (i) or (ii), then we can extract meaningful algorithm output explanations without providing any explicit information about how the algorithm works (theorems, invariants, etc.) Explicitly relational GNN architecture Some graph level tasks (e.g. deciding termination) can be reduced to a logical formula over all nodes – for the algorithms and concepts we consider, termination can be reduced to existence of a node with specific properties. (See graph-level rule extraction). We engineer this logical bias into the termination network τ by adapting PrediNet (Shanahan et al., 2020) to the graph domain. The PrediNet network architecture learns to represent conjunction/disjunction of elementary propositions and is therefore suitable for the termination task. We list the two minor modifications we made to adapt PrediNet to our tasks in Appendix A. Extracting node-level algorithm rules Deciding node-level formulas for algorithm A is achieved by examining the weights of the concept decoder g . To achieve this, we used the open-source A package logic_explained_networks1 (Barbiero et al., 2021) implementing a wide collection of techniques to extract logic-based explanations from concept-bottleneck neural networks (Gori, 2017; Ciravegna et al., 2020). The library takes as inputs (i) node-level output decoder weights, (ii) predicted 1Apache 2.0 Licence. 4
concepts from training data, and (iii) training data ground truth labels, and generates logic formulas in disjunctive normal form as outputs (Mendelson, 2009). By construction, the concept decoder g¯ ≈ g learnt from concepts C to outputs O is a Boolean map. As any Boolean function, it can be A A converted into a logic formula in disjunctive normal form by means of its truth-table (Mendelson, 2009). The weights of the concept decoder g are used to select the most relevant concepts for each A output task. To get concise logic explanations when many concepts are required as inputs (in our experiments this is only the graph coloring task), we add a regularization term in the loss function minimising the L1-norm of the concept decoder weights W , leading to sparse configurations of W . Later, at the training epoch t , first-layer weights are pruned concept-wise, i.e. removing all the prune weights departing from the least relevant concepts: W˜ 1 = W 1I , for i = 1, . . . , |C| (9) j j ||W j1||1≥maxi ||W i1||1/2 where I is the indicator function and W 1 are weights of the first layer. Further details on logic extraction are provided in Appendix B. Extracting algorithm termination rules When deciding whether to continue execution, we use the fact that a number of graph algorithms continue iterating until a node with a specific combination of concepts exists. Since it is unclear what combination of concepts we should supervise towards, we took a full enumeration approach when extracting rules for termination. First, we generate a sample j of the form (U , τ (cid:48)) from the training set from each step for a given graph. τ (cid:48) is the ground j j j truth for whether we should keep iterating, and U = {c(cid:48) , . . . , c(cid:48) } is a set of all unique concepts j 1 k combinations,2 after the algorithm update state has been performed (hence c(cid:48)). Given a set of concept indexes3 I ⊆ P({1..|C|}) and truth assignment T : {1..|C|} → {0, 1} telling us which concepts must be true/false, we check if the following is satisfied: ∀j (cid:0) τ (cid:48) = 1 ⇐⇒ (∃c ∈ U . ∀I ∈ I. c = T (I ))(cid:1) (10) j j i Ii i i.e. we should continue iterating if a special concept combination exists, and we should stop iterating if it does not. We employ a brute-force approach for finding I and T , breaking ties by preferring smaller I.4 The complexity of such an approach is exponential, but if the concept bottleneck is carefully engineered, the number of necessary concepts, and/or the number of concepts in the special combination will be small, making the computation feasible. More importantly, if the same enumeration approach was applied to the raw node input data a I/T combination may not exist. For example, the node-level inputs for the BFS task on each step do not tell us which nodes have visited neigbours (crucial for deciding termination). Additionally, if we have larger number of input features, the brute-force approach may not be computationally feasible – the combinations scale exponentially with the number of node features and concepts are one way to reduce this number. 4 Experimental setup The code for our experiments can be found at https://github.com/HekpoMaH/ algorithmic-concepts-reasoning. Algorithms considered We apply our GNN to the following algorithms: breadth-first search (BFS), parallel coloring (Jones and Plassmann, 1993), a graph coloring heuristic, and Kruskal’s minimum spanning tree (MST) algorithm (Kruskal, 1956). BFS is modelled as a binary classification problem where we predict whether a node is visited or not, parallel coloring – as a classification task over the classes of possible node colors, plus one class for uncolored nodes. Kruskal’s is modelled as two tasks trained in parallel – one, as a classification task to choose the next edge to be considered for the MST and one to help us decide which nodes belong to the same set. As the original Kruskal’s algorithm executes for |E| steps, we do not learn termination for the MST task and stick to a fixed number of steps. We show how we model inputs/outputs for all algorithms in Appendix C. 2k may vary across samples 3{a..b} denotes the set of integers from a to b 4In our case this broke all ties, but, if necessary, one can add tie-break on truth assignment, by e.g. choosing assignments with more true/false values 5
Table 1: Algorithms and their corresponding concepts. We provide some sample ground truth explanations. Visual examples of how the algorithms work can be seen in Figure 1. Example ground-truth explanations Algorithm Concepts (not provided to the model) hasBeenV isited (hBV ) hV N (i) =⇒ y(t) = 1 BFS i hasV isitedN eighbours (hV N ) ∃i.¬hBV (i) ∧ hV N (i) =⇒ τ (t) = 1 iC(i) ∧ c1S(i) ∧ ¬c2S(i) =⇒ y(t) = 2 i isColored (iC), hasP riority (hP ) Coloring colorXSeen (cXS), X ∈ {1, .., 5} (¬iC(i) ∧ hP (i) ∧ c1S(i) ∧ c2S(i) ∧ ¬c3S(i)) =⇒ y(t) = 3 i (lEV (i) ∧ ¬nISS(i) ∧ ¬eIM (i)) lighterEdgesV isited (lEV ) =⇒ y(t) = 1 i Kruskal’s nodesInSameSet (nISS) edgeInM st (eIM ) (nISS(i) ∧ ¬eIM (i)) =⇒ y(t) = 0 i Importantly, all algorithms we experimented with posses the following two properties: (i) node/edge outputs are discrete and can be described in terms of concepts; (ii) continuing the execution can be reduced to the existence of a node with a specific combination of features. Examples of classical algorithms that do not fall into this category are the class of shortest path algorithms: to explain such algorithms, we would need to use arithmetic (e.g. minimum, sum) for the rules – something that concepts cannot directly capture. We leave explanation of such algorithms for future work. To generate our concepts, we took into account what properties of the nodes/neighbourhood the algorithm uses, but we did not provide any details to the model how to use them. Table 1 gives more details on what concepts we chose and some example explanations. We give an outline how one can use these concepts for explaining the algorithms, in Appendix D. Data generation Following prior research on the topic of neural execution (Velicˇkovic´ et al., 2020b), for BFS we generate graphs from a number of categories – ladder, grid, Erdo˝s-Rényi (Erdo˝s and Rényi, 1960), Barabási-Albert (Albert and Barabási, 2002), 4-Community graphs, 4-Caveman graphs and trees. For the coloring task, we limit the number of colors to 5 and then generate graphs where all nodes have fixed degree 5. This made the task both challenging (i.e. there are occassions where 5 colors are necessary) and feasible (we can generate graphs that are 5-colorable). Training data for these tasks graph size is fixed at 20 and we test with graph sizes of 20, 50 and 100 nodes. For Kruskal’s algorithm, we reused most graph categories for the BFS task, except the last three where the graph is either a tree or is not connected. Due to GPU memory constraints, training MST on graphs of size 20 required reducing the batch size by a factor of 4 and making the training very time consuming. Therefore, for the MST task we reduced the size of the training graphs to 8. Testing is still performed on graphs of size 20, 50 and 100. For all tasks we did a 10:1:1 train:validation:testing split.5 More details about the data generation are present in Appendix E. Architectures tested We decided to choose message-passing neural networks (Gilmer et al., 2017) with the max aggregator for the main skeleton of our processor (GNN) architecture as this type of GNN is known to align well with algorithmic execution (Velicˇkovic´ et al., 2020b; Georgiev and Liò, 2020; Velicˇkovic´ et al., 2020a). However, due to the nonlinear nature of some of the tasks (parallel coloring) and the added concept bottleneck we found it beneficial to add a hidden layer to some of the encoders and decoders, rather than simply model them as an affine projection. The Kruskal’s algorithm consists of several steps – masking out visited edges, finding the minimal edge from the unmasked and checking if two nodes are in the same set and unifying if they are not. The architecture for this algorithm, follows the main ideas of Figures 1&2, to implement them we 5When working with multiple graph categories, the ratio is preserved across each category 6
Table 2: Parallel coloring accuracies over 5 runs Model Metric |V | = 20 |V | = 50 |V | = 100 mean-step acc. 99.09±0.86% 98.74±0.44% 97.92±1.50% Standard last-step acc. 99.25±0.56% 99.17±0.20% 99.13±0.29% term. acc. 98.79±0.86% 96.79±1.53% 95.08±2.89% mean-step acc. 99.71±0.11% 99.23±0.21% 98.92±0.59% last-step acc. 99.69±0.13% 99.17±0.23% 99.10±0.22% term. acc. 99.61±0.18% 99.02±0.43% 98.59±0.77% Bottleneck (+L1 and prune) formula mean-step acc. 99.71±0.12% 99.24±0.21% 98.93±0.59% formula last-step acc. 99.69±0.13% 99.16±0.22% 99.08±0.19% formula term. acc. 99.51±0.17% 99.02±0.43% 98.48±0.74% *concepts mean-step acc. 99.85±0.05% 99.60±0.10% 99.45±0.29% *concepts last-step acc. 99.72±0.07% 99.35±0.23% 99.42±0.09% combine the architecture of Yan et al. (2020) for the first two steps and Velicˇkovic´ et al. (2020a) for the third step. More details can be found in Appendix F. Experimental details We train our models using teacher forcing (Williams and Zipser, 1989) for a fixed number of epochs (500 for BFS, 3000 for the parallel coloring, 100 for Kruskal’s). When testing BFS/parallel coloring, we pick the model with the lowest sum of validation losses and when testing Kruskal’s – the model with highest last-step F1-score. For training we use Adam optimizer (Kingma and Ba, 2015) with initial learning rate of 0.001 and batch size 32. We optimise the sum of losses on the concept, output and termination (except for Kruskal’s, see above) predictions – for more details on how we define our losses see Appendix G. We evaluate the ability to strongly generalise on graphs with sizes 50 and 100. Standard deviations are obtained over 5 runs. For parallel coloring we add L1 regularisation and pruning on epoch 2000 to obtain higher quality explanations since every combination of (concepts, output) pair may not be observed during training. Libraries, code, and computing details are described in Appendix L. All hyperparameters were tuned manually. Metrics We use a variety of metrics, such as mean-step F1-score (average F1-score of per-step outputs), last-step F1-score (average F1-score of final algorithm outputs) and termination F1-score (average F1-score of predicting termination). Similarly, we define: concepts mean-step F1-score and concepts last-step F1-score as well formula mean-step F1-score, formula last-step F1-score and formula termination F1-score. The last three are derived by applying the extracted formulas to the predicted concepts for predicting the output/termination instead of using the respective neural network. The motivation behind is that if we achieve high concept accuracies and high formula accuracies then the formulas are likely to be representing the underlying algorithm (or data) accurately. Qualitative analysis We provide several qualitative experiments: (i) We fit a decision tree (DT) for the C → O task (C → T is not possible, due to DTs working on fixed size node-level features). Concepts and targets are obtained from the ground truth concepts and target classes of all training data nodes at each step for each graph. (ii) We also plot the concepts last/mean step F1-score vs epoch for each concept and provide further analysis on which concept the networks find the most difficult. (iii) We provide sample target class explanations for each algorithm. 5 Results and discussion Concept accuracies As can be seen from Tables 2&3 and Table 6, Appendix H (metrics with an asterisk) we are able to learn concepts with high F1-score (99% and higher F1-score for BFS and parallel coloring). Results show that GNNs are capable of producing high-level concepts, capturing either node or neighbourhood information, for these algorithmic tasks and the learned concept extractors strongly generalise – concept F1-score does not drop even for 5× larger graphs. Parallel algorithms: BFS and coloring For the BFS task both the baseline and bottlenecked model perform optimally in line with the state of the art. We therefore present results from the BFS 7
Table 3: Kruskal’s algorithm accuracies over 5 runs Model Metric |V | = 20 |V | = 50 |V | = 100 mean-step acc. 96.75±0.15% 95.41±0.09% 94.68±0.10% Standard last-step acc. 93.70±0.33% 90.10±2.80% 86.69±4.28% mean-step acc. 96.93±0.13% 95.86±0.37% 95.27±0.59% last-step acc. 94.00±0.24% 92.20±0.52% 91.29±0.86% formula mean-step acc. 96.79±0.37% 95.77±0.31% 95.25±0.54% Bottleneck formula last-step acc. 93.70±0.71% 91.92±0.47% 91.15±0.60% *concepts mean-step acc. 97.91±0.08% 97.21±0.22% 96.80±0.35% *concepts last-step acc. 99.56±0.29% 99.49±0.49% 97.09±0.29% Table 4: Sample explanations for each algorithm obtained from the learned model. cXS denotes colorXSeen, nISS is nodesInSameSet, lEV is lighterEdgesV isited, eIM is edgeInM st. Algorithm Thing to explain Explanation n is visited hasV isitedNeighbours(n) BFS continue execution ∃n.¬hasBeenV isited(n) ∧ hasV isitedNeighbours(n) (isColored(n) ∧ ¬hasP riority(n) ∧ c1S(n) ∧ ¬c2S(n))∨ n has color 2 (hasP riority(n) ∧ c1S(n) ∧ ¬c2S(n) ∧ ¬isColored(n)) parallel (isColored(n) ∧ ¬hasP riority(n) ∧ c1S(n) coloring ∧c2S(n) ∧ c3S(n) ∧ c4S(n))∨ n has color 5 (hasP riority(n) ∧ c1S(n) ∧ c2S(n) ∧c3S(n) ∧ c4S(n) ∧ ¬isColored(n)) continue execution ∃n.¬isColored(n) e not in MST (nISS(e) ∧ ¬eIM(e)) ∨ (¬lEV (e) ∧ ¬eIM(e)) Kruskal’s e in MST (lEV (e) ∧ nISS(e) ∧ eIM(e)) ∨ (lEV (e) ∧ ¬nISS(e) ∧ ¬eIM(e)) task in Appendix H. Results from the parallel coloring task are shown in Table 2. Apart from the high F1-score achieved, our results show that: (i) the bottleneck doesn’t have a major impact on the final model F1-score – original metrics6 remain the same or are better for both algorithms; (ii) we are able to learn concepts accurately and (iii) the extracted rules are accurate – applying them to the accurately predicted concepts in order to produce output has no significant negative effect on the predictive F1-score of our model – formula based accuracies do not deviate more than 5-6% than the original metrics. Qualitative analysis: decision trees We visualise the fitted decision trees (DTs) for each algorithm in Appendix I. In all cases the logic of the DT follows the logic of the original algorithm. Additionally, the leaf nodes of all decision trees contain samples from a single class showing that concepts were capable of capturing the complexity of the algorithm. Qualitative analysis: concept learning curves We present per concept learning curves for the parallel coloring in Figure 3 and for Kruskal’s in Figure 4: (i) Parallel coloring exhibits many occasions where there are drops of concept F1-score across almost all concepts. If we observe more carefully Figure 3a, we will notice that they coincide with a drop of the F1-score of hasP riority concept. This drop also explains the lower last-step concept F1-score – changing the coloring order early on may produce quite different final coloring. To confirm this observations, we trained an oracle model that is always provided with the correct value for hasP riority. Such oracle model achieved almost perfect concept F1-score – we provide a plot of the concept learning curves in Appendix J; (ii) The concept instability was present only in the beginning for Kruskal’s, but it converged to a stable solution. The reason edgeInM st concept remained with the lowest last-step F1-score is that the overall last-step F1-score of the model is lower. 6namely mean-, last-step F1-score and termination F1-score 8
(a) Concept mean-step F1-score (b) Concept last-step F1-score Figure 3: Concept accuracies per epoch of the parallel coloring algorithm. (1 point every 50 epochs). cXS is colorXSeen. Note the y axis scale. It can be observed that the hasP riority concept is one of the worst performing concepts. This leads to nodes being colored in a different order and therefore lower last-step concept F1-score for concepts related to colors. Standard deviation obtained from 5 runs. (a) Concept mean-step F1-score (b) Concept last-step F1-score Figure 4: Concept accuracies per epoch of the Kruskal’s algorihm on graphs with 20 nodes. (1 point per epoch). After an initial instability concepts are consistently accurate. Standard deviation obtained from 5 runs. Qualitative analysis: explanations We list examples of obtained explanations in Table 4 and present all explanations obtained from the algorithms in in Appendix K. The extracted rules show that concepts are one way to extract accurate representation of the rules of the algorithm. E.g. we can (re)infer from the listed rules for parallel coloring that for getting a given color that color should not be seen in the neighbourhood and colors coming before that one have already been seen. We additionally observed, that as the number of concepts increases, if we need shorter and more general rules we need more and more data. One way to alleviate such problem is L1 regularisation and pruning – we additionally perform an ablation study in Appendix K showing that without regularisation rules are still usable (giving good formula F1-score) but are less general. 6 Conclusions We presented concept-based reasoning on graph algorithms through Concept Bottleneck Graph Neural Networks. We demonstrated through the surveyed algorithms, that we can accurately learn node-level concepts without impacting performance. Moreover, by examining training data and model weights, we are capable of explaining each node-level output classes with formulas based on the defined concepts. Concepts also allow us perform a unsupervised rule extraction of certain graph-level tasks, such as deciding when to terminate. Extracted rules are interpretable and applying them does not heavily impact F1-score. 9
References Adadi, A. and Berrada, M. (2018). Peeking inside the black-box: a survey on explainable artificial intelligence (xai). IEEE access, 6:52138–52160. Albert, R. and Barabási, A.-L. (2002). Statistical mechanics of complex networks. Reviews of modern physics, 74(1):47. Baldassarre, F. and Azizpour, H. (2019). Explainability techniques for graph convolutional networks. arXiv preprint arXiv:1905.13686. Barbiero, P., Ciravegna, G., Georgiev, D., and Giannini, F. (2021). Lens: a python library for logic explained networks. arXiv preprint. Battaglia, P. W., Pascanu, R., Lai, M., Rezende, D. J., and Kavukcuoglu, K. (2016). Interaction networks for learning about objects, relations and physics. In Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett, R., editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 4502–4510. Cappart, Q., Chételat, D., Khalil, E., Lodi, A., Morris, C., and Velicˇkovic´, P. (2021). Combinatorial optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544. Chen, Z., Bei, Y., and Rudin, C. (2020). Concept whitening for interpretable image recognition. Nature Machine Intelligence, 2(12):772–782. Ciravegna, G., Giannini, F., Melacci, S., Maggini, M., and Gori, M. (2020). A constraint-based approach to learning and explanation. In AAAI, pages 3658–3665. Deac, A., Velickovic, P., Milinkovic, O., Bacon, P., Tang, J., and Nikolic, M. (2020). XLVIN: executed latent value iteration nets. CoRR, abs/2010.13146. Doshi-Velez, F. and Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608. Erdo˝s, P. and Rényi, A. (1960). On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci, 5(1):17–60. Galler, B. A. and Fisher, M. J. (1964). An improved equivalence algorithm. Communications of the ACM, 7(5):301–303. Georgiev, D. and Liò, P. (2020). Neural bipartite matching. In Graph Representation Learning and Beyond (GRL+) workshop. Ghorbani, A., Wexler, J., Zou, J., and Kim, B. (2019). Towards automatic concept-based explanations. arXiv preprint arXiv:1902.03129. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1263–1272. Gori, M. (2017). Machine Learning: A constraint-based approach. Morgan Kaufmann. Hamrick, J. B., Allen, K. R., Bapst, V., Zhu, T., McKee, K. R., Tenenbaum, J., and Battaglia, P. W. (2018). Relational inductive bias for physical construction in humans and machines. In Proceedings of the 40th Annual Meeting of the Cognitive Science Society, CogSci 2018, Madison, WI, USA, July 25-28, 2018. Jones, M. and Plassmann, P. (1993). A parallel graph coloring heuristic. SIAM J. Sci. Comput., 14:654–669. Joshi, C. K., Cappart, Q., Rousseau, L., Laurent, T., and Bresson, X. (2020). Learning TSP requires rethinking generalization. CoRR, abs/2006.07054. Kazhdan, D., Dimanov, B., Jamnik, M., and Liò, P. (2020a). Meme: Generating rnn model explana- tions via model extraction. arXiv preprint arXiv:2012.06954. 10
Kazhdan, D., Dimanov, B., Jamnik, M., Liò, P., and Weller, A. (2020b). Now you see me (cme): Concept-based model extraction. arXiv preprint arXiv:2010.13233. Kazhdan, D., Dimanov, B., Terre, H. A., Jamnik, M., Liò, P., and Weller, A. (2021). Is disentan- glement all you need? comparing concept-based & disentanglement approaches. arXiv preprint arXiv:2104.06917. Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Koh, P. W., Nguyen, T., Tang, Y. S., Mussmann, S., Pierson, E., Kim, B., and Liang, P. (2020). Concept bottleneck models. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 5338–5348. PMLR. Kruskal, J. B. (1956). On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical society, 7(1):48–50. Luo, D., Cheng, W., Xu, D., Yu, W., Zong, B., Chen, H., and Zhang, X. (2020). Parameterized explainer for graph neural network. arXiv preprint arXiv:2011.04573. McCluskey, E. J. (1956). Minimization of boolean functions. The Bell System Technical Journal, 35(6):1417–1444. McColl, H. (1878). The calculus of equivalent statements (third paper). Proceedings of the London Mathematical Society, 1(1):16–28. Mendelson, E. (2009). Introduction to mathematical logic. CRC press. Molnar, C. (2020). Interpretable machine learning. Lulu. com. Moore, E. F. (1959). The shortest path through a maze. In Proc. Int. Symp. Switching Theory, 1959, pages 285–292. Pal, A., Eksombatchai, C., Zhou, Y., Zhao, B., Rosenberg, C., and Leskovec, J. (2020). Pinnersage: Multi-modal user embedding framework for recommendations at pinterest. In Gupta, R., Liu, Y., Tang, J., and Prakash, B. A., editors, KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 2311–2320. ACM. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830. Pope, P. E., Kolouri, S., Rostami, M., Martin, C. E., and Hoffmann, H. (2019). Explainability methods for graph convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10772–10781. Prüfer, H. (1918). Neuer beweis eines satzes über permutationen. Arch. Math. Phys, 27(1918):742– 744. Quine, W. V. (1952). The problem of simplifying truth functions. The American mathematical monthly, 59(8):521–531. Schnake, T., Eberle, O., Lederer, J., Nakajima, S., Schütt, K., Müller, K., and Montavon, G. (2020). Higher-order explanations of graph neural networks via relevant walks. arXiv: 2006.03589. Shanahan, M., Nikiforou, K., Creswell, A., Kaplanis, C., Barrett, D. G. T., and Garnelo, M. (2020). An explicitly relational neural network architecture. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 8593–8603. PMLR. 11
Stokes, J. M., Yang, K., Swanson, K., Jin, W., Cubillos-Ruiz, A., Donghia, N. M., MacNair, C. R., French, S., Carfrae, L. A., Bloom-Ackermann, Z., Tran, V. M., Chiappino-Pepe, A., Badran, A. H., Andrews, I. W., Chory, E. J., Church, G. M., Brown, E. D., Jaakkola, T. S., Barzilay, R., and Collins, J. J. (2020). A Deep Learning Approach to Antibiotic Discovery. Cell, 180(4):688–702.e13. Strathmann, H., Barekatain, M., Blundell, C., and Velicˇkovic´, P. (2021). Persistent message passing. In ICLR 2021 Workshop on Geometrical and Topological Representation Learning. Tang, H., Huang, Z., Gu, J., Lu, B.-L., and Su, H. (2020). Towards scale-invariant graph-related problem solving by iterative homogeneous gnns. Advances in Neural Information Processing Systems, 33. Velicˇkovic´, P., Buesing, L., Overlan, M. C., Pascanu, R., Vinyals, O., and Blundell, C. (2020a). Pointer graph networks. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Velicˇkovic´, P., Ying, R., Padovano, M., Hadsell, R., and Blundell, C. (2020b). Neural execution of graph algorithms. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Vu, M. N. and Thai, M. T. (2020). Pgm-explainer: Probabilistic graphical model explanations for graph neural networks. arXiv preprint arXiv:2010.05788. Watts, D. J. (1999). Networks, dynamics, and the small-world phenomenon. American Journal of sociology, 105(2):493–527. Wijaya, M. A., Kazhdan, D., Dimanov, B., and Jamnik, M. (2021). Failing conceptually: Concept- based explanations of dataset shift. arXiv preprint arXiv:2104.08952. Williams, R. J. and Zipser, D. (1989). A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270–280. Xu, K., Li, J., Zhang, M., Du, S. S., Kawarabayashi, K., and Jegelka, S. (2020). What can neural networks reason about? In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Yan, Y., Swersky, K., Koutra, D., Ranganathan, P., and Hashemi, M. (2020). Neural execution engines: Learning to execute subroutines. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Yeh, C.-K., Kim, B., Arik, S. O., Li, C.-L., Pfister, T., and Ravikumar, P. (2019). On completeness- aware concept-based explanations in deep neural networks. arXiv preprint arXiv:1910.07969. Ying, R., Bourgeois, D., You, J., Zitnik, M., and Leskovec, J. (2019). Gnnexplainer: Generating explanations for graph neural networks. Advances in neural information processing systems, 32:9240. 12
A Adapting PrediNet to graphs As already discussed, PrediNet (Shanahan et al., 2020) is an architecture that operates on the image and learns to capture logical bias. To adapt it to the graph domain and more specifically the domain of our tasks, we made the following two modifications: (i) Instead of flattening a convoluted image we use global graph pooling (max pooling) for the calculation of the query and (ii) we do not concatenate any positional information to the input feature vectors. Everything else is kept the same as the original PrediNet: Qh = pool(H(cid:48)(t) )W h (11) 1 Q1 Qh = pool(H(cid:48)(t) )W h (12) 2 Q2 K = H(cid:48)(t) W (13) k Eh = softmax(QhK(cid:62))H(cid:48)(t) (14) 1 1 Eh = softmax(QhK(cid:62))H(cid:48)(t) (15) 2 2 Dh = EhW − EhW (16) 1 S 2 S out = Linear (cid:0) Dh(cid:1) (17) B Extraction of formulas By construction, the function f¯ ≈ f learnt from concepts C to outputs O is a Boolean map. As any Boolean function, the map f¯ : C → O can be converted into a First-Order Logic (FOL) formula ϕ in Disjunctive Normal Form (DNF) (Mendelson, 2009) by means of its truth-table. The truth table provides a formal mechanism to extract logic rules of increasing complexity for individual observations, for cluster of samples, or for a whole class. FOL extraction A logic formula can be extracted for each sample c ∈ C corresponding to a single row of the truth table: (cid:26) c¯ , if c ≥ t ϕ = c˜ ∧ . . . ∧ c˜ where c˜ := j j , for j = 1, . . . , k, (18) c 1 k j ¬c¯ , if c < t j j Example-level formulas can be formally aggregated providing logic rules ϕ for a cluster of samples S S ⊆ S∗ = {c¯ | f¯(c¯) = t} of the same class t (cid:95) (cid:95) ϕ = ϕ = c˜ ∧ . . . ∧ c˜ (19) S c 1 k c∈S c∈S Any formula ϕ can be thought of as a {0, 1}-valued mapping defined on C, that is equal to 1 exactly S on a cluster S. We may also get an explicit explanation relating ϕ and the Boolean function f¯ by S the FOL formula: ∀c¯ ∈ S : ϕ (c¯) → f¯(c¯) S A formula for a whole class t (e.g. t = 1) can be formally obtained using Eq. 19 by considering S∗, i.e. aggregating all the example-level rules for the same class t. Also in this case it is possible to get a FOL formula relating ϕ and the Boolean map f¯ by means of: ∀c¯ ∈ S∗ : f¯(c¯) ↔ ϕ(c¯) FOL simplification The aggregation of many example-level explanations may increase the length and complexity of the FOL formula being extracted for a cluster of samples or for a whole class. However, existing techniques as the Quine–McCluskey algorithm can be used to get compact and simplified equivalent FOL expressions (McColl, 1878; Quine, 1952; McCluskey, 1956). For instance, the explanation "(person ∧ nose) ∨ (¬person ∧ nose)" can be formally simplified in "nose". 13
C Modelling algorithmic reasoning We begin the modelling by designing the initial states for the algorithm:  1 i = s  BFS : x(1) = 1 i (cid:54)= s coloring : x(1) = (0, binary(p )) Kruskal’s : x(1) = (0, w ) i i i i i 0 otherwise (20) where s is a randomly chosen starting node, p denotes a randomly chosen7 constant integer priority i for node i and w denotes the weight of edge i. For parallel coloring we leave class 0 for denoting i uncolored nodes in the coloring task. The extra 0 in the input for Kruskal’s will be a bit value whether an edge is selected to be in the minimum spanning tree. Similar to Yan et al. (2020) we represent numbers in binary, and learn an embedding for each bit position. We also one hot encode any class information, such as visited/unvisited or color of a node (or no color present). Next step inputs are calculated according to the specific algorithm: for BFS, a node becomes reachable if it had visited neighbours8. For parallel coloring let us assume that there is a total ordering between colors, e.g. color 1 comes before color 2, and so on. A node keeps its color if it was previously colored and becomes colored with the first unseen color in the neighbourhood if it has the highest priority of all uncolored nodes in the neighbourhood. For Kruskal’s algorithm an edge remains in the MST it has been already selected and an edge is selected to be inserted in the MST if all lighter edges have been checked and the nodes, connected by that edge are not in the same set. Note that for Kruskal’s x(t) ranges over the edges:  1 x( it) = 1 BFS : x(t+1) = 1 ∃j.(j, i) ∈ E ∧ x(t) = 1 (21) i j 0 otherwise  x(t) col(t) (cid:54)= 0 (ci ol(t+1), p ) x(t)i = 0 ∧ p = max{p , j ∈ N (cid:48) ∪ {i}} coloring : x(t+1) = i i i i j i (22) i (0, p ) o∧ thc eo rl wi(t+ is1 e) = min{col, col ∈/ seen j} i  Kruskal’s : x(t+1) = x (1( it ,) w i) ii ff la il gre ha ted ry es de gle ec st ve id sited ∧ (a, b) = edge i (23) i set(t)(a) (cid:54)= set(t)(b) (0, w ) otherwise i where for the coloring task col(t) denotes the coloring of node i at step (t), N (cid:48) denotes the set of i i uncolored neighbours, and seen denotes the set of colors observed in the neighbourhood of node i. i For Kruskal’s, (a, b) = edge denotes that edge i connects nodes a and b, and set(t)(n) denotes the i set of node n at timestep t. Before we proceed to the next step, if edge edge = (a, b) is added to the i MST, we unify the sets of the two edge nodes, i.e. set(t+1)(a) = set(t+1)(b) = set(t)(a) ∪ set(t)(b). The ground-truth outputs we supervise towards are yˆ(t) = x(t+1) for BFS, yˆ(t) = col(t+1) for the i i i i coloring task and yˆ(t) = x(t+1)[0] for Kruskal’s (i.e. 0/1 value whether each edge is in the MST). We i i do not aim to reconstruct input node priorities or input edge weights. D Algorithms and concepts Given the concepts algorithms in Table 5, here how each algorithm can be executed with its own concepts: 7We ensure no two nodes connected by an edge have the same priority 8As we utilise self-loops for retention of self-information, this covers the corner case of a starting node being visited without having visited neighbours 14
Table 5: Algorithms and their corresponding concepts. Explanations are added to give a notion of how concepts can be used. Example ground-truth explanations Algorithm Concepts (not provided to the model) hasBeenV isited (hBV ) hV N (i) =⇒ y(t) = 1 BFS i hasV isitedN eighbours (hV N ) ∃i.¬hBV (i) ∧ hV N (i) =⇒ τ (t) = 1 iC(i) ∧ c1S(i) ∧ ¬c2S(i) =⇒ y(t) = 2 i isColored (iC), hasP riority (hP ) coloring colorXSeen (cXS), X ∈ {1, .., 5} (¬iC(i) ∧ hP (i) ∧ c1S(i) ∧ c2S(i) ∧ ¬c3S(i)) =⇒ y(t) = 3 i (lEV (i) ∧ ¬nISS(i) ∧ ¬eIM (i)) lighterEdgesV isited (lEV ) =⇒ y(t) = 1 i Kruskal’s nodesInSameSet (nISS) edgeInM st (eIM ) (lEV (i) ∧ nISS(i) ∧ ¬eIM (i)) =⇒ y(t) = 0 i • BFS – a node is visited if it has a visited neighbour and the execution continues until there is an unvisited node with visited neigbours. • parallel coloring – check if node has been colored, if not, check if it has priority to be colored on this step, if yes, check colors in the neighbourhood. Continue execution until a node with ¬isColored exists. • Kruskal’s algorithm – check if lighter edges have been visited by the algorithm, if not, edge is not in MST. Otherwise, check if the nodes connected by this edge belong in the same set and then if the edge has been previously selected. If the nodes are in the same set, the edge is in the MST for the next iteration if it has been selected for the MST previously. If nodes are not in the same set (this implies that the edge has not been selected for the MST), then this edge is in the MST from now on. E Data generation In general we aimed to maintain 10:1:1 train:validation:test split ratio and to preserve the ratio across different data generation techniques. For the BFS task we generate graphs from the following categories: • *Ladder graphs • *2D grid graphs – during generation we aimed the grid to be as close to square as possible • Trees – uniformly generated from the Prüfer sequence (Prüfer, 1918) (cid:16) (cid:17) • *Erdo˝s-Rényi (Erdo˝s and Rényi, 1960), p of edge is min log 2 |V | , 0.5 |V | • *Barabási-Albert (Albert and Barabási, 2002) graphs, with either 4 or 5 edges attached to each incoming node • 4-Community graphs – generated by creating 4 disjoint Erdo˝s-Rényi graphs with edge probability 0.7 and then interconnecting their nodes with probability of 0.01 • 4-Caveman (Watts, 1999) – intra-clique edges are removed with p = 0.7 and 0.025|V | shortcut edges are inserted after that between cliques. From each category we generate 100 graphs for training and 10 for validation/testing. In total that is 700 for training and 70 for validation/testing. To test strong generalisation, for a given number of nodes in the graph, we generate 10 more test graphs of each category. 15
Figure 5: The dataflow for the Kruskal’s algorithm. Blue variables are input variables, black inputs come from previous steps, supervision is performed on orange outputs. For the parallel coloring task we limit the number of colors to 5. We generate 800/80/80 graphs for training/validation/testing with nodes of fixed degree 5, so as to limit the possibility of 5 colors not being enough. We pick the priority p uniformly from the interval [0, 255]. i For Kruskal’s, we generate 500 graphs of size 8 of each category with a star from the list above for training and 50 for testing/validation of sizes [8, 20, 50, 100]. To facilitate easier retention of self-information in the GNN, for the BFS and parallel coloring tasks, we insert self-loops to all nodes for a given graph similar to Velicˇkovic´ et al. (2020b). For Kruskal’s, all nodes start with a pointer to itself (see Appendix F) F Implementing Kruskal’s algorithm For the implementation of the Kruskal’s algorithm we need to implement a min finding subroutine with masking and a disjoint-set union (DSU) (Galler and Fisher, 1964) data-structure. We architecture the first as a Neural Execution Engine (NEE) (Yan et al., 2020) that takes in the binary edge weights W , last step lighterEdgesV isited concept for all edges and last-step outputs Y(t−1). The NEE b kr outputs a pointer to the next minimal edge m(t) and the next lighterEdgesV isited for all edges. We implement the DSU as a Pointer Graph Network (PGN) (Velicˇkovic´ et al., 2020a) that takes as input: (i) an indicator, which is the two nodes of the minimal edge on the current step (teacher-forced during training), (ii) node priorities (used as a tie-break when unifying sets) and (iii) what’s the last hidden state of the PGN algorithm H(t−1). The PGN’s processor also takes the last step pointers Π(t−1) of the DSU data structure. The PGN predicts: (i) whether the two nodes are in the same set Y(t) ; (ii) a mask µ(t) over which nodes should change their DSU pointers and (iii) an estimate of pgn the new pointer matrix. The next step pointers are updated from (ii) and (iii) as follows: (cid:16) (cid:17) Π(cid:101) ( ijt) = µ( it)Π(cid:101) ( ijt−1) + 1 − µ( it) I j=argmax (cid:16) α(t)(cid:17) Π( ijt) = Π(cid:101) ( ijt) ∨ Π(cid:101) ( jt i) (24) k ik To predict the rest of the concepts9 for every edge e = (u, v) we first concatenate the hidden state of the first NEE transformer Hm with the hidden states of the PGN for nodes u and v, H(t) and H(t). e u v 9apart from lighterEdgesV isited 16
Table 6: BFS accuracies over 5 runs Model Metric |V | = 20 |V | = 50 |V | = 100 mean-step acc. 100.0±0.0% 100.0±0.0% 100.0±0.0% Standard last-step acc. 100.0±0.0% 100.0±0.0% 100.0±0.0% term. acc. 100.0±0.0% 100.0±0.0% 100.0±0.0% mean-step acc 100.0±0.0% 100.0±0.0% 100.0±0.0% last-step acc 100.0±0.0% 100.0±0.0% 100.0±0.0% term. acc. 100.0±0.0% 100.0±0.0% 100.0±0.0% Bottleneck (+next step pool) formula mean-step acc. 100.0±0.0% 100.0±0.0% 100.0±0.0% formula last-step acc. 100.0±0.0% 100.0±0.0% 100.0±0.0% formula term. acc. 100.0±0.0% 100.0±0.0% 100.0±0.0% *concepts mean-step acc. 100.0±0.0% 100.0±0.0% 100.0±0.0% *concepts last-step acc. 100.0±0.0% 100.0±0.0% 100.0±0.0% The concept decoder takes this as input and provides the rest of the concepts nodesInSameSet and edgeInM st, which is then passed to the output decoder to produce final Y(t) whether each edge is kr selected in the MST at step t. The whole process is summarised in Figure 5. Yellow variables are those we supervise on. For a full list of losses used, see Appendix G. G Algorithms and respective losses We optimise our models based on the following losses: • Binary cross-entropy for concepts predictions • Binary cross-entropy for predicting termination (when applicable to the algorithm) Additionally for each algorithm, we add: • BFS: – Binary cross-entropy for reachability predictions • parallel coloring: – categorical cross-entropy for reachability predictions • Kruskal’s: – categorical cross-entropy for predicting the position of the next selected edge – binary cross-entropy for predicting whether two nodes are in the same set (cf. Velicˇkovic´ et al. (2020a)) – binary cross-entropy for predicting mask of the pointers to be changed (cf. Velicˇkovic´ et al. (2020a)) – loss from positive examples is scaled by 1 10 H BFS accuracies We present BFS F1-score in Table 6. Since BFS is relatively simple task both the baseline and the concept-bottleneck model achieve perfect F1-score. I Node-level decision tree The decision trees are shown in Figures 6a&7. In all cases the rules follow the logic of the original algorithm. It should be noted that the decision tree for the BFS task consists of just a single concept – why are concepts necessary then? The answer is simple – the addition of termination requires us to use the concepts hasBeenV isited. 17
(a) The decision tree for the BFS task. Due to the presence of self-loops the rule is simple – check if (b) The decision tree for the Kruskal’s algorithm. visited neighbours exist (same node is always present in the neighbours list). Figure 6: Decision trees for BFS and Kruskal’s Table 7: BFS explanations. The model explanations match the ground-truth ones. Available concepts are hasV isitedN eighbours and hasBeenV isited. Max number of explanation occurrences is 5. Thing to explain Explanation # of occurrences n is NOT visited ¬hasBeenV isited(n) ∧ ¬hasV isitedNeighbours(n) 5 n is visited hasV isitedNeighbours(n) 5 continue execution ¬hasBeenV isited(n) ∧ hasV isitedNeighbours(n) 5 Although not displayed in Figure 7 due to space constraints, the decision tree helped discover a ’bug’ – one of the concepts (color5Seen) was never used, which suggests that it may not be necessary. J Oracle model for parallel coloring In the main body of the paper (Figure 3) we showed that quite often there were drops of F1-score for almost all concepts for the parallel coloring task. Based on more careful observation it was hypothesised that the hasP riority is too critical for the task. To prove this we plot the per-step concept F1-score of an oracle model that is always provided with the correct value for that concept. Figure 8 clearly shows the importance of the hasP riority concept – hardcoding its value to the ground-truth allows us to learn the task almost perfectly. K Example explanations of algorithms We list example explanations for BFS in Table 7 and for Kruskal’s. Our observations are that for simpler algorithms with fewer possible concept combinations, such as BFS, explanations match very closely the ground truth – the only difference is that unvisited rule relies on ¬hasBeenV isited. This, however, would not break the execution – in fact the network learns to use this concept to reinforce its outputs since ¬hasBeenV isited always holds when ¬hasV isitedN eighbours. For the parallel coloring task, we first draw the attention to the results without using any regularisation techniques in Table 9. The main observation is that regularisation is not necessary to achieve high F1-score – formula-based metrics do not deviate largely from formulas obtained without using metrics. However, as can be seen from Table 10 when the possible combinations and outputs grow, some combinations may not be observed during training and therefore the explanations may contain unnecessary concepts (e.g., whether color 5 is seen in the exlanation for uncolored node n) or sometimes not consider some concept (red in Table 10). Given the applications of the formulas still achieved very high F1-score, our hypothesis was that such algorithm require regularisation techniques, such as adding auxiliary L1 loss and pruning, in order to force the model to rely on as few concepts 18
Table 8: Kruskal’s explanations. The model explanations match the ground-truth ones. Available concepts are nodesInSameSet, lighterEdgesV isited (lEV ) and edgeInM st. Max number of explanation occurrences is 5. Thing to explain Explanation # of occurrences (nodesInSameSet(e) ∧ ¬edgeInMst(e))∨ e NOT in MST 5 (¬lEV (e) ∧ ¬edgeInMst(e)) (lEV (e) ∧ edgeInMst(e))∨ 1 (lEV (e) ∧ ¬nodesInSameSet(e)) e in MST (lEV (e) ∧ nodesInSameSet(e) ∧ edgeInMst(e))∨ 4 (lEV (e) ∧ ¬nodesInSameSet(e) ∧ ¬edgeInMst(e)) Table 9: Bottleneck model without pruning/L1 loss gives applicable accuracies – formula accuracies are as high as their counterparts. But are they as interpretable? Parallel coloring |V | = 20 |V | = 50 |V | = 100 mean-step acc. 99.55±0.17% 99.39±0.22% 99.22±0.32% last-step acc. 99.33±0.23% 99.06±0.37% 98.95±0.52% term. acc. 99.69±0.27% 99.41±0.21% 99.07±0.38% formula mean-step acc. 99.55±0.18% 99.39±0.22% 99.22±0.33% formula last-step acc. 99.31±0.18% 99.03±0.36% 98.93±0.52% formula term. acc. 99.66±0.27% 99.41±0.21% 99.07±0.38% concepts mean-step acc. 99.81±0.08% 99.68±0.11% 99.6±0.16% concepts last-step acc. 99.61±0.16% 99.38±0.2% 99.27±0.27% as possible. Results confirmed our hypothesis – Table 11 shows that 4/5 times all explanations are correct and only once the rule for color 2 contained an extra clause. L Software The code for the experiments is implemented in Python 3, relying upon open-source libraries such as Scikit-learn (BSD license) (Pedregosa et al., 2011) and Pytorch (BSD license) (Paszke et al., 2019). All the experiments have been run on an NVIDIA Titan Xp 12 GB GPU. 19
Table 10: Some example parallel coloring explanations for node n, no regularisation applied. (For brevity we omit listing every single explanation.) Class explanations do not match the ground truth and sometimes contain usage of variables, whose value can be inferred from the values of other variables. Available concepts are isColored (iC), hasP riority (hP ) and colorXSeen, X ∈ {1, .., 5}. Max number of explanation occurrences is 5. Thing to explain Explanation # of occurrences (¬iC(n) ∧ ¬hP (n) ∧ ¬color5Seen(n))∨ n is not colored (color5Seen(n) ∧ color4Seen(n) ∧ ¬iC(n) ∧ ¬hP (n) 1 ¬color1Seen(n) ∧ ¬color2Seen(n)) (iC(n) ∧ ¬hP (n) ∧ color1Seen(n) ∧ ¬color2Seen(n))∨ n has color 2 2 (hP (n) ∧ color1Seen(n) ∧ ¬iC(n) ∧ ¬color2Seen(n)) (hP (n) ∧ color2Seen(n) ∧ color3Seen(n) ∧color4Seen(n) ∧ ¬color5Seen(n) ∧ ¬iC(n))∨ n has color 5 2 (iC(n) ∧ color1Seen(n) ∧ color2Seen(n)∧ color3Seen(n) ∧ color4Seen(n) ∧ ¬color5Seen(n)) (hP (n)∧color1Seen ∧ color2Seen(n) ∧ color3Seen(n) ∧color4Seen(n) ∧ ¬color5Seen(n) ∧ ¬iC(n))∨ n has color 5 3 (iC(n) ∧ color1Seen(n) ∧ color2Seen(n)∧ color3Seen(n) ∧ color4Seen(n) ∧ ¬color5Seen(n)) Table 11: Parallel coloring explanations for node n, with regularisation applied. Class explanations are concise, consistent across seeds and very close to the ground truth. Available concepts are isColored (iC), hasP riority (hP ) and colorXSeen, X ∈ {1, .., 5}. Max number of explanation occurrences is 5. Thing to explain Explanation # of occurrences n is not colored ¬iC(n) ∧ ¬hP (n) 5 (iC(n) ∧ ¬hP (n) ∧ ¬color1Seen(n))∨ n has color 1 5 (hP (n) ∧ ¬color1Seen(n) ∧ ¬iC(n)) (iC(n) ∧ ¬hP (n) ∧ color1Seen(n) ∧ ¬color2Seen(n))∨ 4 (hP (n) ∧ color1Seen(n) ∧ ¬color2Seen(n) ∧ ¬iC(n)) n has color 2 (iC(n) ∧ ¬hP (n) ∧ color1Seen(n) ∧ ¬color2Seen(n))∨ (hP (n) ∧ color1Seen(n) ∧ ¬iC(n) ∧ ¬color2Seen(n))∨ 1 (hP (n) ∧ color4Seen(n) ∧ ¬iC(n)∧ ¬color2Seen(n) ∧ ¬color3Seen(n)) (iC(n) ∧ ¬hP (n) ∧ color1Seen(n) ∧color2Seen(n) ∧ ¬color3Seen(n))∨ n has color 3 5 (hP (n) ∧ color1Seen(n) ∧ color2Seen(n) ∧¬color3Seen(n) ∧ ¬iC(n)) (iC(n) ∧ ¬hP (n) ∧ color1Seen(n) ∧color2Seen(n) ∧ color3Seen(n) ∧ ¬color4Seen(n))∨ n has color 4 5 (hP (n) ∧ color1Seen(n) ∧ color2Seen(n) ∧color3Seen(n) ∧ ¬color4Seen(n) ∧ ¬iC(n)) (iC(n) ∧ ¬hP (n) ∧ color1Seen(n) ∧color2Seen(n) ∧ color3Seen(n) ∧ color4Seen(n))∨ n has color 5 5 (hP (n) ∧ color1Seen(n) ∧ color2Seen(n) ∧color3Seen(n) ∧ color4Seen(n) ∧ ¬iC(n)) 20
Figure 7: The decision tree for the parallel coloring task. The classifier first checks colors seen around, then priority and only then, whether a node is colored or not. 21
(a) Concept mean-step F1-score (b) Concept last-step F1-score (c) Concept mean-step F1-score, hasP riority pro-(d) Concept last-step F1-score, hasP riority pro- vided vided Figure 8: Concept accuracies per epoch of the parallel coloring algorithm with and without hardcoding the hasP riority concept. Clearly, hardcoding this concept makes our morel much more stable. 22
