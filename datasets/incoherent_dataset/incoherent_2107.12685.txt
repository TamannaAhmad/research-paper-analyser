On the Role of Optimization in Double Descent: A Least Squares Study Ilja Kuzborskij Csaba Szepesvári Omar Rivasplata DeepMind DeepMind, Canada University College London University of Alberta, Edmonton Amal Rannen-Triki Razvan Pascanu DeepMind DeepMind Abstract Empirically it has been observed that the performance of deep neural networks steadily improves as we increase model size, contradicting the classical view on overfitting and generalization. Recently, the double descent phenomena has been proposed to reconcile this observation with theory, suggesting that the test error has a second descent when the model becomes sufficiently overparametrized, as the model size itself acts as an implicit regularizer. In this paper we add to the growing body of work in this space, providing a careful study of learning dynamics as a function of model size for the least squares scenario. We show an excess risk bound for the gradient descent solution of the least squares objective. The bound depends on the smallest non-zero eigenvalue of the covariance matrix of the input features, via a functional form that has the double descent behaviour. This gives a new perspective on the double descent curves reported in the literature. Our analysis of the excess risk allows to decouple the effect of optimisation and generalisation error. In particular, we find that in case of noiseless regression, double descent is explained solely by optimisation-related quantities, which was missed in studies focusing on the Moore-Penrose pseudoinverse solution. We believe that our derivation provides an alternative view compared to existing work, shedding some light on a possible cause of this phenomena, at least in the considered least squares setting. We empirically explore if our predictions hold for neural networks, in particular whether the covariance of intermediary hidden activations has a similar behaviour as the one predicted by our derivations. 1 Introduction Deep Neural Networks have shown amazing versatility across a large range of domains. Among one of their main features is their ability to perform better with scale. Indeed, some of the most impressive results [see e.g. Brock et al., 2021, Brown et al., 2020, Senior et al., 2020, Schrittwieser et al., 2020, Silver et al., 2017, He et al., 2016 and references therein] have been obtained often by exploiting this fact, leading to models that have at least as many parameters as the number of examples in the dataset they are trained on. Empirically, the limitation on the model size seems to be mostly imposed by hardware or compute. From a theoretical point of view, however, this property is quite surprising and counter-intuitive, as one would expect that in such extremely overparametrized regimes the learning would be prone to overfitting [Hastie et al., 2009, Shalev-Shwartz and Ben-David, 2014]. Recently Belkin et al. [2019] proposed Double Descent (DD) phenomena as an explanation. They argue that the classical view of overfitting does not apply in extremely over-parameterized regimes, which were 1 1202 luJ 72 ]GL.sc[ 1v58621.7012:viXra
less studied prior to the emergence of the deep learning era. The classical view in the parametric learning models was based on error curves showing that the training error decreases monotonically when plotted against model size, while the corresponding test errors displayed a U-shape curve, where the model size for the bottom of the U-shape was taken to achieve the ideal trade-off between model size and generalization, and larger model sizes than that were thought to lead to ‘overfitting’ since the gap between test errors and training errors increased. Figure 1: Evaluation of a synthetic setting inspired by Belkin et al. [2020]. We consider a linear regression problem (n = 20, d ∈ [100]), where regression parameters are fixed, and instances are sampled from [−1, 1]-truncated normal density. GD is run with α = 0.05 and initialization variance is set as ν2 = 1/d. The first row demonstrates behavior init of (??), the second shows an estimate of the excess risk (on 104 held-out points), and the third an estimate of the optimization error. The classical U-shape error curve dwells in what is now called the under-parameterized regime, where the model size is smaller than the size of the dataset. Arguably, the restricted model sizes used in the past were tied to the available computing power. By contrast, it is common nowadays for model sizes to be larger than the amount of available data, which we call the over-parameterized regime. The divide between these two regimes is marked by a point where model size matches dataset size, which Belkin et al. [2019] called the interpolation threshold. The work of Belkin et al. [2019] argues that as model size grows beyond the interpolation threshold, one will observe a second descent of the test error that asymptotes in the limit to smaller values than those in the underparameterized regime, which indicates better generalization rather than overfitting. To some extent this was already known in the nonparametric learning where model complexity scales with the amount of data by 2
design (such as in nearest neighbor rules and kernels), yet one can generalize well and even achieve statistical consistency [Györfi et al., 2002]. This has lead to a growing body of works trying to identify the mechanisms behind DD, to which the current manuscript belongs too. We refer the reader to Section 2, where the related literature is discussed. Similar to these works, our goal is also to understand the cause of DD. Our approach is slightly different: we explore the least squares problem that allows us to work with analytic expressions for all the quantities involved. Fig. 1 provides a summary of our findings. In particular, it shows the behaviour of the excess risk in a setting with random inputs and noise-free labels, for which in Section 3 we prove a (cid:104) (cid:105) bound that has the form E (1 − αλ(cid:98)+ )2T (cid:107)w(cid:63)(cid:107)2 + (cid:107)w √(cid:63)(cid:107)2 , for a rapidly decaying spectrum of the sample min n covariance. In this setting, the linear predictors project d-dimensional features by dot product with a weight vector which must be learned from data; then w(cid:63) refers to the optimal solution, α is a constant learning rate, and n is the number of examples in the training set. Note that the feature dimension d coincides with the number of parameters in this particular setting, hence d > n is the overparameterized regime. The quantity λ(cid:98)+ is of special importance: It is the smallest positive eigenvalue of the sample covariance matrix of the min features. In particular, we observe that the excess risk is controlled by the smallest non-zero eigenvalue of the covariance of the features, and its functional dependence exhibits a profile similar to the DD curve. This offers a new perspective on the problem. In Fig. 1 we observe a peaking behavior, not only in the excess risk, but also in the quantity that we label ‘optimization error’ which is a special term of the excess risk bound that is purely related to optimization. The peaking behaviour of the excess risk (MSE in case of the square loss) was observed and studied in a number of settings [Belkin et al., 2019, Mei and Montanari, 2019, Derezinski et al., 2020]; however, the connection between the peaking behavior and optimization so far received less attention. This pinpoints a less-studied setting and we conjecture that the DD phenomenon occurs due to λ(cid:98)+ . In the absence of label min noise, we conclude that DD manifests due to the optimization process. On the other hand, when label noise is present, in addition to the optimization effect, λ(cid:98)+ also has an effect on the generalization error. min Our contributions: Our main theoretical contribution is provided in Section 3. In particular, Section 3.1 focuses on the noise-free least squares problem, Section 3.2 adds noise to the problem, and Section 3.3 deals with concentration of the sample-dependent λ(cid:98)+ around its population counterpart. Sections 4 and 5 provide min an in-depth discussion on the implications of our findings and an empirical exploration of the question whether simple neural networks have a similar behaviour. Notation: The linear algebra/analysis notation used in this work is defined in Appendix A. We briefly mention here that we denote column vectors and matrices with small and capital bold letters, respectively, e.g. α = [α 1, α 2, . . . , α d](cid:62) ∈ Rd and A ∈ Rd1×d2. Singular values of a rectangular matrix A ∈ Rn×d are denoted by s (A) = s (A) ≥ . . . ≥ s (A) = s (A). The rank of A is r = max{k | s (A) > max 1 n∧d min k 0}. Eigenvalues of a Positive Semi-Definite (PSD) matrix M ∈ Rd×d are non-negative and are denoted λ (M ) = λ (M ) ≥ . . . ≥ λ (M ) = λ (M ), while the smallest non-zero eigenvalue is denoted max 1 d min λ+ (M ). min Next, we set the learning theory notation. In a parametric statistical learning problem the learner is given a training set S = (Z , . . . , Z ), which is an n-tuple consisting of independent random elements, called 1 n training examples, distributed according to some unknown distribution D ∈ M (Z), where Z is called the 1 example space. The learner’s goal is to select parameter w from some parameter space W so as to minimize (cid:82) the population loss L(w) = (cid:96)(w, z)D(dz), where (cid:96) : W × Z → [0, 1] is some given loss function. A Z learner following the Empirical Risk Minimization (ERM) principle selects a w with the smallest empirical 3
loss Lˆ (w) = ((cid:96)(w, Z ) + · · · + (cid:96)(w, Z ))/n over the training set. In this report we consider a Euclidean S 1 n parameter space: W = Rd. We consider a least squares regression problem. In this setting, each example is an instance-label pair: Z = (X , Y ) ∈ B × [0, 1]. We assume that inputs X are from the Euclidean ball of unit radius B ⊂ Rd, i i i 1 i 1 and labels Y are in the unit interval [0, 1]. For a suitably chosen parameter vector w, the noiseless regression i model is f (X) = X(cid:62)w and the model with label noise is f (X) = X(cid:62)w + (cid:15) where (cid:15) ∼ N (0, σ2). The loss function is the square loss: (cid:96)(w, Z ) = (f (X ) − Y )2/2. i i i 2 Related Work The literature on the DD of the test error has mainly focused on the ordinary least squares with the explicit solution given by the Moore-Penrose pseudo-inverse. Early works have focused on instance-specific settings (making distributional assumptions on the inputs) while arguing when the analytic pseudo-inverse solutions yield DD behaviour [Belkin et al., 2020]. This was later extended to a more general setting showcasing the control of DD by the spectrum of the feature matrix [Derezinski et al., 2020]. In this paper we also argue that the spectrum of the covariance matrix has a critical role in DD, however we take into account the effect of GD optimization, which was missed by virtually all the previous literature due to their focusing on analytic solutions. The effect of the smallest non-zero eigenvalue on DD, through a condition number, was briefly noticed by Rangamani et al. [2020]. In this work we carry out a more comprehensive analysis and show how the excess risk of GD is controlled the smallest eigenvalue. In particular, λ(cid:98)+ has a “U”-shaped behaviour min as the number of features increases, and we give a high-probability characterization of this behavior when inputs are subgaussian. To some extent, this is a non-asymptotic manifestation of the Bai-Yin law, whose connection to DD in an asymptotic setting was noted by Hastie et al. [2019, Theorem 1]. Some interest was also dedicated to the effect of bias and variance of DD [Mei and Montanari, 2019] in the same pseudo-inverse setting, while more involved fine-grained analysis was later carried out by Adlam and Pennington [2020]. In this work we focus on the influence of the optimization error, which is complementary to the bias-variance effects (typically we care about it once optimization error is negligible). DD behaviour was also observed beyond least squares, in neural networks and other interpolating models [Belkin et al., 2019]. To some extent a formal connection to neural networks was first made by Mei and Montanari [2019] who studied asymptotic behaviour of the risk under the random feature model, when n, dinput, dRF → ∞ while having n and dRF fixed. Later on, with popularity of Neural Tangent dinput dinput Kernel (NTK) the connection became clearer as within NTK interpretation shallow neural networks can be paralleled with kernelized predictors [Bartlett et al., 2021]. A detailed experimental study of DD in deep neural networks was carried out by [Nakkiran et al., 2019], who showed that various forms of regularization mitigate DD. In this work, we explain DD in least-squares solution obtained by GD through the spectrum of the features, where optimization error has a visible role. While we do not present formal results for neural networks, but we empirically investigate whether our conclusions extend to shallow neural nets as would be suggested by NTK theory. 3 Excess Risk of the Gradient Descent Solution We focus on learners that optimize parameters via the Gradient Descent algorithm. We treat GD as a measurable map A : S × Rd → Rd, where S = Zn is the space of size-n training sets. Given a training set S ∈ S and an initialization point w ∈ W, we write A (w ) to indicate the output obtained recursively by 0 S 0 4
the standard GD update rule with some fixed step size α > 0, i.e. A (w ) = w , where S 0 T w t = w t−1 − α∇L(cid:98)S(w t−1), t = 1, . . . , T . We look at the behavior of GD in the overparameterized regime (d > n) when the initialization parameters are sampled from an isotropic Gaussian density, that is W ∼ N (0, ν2 I ) with some initialization 0 init d×d variance ν2 . It is well-known that in the overparameterized regime, GD is able to achieve zero empirical init loss. Therefore, rather than focusing on the generalization gap L(A (W )) − Lˆ (A (W )) it is natural to S 0 S S 0 compare the loss of A (W ) to that of the best possible predictor. Thus, we consider the excess risk defined S 0 as E(w(cid:63)) = L(A (W )) − L(w(cid:63)) , w(cid:63) ∈ arg min L(w) . S 0 w∈Rd Our results are based on a the requirement that A satisfies the following regularity condition: S Definition 1. A map f : Rd → Rd is called (∆, M )-admissible, where M is a fixed PSD matrix and ∆ ≥ 0, if for all w, w(cid:48) ∈ Rd the following holds: (cid:107)f (w) − f (w(cid:48))(cid:107) ≤ ∆(cid:107)w − w(cid:48)(cid:107) . M Notice that the norm on the left-hand side is (cid:107) · (cid:107) , while that on the right-hand side is the standard M Euclidean norm. Also note that this inequality entails a Lipschitz condition with Lipschitz factor ∆. Our first main result gives an upper bound on the excess risk of GD output, assuming that the output of A is of low-rank, in the sense that for some low-rank orthogonal projection M ∈ Rd×d we assume that S M A (w) = A (w) almost surely (a.s.) with respect to S, for any initialization w. This condition is of S S interest in the overparameterized regime, where the learning dynamics effectively happens in a subspace which is arguably of much smaller dimension than the whole parameter space. The following theorem bounds the excess risk (with respect to a possibly non-convex but smooth loss) of any algorithm that satisfies Definition 1 with some (∆, M ). Later it will become apparent that in a particular learning problem this pair consists of data-dependent quantities. Importantly, the theorem demonstrates how the excess risk is controlled by the learning dynamics on the subspace spanned by M (the first and the second terms on the right hand side). It also shows how much is lost due to not learning on the complementary subspace (the third term). The first two terms will become crucial in our analysis of the double descent, while we will show that the last term will vanish as n → ∞. Theorem 1 (Excess Risk). Assume that W ∼ N (0, ν2 I ), and assume that A is (∆, M )-admissible 0 init d×d S (Definition 1), where ∆ and W are independent. Further assume M A (w) = A (w) for any w, and that 0 S S L and Lˆ are H-smooth. Then, for any w(cid:63) ∈ arg min L(w) we have w∈Rd   E[E(w(cid:63))] ≤ H E[∆2] (cid:0) (cid:107)w(cid:63)(cid:107)2 + ν2 (2 + d)(cid:1) + E[(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 ] + 1 E[(cid:107)w(cid:63)(cid:107)2 ] .  init S M 2 I−M  (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (1) (2) (3) In particular for GD, having α ≤ 1/H, E[(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 ] ≤ 2αT L(w(cid:63)) . S M The proof is in Appendix C. The main steps are using the H-smoothness of L to upper-bound E(w(cid:63)) in terms of the squared norm of A (W ) − w(cid:63) and decomposing the latter as the sum of the squared norms of S 0 5
its projections onto the space spanned by M and its orthogonal complement, by the Pythagorean theorem. Then A (W ) − w(cid:63) = A (W ) − A (w(cid:63)) + A (w(cid:63)) − w(cid:63) is used on the subspace spanned by M : the S 0 S 0 S S norm of A (W ) − A (w(cid:63)) is controlled by using the admissibility of A and Gaussian integration, and the S 0 S S norm of A (w(cid:63)) − w(cid:63) is controlled by the accumulated squared norms of gradients of Lˆ over T steps of S S gradient descent, which is conveniently bounded by 2αT Lˆ (w(cid:63)) when α ≤ 1/H due to the H-smoothness S of Lˆ . S We will rely on Theorem 1 for our analysis of the Least-Squares problem as follows. 3.1 Least-Squares with Random Design and No Label Noise Consider a noise-free linear regression model with random design: Y = X(cid:62)w(cid:63) where instances X are distributed according to some unknown distribution P supported on a d-dimensional X unit Euclidean ball. After observing a training sample S = ((X , Y ))n , we run GD on the given empirical i i i=1 square loss n 1 (cid:88) Lˆ (w) = (w(cid:62)X − Y )2 . S i i 2n i=1 In the setting of our interest, the sample covariance matrix Σ(cid:98) = (X 1X(cid:62) 1 + · · · + X nX(cid:62) n )/n might be degenerate, and therefore we will occasionally refer to the non-degenerate subspace U = [u , . . . , u ], r 1 r where U is given by the Singular Value Decomposition (SVD): Σ(cid:98) = U SV (cid:62) and u 1, . . . , u r are the eigenvectors corresponding to the eigenvalues λˆ 1, . . . , λˆ r, where λˆ i = λ i(Σ(cid:98) ), arranged in decreasing order: λ 1(Σ(cid:98) ) ≥ λ 2(Σ(cid:98) ) ≥ · · · ≥ λ r(Σ(cid:98) ) > 0 and r = rank(Σ(cid:98) ). We write λ(cid:98)+ min = λ+ min(Σ(cid:98) ) = λ r(Σ(cid:98) ) for the minimal non-zero eigenvalue, and we denote 2 M(cid:99) = U rU (cid:62) r . Note that M(cid:99) = M(cid:99). Now we state our main result in this setting. Theorem 2. Assume that W ∼ N (0, ν2 I). Then, for any w(cid:63) ∈ arg min L(w) and any x > 0, with 0 init w∈Rd probability 1 − e−x over random samples S we have E[E(w(cid:63))] ≤ E (cid:104) (1 − αλ(cid:98)+ )2T (cid:105) (cid:0) (cid:107)w(cid:63)(cid:107)2 + ν2 (2 + d)(cid:1) + 1 E[(cid:107)w(cid:63)(cid:107)2 ] . min init 2 I−M(cid:99) The proof is in Appendix C. This is a consequence of Theorem 1, modulo showing that GD with the least squares objective is (∆, M(cid:99))-admissible with ∆ = (1 − αλ(cid:98)+ )T , and upper-bounding E[(cid:107)w(cid:63)(cid:107)2 ] by min I−M(cid:99) controlling the expected squared norm of the projection onto the orthogonal complement of the space spanned by U . The later comes up in the analysis of PCA (see e.g. Shawe-Taylor et al. [2005, Theorem 1]) and, as r we show in Appendix E, this term is expected to be small enough whenever the eigenvalues have exponential √ decay, in which case with high probability we have E[(cid:107)w(cid:63)(cid:107)2 ] (cid:46) (cid:107)w(cid:63)(cid:107)2/ n as n → ∞. Note that the I−M(cid:99) 2 middle term in the upper bound of our Theorem 1 vanishes in the noise-free case: E[(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 ] = 0. S M(cid:99) Looking at Theorem 2, we can see that the excess risk is bounded by the sum of two terms. Note that the second term is negligible in many cases (consider the limit of infinite data) and additionally it is a term that remains constant during training as it does not depend on training data. Therefore, we are particularly interested in the first term of the bound, which is data-dependent. This term depends on λ(cid:98)+ via a functional min form that has a double descent behaviour if plotted against d for fixed n. Before going into that analysis, let us also consider the scenario with label noise. 6
3.2 Least-Squares with Random Design and Label Noise Now, in addition to the random design we introduce label noise into our model: Y = X(cid:62)w(cid:63) + ε , where we have random noise ε such that E[ε] = 0 and E[ε2] = σ2, independent of the instances. Theorem 3. Assume that W ∼ N (0, ν2 I). Then, for any w(cid:63) ∈ arg min L(w) and any x > 0, with 0 init w∈Rd probability 1 − e−x over random samples S we have E[E(w(cid:63))] ≤ E (cid:104) (1 − αλ(cid:98)+ )2T (cid:105) (cid:0) (cid:107)w(cid:63)(cid:107)2 + ν2 (2 + d)(cid:1) + 4σ2 E (cid:20) (cid:16) λ(cid:98)+ (cid:17)−2(cid:21) + 1 E[(cid:107)w(cid:63)(cid:107)2 ] . min init n min 2 I−M(cid:99) The proof is in Appendix C. Again, this follows from Theorem 1, by the same steps used in the proof of (cid:104) (cid:105) Theorem 2, except that the term E (cid:107)w(cid:63) − A (w(cid:63))(cid:107)2 is now handled by conditioning on the sample and S M(cid:99) analyzing the expectation with respect to the random noise (Lemma 4 and its proof in Appendix C.2), leading (cid:104) (cid:105) to the new term 4σ2 E (cid:0) λ(cid:98)+ (cid:1)−2 . The latter closely resembles the term one would get for ridge regression n min [Shalev-Shwartz and Ben-David, 2014, Cor. 13.7] due to algorithmic stability [Bousquet and Elisseeff, 2002], but here we have a dependence on the smallest non-zero eigenvalue instead of a regularization parameter. 3.3 Concentration of the Smallest Non-zero Eigenvalue In this section we take a look at the behaviour of λ(cid:98)+ min assuming that input instances X 1, . . . , X n are i.i.d. random vectors, sampled from some underlying marginal density that meets some regularity requirements (Definitions 2 and 3 below) so that we may use the results from random matrix theory [Vershynin, 2012]. Recall that the covariance matrix of the input features is Σ(cid:98) = (X 1X(cid:62) 1 + · · · + X nX(cid:62) n )/n. We focus on the concentration of λ(cid:98)+ = λ+ (Σ(cid:98) ) around its population counterpart λ+ = λ+ (Σ), where Σ is the min min min min population covariance matrix: Σ = E[X X(cid:62)]. 1 1 In particular, the Bai-Yin limit characterization of the extreme eigenvalues of sample covariance matri- ces [Bai and Yin, 1993] implies that λ(cid:98)+ has almost surely an asymptotic behavior (1 − (cid:112) d/n)2 as the min dimensions grow to infinity, assuming that the matrix X := [X , . . . , X ] ∈ Rd×n has independent entries. 1 n We are interested in the non-asymptotic version of this result. However, unlike Bai and Yin [1993], we do not assume independence of all entries, but rather independence of observation vectors (columns of X). This will be done by introducing a distributional assumption: we assume that observations are sub-Gaussian and isotropic random vectors. Definition 2 (Sub-Gaussian random vectors). A random vector X ∈ Rd is sub-Gaussian if the random variables X(cid:62)y are sub-Gaussian for all y ∈ Rd. The sub-Gaussian norm of a random vector X ∈ Rd is defined as (cid:26) (cid:27) (cid:107)X(cid:107) ψ2 = sup sup √1 p E[|X(cid:62)y|p] p1 . (cid:107)y(cid:107)=1 p≥1 Definition 3 (Isotropic random vectors). A random vector X ∈ Rd is called isotropic if its covariance is the identity: E (cid:2) XX(cid:62)(cid:3) = I. Equivalently, X is isotropic if E[(X(cid:62)x)2] = (cid:107)x(cid:107)2 for all x ∈ Rd. Let Σ† be the Moore-Penrose pseudoinverse of Σ. In Appendix D we prove the following.1 1(x) = max {0, x} + 7
Lemma 1 (Smallest non-zero eigenvalue of sample covariance matrix). Let X = [X , . . . , X ] ∈ Rd×n 1 n be a matrix with i.i.d. columns, such that max i (cid:107)X i(cid:107) ψ2 ≤ K, and let Σ(cid:98) = XX(cid:62)/n, and Σ = E[X 1X(cid:62) 1 ]. Then, for every x ≥ 0, with probability at least 1 − 2e−x, we have (cid:32) (cid:32) (cid:114) (cid:114) (cid:33)(cid:33)2 d x λ+ (Σ(cid:98) ) ≥ λ+ (Σ) 1 − K2 c + for n ≥ d , min min n n + √ and furthermore, assuming that (cid:107)X (cid:107) = d a.s. for all i ∈ [n], we have i Σ† (cid:32)(cid:114) (cid:18) (cid:114) (cid:19)(cid:33)2 d x λ+ (Σ(cid:98) ) ≥ λ+ (Σ) − K2 c + 6 for n < d , min min n n + (cid:112) where we have an absolute constant c = 23.5 ln(9). Lemma 1 is a non-asymptotic result that allows us to understand the behaviour of λ(cid:98)+ , and hence the min behaviour of the excess risk that depends on this quantity, for fixed dimensions. We will exploit this fact in the following section in which we discuss the implications of our findings. 4 Excess risk as a function of over-parameterization First we note that, in the noise-free case, the middle term in the upper bound of Theorem 1 vanishes: E[(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 ] = 0. Thus, as in Theorem 2, the upper bound consists only of the term involving the S M(cid:99) smallest positive eigenvalue λ(cid:98)+ and the term involving E[(cid:107)w(cid:63)(cid:107)2 ]. The behaviour of the former was min I−M(cid:99) clarified in Section 3.3, and the latter is controlled as explained in Appendix E. Thus, in the overparametrized regime (d > n) we have: 2 (cid:16) α √ √ (cid:17)2T (cid:104) (cid:105) E[E(w(cid:63))] (cid:46) 1 − ( d − n − 1)2 (cid:107)w(cid:63)(cid:107)2 + E (cid:107)w(cid:63)(cid:107)2 . n + I−M(cid:99) √ √ A similar bound holds in the underparameterized case (d < n) but replacing the term ( d − n − 1)2 with √ √ (cid:112) √ + ( n − d − 1)2 . Note that the term multiplying the learning rate is ( d/n − 1 − 1/ n)2, in accordance + with the Bai-Yin limit which says that asymptotically λ(cid:98)+ ∼ ((cid:112) d/n − 1)2. It is interesting to see how min (cid:0) 1 − α((cid:112) d/n − 1)2 (cid:1)2T varies with model size d for a given fixed dataset size n and fixed number of gradient + updates T . Setting y = d/n and considering the cases y → 0 (underparameterized regime), y ∼ 1 (the peak), and y > 1 (overparameterized regime) it becomes evident that this term has a double descent behaviour. Thus, the double descent is captured in the part of the excess risk bound that corresponds to learning dynamics on the space spanned by M(cid:99). Similarly, we can now consider the scenario with label noise: we can similarly bound the excess risk, following the same logic as for noise-free case; however we have an additional dependence on σ2 via the (cid:104) (cid:105) term 4σ2 E (cid:0) λ(cid:98)+ (cid:1)−2 . While this does not interfere with the DD shape as we change model size, it does n min imply that the peak is dependent on the amount of noise. In particular, the more noise we have in the learning problem the larger we expect the peak at the interpolation boundary to be. 2We use f (cid:46) g when there exists a universal constant C > 0 such that f ≤ Cg uniformly over all arguments. 8
While the presence of the double descent has been studied by several works, our derivation provides two potentially new interesting insights. The first one is that there is a dependency between the noise in the learning problem and the shape of the curve, the larger the noise is, the larger the peak in DD curve. This agrees with the typical intuition in the underparmetrized regime that the model fits the noise when it has enough capacity, leading towards a spike in test error. However, due to the dependence on λ(cid:98)+ , it is min subdued as the model size grows. Secondly, and maybe considerably more interesting, there seems to be a connection between the double descent curve of the excess risk and the optimization process. In particular, our derivation is specific to gradient descent. In this case the excess risk seems to depend on the conditioning of the features in the least squares problem on the subspace spanned by the data through λ(cid:98)+ , which also min affects convergence of the optimization process. For the least squares problem this can easily be seen, as the sample covariance of the features corresponds to the Gauss-Newton approximation of the Hessian [e.g. Nocedal and Wright, 2006], hence it impacts the convergence. In a more precise way, conditioning of any matrix is measured by the ratio s /s (the ‘condition number’) which is determined solely by the max min smallest singular value s in cases when s is of constant order, such as the case that we studied here: min max Note that by our boundedness assumption, s is constant, but in general one needs to consider both s max max and s in order to characterize the condition numbers, which interestingly have been observed to display a min double descent as well Poggio et al. [2019]. More generally, normalization, standardization, whitening and various other preprocessing of the input data have been a default step in many computer vision systems [e.g. LeCun et al., 1998, Krizhevsky, 2009] where it has been shown empirically that they greatly affect learning. Such preprocessing techniques are usually aimed to improve conditioning of the data. Furthermore, various normalization layers like batch- norm [Ioffe and Szegedy, 2015] or layer-norm [Ba et al., 2016] are typical components of recent architectures, ensuring that features of intermediary layers are well conditioned. Furthermore, it has been suggested that model size improves conditioning of the learning problem [Li et al., 2018], which is in line with our expectation given the behaviour of λ(cid:98)+ . Taking inspiration from the optimization literature, it is natural for min us to ask whether for neural networks, we can also connect the conditioning or λ(cid:98)+ of intermediary features min and double descent. This particular might be significant if we think of the last layer of the architecture as a least squares problem (assuming we are working with mean square error), and all previous layers as some random projection, ignoring that learning is affecting this projection as well. This relationship between generalization and double descent on one hand, and the conditioning of the features and optimization process raises some additional interesting questions, particularly since, compared to the typical least squares setting, the conditioning of the problem for deep architectures does not solely depend on size. In the next section we empirically look at some of these questions. 5 Empirical exploration in neural networks The first natural question to ask is whether the observed behaviour for the least squares problem is reflected when working with neural networks. To explore this hypothesis, and to allow tractability of computing various quantities of interest (like λ(cid:98)+ ), we focus on one hidden layer MLPs on the MNIST and FashionMNIST min datasets. We follow the protocol used by Belkin et al. [2019], relying on a squared error loss. In order to increase the model size we simply increase the dimensionality of the latent space, and rely on gradient descent with a fixed learning rate and a training set to 1000 randomly chosen examples for both datasets. More details can be found in Appendix G. Figure 2 provides the main findings on this experiment. Similar to the Figure 1, we depict 3 columns showing snapshots at different number of gradient updates: 1000, 10000 and 100000. The first row shows 9
0.35 0.30 0.25 0.20 0.15 Min. eigenvalue at initialization 0.10 10 3 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 10 4 10 5 10 6 10 7 10 8 10 9 500 1000 1500 2000 (25a00)3000 3500 4000 4500 5000 rorre tseT number of iterations = 1000 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 eulavnegie muminiM number of iterations = 10000 number of iterations = 100000 0.15 alpha = 0.01 0.094 0.14 alpha = 0.05 0.092 0.13 alpha = 0.1 0.090 0.12 0.088 0.11 0.086 0.10 0.084 0.09 0.08 0.082 500 1000 1500 2000 2500 3000 3500 4000 4500 50000.080500 1000 1500 2000 2500 3000 3500 4000 4500 5000 10 3 10 4 10 4 10 5 10 6 10 6 10 7 10 8 10 8 10 9 10 10 500 1000 1500 200(0 b250)0 3000 3500 4000 4500 5000 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 MNIST 0.375 0.350 0.325 0.300 0.275 0.250 0.225 Min. eigenvalue at initialization 0.200 10 3 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 10 6 10 9 10 12 10 15 10 18 500 1000 1500 2000 (250a0 )3000 3500 4000 4500 5000 rorre tseT number of iterations = 1000 10 3 10 6 10 9 10 12 10 15 10 18 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 eulavnegie muminiM number of iterations = 10000 number of iterations = 100000 0.22 0.22 alpha = 0.01 0.21 alpha = 0.05 0.21 alpha = 0.1 0.20 0.20 0.19 0.19 0.18 0.18 0.17500 1000 1500 2000 2500 3000 3500 4000 4500 50000.17500 1000 1500 2000 2500 3000 3500 4000 4500 5000 10 4 10 3 10 7 10 6 10 10 10 9 10 13 10 12 10 16 10 15 10 19 10 18 500 1000 1500 200(0b250)0 3000 3500 4000 4500 5000 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 FashionMNIST Figure 2: Training one hidden layer networks of increasing width on MNIST (top) and FashionMNIST (bottom): (a) Minimum positive eigenvalue of the intermediary features at initialization - (b) Test error and corresponding minimum eigenvalue of the intermediary features at different iterations 10
test error (number of miss-classified examples out of the test examples) computed on the full test set of 10000 data points which as expected shows the double descent curve with a peak around 1000 hidden units. Note that the peak is relatively small, however the behaviour seems consistent under 5 random seeds for the MNIST experiment.3 The second row and potentially the more interesting one looks at the λ(cid:98)+ computed on min the covariance of the activations of the hidden layer, which as predicted by our theoretical derivation shows a dip around the interpolation threshold, giving the expected U-shape. Even more surprisingly this shape seems to be robust throughout learning, and the fact that the input weights and biases are being trained seems not to alter it, thus suggesting that our derivation might provide insights in the behaviour of deep models. 0.5 0.4 0.3 0.2 Min. eigenvalue at initialization 1000 2000 3000 4000 5000 10 5 10 8 10 11 10 14 10 17 10 20 1000 20(00a) 3000 4000 5000 rorre tseT number of iterations = 1000 10 5 10 8 10 11 10 14 10 17 10 20 1000 2000 3000 4000 5000 eulavnegie muminiM number of iterations = 10000 number of iterations = 100000 0.15 0.100 0.14 0.095 0.13 0.12 0.090 0.11 0.085 0.10 0.080 0.09 0.075 0.08 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 10 5 10 5 10 9 10 9 10 13 10 13 10 17 10 17 10 21 10 21 10 25 10 25 10 29 d de ep pt th h = = 3 1 - same width 10 29 1000 2(00b0 ) 3000 4000 5000 1000 2000 3000 4000 5000 Figure 3: Training networks of increasing width with 1 and 3 hidden layers on MNIST: (a) Minimum positive eigenvalue of the intermediary features at initialization - (b) Test error and corresponding minimum eigenvalue of the intermediary features at different iterations Following this, if we think of the output layer as solving a least squares problem, while the rest of the network provides a projection of the data, we can consider what can affect the conditioning of the last latent space of the network. We put forward the hypothesis that λ(cid:98)+ is not simply affected by the number of min parameters, but actually the distribution of these parameters in the architecture matters. To test this hypothesis, we conduct an experiment where we compare the behavior of a network with a single hidden layer and a network with three hidden layers. For both networks, we increase the size of the hidden layers. For the deeper network, we consider either increasing the size of all the hidden layers or grow only the last hidden layer while keeping the others to a fixed small size, creating a strong bottleneck in the network. Figure 3 shows the results obtained with the former, while the effect of the bottleneck can be seen in Appendix F. We first observe that for the three tested networks, the drop in the minimum eigenvalues happens when the size of the last hidden layer reaches the number of training samples, as predicted by the theory. The magnitude of this drop and behavior across the different tested sizes depends however on the previous layers. In particular, we observe that the bottleneck yields features that are more ill-conditioned than the network with wide hidden layers, where the width of the last layer on its own can not compensate for the existence of the bottleneck. Moreover, from Figure 3, we can clearly see that the features obtained by the deeper network have a bigger drop in the minimum eigenvalue, which results, as expected in a higher increase in the test error around the interpolation threshold. It is well known that depth can harm optimization making the problem ill-conditioned, hence the reliance 3The error bars for the test error in all the other experiments are estimated by splitting the test set into 10 subsets. 11
on skip-connections and batch normalization De and Smith [2020] to train very deep architecture. Our construction provides a way of reasoning about double descent that allows us to factor in the ill-conditioning of the learning problem. Rather than focusing simply on the model size, it suggests that for neural networks the quantity of interest might also be λ(cid:98)+ for intermediary features, which is affected by size of the model min but also by the distribution of the weights and architectural choices. For now we present more empirical explorations and ablations in Appendix G, and put forward this perspective as a conjecture for further exploration. 6 Conclusion and Future Work In this work we analyse the double descent phenomenon in the context of the least squares problem. We make the observation that the excess risk of gradient descent is controlled by the smallest positive eigenvalue, λ(cid:98)+ , min of the feature covariance matrix. Furthermore, this quantity follows the Bai-Yin law with high probability under mild distributional assumptions on features, that is, it manifests a U-shaped behaviour as the number of features increases, which we argue induces a double descent shape of the excess risk. Through this we provide a connection between the widely known phenomena and optimization process and conditioning of the problem. We believe this insight provides a different perspective compared to existing results focusing on the Moore-Penrose pseudo-inverse solution. In particular our work conjectures that the connection between the known double descent shape and model size is through λ(cid:98)+ of the features at intermediary layers. For min the least squares problem λ(cid:98)+ correlates strongly with model size (and hence feature size). However this min might not necessarily be always true for neural networks. For example we show empirically that while both depth and width increase the model size, they might affect λ(cid:98)+ differently. We believe that our work could min enable much needed effort, either empirical or theoretical, to disentangle further the role of various factors, like depth and width or other architectural choices like skip connections on double descent. References Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. arXiv:2102.06171, 2021. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein structure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354–359, 2017. 12
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2 edition, 2009. Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014. Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32): 15849–15854, 2019. Previously arXiv:1812.11118. Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM Journal on Mathematics of Data Science, 2(4):1167–1180, 2020. Accessed from arXiv:1903.07571. László Györfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A distribution-free theory of nonparametric regression, volume 1. SPRINGER, 2002. Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and double descent curve. arXiv:1908.05355, 2019. Michal Derezinski, Feynman T Liang, and Michael W Mahoney. Exact expressions for double descent and implicit regularization via surrogate random design. In Advances in Neural Information Processing Systems [NeurIPS 2020], 2020. Akshay Rangamani, Lorenzo Rosasco, and Tomaso Poggio. For interpolating kernel machines, minimizing the norm of the erm solution minimizes stability, 2020. Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. arXiv:1903.08560, 2019. Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-variance decomposition. In Advances in Neural Information Processing Systems [NeurIPS 2020], 2020. Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. arXiv:2103.09177, 2021. Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. In International Conference on Learning Representations, 2019. John Shawe-Taylor, Christopher KI Williams, Nello Cristianini, and Jaz Kandola. On the eigenspectrum of the gram matrix and the generalization error of kernel-pca. IEEE Transactions on Information Theory, 51 (7):2510–2522, 2005. Olivier Bousquet and André Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2:499–526, 2002. Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed Sensing, Theory and Applications, pages 210–268. Cambridge University Press, 2012. Accessed from arXiv:1011.3027. 13
Zhi-Dong Bai and Yong-Qua Yin. Limit of the smallest eigenvalue of a large dimensional sample covariance matrix. The Annals of Probability, 21(3):1275–1294, 1993. Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA, second edition, 2006. Tomaso Poggio, Gil Kur, and Andrzej Banburski. Double descent in the condition number. Technical Report CBMM Memo No. 102, MIT, 2019. Accessed from arXiv:1912.06190. Yann LeCun, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller. Efficient backprop. In Neural Networks: Tricks of the Trade (2nd ed.), Lecture Notes in Computer Science, pages 9–48. Springer, 1998. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, page 448–456. JMLR.org, 2015. Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016. Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. Soham De and Sam Smith. Batch normalization biases residual blocks towards the identity function in deep networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 19964–19975. Curran Associates, Inc., 2020. 14
A Definitions from Linear Analysis We denote column vectors and matrices with small and capital bold letters, respectively, e.g. α = [α 1, α 2, . . . , α d](cid:62) ∈ Rd and A ∈ Rd1×d2. Singular values of a rectangular matrix A ∈ Rn×d are denoted by s (A) = s (A) ≥ . . . ≥ s (A) = s (A). The rank of A is r = max{k | s (A) > 0}. max 1 n∧d min k Eigenvalues of a Positive Semi-Definite (PSD) matrix M ∈ Rd×d are nonnegative and are denoted λ (M ) = λ (M ) ≥ . . . ≥ λ (M ) = λ (M ), while the smallest non-zero eigenvalue is denoted max 1 d min λ+ (M ). min √ When M ∈ Rd×d is positive definite, we define (cid:107)x(cid:107) for x ∈ Rd by (cid:107)x(cid:107) = x(cid:62)M x . It is easy to M M check that (cid:107) · (cid:107) is indeed a norm on Rd, hence it induces a metric over Rd, with the distance between x M (cid:112) and y given by (cid:107)x − y(cid:107) = (x − y)(cid:62)M (x − y). If M is only semi-definite, these definitions would M give a semi-norm and semi-metric. Note that (cid:107)x(cid:107) = (cid:107)M 1/2x(cid:107) where M 1/2 is the matrix square root of M M . If we set M = I, the identity matrix, then the norm (cid:107) · (cid:107) reduces to the standard Euclidean norm: √ M (cid:107)x(cid:107) = x(cid:62)x. Combining the Cauchy-Schwarz inequality and the definition of operator norm (cid:107)M (cid:107) = s (M ), which max implies (cid:107)M x(cid:107) ≤ (cid:107)M (cid:107)(cid:107)x(cid:107), we get the inequality (cid:107)x(cid:107)2 ≤ (cid:107)x(cid:107)2(cid:107)M (cid:107). M The distance from a point x to some set B ⊆ Rd is defined as usual ρ(x, B) = inf (cid:107)x − y(cid:107) y∈B and for a positive defninte matrix M we define similarly ρ (x, B) = inf (cid:107)x − y(cid:107) . M M y∈B Given x ∈ Rd and (cid:15) > 0, the Euclidean ball of radius (cid:15) centered at x is defined as 0 0 (cid:110) (cid:111) B(x , (cid:15)) = x ∈ Rd | (cid:107)x − x (cid:107) ≤ (cid:15) 0 0 and for a positive definite matrix M the ellipsoid w.r.t. metric (cid:107) · (cid:107) is defined as M (cid:110) (cid:12) (cid:111) E (x , (cid:15)) = x ∈ Rd (cid:12) (cid:107)x − x (cid:107) ≤ (cid:15) M 0 (cid:12) 0 M B Minimum eigenvalue and condition number Previous works considered the link between the condition number of the features and the DD behavior Ranga- mani et al. [2020]. In this work, the analysis focuses more particularly on the minimum eigenvalue. In the following small experiments, we empirically show that in the experiments shown in the main paper, the condition number is driven by the minimum eigenvalue, and that the maximum eigenvalue stays close to a constant order when we increase the size of the features. In Figure 4, we use the same setting as in the MNIST experiment in Figure 2 is the main paper. We obseve that the behavior of the condition number follows the minimum eigenvalue, while the maximum eigenvalue stays between 10 and 100 as we increase the width of the networks. 15
102 101 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 eulavnegie mumixaM number of iterations = 1000 10 3 10 5 10 7 10 9 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 eulavnegie muminiM 1023 1020 1017 1014 1011 108 105 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 rebmun noitidnoC number of iterations = 10000 number of iterations = 100000 102 102 alpha = 0.01 101 alpha = 0.05 101 alpha = 0.1 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 10 3 10 4 10 4 10 5 10 6 10 6 10 7 10 8 10 8 10 9 10 10 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 1023 1020 1020 1017 1017 1014 1014 1011 1011 108 108 105 105 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 Figure 4: Maximum and minimum eigenvalues and condition numbers of the features of one hidden layer networks of variable width: MNIST - 1000 samples for training, networks trained with gradient descent and different step sizes. C Excess Risk of Gradient Descent In this section we consider the standard GD algorithm, that is A (w ) = w , which is obtained recursively S 0 T by applying the update rule w = w − α∇Lˆ (w ) with some step size α > 0 and initialization w ∈ Rd. t+1 t S t 0 The rule is iterated for t = 0, . . . , T − 1. We will pay attention to A which satisfy the following regularity S condition: Definition 1. A map f : Rd → Rd is called (∆, M )-admissible, where M is a fixed PSD matrix and ∆ ≥ 0, if for all w, w(cid:48) ∈ Rd the following holds: (cid:107)f (w) − f (w(cid:48))(cid:107) ≤ ∆(cid:107)w − w(cid:48)(cid:107) . M Notice that the norm on the left-hand side is (cid:107) · (cid:107) , while that on the right-hand side is the standard M Euclidean norm. Also notice that this inequality entails a Lipschitz condition with Lipschitz factor ∆. The excess risk of A (W ) is defined as S 0 E(w(cid:63)) = L(A (W )) − L(w(cid:63)) w(cid:63) ∈ arg min L(w) . S 0 w∈Rd Next we give upper bounds on the excess risk of GD output, assuming that the output of A is of low-rank, S which is of interest in the overparameterized regime. Specifically, for some low-rank orthogonal projection M ∈ Rd×d we assume that M A (w) = A (w) a.s. with respect to random samples S, for any initialization S S w. The following theorem gives us a general bound on the excess risk of any admissible algorithm in a sense of Definition 1 w.r.t. to any smooth loss (not necessarily convex). In the following we will demonstrate that GD satisfies Definition 1. 16
Theorem 1 (Excess Risk of Admissible Algorithm). Assume that W ∼ N (0, ν2 I ), and assume that 0 init d×d A is (∆, M )-admissible (Definition 1), where ∆ and W are independent. Further assume M A (w) = S 0 S A (w) for any w, and that L and Lˆ are H-smooth. Then, for any w(cid:63) ∈ arg min L(w) we have S w∈Rd (cid:18) (cid:19) E[E(w(cid:63))] ≤ H E[∆2] (cid:0) (cid:107)w(cid:63)(cid:107)2 + ν2 (2 + d)(cid:1) + E[(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 ] + 1 E[(cid:107)w(cid:63)(cid:107)2 ] . init S M 2 I−M In particular, having α ≤ 1/H, E[(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 ] ≤ 2αT L(w(cid:63)) . S M Proof. By the H-smoothness of L, and noting that ∇L(w(cid:63)) = 0 since w(cid:63) is a minimizer, H L(A (W )) − L(w(cid:63)) ≤ (cid:107)A (W ) − w(cid:63)(cid:107)2 S 0 S 0 2 H H = (cid:107)A (W ) − w(cid:63)(cid:107)2 + (cid:107)A (W ) − w(cid:63)(cid:107)2 2 S 0 M 2 S 0 I−M H H = (cid:107)A (W ) − w(cid:63)(cid:107)2 + (cid:107)w(cid:63)(cid:107)2 2 S 0 M 2 I−M where the last equality is justified since M is an orthogonal projection satisfying the assumption M A (W ) = S 0 A (W ). S 0 Focusing on the first term above, we get (cid:107)A (W ) − w(cid:63)(cid:107)2 ≤ 2(cid:107)A (W ) − A (w(cid:63))(cid:107)2 + 2(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 S 0 M S 0 S M S M ≤ 2∆2(cid:107)W − w(cid:63)(cid:107)2 + 2(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 0 S M where the first inequality is due to the following inequality for squared Euclidean norms: (cid:107)a + · · · + a (cid:107)2 ≤ 1 n n((cid:107)a (cid:107)2 + · · · + (cid:107)a (cid:107)2), and the last inequality is due to (∆, M )-admissibility of A . Taking expectation 1 n S on both sides we have E (cid:2) (cid:107)A (W ) − w(cid:63)(cid:107)2 (cid:3) ≤ 2 E[∆2] E[(cid:107)W − w(cid:63)(cid:107)2] + 2 E[(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 ] S 0 M 0 S M where ∆ is random only due to the sample (recall that ∆ is independent of W by assumption). The term 0 E[(cid:107)W − w(cid:63)(cid:107)2] is a standard Gaussian integral, whose calculation is summarized in following lemma: 0 Lemma 2 (Expectation of a squared norm of the Gaussian random vector). For any ν > 0 and x ∈ Rd: 0 (cid:90) 1 (cid:107)x − x 0(cid:107)2e− 2ν1 2 (cid:107)x(cid:107)2 dx = (cid:107)x 0(cid:107)2 + ν2(2 + d) Z Rd where Z = (cid:82) Rd e− 2ν1 2 (cid:107)x(cid:107)2 dx is the normalization constant. Therefore, if X ∼ N (0, ν2I d×d) then for any x ∈ Rd we have the identity E[(cid:107)X − x (cid:107)2] = (cid:107)x (cid:107)2 + ν2(2 + d). 0 0 0 Proof of Lemma 2. (cid:90) (cid:90) 1 (cid:107)x − x 0(cid:107)2e− 2ν1 2 (cid:107)x(cid:107)2 dx = (cid:107)x 0(cid:107)2 + 1 (cid:107)x(cid:107)2e− 2ν1 2 (cid:107)x(cid:107)2 dx Z Z Rd Rd Γ (cid:0) d+2 (cid:1) = (cid:107)x (cid:107)2 + ν2 E[(cid:107)X(cid:107)2] = (cid:107)x (cid:107)2 + 2ν2 2 0 0 Γ (cid:0) d (cid:1) 2 = (cid:107)x (cid:107)2 + ν2(2 + d) , 0 since (cid:107)X(cid:107)2 is χ2-distributed with d degrees of freedom. 17
For our case with W ∼ N (0, ν2 I ) this gives E[(cid:107)W − w(cid:63)(cid:107)2] ≤ (cid:107)w(cid:63)(cid:107)2 + ν2 (2 + d). 0 init d×d 0 init The term E[(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 ] is bounded next by using the standard “descent lemma”. S M Lemma 3 (Descent Lemma). Assuming that α ≤ 1/H, T −1 (cid:88) 2 (cid:16) (cid:17) (cid:107)∇Lˆ (w )(cid:107)2 ≤ Lˆ (w ) − Lˆ (w ) S t S 0 S T α t=0 Proof of Lemma 3. Since Lˆ is H-smooth, Taylor expansion and the gradient descent rule give us α2H Lˆ (w ) − Lˆ (w ) ≤ −α(cid:107)∇Lˆ (w )(cid:107)2 + (cid:107)∇Lˆ (w )(cid:107)2 , S t+1 S t S t S t 2 and rearranging we have (cid:18) α2H (cid:19) α − (cid:107)∇Lˆ (w )(cid:107)2 ≤ Lˆ (w ) − Lˆ (w ) . S t S t S t+1 2 Summing over t = 0, . . . , T − 1 we arrive at (cid:18) α2H (cid:19) T (cid:88)−1 α − (cid:107)∇Lˆ (w )(cid:107)2 ≤ Lˆ (w ) − Lˆ (w ) . S t S 0 S T 2 t=0 Finally, note that α − α2H > α by the assumption that α ≤ 1/H. 2 2 In particular, if w(cid:63) are the iterates of GD when starting from w(cid:63) (so that w(cid:63) = w(cid:63)), then t 0 (cid:13) (cid:13)2 T −1 (cid:13) (cid:88) (cid:13) (cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 = (cid:13)α ∇Lˆ (w(cid:63))(cid:13) S M (cid:13) S t (cid:13) (cid:13) (cid:13) t=0 M T −1 (cid:88) ≤ α2T (cid:107)∇Lˆ (w(cid:63))(cid:107)2 S t M t=0 2T (cid:16) (cid:17) ≤ α2 Lˆ (w(cid:63)) − Lˆ (w(cid:63) ) α S S T ≤ 2αT Lˆ (w(cid:63)) S and taking expectation on both sides we have E[(cid:107)A (w(cid:63)) − w(cid:63)(cid:107)2 ] ≤ 2αT L(w(cid:63)) . S M Putting all together completes the proof of Theorem 1. C.1 Least-Squares with Random Design and without Label Noise Consider a noise-free linear regression model Y = X(cid:62)w(cid:63) , 18
where instances are distributed according to some unknown distribution P supported on a d-dimensional X unit Euclidean ball. After observing a training sample S = ((X , Y ))n , we run GD on the given empirical i i i=1 square loss n 1 (cid:88) Lˆ (w) = (w(cid:62)X − Y )2 . S i i 2n i=1 Let a sample covariance matrix be defined as Σ(cid:98) = (X 1X(cid:62) 1 + · · · + X nX(cid:62) n )/n, and let Σ(cid:98) = U SV (cid:62) be the SVD of Σ(cid:98) . We will use a subscript notation U r = [u 1, . . . , u r], V r = [v 1, . . . , v r], and S r = diag(s 1(Σ(cid:98) ), . . . , s r(Σ(cid:98) )), where r = rank(Σ(cid:98) ) to indicate non-degenerate orthonormal bases and their scaling matrix. In the setting of our interest Σ(cid:98) might be degenerate, and therefore we will occasionally refer to the non-degenerate subspace U r. We write λ(cid:98)+ min = λ+ min(Σ(cid:98) ) = λ r(Σ(cid:98) ) for the minimal non-zero eigenvalue, and 2 we denote M(cid:99) = U rU (cid:62) r . Note that M(cid:99) = M(cid:99). Now we state the main result of this section. Theorem 2. Assume that W ∼ N (0, ν2 I). Then, for any w(cid:63) ∈ arg min L(w), 0 init w∈Rd E[E(w(cid:63))] ≤ E (cid:104) (1 − αλ(cid:98)+ )2T (cid:105) (cid:0) (cid:107)w(cid:63)(cid:107)2 + ν2 (2 + d)(cid:1) + 1 E[(cid:107)w(cid:63)(cid:107)2 ] . min init 2 I−M(cid:99) Proof. The proof is a consequence of Theorem 1, modulo showing that GD with the least-squares objective is ((1 − αλ(cid:98)+ )T , M(cid:99))-admissible (Corollary 1 below). min Proposition 1. For a T -step gradient descent map A : Rd → Rd with step size α > 0 applied to the S least-squares, and for all w ∈ Rd, we have a.s. that 0 T −1 (cid:32) n (cid:33) (cid:88) 1 (cid:88) A S(w 0) = (I − αΣ(cid:98) )T w 0 + α (I − αΣ(cid:98) )t X iY i . n t=0 i=1 Proof of Proposition 1. Abbreviate C = (X 1Y 1 + · · · + X nY n)/n. Since ∇L(cid:98)S(w) = Σ(cid:98) w − C, observe that w t = w t−1 − α(Σ(cid:98) w t−1 − C) = (I − αΣ(cid:98) )w t−1 + αC . A simple recursive argument reveals that for every w ∈ Rd 0 A S(w 0) = w T = (I − αΣ(cid:98) )w T −1 + αC = (I − αΣ(cid:98) )2w T −2 + α(I − αΣ(cid:98) )C + αC = (I − αΣ(cid:98) )3w T −3 + α(I − αΣ(cid:98) )2C + α(I − αΣ(cid:98) )C + αC · · · T −1 (cid:88) = (I − αΣ(cid:98) )T w 0 + α (I − αΣ(cid:98) )tC . t=0 Proposition 1 implies the following simple fact. Corollary 1 (Admissibility of GD). The T -step gradient descent map A : Rd → Rd with step size α > 0 S applied to the least-squares problem satisfies, for all w , u ∈ Rd, 0 0 (cid:107)A S(w 0) − A S(u 0)(cid:107) M(cid:99) ≤ (1 − αλ(cid:98)+ min)T (cid:107)w 0 − u 0(cid:107) . 19
Proof of Corollary 1. By Proposition 1 for any w , u ∈ Rd: 0 0 (cid:107)A S(w 0) − A S(u 0)(cid:107) UrU(cid:62) r = (cid:107)(I − αΣ(cid:98) )T (w 0 − u 0)(cid:107) UrU(cid:62) r = (cid:107)U (cid:62) r (I − αΣ(cid:98) )T (w 0 − u 0)(cid:107) ≤ (cid:107)U (cid:62) r (I − αΣ(cid:98) )T (cid:107)(cid:107)(w 0 − u 0)(cid:107) . Now, U (cid:62) r (I − αΣ(cid:98) )T = U (cid:62) r U (I − αS)T V (cid:62) = I r×d(I − αS)T V (cid:62) = (I r×d − αS r×d)T V (cid:62) where subscript r × n stands for clipping the matrix to r rows and d columns. The above implies that the operator norm of U (cid:62)(I − αΣ(cid:98) )T satisfies (cid:107)U (cid:62)(I − αΣ(cid:98) )T (cid:107) ≤ (1 − αλ+ (Σ(cid:98) ))T . r r min Finally, note that in the overparametrized case (d > n) we have r = n ∧ d = n. C.2 Least-Squares with Random Design and Label Noise Now, in addition to the random design we introduce a label noise into our model: Y = X(cid:62)w(cid:63) + ε , where we have independent noise ε such that E[ε] = 0 and E[ε2] = σ2. Theorem 3. Assume that W ∼ N (0, ν2 I). Then, for any w(cid:63) ∈ arg min L(w), 0 init w∈Rd E[E(w(cid:63))] ≤ E (cid:104) (1 − αλ(cid:98)+ )2T (cid:105) (cid:0) (cid:107)w(cid:63)(cid:107)2 + ν2 (2 + d)(cid:1) + 4σ2 E (cid:20) (cid:16) λ(cid:98)+ (cid:17)−2(cid:21) + 1 E[(cid:107)w(cid:63)(cid:107)2 ] . min init n min 2 I−M(cid:99) (cid:104) (cid:105) Proof. The proof is almost identical to the one of Theorem 2 except E (cid:107)w(cid:63) − A (w(cid:63))(cid:107)2 is handled by S M(cid:99) the following Lemma 4. Lemma 4. Let M(cid:99) be defined as in Section 3.1. For any T > 0, GD achieves (cid:104) (cid:105) 4σ2 (cid:20) (cid:16) (cid:17)−2(cid:21) E (cid:107)w(cid:63) − A S(w(cid:63))(cid:107)2 M(cid:99) ≤ n E λ(cid:98)+ min . Proof. Recall that E[ε ] = 0 and E[ε2] = σ2 for i ∈ [n]. Throughout the proof abbreviate E[· | X , . . . , X ] = i i 1 n E [·]. ε We begin by noting that the integral form of Taylor theorem gives us that for any w(cid:63) ∈ arg min Lˆ(w) w∈Rd and any w ∈ Rd, 1 (cid:18)(cid:90) 1 (cid:19) Lˆ(w) − Lˆ(w(cid:63)) = (w − w(cid:63))(cid:62) ∇2Lˆ(τ w + (1 − τ )w(cid:63)) dτ (w − w(cid:63)) 2 0 1 ≥ · λ(cid:98)+ (w − w(cid:63))(cid:62)M(cid:99)(w − w(cid:63)) . 2 min 20
Thus, taking w = A (w(cid:63)), we have S (cid:104) (cid:105) 1 (cid:16) (cid:104) (cid:105)(cid:17) E (cid:107)w(cid:63) − A (w(cid:63))(cid:107)2 ≤ E Lˆ(w(cid:63)) − E Lˆ(A (w(cid:63))) ε S ε ε S M(cid:99) λ(cid:98)+ min 1 (cid:16) (cid:104) (cid:105)(cid:17) = σ2 − E Lˆ(A (w(cid:63))) . ε S λ(cid:98)+ min Now, let’s focus on the loss term on the r.h.s.: n (cid:20) (cid:21) (cid:104) (cid:105) 1 (cid:88) (cid:16) (cid:17)2 E Lˆ (w(cid:63) ) = E (w(cid:63) − w(cid:63))(cid:62) X − ε ε S T n ε T 0 i i i=1 n 2 (cid:88) (cid:104) (cid:105) (cid:104) (cid:105) = σ2 − n E ε ε i (w(cid:63) T − w(cid:63) 0)(cid:62) X i + E ε (w(cid:63) T − w(cid:63) 0)(cid:62) Σ(cid:98) (w(cid:63) T − w(cid:63) 0) i=1 n 2 (cid:88) (cid:104) (cid:105) ≥ σ2 − E ε (w(cid:63) − w(cid:63))(cid:62) X n ε i T 0 i i=1 n 2 (cid:88) (cid:104) (cid:105) = σ2 − E ε w(cid:63) (cid:62)X n ε i T i i=1 where the last term is small when label noise is not too correlated with the output w(cid:63) . Hence to control the T term, we need to measure the effect of the noise on GD. To do so we will introduce an additional iterates (w˜ ) constructed by running GD on labels without noise, that is t t n 1 (cid:88) (cid:16) (cid:17)2 w˜ (cid:63) = w˜ (cid:63) − α∇L˜ (w˜ (cid:63)) where L˜(w) = w(cid:62)X − w(cid:63)(cid:62)X . t+1 t S t 2n i i i=1 The plan is then to bound the deviation (cid:107)w(cid:63) − w˜ (cid:63) (cid:107) which we will do recursively. We proceed: T T M(cid:99) n 2 (cid:88) (cid:104) (cid:105) E ε w(cid:63) (cid:62)X n ε i T i i=1 n 2 (cid:88) (cid:104) (cid:105) = E ε (w(cid:63) − w˜ (cid:63) )(cid:62)X (Note that E [w˜ (cid:63) | X ] = 0) n ε i T T i ε T i i=1 n 2 (cid:88) (cid:104) (cid:105) = n E ε ε i(w(cid:63) T − w˜ (cid:63) T )(cid:62)M(cid:99)X i (Since M(cid:99)X i = X i) i=1 (cid:34)(cid:13) n (cid:13) (cid:35) 2 (cid:13)(cid:88) (cid:13) (cid:13) (cid:13) ≤ n E ε (cid:13) (cid:13) ε iX i(cid:13) (cid:13) (cid:13) (cid:13)M(cid:99)(w(cid:63) T − w˜ (cid:63) T )(cid:13) (cid:13) (Cauchy-Schwarz) (cid:13) (cid:13) i=1 (cid:13) (cid:13) Now we will handle (cid:13)M(cid:99)(w(cid:63) − w˜ (cid:63) )(cid:13) = (cid:107)w(cid:63) − w˜ (cid:63) (cid:107) by following a recursive argument. First, observe (cid:13) T T (cid:13) T T M(cid:99) that for any t = 0, 1, 2, . . . n 1 (cid:88) ∇Lˆ(w˜ (cid:63) t ) = Σ(cid:98) w˜ (cid:63) t − n X iX(cid:62) i w(cid:63) 0 = Σ(cid:98) (w˜ (cid:63) t − w(cid:63) 0) , i=1 21
and at the same time n n n 1 (cid:88) 1 (cid:88) 1 (cid:88) ∇Lˆ(w(cid:63) t ) = Σ(cid:98) w(cid:63) t − n X iX(cid:62) i w(cid:63) 0 − n X iε i = Σ(cid:98) (w(cid:63) t − w(cid:63) 0) − n X iε i . i=1 i=1 i=1 Thus, (cid:13) (cid:16) (cid:17)(cid:13) (cid:107)w(cid:63) − w˜ (cid:63) (cid:107) = (cid:13)w(cid:63) − w˜ (cid:63) − α ∇Lˆ(w(cid:63)) − ∇Lˆ(w˜ (cid:63)) (cid:13) (1) t+1 t+1 M(cid:99) (cid:13) t t t t (cid:13) M(cid:99) (cid:13) (cid:13) n (cid:13) α (cid:88) (cid:13) = (cid:13) (cid:13)w(cid:63) t − w˜ (cid:63) t − αΣ(cid:98) (w(cid:63) t − w˜ (cid:63) t ) − n X iε i(cid:13) (cid:13) (cid:13) (cid:13) i=1 M(cid:99) (cid:13) (cid:13) n (cid:13) (cid:13) α (cid:13)(cid:88) (cid:13) = (cid:13) (cid:13)(I − αΣ(cid:98) )(w(cid:63) t − w˜ (cid:63) t )(cid:13) (cid:13) M(cid:99) + n (cid:13) (cid:13) (cid:13) X iε i(cid:13) (cid:13) (cid:13) i=1 M(cid:99) (cid:13) (cid:13) n (a) α (cid:13)(cid:88) (cid:13) ≤ (cid:107)I − αΣ(cid:98) (cid:107) M(cid:99)(cid:107)w(cid:63) t − w˜ (cid:63) t (cid:107) M(cid:99) + n (cid:13) (cid:13) X iε i(cid:13) (cid:13) (cid:13) (cid:13) i=1 M(cid:99) (cid:13) (cid:13) n α (cid:13)(cid:88) (cid:13) ≤ (1 − αλ(cid:98)+ min)(cid:107)w(cid:63) t − w˜ (cid:63) t (cid:107) M(cid:99) + n (cid:13) (cid:13) X iε i(cid:13) (cid:13) . (2) (cid:13) (cid:13) i=1 M(cid:99) 2 where in the step (a) we note that M(cid:99)(I − αΣ(cid:98) )(w(cid:63) − w˜ (cid:63)) = M(cid:99)(I − αΣ(cid:98) )M(cid:99)(w(cid:63) − w˜ (cid:63)) (since M(cid:99) = M(cid:99) t t t t and Σ(cid:98) M(cid:99) = Σ(cid:98) ). Now we use the fact that an elementary recursive relation x ≤ a x + b with x = 0 unwinds to t+1 t t t 0 x ≤ (cid:80)T b (cid:81)T a , which gives T t=1 t k=t+1 k (cid:13) (cid:13) n T α (cid:13)(cid:88) (cid:13) (cid:88) (cid:107)w(cid:63) T − w˜ (cid:63) T (cid:107) M(cid:99) ≤ n (cid:13) (cid:13) X iε i(cid:13) (cid:13) (1 − αλ(cid:98)+ min)T −t (cid:13) (cid:13) i=1 M(cid:99) t=1 (cid:13) (cid:13) α (cid:13)(cid:88)n (cid:13) 1 − (1 − αλ(cid:98)+ )T ≤ (cid:13) X ε (cid:13) min . n (cid:13) (cid:13) i i(cid:13) (cid:13) αλ(cid:98)+ i=1 M(cid:99) min Thus, n (cid:13) n (cid:13)2  2 (cid:88) (cid:104) (cid:105) 2 1 (cid:13)(cid:88) (cid:13) 1 n E ε ε i (w(cid:63) T − w(cid:63) 0)(cid:62) X i ≤ n · n E ε (cid:13) (cid:13) (cid:13) X iε i(cid:13) (cid:13) (cid:13) λ(cid:98)+  i=1 i=1 min 2σ2 1 ≤ · n λ(cid:98)+ min where we used a basic fact that (cid:13) n (cid:13)2 (cid:12) (cid:12)  n E ε (cid:13) (cid:13) (cid:13)(cid:88) X iε i(cid:13) (cid:13) (cid:13) (cid:12) (cid:12) X 1, . . . , X n = σ2 (cid:88) (cid:107)X i(cid:107)2 ≤ σ2n . (cid:12) (cid:13) (cid:13) i=1 (cid:12) i=1 Putting all together completes the proof. 22
C.3 Concentration of the Smallest Non-zero Eigenvalue In this section we take a look at the behaviour of λ(cid:98)+ assuming that training instances are now random min independent vectors X , . . . , X sampled i.i.d. from some underlying marginal density. Recall that the 1 n sample covariance matrix is Σ(cid:98) = (X 1X(cid:62) 1 + · · · + X nX(cid:62) n )/n. We focus on the concentration of λ(cid:98)+ min = λ+ (Σ(cid:98) ) around its population counterpart λ+ = λ+ (Σ), where Σ is the population covariance matrix: min min min Σ = E[X 1X(cid:62) 1 ]. Note that defining X = [X 1, . . . , X n] ∈ Rd×n we have: Σ(cid:98) = XX(cid:62)/n. In particular, we are concerned with a non-asymptotic version of the Bai-Yin law [Bai and Yin, 1993], which says that the smallest eigenvalue (for d ≤ n), or the (d − n + 1)-th smallest eigenvalue (for d > n), of (cid:112) a sample covariance matrix with independent entries has almost surely an asymptotic behavior (1 − d/n)2 as n → ∞. The setting d > n is essential for our case, as it corresponds to overparametrization. However, unlike Bai and Yin [1993], we do not assume independence of entries, but rather independence of observations (columns of X). This will be done by introducing a distributional assumption on observations: we assume that observations are sub-Gaussian. Definition 2 (Sub-Gaussian random vectors). A random vector X ∈ Rd is sub-Gaussian if the random variables X(cid:62)y are sub-Gaussian for all y ∈ Rd. The sub-Gaussian norm of a random vector X ∈ Rd is defined as (cid:26) (cid:27) (cid:107)X(cid:107) ψ2 = sup sup √1 p E[|X(cid:62)y|p] p1 . (cid:107)y(cid:107)=1 p≥1 We will also require the following definition. Definition 3 (Isotropic random vectors). A random vector X ∈ Rd is called isotropic if its covariance is the identity: E (cid:2) XX(cid:62)(cid:3) = I. Equivalently, X is isotropic if E[(X(cid:62)x)2] = (cid:107)x(cid:107)2 for all x ∈ Rd. Let Σ† be the Moore-Penrose pseudoinverse of Σ. In Appendix D we prove the following. Lemma 1. Let X = [X , . . . , X ] ∈ Rd×n be a matrix with i.i.d. columns, such that max (cid:107)X (cid:107) ≤ K, 1 n i i ψ2 and let Σ(cid:98) = XX(cid:62)/n, and Σ = E[X 1X(cid:62) 1 ]. Then, for every x ≥ 0, with probability at least 1 − 2e−x, we have (cid:32) (cid:32) (cid:114) (cid:114) (cid:33)(cid:33)2 d x λ+ (Σ(cid:98) ) ≥ λ+ (Σ) 1 − K2 c + for n ≥ d , min min n n + √ and furthermore, assuming that (cid:107)X (cid:107) = d a.s. for all i ∈ [n], we have i Σ† (cid:32)(cid:114) (cid:18) (cid:114) (cid:19)(cid:33)2 d x λ+ (Σ(cid:98) ) ≥ λ+ (Σ) − K2 c + 6 for n < d , min min n n + (cid:112) where we have an absolute constant c = 23.5 ln(9). Next we present the proof of the Lemma. D Concentration of the Smallest Non-zero Eigenvalue: Proof The next theorem gives us a non-asymptotic version of Bai-Yin law [Bai and Yin, 1993] for rectangular matrices whose rows are sub-Gaussian isotropic random vectors. 23
Theorem 4 ([Vershynin, 2012, Theorem 5.39]). Let A ∈ Rn×d whose rows (A(cid:62)) are independent sub- i Gaussian isotropic random vectors in Rd, such that K = max (cid:107)(A(cid:62)) (cid:107) . Then for every x ≥ 0, with i∈[n] i ψ2 probability at least 1 − 2e−x one has √ (cid:112) √ √ (cid:16)(cid:112) √ (cid:17) n − 23.5K2( ln(9)d + x) ≤ s (A) ≤ s (A) ≤ n + 23.5K2 ln(9)d + x . min max Theorem 5 ([Vershynin, 2012, Theorem 5.58]). Let A ∈ Rd×n whose columns A are independent sub- √ i Gaussian isotropic random vectors in Rd with (cid:107)A (cid:107) = d a.s., such that K = max (cid:107)A (cid:107) . Then for i i∈[n] i ψ2 every x ≥ 0, with probability at least 1 − 2e−x one has √ (cid:112) √ √ (cid:112) √ d − 23.5K2( ln(9)n + 6 x) ≤ s (A) ≤ s (A) ≤ d + 23.5K2( ln(9)n + 6 x) min max Above two theorems lead to the following non-asymptotic version of a Bai-Yin law. Proof of Lemma 1. The proof considers two cases: 1) when number of observations exceeds the dimension, which is handled by the concentration of a minimal non-zero eigenvalue of a covariance matrix; 2) when dimension exceeds number of observations, which is handled by concentration of the Gram matrix. Case n ≥ d. We will apply Theorem 4 with A = (Σ† 1 2 X)(cid:62) whose rows are independent and isotropic, and in addition by Cauchy-Schwarz inequality: (cid:107)Σ† 21 (cid:107)s min(X(cid:62)) ≥ s min (cid:16) (Σ† 1 2 X)(cid:62)(cid:17) ≥ √ n − 23.5K2((cid:112) ln(9)d + √ x) with probability at least 1 − e−x for x > 0. Observing that (cid:107)Σ† 1 2 (cid:107) = s+ min(Σ)−1/2, this implies that (cid:113) (cid:16)√ (cid:16)(cid:112) √ (cid:17)(cid:17) s (X(cid:62)) ≥ s+ (Σ) n − 23.5K2 ln(9) d + x , min min √ while dividing through by n, taking the non-negative part of the r.h.s. and squaring gives us (cid:32) (cid:32)(cid:114) (cid:114) (cid:33)(cid:33)2 d x λ min(Σ(cid:98) ) ≥ λ+ min(Σ) 1 − 23.5K2 ln(9) n + n . + Case n < d. In this case we essentially study concentration of a smallest singular value of a Gram matrix G(cid:98) = 1 X(cid:62)X. For the case n < d, Theorem 4 would give us a vacuous estimate, and therefore we rely d √ on Theorem 5 which requires additional assumption that columns of X lie on a (elliptic) sphere of radius d. In particular, similarly a √s before, applying Theorem 5 to the matrix Σ† 1 2 X with isotropic columns Σ† 1 2 X i satisfying (cid:107)Σ† 1 2 X i(cid:107) = d a.s. for all i ∈ [n], we get (cid:107)Σ† 21 (cid:107)s min(X) ≥ s min (cid:16) Σ† 1 2 X(cid:17) ≥ √ d − 23.5K2((cid:112) ln(9)n + 6√ x) with probability at least 1 − e−x for x > 0. Again, this gives us (cid:113) (cid:16)√ (cid:16)(cid:112) √ (cid:17)(cid:17) s (X) ≥ s+ (Σ) d − 23.5K2 ln(9) n + 6 x , min min 24
√ while dividing through by d, taking the non-negative part of the r.h.s. and squaring gives us (cid:18) (cid:18)(cid:114) (cid:114) (cid:19)(cid:19)2 n x λ min(G(cid:98)) ≥ λ+ min(Σ) 1 − 23.5K2 ln(9) d + 6 d . + Now we relate λ min(G(cid:98)) to the smallest non-zero eigenvalue of Σ(cid:98) (see also [Bai and Yin, 1993, Remark 1]). The smallest eigenvalue of dG(cid:98) corresponds to d−n+1-th smallest eigenvalue of nΣ(cid:98) , that is dλ min(G(cid:98)) = nλ+ (Σ(cid:98) ). That said, multiplying the previous inequality through by d/n and rearranging, we get min (cid:32)(cid:114) (cid:18) (cid:114) (cid:19)(cid:33)2 d (cid:112) x λ+ (Σ(cid:98) ) ≥ λ+ (Σ) − 23.5K2 ln(9) + 6 min min n n + The proof is now complete. E Bounding the third term (orthogonal complement) Finally, we take care of E[(cid:107)w(cid:63)(cid:107)2 ]. Clearly, in the underparameterized case d ≥ n, E[(cid:107)w(cid:63)(cid:107)2 ] = 0 I−M(cid:99) I−M(cid:99) and so we will not consider such a case. On the other hand, in the overparameterized case, we argue that √ whenever the spectrum of M(cid:99) decays sufficiently quickly, the term of interest will behave as (cid:107)w(cid:63)(cid:107)2/ n. Consider the following theorem due to [Shawe-Taylor et al., 2005, Theorem 1], which is concerned with the magnitude of projection onto partial eigenbasis of a covariance matrix (they state the theorem for Kernel-PCA, however we adapt it here for the Euclidean space): Theorem 6 ([Shawe-Taylor et al., 2005, Theorem 1]). Denote the k-“tail” of eigenvalues of Σ(cid:98) as n (cid:88) λˆ>k = λˆ . i i=k+1 Then, for any z ∈ Rd, with probability at least 1 − δ over S, for all r ∈ [n],  √ (cid:118)  (cid:115) E (cid:104) (cid:107)P U⊥ r (z)(cid:107)2 2(cid:105) ≤ km ∈i [n r]   n1 · λˆ>k + 1 + √ n k (cid:117) (cid:117) (cid:116) n2 (cid:88)n (cid:107)X i(cid:107)2  + (cid:107)z(cid:107)2 2 1 n8 · ln (cid:18) 2 δn (cid:19) . i=1 Since the rank of the covariance matrix in our case is n and inputs lie on a unit sphere, we have that w.p. at least 1 − δ over S, (cid:104) (cid:105) (cid:40) 1 (cid:16) √ (cid:17) (cid:114) 2 (cid:41) (cid:115) 18 (cid:18) 2n (cid:19) E (cid:107)w(cid:63)(cid:107)2 ≤ min · λˆ>k + 1 + k + (cid:107)w(cid:63)(cid:107)2 · ln (3) I−M(cid:99) k∈[n] n n 2 n δ Thus, assuming that eigenvalues decay quickly enough that is λˆ = Cb−i for some constants C > 0, b > i 1, i ∈ N, the above projection behaves as (w.h.p. over S) (cid:104) (cid:105) (cid:18) (cid:107)w(cid:63)(cid:107)2 (cid:19) E (cid:107)w(cid:63)(cid:107)2 = O˜ √ 2 as n → ∞ . I−M(cid:99) n 25
A natural question is whether we indeed typically observe a polynomial decay of the spectrum. As an illustrative example we consider a simulation where inputs are sampled uniformly from a unit sphere for the sample size n ∈ (cid:8) 2i : i ∈ [13](cid:9) and d = 10n. The min {. . .} term in Eq. (3) is plotted against the k∈[n] sample size in Fig. 5. We observe that the term exhibits polynomial decay. 100 10 1 0 2000 4000 6000 8000 Sample size k +1 +k> 1 nim } n{ n ]n[ k Figure 5: Decay of the min {. . .} term in Eq. (3) for inputs distributed on a unit sphere. Here d = 10n. k∈[n] Error bars are omitted due to insignificant scale. F More on the effect of depth In section 5, we suggested that the ill-conditioning of the intermediary features of a neural network is not only due to the size of the network, but also to the weights distribution across the layers. More particularly, we suggest here that the optimization difficulty we observe for deep neural networks is linked among other factors to the minimum eigenvalue of the activations of the penultimate layer. To support our hypothesis, we run an experiment where we train networks of a fixed width (equal to 500) and depth varying from 2 to 10. We track the test error at various stages of training and the minimum eigenvalue of the features of the last layer. In Figure 6, we can observe that as expected, the deeper the network, the harder it is to train them. This is reflected in the increasing test error. For the deepest network, simple gradient descent fails to obtain a reasonable performance even after 10000 iterations. Moreover, we observe that the deeper the network, the smaller is the minimum eigenvalue, and the most ill-conditioned settings get even worse with training. 26
0.970 0.968 0.966 0.964 0.962 0.960 0.958 0.956 2 4 6 8 10 rorre tseT number of training iterations = 1000 10 7 10 11 10 15 10 19 10 23 10 27 10 31 10 35 2 4 6 8 10 eulavnegie muminiM number of training iterations = 5000 number of training iterations = 10000 0.984 0.980 0.982 0.975 0.980 0.970 0.978 0.976 0.965 0.974 0.960 0.972 2 4 6 8 10 2 4 6 8 10 10 7 10 6 10 10 10 12 10 14 10 17 10 18 10 22 10 22 10 27 10 26 10 32 10 30 10 37 10 34 2 4 6 8 10 2 4 6 8 10 Figure 6: Mean test error and minimum eigenvalue for networks of fixed width = 500 and varying depth: MNIST - 1000 samples for training, 10000 samples for test, networks trained with gradient descent and step size 0.01. To further this analysis, we also compare networks with 3 hidden layers where we increase the width in all the layers and in the penultimate layer only, creating bottleneck in the earlier layers. This experiment complements Figure 3. In Figure 7, we observe that the bottleneck results in a more important drop in the eigenvalue around the width 1000 (width of the last layer in this case). Moreover, the minimum eigenvalue stays smaller than the other considered architectures when we increase the depth. This is reflected in a higher test error, confirming once more the effect of the conditioning of the last layer features on the final performance of the network when trained with gradient descent. 27
0.8 0.7 0.6 0.5 0.4 0.3 Min. eigenvalue at initialization 0.2 10 4 0.1 1000 2000 3000 4000 5000 10 8 10 12 10 16 10 20 10 24 10 28 1000 200(0 a) 3000 4000 5000 rorre tseT number of iterations = 1000 10 5 10 9 10 13 10 17 10 21 10 25 10 29 10 33 1000 2000 3000 4000 5000 eulavnegie muminiM number of iterations = 10000 number of iterations = 100000 0.250 0.20 0.225 0.18 0.200 0.16 0.175 0.14 0.150 0.12 0.125 0.10 0.100 0.08 0.075 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 10 4 10 4 10 8 10 8 10 12 10 12 10 16 10 16 10 20 10 20 10 24 10 24 10 28 depth = 3 - same width 10 28 10 32 d de ep pt th h = = 3 1 - with bottleneck 10 32 1000 2(00b0 ) 3000 4000 5000 1000 2000 3000 4000 5000 Figure 7: Training networks of increasing width with 1 and 3 hidden layers on MNIST - For the version with bottleneck, only the size of the last hidden layer is increased, while the other layers are composed of 10 neurons: (a) Minimum positive eigenvalue of the intermediary features at initialization - (b) Test error and corresponding minimum eigenvalue of the intermediary features at different iterations G Additional Empirical Evaluation G.1 Experimental settings - More details In our experiments, we considered two datasets: MNIST and FashionMNIST. Both datasets have an input dimension of 784 and a training set of 6.104 samples. As our theory predicts that the drop in the minimum eigenvalue and the performance of the models happens when the feature size reaches the size of the training set, and in order to keep our model tractable, we use subsets of size 1000 of the training sets. These subsets are randomly chosen and kept the same when the size of the model increases. All the models are trained with plain gradient descent, with a fixed step size. We use a step size of 0.01 unless stated otherwise. All the weights of the networks are initialized from a truncated normal distribution with a scaled variance. Finally, for the MNIST experiment in Figure 2, the mean and standard errors are estimated from runs with different seeds. For the other experiments, the mean and standard errors of the test error are estimated by splitting the test set into 10 subsets. G.2 More on the effect of architectural choices In section 5, we suggested that for neural networks the quantity of interest might also be λ(cid:98)+ for intermediary min features, which is affected by size of the model but also by the distribution of the weights and architectural choices. Section F shows some experiments that validate this hypothesis. To further our analysis, we question here the impact of skip connections on the eigenvalue of features at initialization. The difficulty that depth cause for the optimization of neural networks led to our reliance on skip connections among other tricks [De and Smith, 2020]. Here, we hypothesize that skip connections make the optimization of deep networks easier thanks to a better conditioning of the feature, through a less severe drop in the minimum eigenvalue around the interpolation threshold. Figure 8 shows that for a deep network with skip connection, the minimum eigenvalue of the penultimate layer activations behaves like this of a shallow neural network. 28
Eigenvalue of features at initialization with and without skip connection 10 5 10 8 10 11 depth = 3 - with skip connection depth = 3 - without skip connection depth = 1 10 14 10 17 10 20 1000 2000 3000 4000 5000 Figure 8: Mean minimum eigenvalue at initialization for networks of depths 1 and 3 and varying width. For the network of depth 3, we show two variants: with and without skip connection. The skip connection makes the deep network eigenvalue behave like the shallow network’s. 29
