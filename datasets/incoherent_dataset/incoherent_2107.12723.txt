Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel Dominic Richards Ilja Kuzborskij Department of Statistics DeepMind University of Oxford London 24-29 St Giles’, Oxford, OX1 3LB Dominic.Richards94@gmail.com iljak@deepmind.com Abstract We revisit on-average algorithmic stability of Gradient Descent (GD) for train- ing overparameterised shallow neural networks and prove new generalisation and excess risk bounds without the Neural Tangent Kernel (NTK) or Polyak- Łojasiewicz (PL) assumptions. In particular, we show oracle type bounds which reveal that the generalisation and excess risk of GD is controlled by an interpolating network with the shortest GD path from initialisation (in a sense, an interpolating network with the smallest relative norm). While this was known for kernelised interpolants, our proof applies directly to networks trained by GD without inter- mediate kernelisation. At the same time, by relaxing oracle inequalities developed here we recover existing NTK-based risk bounds in a straightforward way, which demonstrates that our analysis is tighter. Finally, unlike most of the NTK-based analyses we focus on regression with label noise and show that GD with early stopping is consistent. 1 Introduction In a canonical statistical learning problem the learner is given a tuple of independently sampled training examples S = (z , . . . , z ), where each example z = (x , y ) consists of an input x and 1 n i i i i label y jointly distributed according to some unknown probability measure P . In the following we i assume that inputs belong to an Euclidean ball Bd(C ) of radius C and labels belong to [−C , C ]. 2 x x y y Based on training examples the goal of the learner is to select parameters W from some parameter space W in order to minimise the statistical risk 1 (cid:90) L(W) d =ef (f (x) − y)2 dP 2 W where f is a predictor parameterised by W. The best possible predictor in this setting is the W regression function f (cid:63), which is defined as f (cid:63)(x) = (cid:82) y dP , while the minimum possible risk Y |X=x is equal to the noise-rate of the problem, which is given by σ2 = (cid:82) (f (cid:63)(x) − y)2 dP . Z In this paper we will focus on a shallow neural network predictor that takes the form m f (x) d =ef (cid:88) u φ ((cid:104)W , x(cid:105)) (cid:0) x ∈ Rd, W ∈ Rd×m(cid:1) W k k k=1 defined with respect to some activation function φ : R → R, fixed output layer u ∈ {±1/√ m}m, and a tunable (possibly randomised) hidden layer W. In particular, we will consider f , where the WT Preprint. Under review. 1202 voN 9 ]LM.tats[ 2v32721.7012:viXra
def hidden layer is obtained by minimising an empirical proxy of L called the empirical risk L (W) = S (2n)−1 (cid:80)n (f (x ) − y )2 by running a Gradient Descent (GD) procedure: For t ∈ [T ] steps with i=1 W i i initial parameters W and a step size η > 0, we have iterates W = W − η∇L (W ) where 0 t+1 t t t ∇L (W) is the first order derivative of L (W). t t Understanding the behaviour of the statistical risk for neural networks has been a long-standing topic of interest in the statistical learning theory [Anthony and Bartlett, 1999]. The standard approach to this problem is based on uniform bounds on the generalisation gap (cid:15)Gen(W ) d =ef L(W ) − L (W ) , T T S T which, given a parameter space W, involves controlling the gap for the worst possible choice of W ∈ W under some unknown data distribution. The theory then typically leads to the capacity-based (Rademacher complexity, VC-dimension, or metric-entropy based) bounds which hold with high probability (w.h.p.) over S [Bartlett and Mendelson, 2002, Golowich et al., 2018, Neyshabur et al., 2018]: 1 (cid:114) capacity(W) sup |(cid:15)Gen(W)| (cid:46) . (1) n W∈W Thus, if one could simultaneously control the empirical risk and the capacity of the class of neural networks, one could control the statistical risk. Unfortunately, controlling the empirical risk L turns T out to be a challenging part here since it is non-convex, and thus, it is not clear whether GD can minimise it up to a desired precision. This issue has attracted considerable attention in recent years with numerous works [Du et al., 2018, Lee et al., 2019, Allen-Zhu et al., 2019, Oymak and Soltanolkotabi, 2020] demonstrating that overparameterised shallow networks (in a sense m (cid:38) poly(n)) trained on subgaussian inputs converge to global minima exponentially fast, namely, L S(W T ) (cid:46) (1 − η · d/n)T . Loosely speaking, these proofs are based on the idea that a sufficiently overparameterised network trained by GD initialised at W with Gaussian entries, predicts closely to a solution of a Kernelised Least-Squares 0 (KLS) formulation minimised by GD, where the kernel function called the Neural Tangent Kernel (NTK) [Jacot et al., 2018] is implicitly given by the activation function. This connection explains the observed exponential rate in case of shallow neural networks: For the NTK kernel matrix K the convergence rate of GD is (1 − ηλ (K))T , where λ (K) is its smallest eigenvalue, and as it min min turns out, for subgaussian inputs λ (K) (cid:38) d/n [Bartlett et al., 2021]. 2 Naturally, the convergence min was exploited to state bounds on the statistical risk: Arora et al. [2019] showed that for noise-free regression (σ2 = 0) when T (cid:38) 1/(ηλ (K)), min (cid:114) (cid:104)y, (nK)−1y(cid:105) L(W ) (cid:46) . (2) T n Clearly, the bound is non-vacuous whenever (cid:10) y, K−1y(cid:11) (cid:46) n2α for some α ∈ [0, 1), and Arora et al. [2019] present several examples of smooth target functions which satisfy this. More generally, for f (cid:63) which belongs to the Reproducing kernel Hilbert space (RKHS) induced by NTK one has (cid:10) y, K−1y(cid:11) ≤ (cid:107)f (cid:63)(cid:107)2 [Scho¨lkopf and Smola, 2002]. The norm-based control of the risk is HNTK standard in the literature on kernels, and thus, one might wonder to which extent neural networks are kernel methods in disguise? At the same time, some experimental and theoretical evidence [Bai and Lee, 2019, Seleznova and Kutyniok, 2020, Suzuki and Akiyama, 2021] suggest that connection to kernels might be good only at explaining the behaviour of very wide networks, much more overparameterised than those used in practice. Therefore, an interesting possibility is to develop alternative ways to analyse generalisation in neural networks: Is there a more straightforward kernel- free optimisation-based perspective? In this paper we take a step in this direction and explore a kernel-free approach, which at the same time avoids worst-case type uniform bounds such as Eq. (1). In particular, we focus on the notion of the algorithmic stability: If an algorithm is insensitive to replacement (or removal) of an observation 1Throughout this paper, we use f (cid:46) g to say that there exists a universal constant c > 0 and some k ∈ N such that f ≤ cg logk(g) holds uniformly over all arguments. 2Which is a tightest known bound, Oymak and Soltanolkotabi [2020] prove a looser bound without distribu- tional assumption on the inputs. 2
in a training tuple, then it must have a small generalisation gap. Thus, a natural question is whether GD is sufficiently stable when training overparameterised neural networks. The stability of GD (and its stochastic counterpart) when minimising convex and non-convex smooth objective functions was first explored by Hardt et al. [2016]. Specifically, for a time-dependent choice of a step size η = 1/t and a problem-dependent constant α ∈ (0, 1) they show that t E (cid:2) (cid:15)Gen(W T ) (cid:12) (cid:12) W 0, u(cid:3) (cid:46) ln(T )n−α . Unfortunately, when combined with the NTK-based convergence rate of the empirical risk we have a vacuous bound since L (W ) (cid:46) 1. 3 This is because the stability is enforced through a S T quickly decaying step size rather than by exploiting a finer structure of the loss, and turns out to be insufficient to guarantee the convergence of the empirical risk. That said, several works Charles and Papailiopoulos [2018], Lei and Ying [2021] have proved stability bounds exploiting an additional structure in mildly non-convex losses. Specifically, they studied the stability of GD minimising a gradient-dominated empirical risk, meaning that for all W in some neighbourhood W and a problem-dependent quantity µ, it is assumed that L (W) − min L (W(cid:48)) ≤ (cid:107)∇L (W)(cid:107)2/(2µ). S W(cid:48) S S Having iterates of GD within W and assuming that the gradient is ρ-Lipschitz, this allows to show E (cid:2) (cid:15)Gen(W T ) (cid:12) (cid:12) W 0, u(cid:3) (cid:46) µρ · n1 . As it turns out, the condition is satisfied for the iterates of GD training overparameterised networks [Du et al., 2018] with high probability over W . The key quantity that controls the bound is ρ/µ (cid:46) 0 1/λ (K) which can be interpreted as a condition number of the NTK matrix. However, it is min known that for subgaussian inputs the condition number behaves as n/d which renders the bound vacuous [Bartlett et al., 2021]. 2 Our Contributions In this paper we revisit algorithmic stability of GD for training overparameterised shallow neural networks, and prove new risk bounds without the Neural Tangent Kernel (NTK) or Polyak- Łojasiewicz (PL) machinery. In particular, we first show a bound on the generalisation gap and then specialise it to state risk bounds for regression with and without label noise. In the case of learning with noise we demonstrate that GD with a form of early stopping is consistent, meaning that the risk asymptotically converges to the noise rate σ2. Our analysis brings out a key quantity, which controls all our bounds, the Regularised Empirical Risk Minimisation (R-ERM) Oracle defined as (cid:18) (cid:107)W − W (cid:107)2 (cid:19) ∆oracle d =ef min L (W) + O 0 F as ηT → ∞ , (3) S W∈Rd×m S ηT which means that ∆oracle is essentially an empirical risk of solution closest to initialisation (an S interpolant when for m large enough). We first consider a bound on the generalisation gap. 2.1 Generalisation Gap For simplicity of presentation in the following we assume that the activation function φ, its first and second derivatives are bounded. Assuming parameterisation m (cid:38) (ηT )5 we show (Corollary 1) that the expected generalisation gap is bounded as (cid:18) (cid:19) E (cid:2) (cid:15)Gen(W T ) (cid:12) (cid:12) W 0, u(cid:3) ≤ C · η nT 1 + η nT E (cid:2) ∆o Sracle (cid:12) (cid:12) W 0, u(cid:3) , (4) where C is a constant independent from n, T, η. 3If η = 1 we have L (W ) (cid:46) exp(µ (cid:80)T 1 ) ≈ 1 , thus, if µ ≈ 1 we then require T ∼ (cid:15)−n for s s S T j=1 j Tµ n L (W ) (cid:46) (cid:15). Plugging this into the Generalisation Error bound we get that log(T )n−α = n1−α log(1/(cid:15)) S T which is vacuous as n grows. 3
Dependence of m on the total number of steps T might appear strange at first, however things clear out once we pay attention to the scaling of the bound. Setting the step size η to be constant, T = nα, and overparameterisation m (cid:38) n5α for some free parameter α ∈ (0, 1] we have E (cid:2) (cid:15)Gen(W T ) (cid:12) (cid:12) W 0, u(cid:3) = O (cid:18) n1 E (cid:104) (cid:107)Wˆ − W 0(cid:107)2 F (cid:12) (cid:12) (cid:12) W 0, u(cid:105)(cid:19) as n → ∞ , where Wˆ is chosen as parameters of a minimal-norm interpolating network, in a sense Wˆ ∈ argmin {(cid:107)W − W (cid:107)2 | L (W) = 0}. Thus, as Eq. (4) suggests, the generalisation gap is W∈Rd×m 0 F S controlled by a minimal relative norm of an interpolating network. In comparison, previous work in the NTK setting obtained results of a similar type where in place of Wˆ one has an interpolating minimal-norm solution to an NTK least-squares problem. Specifically, in Eq. (2) the generalisation gap is controlled by (cid:10) y, (nK)−1y(cid:11) , which is a squared norm of such solution. The generalisation gap of course only tells us a partial story, since the behaviour of the empirical risk is unknown. Next, we take care of this and present bounds excess risk bounds. 2.2 Risk Bound without Label Noise We first present a bound on the statistical risk which does not use the usual NTK arguments to control the empirical risk. For now assume that σ2 = 0 meaning that there is no label noise and randomness is only in the inputs. NTK-free Risk Bound. In Corollary 2 we show that the risk is bounded as (cid:18) (cid:18) (cid:19)(cid:19) E [L(W T ) | W 0, u] ≤ 1 + C · η nT 1 + η nT E (cid:2) ∆o Sracle (cid:12) (cid:12) W 0, u(cid:3) . (5) Note that this bound looks very similar compared to the bound on generalisation gap. The difference lies in a “1+” term which accounts for the fact that L(W ) ≤ ∆oracle as we show in Lemma 2. T S As before we let the step size be constant, set T = nα, and let overparameterisation be m (cid:38) n5α for some α ∈ (0, 1]. Then our risk bound implies that (cid:18) 1 (cid:104) (cid:12) (cid:105)(cid:19) E [L(W ) | W , u] = O E (cid:107)Wˆ − W (cid:107)2 (cid:12) W , u as n → ∞ , T 0 nα 0 F (cid:12) 0 which comes as a simple consequence of bounding ∆oracle while choosing Wˆ to be parameters of a S minimal relative norm interpolating network as before. Recall that y = f (cid:63)(x ): The bound suggests i i that target functions f (cid:63) are only learnable if n−α E[(cid:107)Wˆ − W (cid:107)2 | W , u] → 0 as n → ∞ for a 0 F 0 fixed α and input distribution. A natural question is whether a class of such functions is non-empty. Indeed, the NTK theory suggests that it is not and in the following we will recover NTK-based results by relaxing our oracle inequality. Similarly as in Section 2.1 we choose T = nα and so we require m (cid:38) n5α, which trades off overparameterisation and convergence rate through the choice of α ∈ (0, 1]. To the best of our knowledge this is the first result of this type, where one can achieve a smaller overparameterisation compared to the NTK literature at the expense of a slower convergence rate of the risk. Finally, unlike the NTK setting we do not require randomisation of the initialisation and the risk bound we obtain holds for any W . 4 0 Comparison to NTK-based Risk Bound. Our generalisation bound can be naturally relaxed to obtain NTK-based empirical risk convergence rates. In this case we observe that our analysis is general enough to recover results of Arora et al. [2019] (up to a difference that our bounds hold in expectation rather than in high probability). By looking at Eq. (5), our task is in controlling ∆oracle: by its definition in Eq. (3) we can see S that it can be bounded by the regularised empirical risk of any model, and we choose a Moore- Penrose pseudo-inverse solution to the least-squares problem supplied with the NTK feature map x (cid:55)→ (∇ f (x))(W ). Here, entries of W are drawn from N (0, ν2 ) for some appropriately W W 0 0 init 4In the NTK literature randomisation of W is required to guarantee that λ (K) > 0 [Du et al., 2018]. 0 min 4
chosen ν2 , and entries of the outer layer u are distributed according to the uniform distribution init over {±1/√ m}, independently from each other and the data. It is not hard to see that aforementioned pseudo-inverse solution Wpinv will have zero empirical risk, and so we are left with controlling (cid:107)Wpinv − W (cid:107)2 as can be seen from the definition of ∆oracle. Note that it is straightforward to do 0 F S since Wpinv has an analytic form. In Theorem 2 we carry out these steps and show that with high probability over initialisation (W , u), 0 (cid:18) (cid:19) ∆oracle = O˜ 1 (cid:10) y, (nK)−1y(cid:11) as n → ∞ , S P n which combined with Eq. (5) recovers the result of Arora et al. [2019]. Note that we had to relax the oracle inequality, which suggests that the risk bound we give here is tighter compared to the NTK- based bound. Similarly, Arora et al. [2019] demonstrated that (cid:107)W −W (cid:107)2 = O˜ (cid:0)(cid:10) y, (nK)−1y(cid:11)(cid:1) T 0 F P holds w.h.p. over initialisation and ReLU activation function, however their proof requires a much more involved “coupling” argument where iterates (W )T are shown to be close to the GD iterates t t=1 minimising a KLS problem with NTK matrix. 2.3 Risk Bound with Label Noise and Consistency So far we considered regression with random inputs, but without label noise. In this section we use our bounds to show that GD with early stopping is consistent in the regression with label noise. Here labels are generated as y = f (cid:63)(x ) + ε where zero-mean random variables (ε )n are i.i.d., almost i i i i i=1 surely bounded,5 and E[ε2] = σ2. 1 For now we will use the same settings of parameters as in Section 2.2: Recall that η is constant, T = nα, m (cid:38) n5α for a free parameter α ∈ (0, 1]. In addition assume a well-specified scenario which means that m is large enough such that for some subset of parameters W we achieve L(W(cid:63)) = σ2 for some parameters W(cid:63). Employing our risk bound of Eq. (5), we relax R-ERM oracle as E[∆oracle | W , u] (cid:46) σ2 + n−α(cid:107)W(cid:63) − W (cid:107)2 , where W(cid:63) ∈ argmin L(W) . S 0 0 F W∈Rd×m Then we immediately have an α-dependent risk bound (cid:18) C (cid:19) (cid:18) (cid:18) (cid:107)W(cid:63) − W (cid:107)2 (cid:19)(cid:19) E [L(W ) | W , u] ≤ 1 + σ2 + O 0 F as n → ∞ . (6) T 0 n1−α nα It is important to note that for any tuning α ∈ (0, 1), as long as W(cid:63) is constant in n (which is the case in the parametric setting), we have E [L(W ) | W , u] → σ2 as n → ∞, and so we achieve T 0 consistency. Adaptivity to the noise achieved here is due to early stopping: Indeed, recall that we take T = nα steps and having smaller α mitigates the effect of the noise as can be seen from Eq. (6). This should come as no surprise as early stopping is well-known to have a regularising effect in GD methods [Yao et al., 2007]. The current literature on the NTK deals with this through a kernelisation perspective by introducing an explicit L2 regularisation [Hu et al., 2021], while risk bound of [Arora et al., 2019] designed for a noise-free setting would be vacuous in this case. 2.4 Future Directions and Limitations We presented generalisation and risk bounds for shallow nets controlled by the Regularised Empirical Risk Minimisation oracle ∆oracle. By straightforward relaxation of ∆oracle, we showed that the risk S S can be controlled by the minimal (relative) norm of an interpolating network which is tighter than the NTK-based bounds. There are several interesting venues which can be explored based on our results. For example, by assuming a specific form of a target function f (cid:63) (e.g., a “teacher” network), one possibility is to characterise the minimal norm. This would allow us to understand better which such target function are learnable by GD. One limitation of our analysis is smoothness of the activation function and its boundedness. While boundedness can be easily dealt with by localising the smoothness analysis (using a Taylor approxi- mation around each GD iterate) and randomising (W , u), extending our analysis to non-smooth 0 activations (such as ReLU φ(x) = max(x, 0)) appears to be non-trivial because our stability analysis 5We assume boundedness of labels throughout our analysis to ensure that L (W ) is bounded almost surely S 0 for simplicity. This can be relaxed by introducing randomised initialisation, e.g. as in [Du et al., 2018]. 5
crucially relies on the control of the smallest eigenvalue of the Hessian. Another peculiarity of our analysis is a time-dependent parameterisation requirement m (cid:38) (ηT )5. Making parameterisation n-dependent introduces an implicit early-stopping condition, which helped us to deal with label noise. On the other hand, such requirement might be limiting when dealing with noiseless interpolating learning where one could require to have ηT → ∞ while keeping n finite. Notation In the following denote (cid:96)(w, (x, y)) d =ef 1 (f (x)−y)2. Operator norm is denoted as (cid:107)·(cid:107) , while L 2 W op 2 def def norm is denoted by (cid:107) · (cid:107) or (cid:107) · (cid:107) . We use notation (a ∨ b) = max {a, b} and (a ∧ b) = min {a, b} 2 F throughout the paper. Let (W(i)) be the iterates of GD obtained from the data set with a resampled t t data point: S(i) d =ef (z , . . . , z , z , z , . . . , z ) 1 i−1 (cid:101)i i+1 n where z is an independent copy of z . Moreover, denote a remove-one version of S by (cid:101)i i S\i d =ef (z , . . . , z , z , . . . , z ) . 1 i−1 i+1 n 3 Main Result and Proof Sketch This section presents both the main results as well as a proof sketch. Precisely, Section 3.1 presents the main result and Section 3.2 the proof sketch. 3.1 Main Results We formally make the following assumption regarding the regularity of the activation function. Assumption 1 (Activation). The activation φ(u) is continuous and twice differentiable with constant B , B , B ≥ 0 bounding |φ(u)| ≤ B , |φ(cid:48)(u)| ≤ B and |φ(cid:48)(cid:48)(u)| ≤ B for any u ∈ R. φ φ(cid:48) φ(cid:48)(cid:48) φ φ(cid:48) φ(cid:48)(cid:48) This is satisfied for sigmoid as well as hyperbolic tangent activations. Assumption 2 (Inputs, labels, and the loss function). For constants C , C , C > 0, inputs belong x y 0 to Bd(C ), labels belong to [−C , C ], and loss is uniformly bounded by C almost surely. 2 x y y 0 A consequence of the above is an essentially constant smoothness of the loss and the fact that the √ Hessian scales with 1/ m (see Appendix A for the proof). These facts will play a key role in our stability analysis. 6 Lemma 1 (Smoothness and curvature). Fix W, W(cid:102) ∈ Rd×m. Consider Assumption 1, Assumption 2, and assume that L S(W(cid:102) ) ≤ C 02. Then, for any S, (cid:18) (cid:19) B C λ (∇2L (W)) ≤ ρ where ρ d =ef C2 B2 + B B + √φ(cid:48)(cid:48) y , max S x φ(cid:48) φ(cid:48)(cid:48) φ m (cid:0) (cid:1) B B C + C αm ∈[i 0n ,1]λ min(∇2L S(W(cid:102) + α(W − W(cid:102) ))) ≥ − φ(cid:48)(cid:48) φ √(cid:48) mx 0 · (1 ∨ (cid:107)W − W(cid:102) (cid:107) F ) . Given these assumptions we are ready to present a bound on the Generalisation Error of the gradient descent iterates. Theorem 1 (Generalisation Error). Consider Assumptions 1 and 2. Fix t > 0. If η ≤ 1/(2ρ) and (cid:16) √ √ (cid:17)2 m ≥ 144(ηt)2C4C2B2 4B C ηt + 2 (7) x 0 φ(cid:48)(cid:48) φ(cid:48) x then E (cid:2) (cid:15)Gen(W t+1) (cid:12) (cid:12) W 0, u(cid:3) ≤ b (cid:18) nη + η n2 2t (cid:19) (cid:88)t E [L S(W j) | W 0, u] j=0 3 3 where b = 16e3C x2 B φ2 (cid:48)(1 + C x2 B φ2 (cid:48)). 6We use notation (a ∨ b) d=ef max {a, b} and (a ∧ b) d=ef min {a, b} throughout the paper. 6
Proof. For full proof see Appendix C with sketch proof in Section 3.2. Theorem 1 then provides a formal upper bound on the Generalisation Gap of a shallow neural network trained with GD. Specifically, provided the network is sufficiently wide the generalisation gap can be controlled through both: the gradient descent trajectory evaluated at the Empirical Risk (cid:80)T E[L (W ) | W , u], as well as step size and number of iterations in the multiplicative factor. t=0 S t 0 Similar bounds for the test performance of gradient descent have been given by Lei and Ying [2020], although in our case the risk is non-convex, and thus, a more delicate bound on the Optimisation Gap is required. This is summarised within the lemma, which is presented in an “oracle” inequality manner. Lemma 2 (Optimisation Error). Consider Assumptions 1 and 2. Fix t > 0. If η ≤ 1/(2ρ), then 1 t (cid:88)t L S(W j) ≤ W∈m Ri dn ×m (cid:110) L S(W) + (cid:107)W − ηW t 0(cid:107)2 F + (cid:101)b(cid:107)W √− mW 0(cid:107)3 F (cid:111) + (cid:101)bC 0 · ( √ηt m) 3 2 j=0 where (cid:101)b = C x2B φ(cid:48)(cid:48) (B φ(cid:48)C x + C 0). Proof. Full proof is given in Appendix B with sketch proof in Section 3.2. By combining Theorem 1 and Lemma 2 we get the following where ∆oracle is defined in Eq. (3): S Corollary 1. Assume the same as in Theorem 1 and Lemma 2. Then, (cid:18) (cid:19) E (cid:2) (cid:15)Gen(W T ) (cid:12) (cid:12) W 0, u(cid:3) ≤ C · η nT 1 + η nT E (cid:2) ∆o Sracle (cid:12) (cid:12) W 0, u(cid:3) where ∆oracle is defined in Eq. (3) and C is a constant independent from n, T, η. S The above combined with Lemma 2, and the fact that L (W ) = min L (W ) gives us: S T t∈[T ] S t Corollary 2. Assume the same as in Theorem 1 and Lemma 2. Then, (cid:18) (cid:18) (cid:19)(cid:19) E [L(W T ) | W 0, u] ≤ 1 + C · η nT 1 + η nT E (cid:2) ∆o Sracle (cid:12) (cid:12) W 0, u(cid:3) . Proof. The proof is given in Appendix E. Finally, ∆oracle is controlled by the norm of the NTK solution, which establishes connection to the S NTK-based risk bounds: Theorem 2 (Connection between ∆oracle and NTK). Consider Assumption 1 and that ηT = n. S Moreover, assume that entries of W are i.i.d., that λ (K) (cid:38) 1/n, and assume that u ∼ √ 0 min m unif ({±1/ m}) independently from all sources of randomness. Then, with probability least 1 − δ for δ ∈ (0, 1), over (W , u), 0 (cid:18) (cid:19) ∆oracle = O˜ 1 (cid:10) y, (nK)−1y(cid:11) as n → ∞ . S P n Proof. The proof is given in Appendix D. 3.2 Proof Sketch of Theorem 1 and Lemma 2 Throughout this sketch let expectation be understood as E[·] = E[· | W , u]. For brevity we will 0 also vectorise parameter matrices, so that W ∈ Rdm and thus (cid:107) · (cid:107) = (cid:107) · (cid:107) . 2 F Let us begin with the bound on the generalisation gap, for which we use the notion of algorithmic stability [Bousquet and Elisseeff, 2002]. With W(i) denoting a GD iterate with the dataset with the t resampled data point S(i), the Generalisation Gap can be rewritten [Shalev-Shwartz and Ben-David, 2014, Chap. 13] n (cid:15)Gen = E[L(W ) − L (W )] = 1 (cid:88) E[(cid:96)(W , z ) − (cid:96)(W(i), z )] . T S T n T (cid:101)i T (cid:101)i i=1 7
The equality suggests that if trained parameters do not vary much when a data point is resampled, that is W ≈ W(i), then the Generalisation Gap will be small. Indeed, prior work [Hardt et al., T T 2016, Kuzborskij and Lampert, 2018] has been dedicated to proving uniform bounds on L norm 2 max (cid:107)W − W(i)(cid:107) . In our case, we consider the more refined squared L expected stability i∈[n] T T 2 2 and consider a bound for smooth losses similar to that of Lei and Ying [2020]: Lemma 3. Consider Assumptions 1 and 2. Then, (cid:118) (cid:117) n n (cid:15)Gen (cid:46) (cid:112) E [L S(W T )](cid:117) (cid:116) n1 (cid:88) E (cid:104) (cid:107)W T − W T(i)(cid:107)2 op(cid:105) + n1 (cid:88) E (cid:104) (cid:107)W T − W T(i)(cid:107)2 op(cid:105) . i=1 i=1 Proof. The proof is given in Appendix C.1. To this end, we bound (cid:107)W − W(i)(cid:107)2 recursively by using the definition of the gradient iterates and T T 2 applying Young’s inequality (a + b)2 ≤ (1 + 1 )a2 + (1 + t)b2 to get that for any i ∈ [n]: t (cid:107)W − W(i) (cid:107)2 (8) t+1 t+1 2 = (cid:107)W − W(i) − η(∇L (W ) − ∇L (W(i)))(cid:107)2 t t S t S(i) t 2 η = (cid:107)W − W(i) − η(∇L (W ) − ∇L (W(i))) + (∇(cid:96)(W , z ) − ∇(cid:96)(W(i), z˜ )(cid:107)2 t t S\i t S\i t n t i t i 2 1 (cid:16) (cid:17) ≤ (1 + ) (cid:107)W − W(i) − η ∇L (W ) − ∇L (W(i)) (cid:107)2 t t t S\i t S\i t 2 (cid:124) (cid:123)(cid:122) (cid:125) Expansiveness of the Gradient Update η2(1 + t) (cid:16) (cid:17) + (cid:107)∇(cid:96)(W , z )(cid:107)2 + (cid:107)∇(cid:96)(W(i), z )(cid:107)2 . (9) n2 t i 2 t (cid:101)i 2 The analysis of the expansiveness of the gradient update then completes the recursive relation- ship [Hardt et al., 2016, Kuzborskij and Lampert, 2018, Lei and Ying, 2020, Richards and Rab- bat, 2021]. Clearly, a trivial handling of the term (using smoothness) would yield a bound (1 + ηρ)2(cid:107)W − W(i)(cid:107)2, which leads to an exponential blow-up when unrolling the recursion. t t 2 So, we must ensure that the expansivity coefficient is no greater than one. While the classic analysis of Hardt et al. [2016] controls this by having a polynomially decaying learning rate schedule, here we leverage on observation of Richards and Rabbat [2021] that the negative Eigenvalues of the loss’s Hessian ∇2L (·) ∈ Rdm×dm can control the expansiveness. This boils down to Lemma 1: S\i (cid:18)(cid:90) 1 (cid:19) 1 ∨ (cid:107)W − W(i)(cid:107) (cid:114) ηt λ ∇2L (W(i) + α(W − W(i))) dα (cid:38) − √t t 2 (cid:38) − min S\i t t t m m 0 √ where (cid:107)W − W(i)(cid:107) (cid:46) ηt comes from smoothness and standard analysis of GD (see Lemma 6). t t 2 √ Note that the eigenvalue scales 1/ m because Hessian has a block-diagonal structure. Controlling expansiveness of GD updates. Abbreviate ∆ = ∇L (W ) − ∇L (W(i)) and S\i t S\i t ∇2 = (cid:82) 1 ∇2L (W(i) + α(W − W(i))) dα. Now we “open” the squared norm 0 S\i t t t (cid:68) (cid:69) (cid:107)W − W(i) − η∆(cid:107)2 = (cid:107)W − W(i)(cid:107)2 + η2 (cid:107)∆(cid:107)2 − 2η W − W(i), ∆ t t 2 t t 2 2 t t and use Taylor’s theorem for gradients to have η2 (cid:107)∆(cid:107)2 − 2η (cid:68) W − W(i), ∆(cid:69) = (cid:68) ∆, (cid:0) η2∇2 − 2ηI(cid:1) (W − W(i))(cid:69) 2 t t t t = (cid:68) W − W(i), η∇2 (cid:0) η∇2 − 2I(cid:1) (W − W(i))(cid:69) t t t t (cid:32) (cid:114) (cid:33) ηt ηt (cid:46) η + η2 (cid:107)W − W(i)(cid:107)2 m m t t 2 where we noted that (η∇2 − 2I) has only negative eigenvalues due to assumption η ≤ 1/(2ρ). 8
By rearranging this inequality one can verify that the gradient operator is approximately co- coercive [Hardt et al., 2016]. This result, crucial to our proof, is supported by some empirical evidence: In Fig. 1 we show a synthetic experiment where we train a shallow neural network with sigmoid activation. (cid:32) (cid:114) (cid:33) ηt (cid:107)W − W(i) − η∆(cid:107)2 (cid:46) 1 + η (cid:107)W − W(i)(cid:107)2 . t t 2 m t t 2 Plugging this back into Eq. (8)-(9) we unroll the recursion: (cid:107)W − W(i) (cid:107)2 (cid:46) η2t (cid:88)t (cid:18) 1 + 1 (cid:19)t (cid:32) 1 + η(cid:114) ηt (cid:33)t (cid:16) (cid:107)∇(cid:96)(W , z )(cid:107)2 + (cid:107)∇(cid:96)(W(i), z )(cid:107)2(cid:17) . t+1 t+1 2 n2 t m j i 2 j (cid:101)i 2 j=0 The above suggest that to prevent exponential blow-up (and ensure that the expansiveness remains (cid:113) under control), it’s enough to have η ηt ≤ 1/t, which is an assumption of the theorem, enforcing m relationship between ηt and m. On-average stability bound. Now we are ready to go back to Lemma 3 and complete the proof of the generalisation bound. Since we have a squared loss we have (cid:107)∇(cid:96)(W , z )(cid:107)2 (cid:46) (cid:96)(W , z ), and j i 2 j i thus, when summing over i ∈ [n] we recover the Empirical risk 1 (cid:80)n (cid:107)∇(cid:96)(W , z )(cid:107)2 (cid:46) L (W ). n i=1 j i 2 S j Finally, noting that E[(cid:96)(W , z )] = E[(cid:96)(W(i), z )] we then arrive at the following bound on the j i j (cid:101)i expected squared L stability 2 1 (cid:88)n E[(cid:107)W − W(i) (cid:107)2] (cid:46) η2t (cid:88)t E[L (W )] . n t+1 t+1 2 n2 S j i=1 j=0 Note that the Generalisation Gap is directly controlled by the Optimisation Performance along the path of the iterates, matching the shape of bounds by Lei and Ying [2020]. Using smoothness of the loss as well as that L (W ) ≤ 1 (cid:80)T L (W ), the risk is bounded when m (cid:38) (ηt)3 as S T T j=0 S j (cid:18) (cid:18) (cid:19)(cid:19) T ηT ηT 1 (cid:88) E[L(W )] (cid:46) 1 + 1 + E[L (W )] . T n n T S j j=0 Optimisation error. We must now bound the Optimisation Error averaged across the iterates. Following standard optimisation arguments for gradient descent on a smooth objective we then get η L (W ) ≤ L (W ) − (cid:107)∇L (W )(cid:107)2 S t+1 S t 2 S t 2 At this point convexity would usually be applied to upper bound L (W ) ≤ L (Wˆ ) − S t S (cid:104)∇L (W ), Wˆ − W (cid:105) where Wˆ is a minimiser of L (·). The analysis differs from this approach S t t S in two respects. Firstly, we do not consider a minimiser of L (·) but a general point W, which is S then optimised over at the end. Secondly, the objective is not convex, and therefore, we leverage that the Hessian negative Eigenvalues are on the order of − √1 to arrive at the upper bound (Lemma 1): m 1 L (W ) (cid:46) L (W) − (cid:104)∇L (W ), W − W (cid:105) + √ (cid:107)W − W (cid:107)3 S t S S t t m t 2 (cid:124) (cid:123)(cid:122) (cid:125) Convex Component (cid:124) (cid:123)(cid:122) (cid:125) Negative Hessian Eigenvalues After this point, the standard optimisation analysis for gradient descent can be performed i.e. −(cid:104)∇L (W ), W − W (cid:105) − η (cid:107)L (W )(cid:107)2 = 1 (cid:0) (cid:107)W − W (cid:107)2 − (cid:107)W − W (cid:107)2(cid:1) , which then S t t 2 S t 2 η t 2 t+1 2 yields to a telescoping sum over t = 0, 1, . . . , T − 1. This leads to the upper bound for W ∈ Rm×d 1 (cid:88)T E[L (W )] (cid:46) L (W) + (cid:107)W − W 0(cid:107)2 2 + √1 · 1 (cid:88)T (cid:107)W − W (cid:107)3 T S j S ηT m T j 2 j=0 j=0 where we plug this into the generalisation bound and take a minimum over W ∈ Rmd. 9
Figure 1: The gradient operator of an overparameterised shallow neural network is almost-monotone: Here, inputs are uniformly distributed on a 10-sphere and labels are generated by x (cid:55)→ e(cid:104)W(cid:63),x(cid:105) 1+e(cid:104)W(cid:63),x(cid:105) where W(cid:63) is once sampled from N (0, I ) at the beginning of the training. The initialisation W is 10 0 sampled from N (0, I ) and each experiment is performed 10 times (which is reflected by standard 10 deviation). The shallow network is then trained by GD with η = 1, T = 104, and sigmoid activation. 4 Additional Related Literature Stability of Gradient Descent. Algorithmic stability [Bousquet and Elisseeff, 2002] has been used to investigate the generalisation performance of GD in a number of works [Hardt et al., 2016, Kuzborskij and Lampert, 2018, Chen et al., 2018, Lei and Ying, 2020, Richards and Rabbat, 2021]. While near optimal bounds are achieved for convex losses, the non-convex case aligning with this work is more challenging. Within [Hardt et al., 2016, Kuzborskij and Lampert, 2018] in particular a restrictive 1/t step size is required if t iterations of gradient descent are performed. This was partially alleviated within [Richards and Rabbat, 2021] which demonstrated that the magnitude of Hessian’s negative eigenvalues can be leveraged to allow for much larger step sizes to be taken. In this work we also utilise the magnitude of Hessian’s negative Eigenvalue, although a more delicate analysis is required as both: the gradient is potentially unbounded in our case; and the Hessian’s negative eigenvalues need to be bounded when evaluated across multiple points. We note the scaling of the Hessian network has also previously been observed within [Liu et al., 2020a,b], with empirical investigations into the magnitude of the Hessian’s negative Eigenvalues conducted within [Sagun et al., 2016, Yuan et al., 2019]. Stability-based generalisation bounds were also shown for certain types of non-convex functions, such Polyak-Łojasiewicz functions [Charles and Papailiopoulos, 2018, Lei and Ying, 2021] and functions with strict saddles among stationary points [Gonen and Shalev-Shwartz, 2017]. In line with our results, Rangamani et al. [2020] shows that for interpolating kernel predictors, minimizing the norm of the ERM solution minimizes stability. Several works have also recently demonstrated consistency of GD with early stopping for training shallow neural networks in the presence of label noise. The concurrent work [Ji et al., 2021] also showed that shallow neural networks trained with gradient descent are consistent, although their approach distinctly different to ours e.g. leveraging structure of logistic loss as well as connections between shallow neural networks and random feature models / NTK. Earlier, Li et al. [2020] showed consistency under a certain gaussian-mixture type parametric classification model. In a more general nonparametric setting, Kuzborskij and Szepesva´ri [2021] showed that early-stopped GD is consistent when learning Lipschitz regression functions. Certain notions of stability, such the uniform stability (taking sup over the data rather than expectation) allows to prove high-probability risk bounds (h.p.) [Bousquet and Elisseeff, 2002] which are known to be optimal up to log-terms [Feldman and Vondrak, 2019, Bousquet et al., 2020]. In this work we show bounds in expectation, or equivalently, control the first moment of a generalisation gap. To prove a h.p. bounds while enjoying benefits of a stability in expectation, one would need to control higher order moments [Maurer, 2017, Abou-Moustafa and Szepesva´ri, 2019], however often this is done through a higher-order uniform stability: Since our proof closely relies on the stability in expectation, it is not clear whether our results can be trivially stated with high probability. 10
Acknowledgements D.R. is supported by the EPSRC and MRC through the OxWaSP CDT programme (EP/L016710/1), and the London Mathematical Society ECF-1920-61. References K. Abou-Moustafa and Cs. Szepesva´ri. An exponential efron-stein inequality for l q stable learning rules. In Algorithmic Learning Theory (ALT), 2019. Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization. In International Conference on Machine Learing (ICML), 2019. M. Anthony and P. L. Bartlett. Neural network learning: Theoretical foundations. Cambridge University Press, 1999. S. Arora, S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learing (ICML), 2019. Y. Bai and J. D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks. In International Conference on Learning Representations (ICLR), 2019. P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002. P. L. Bartlett, A. Montanari, and A. Rakhlin. Deep learning: a statistical viewpoint. Acta Numerica, 2021. URL https://arxiv.org/abs/2103.09177. To appear. O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2:499–526, 2002. O. Bousquet, Y. Klochkov, and N. Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In Conference on Computational Learning Theory (COLT), 2020. Z. Charles and D. Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In International Conference on Machine Learing (ICML), 2018. Y. Chen, C. Jin, and B. Yu. Stability and convergence trade-off of iterative optimization algorithms. arXiv preprint arXiv:1804.01619, 2018. S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations (ICLR), 2018. V. Feldman and J. Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Conference on Computational Learning Theory (COLT), 2019. N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks. In Conference on Computational Learning Theory (COLT), 2018. A. Gonen and S. Shalev-Shwartz. Fast rates for empirical risk minimization of strict saddle problems. In Conference on Computational Learning Theory (COLT), 2017. M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learing (ICML), 2016. T. Hu, W. Wang, C. Lin, and G. Cheng. Regularization matters: A nonparametric perspective on overparametrized neural network. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2021. A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: convergence and generalization in neural networks. In Conference on Neural Information Processing Systems (NeurIPS), pages 8580–8589, 2018. 11
Z. Ji, J. D. Li, and M. Telgarsky. Early-stopped neural networks are consistent. Conference on Neural Information Processing Systems (NeurIPS), 2021. I. Kuzborskij and C. H. Lampert. Data-dependent stability of stochastic gradient descent. In International Conference on Machine Learing (ICML), 2018. I. Kuzborskij and Cs. Szepesva´ri. Nonparametric regression with shallow overparameterized neural networks trained by GD with early stopping. In Conference on Computational Learning Theory (COLT), 2021. J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. Conference on Neural Information Processing Systems (NeurIPS), 2019. Y. Lei and Y. Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. In International Conference on Machine Learing (ICML), 2020. Y. Lei and Y. Ying. Sharper generalization bounds for learning with gradient-dominated objective functions. In International Conference on Learning Representations (ICLR), 2021. M. Li, M. Soltanolkotabi, and S. Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2020. C. Liu, L. Zhu, and M. Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. Conference on Neural Information Processing Systems (NeurIPS), 33, 2020a. Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. arXiv preprint arXiv:2003.00307, 2020b. A. Maurer. A second-order look at stability and generalization. In Conference on Computational Learning Theory (COLT), 2017. Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2003. B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, and N. Srebro. The role of over-parametrization in generalization of neural networks. In International Conference on Learning Representations (ICLR), 2018. S. Oymak and M. Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory, 1(1):84–105, 2020. A. Rangamani, L. Rosasco, and T. Poggio. For interpolating kernel machines, minimizing the norm of the ERM solution minimizes stability. arXiv:2006.15522, 2020. D. Richards and M. Rabbat. Learning with gradient descent and weakly convex losses. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2021. L. Sagun, L. Bottou, and Y. LeCun. Eigenvalues of the hessian in deep learning: Singularity and beyond. arXiv preprint arXiv:1611.07476, 2016. B. Scho¨lkopf and A. J. Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. The MIT Press, 2002. M. Seleznova and G. Kutyniok. Analyzing finite neural networks: Can we trust neural tangent kernel theory? arXiv preprint arXiv:2012.04477, 2020. S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge University Press, 2014. T. Suzuki and S. Akiyama. Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods. In International Conference on Learning Representations (ICLR), 2021. 12
Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289–315, 2007. Z. Yuan, Y. Yan, R. Jin, and T. Yang. Stagewise training accelerates convergence of testing error over SGD. Conference on Neural Information Processing Systems (NeurIPS), 2019. 13
Notation In the following denote (cid:96)(w, (x, y)) d =ef 1 (f (x) − y)2. Unless stated otherwise, we work with 2 W vectorised quantities so W ∈ Rdm and therefore simply interchange (cid:107) · (cid:107) with (cid:107) · (cid:107) . We also 2 F use notation (W) so select k-th block of size d, that is (W) = [W , . . . , W ](cid:62). We use k k (d−1)k+1 dk notation (a ∨ b) d =ef max {a, b} and (a ∧ b) d =ef min {a, b} throughout the paper. Let (W(i)) be the t t iterates of GD obtained from the data set with a resampled data point: S(i) d =ef (z , . . . , z , z , z , . . . , z ) 1 i−1 (cid:101)i i+1 n where z is an independent copy of z . Moreover, denote a remove-one version of S by (cid:101)i i S\i d =ef (z , . . . , z , z , . . . , z ) . 1 i−1 i+1 n A Smoothness and Curvature of the Empirical Risk (Proof of Lemma 1) Lemma 1 (restated). Fix W, W(cid:102) ∈ Rd×m. Consider Assumption 1, Assumption 2, and assume that L S(W(cid:102) ) ≤ C 02. Then, for any S, (cid:18) (cid:19) B C λ (∇2L (W)) ≤ ρ where ρ d =ef C2 B2 + B B + √φ(cid:48)(cid:48) y , max S x φ(cid:48) φ(cid:48)(cid:48) φ m (cid:0) (cid:1) B B C + C αm ∈[i 0n ,1]λ min(∇2L S(W(cid:102) + α(W − W(cid:102) ))) ≥ − φ(cid:48)(cid:48) φ √(cid:48) mx 0 · (1 ∨ (cid:107)W − W(cid:102) (cid:107) F ) . (10) Proof. Vectorising allows the loss’s Hessian to be denoted ∇2(cid:96)(W, z) = ∇f (x)∇f (x)(cid:62) + ∇2f (x)(f (x) − y) (11) W W W W where  u xφ(cid:48) ((cid:104)(W) , x(cid:105))  1 1 u xφ(cid:48) ((cid:104)(W) , x(cid:105)) ∇f W(x) =    2 . . . 2    ∈ Rdm u xφ(cid:48) ((cid:104)(W) , x(cid:105)) m m and ∇2f (x) ∈ Rdm×dm with W  u xx(cid:62)φ(cid:48)(cid:48)((cid:104)(W) , x(cid:105)) 0 0 . . . 0  1 1  0 u 2xx(cid:62)φ(cid:48)(cid:48)((cid:104)(W) 2, x(cid:105)) 0 . . . 0  ∇2f W(x) =    . . . . . . . . . . . . . . .    0 0 0 . . . u xx(cid:62)φ(cid:48)(cid:48)((cid:104)(W) , x(cid:105)) m m Note that we then immediately have with v = (v , v , . . . , v ) ∈ Rdm with v ∈ Rd 1 2 m i m (cid:88) (cid:107)∇2f (x)(cid:107) = max u (cid:104)v , x(cid:105)2φ(cid:48)(cid:48)((cid:104)(W) , x(cid:105)) W 2 j j j v:(cid:107)v(cid:107)2≤1 j=1 m 1 (cid:88) ≤ √ (cid:107)x(cid:107)2B max (cid:107)v (cid:107)2 m 2 φ(cid:48)(cid:48) v:(cid:107)v(cid:107)2≤1 j=1 j 2 C2B ≤ √x φ(cid:48)(cid:48) . (12) m We then see that the maximum Eigenvalue of the Hessian is upper bounded for any W ∈ Rdm, that is (cid:107)∇2(cid:96)(W, z)(cid:107) ≤ (cid:107)∇f (x)(cid:107)2 + (cid:107)∇2f (x)(cid:107) |f (x) − y| (13) 2 W 2 W 2 W C2B √ ≤ C2B2 + √x φ(cid:48)(cid:48) ( mB + C ) (14) x φ(cid:48) m φ y 14
and therefore the objective is ρ-smooth with ρ = C x2(cid:0) B φ2 (cid:48) + B φ(cid:48)(cid:48)B φ + B √φ(cid:48)(cid:48) mCy (cid:1) . Let us now prove the lower bound (10). For some fixed W, W(cid:102) ∈ Rd×m define def W(α) = W(cid:102) + α(W − W(cid:102) ) α ∈ [0, 1] . Looking at the Hessian in (11), the first matrix is positive semi-definite, therefore n λ (∇2L (W(α))) ≥ −(cid:16) max (cid:8) (cid:107)∇2f (x )(cid:107) (cid:9)(cid:17) 1 (cid:88) |f (x ) − y | min S i=1,...,n W(α) i 2 n W(α) i i i=1 ≥ − C √x2B φ(cid:48)(cid:48) · 1 (cid:88)n |f (x ) − y | m n W(α) i i i=1 where we have used the upper bound on (cid:107)∇2f (x )(cid:107) . Adding and subtracting f (x ) inside the W i 2 W(cid:102) i absolute value we then get n n n 1 (cid:88) 1 (cid:88) 1 (cid:88) |f (x ) − y | ≤ |f (x ) − f (x )| + |f (x ) − y | n W(α) i i n W(α) i W(cid:102) i n W(cid:102) i i i=1 i=1 i=1 (cid:113) ≤ B φ(cid:48)C x(cid:107)W(α) − W(cid:102) (cid:107) 2 + L S(W(cid:102) ) (cid:112) ≤ B φ(cid:48)C x(cid:107)W(α) − W(cid:102) (cid:107) 2 + L S(W 0) (cid:0) (cid:1) ≤ B φ(cid:48)C x + C 0 (1 ∨ (cid:107)W(α) − W(cid:102) (cid:107) 2) where for the second term we have simply applied Cauchy-Schwarz inequality. For the first term, we used that for any W, W(cid:102) ∈ Rdm we see that m 1 (cid:88) |f W(x) − f W(cid:102) (x)| ≤ √ m |φ((cid:104)(W) i, x(cid:105)) − φ((cid:104)(W(cid:102) ) i, x(cid:105))| (15) i=1 ≤ √B mφ(cid:48) (cid:88)m (cid:12) (cid:12) (cid:12)(cid:104)(W) i − (W(cid:102) ) i, x(cid:105)(cid:12) (cid:12) (cid:12) i=1 ≤ C xB φ(cid:48)(cid:107)W − W(cid:102) (cid:107) 2. (16) Bringing everything together yields the desired lower bound λ min(∇2L S(W(α))) ≥ − √C mx2 B φ(cid:48)(cid:48)(cid:0) B φ(cid:48)C x + C 0(cid:1) (1 ∨ (cid:107)W(α) − W(cid:102) (cid:107) 2) ≥ − √C mx2 B φ(cid:48)(cid:48)(cid:0) B φ(cid:48)C x + C 0(cid:1) (1 ∨ (cid:107)W − W(cid:102) (cid:107) 2) . This holds for any α ∈ [0, 1], therefore, we took the minimum. B Optimisation Error Bound (Proof of Lemma 2) In this section we present the proof for the Optimisation Error term. We begin by quoting the result which we set to prove. Lemma 2 (restated). Consider Assumptions 1 and 2. Fix t > 0. If η ≤ 1/(2ρ), then 1 t (cid:88)t L S(W j) ≤ W∈m Ri dn ×m (cid:110) L S(W) + (cid:107)W − ηW t 0(cid:107)2 F + (cid:101)b(cid:107)W √− mW 0(cid:107)3 F (cid:111) + (cid:101)bC 0 · ( √ηt m) 3 2 j=0 where (cid:101)b = C x2B φ(cid:48)(cid:48) (B φ(cid:48)C x + C 0). Proof. Using Lemma 1 as well as that ηρ ≤ 1 from the assumption within the theorem yields for t ≥ 0 L (W ) ≤ L (W ) − η(cid:0) 1 − ηρ (cid:1) (cid:107)∇L (W )(cid:107)2 S t+1 S t 2 S t 2 η ≤ L (W ) − (cid:107)∇L (W )(cid:107)2. S t 2 S t 2 15
Fix some W ∈ Rdm. We then use the following inequality which will be proven shortly: (cid:101)b (cid:0) (cid:1)3 L (W ) ≤ L (W) − (cid:104)W − W , ∇L (W )(cid:105) + √ 1 ∨ (cid:107)W − W (cid:107) (17) S t S t S t m t 2 Plugging in this inequality we then get L (W ) ≤ L (W) − (cid:104)W − W , ∇L (W )(cid:105) − η (cid:107)∇L (W )(cid:107)2 + √(cid:101)b (cid:0) 1 ∨ (cid:107)W − W (cid:107) (cid:1)3 . S t+1 S t S t 2 S t 2 m t 2 Note that we can rewrite η − (cid:104)W − W , ∇L (W )(cid:105) − (cid:107)∇L (W )(cid:107)2 t S t 2 S t 2 1 1 = (cid:104)W − W , W − W (cid:105) − (cid:107)W − W (cid:107)2 η t t+1 t 2η t+1 t 2 = 1 (cid:0) (cid:107)W − W (cid:107)2 − (cid:107)W − W(cid:107)2(cid:1) η t 2 t+1 2 where we used that for any vectors x, y, z: 2(cid:104)x − y, x − z(cid:105) = (cid:107)x − y(cid:107)2 + (cid:107)x − z(cid:107)2 − (cid:107)y − z(cid:107)2 2 2 2 (which is easier to see if we relabel 2(cid:104)a, b(cid:105) = (cid:107)a(cid:107)2 + (cid:107)b(cid:107)2 − (cid:107)a − b(cid:107)2). Plugging in and summing 2 2 2 up we get 1 (cid:88)t L (W ) ≤ L (W) + (cid:107)W − W 0(cid:107)2 2 + √(cid:101)b · 1 (cid:88)t (cid:0) 1 ∨ (cid:107)W − W (cid:107) (cid:1)3 . t S t S ηt m t t 2 s=0 s=0 Since the choice of W was arbitrary, we can simply take the minimum. Proof of Eq. (17). Let us now prove the key Eq. (17). Fix t ≥ 0, and let us define the following functions for α ∈ [0, 1] def W(α) = W + α(W − W ) , t t g(α) d =ef L (W(α)) + √(cid:101)b · α2 (cid:0) 1 ∨ (cid:107)W − W (cid:107) (cid:1)3 . S m 2 t 2 Note that computing the derivative we have g(cid:48)(cid:48)(α) = (W − W )(cid:62)∇2L (W(α))(W − W ) + √(cid:101)b (cid:0) 1 ∨ (cid:107)W − W (cid:107) (cid:1)3 . t S t m t 2 On the other hand by Lemma 1 we have min λ (∇2L (W(α))) ≥ − √(cid:101)b (cid:0) 1 ∨ (cid:107)W − W (cid:107) (cid:1) α∈[0,1] min S m t 2 and we immediately have g(cid:48)(cid:48)(α) ≥ 0, and thus, g(·) is convex on [0, 1]. Inequality (17) then arises from g(1) − g(0) ≥ g(cid:48)(0), in particular (cid:101)b (cid:0) (cid:1)3 g(1) − g(0) = L (W) + √ 1 ∨ (cid:107)W − W (cid:107) − L (W ) S m t 2 S t ≥ (cid:104)W − W , ∇L (W )(cid:105) t S t = g(cid:48)(0) as required. C Generalisation Gap Bound (Proof of Theorem 1) In this section we prove: 16
Theorem 1 (restated). Consider Assumptions 1 and 2. Fix t > 0. If η ≤ 1/(2ρ) and (cid:16) √ √ (cid:17)2 m ≥ 144(ηt)2C4C2B2 4B C ηt + 2 x 0 φ(cid:48)(cid:48) φ(cid:48) x then E (cid:2) (cid:15)Gen(W t+1) (cid:12) (cid:12) W 0, u(cid:3) ≤ b (cid:18) nη + η n2 2t (cid:19) (cid:88)t E [L S(W j) | W 0, u] j=0 3 3 where b = 16e3C x2 B φ2 (cid:48)(1 + C x2 B φ2 (cid:48)). To prove this result we use algorithmic stability arguments. Recall that we can write [Shalev-Shwartz and Ben-David, 2014, Chapter 13], E [L(W ) − L (W ) | W , u] = 1 (cid:88)n E (cid:104) (cid:96)(W , z ) − (cid:96)(W(i) , z ) (cid:12) (cid:12) W , u(cid:105) . t+1 S t+1 0 n t+1 (cid:101)i t+1 (cid:101)i (cid:12) 0 i=1 The following lemma shown in Appendix C.1 then bounds the Generalisation error in terms of a notation of stability. Lemma 3 (restated). Consider Assumptions 1 and 2. Then, for any t ≥ 0, E [L(W ) − L (W ) | W , u] t S t 0 (cid:118) ≤ B φ(cid:48)(cid:112) C x(cid:112) E [L S(W t) | W 0, u](cid:117) (cid:117) (cid:116) n1 (cid:88)n E (cid:104) (cid:107)W t − W t(i)(cid:107)2 op (cid:12) (cid:12) (cid:12) W 0, u(cid:105) i=1 + C B2 · 1 (cid:88)n E (cid:104) (cid:107)W − W(i)(cid:107)2 (cid:12) (cid:12) W , u(cid:105) x φ(cid:48) n t t op (cid:12) 0 i=1 where (cid:107) · (cid:107) denotes the spectral norm. op We note while the stability is only required on the spectral norm, our bound will be on the element wise L -norm i.e. Frobenius norm, which upper bounds the spectral norm. It is summarised within 2 the following lemma shown in Appendix C.2. Lemma 4 (Bound on On-Average Parameter Stability). Consider Assumptions 1 and 2. Fix t > 0. If η ≤ 1/(2ρ), then n1 (cid:88)n E (cid:104) (cid:107)W t+1 − W t( +i) 1(cid:107)2 F (cid:12) (cid:12) (cid:12) W 0, u(cid:105) ≤ 8e η n2 2t (cid:16) 1 −1 2η(cid:15) (cid:17)t n1 (cid:88)n (cid:88)t E (cid:2) (cid:107)∇(cid:96)(W j, z i)(cid:107)2 2 (cid:12) (cid:12) W 0, u(cid:3) i=1 i=1 j=0 where (cid:15) = 2 · C x2√ √C m0B φ(cid:48)(cid:48) (cid:0) 4B φ(cid:48)C x√ ηt + √ 2(cid:1) . Theorem 1 then arises by combining Lemma 3 and Lemma 4, and noting the following three points. Firstly, recall that n n 1 (cid:88) (cid:107)∇(cid:96)(W, z )(cid:107)2 ≤ (cid:0) max (cid:107)∇f (x )(cid:107)2(cid:1) 1 (cid:88) (f (x ) − y )2 n i 2 i=1,...,n W i 2 n W i i i=1 i=1 ≤ 2C2B2 L (W). x φ(cid:48) S Secondly, note that we have (cid:0) 1 (cid:1)t ≤ exp( 2ηt(cid:15) ) ≤ e2 when 2ηt(cid:15) ≤ 2/3. For this to occur we 1−2η(cid:15) 1−2ηt(cid:15) then require √ (cid:15) = 2 · C x2 √C 0B φ(cid:48)(cid:48) · (cid:0) 4B C √ ηt + √ 2(cid:1) ≤ 1 , m φ(cid:48) x 3ηt which is satisfied by scaling m sufficient large, in particular, as required within condition (7) within the statement of Theorem 1. This allows us to arrive at the bound on the L -stability 2 1 (cid:88)n E (cid:104) (cid:107)W − W(i) (cid:107)2 (cid:12) (cid:12) W , u(cid:105) ≤ η2t · 16e3C2B2 (cid:88)t E [L (W ) | W , u] . n t+1 t+1 F (cid:12) 0 n2 x φ(cid:48) S j 0 i=1 j=0 17
Third and finally, note that we can bound (cid:118) (cid:112) (cid:117) (cid:117) η2t (cid:88)t E [L S(W t+1) | W 0, u](cid:116) n2 E [L S(W j) | W 0, u] j=0 (cid:118) (cid:117) t η (cid:112) (cid:117)(cid:88) = n tE [L S(W t+1) | W 0, u](cid:116) E [L S(W j) | W 0, u] j=0 t η (cid:88) ≤ E [L (W ) | W , u] n S j 0 j=0 since L (W ) ≤ 1 (cid:80)t L (W ). This then results in S t+1 t j=1 S j E [L(W ) − L (W ) | W , u] t+1 S t+1 0 ≤ (cid:16) η (cid:0) 4e2C3/2B2 (cid:1) + η2t (cid:0) 16e3C3B4 (cid:1)(cid:17) (cid:88)t E [L (W ) | W , u] n x φ(cid:48) n2 x φ(cid:48) S j 0 j=0 (cid:16) η η2t (cid:17) (cid:88)t ≤ 16e3C3/2B2 (1 + C3/2B2 ) + E [L (W ) | W , u] x φ(cid:48) x φ(cid:48) n n2 S j 0 j=0 as required. C.1 Proof of Lemma 3: From loss stability to parameter stability Recall that z = (x , y ) ∈ Bd(C ) × [−C , C ]. Expanding the square loss and some basic algebra (cid:101)i (cid:101)i i 2 x y y gives us: (cid:16) (cid:17) 2 (cid:96)(W , z ) − (cid:96)(W(i), z ) t (cid:101)i t (cid:101)i (cid:16) (cid:17)2 = (f (x ) − y )2 − f (x ) − y Wt (cid:101)i (cid:101)i W(i) (cid:101)i (cid:101)i t (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17) = (f (x ) − y ) f (x ) − f (x ) + f (x ) − y f (x ) − f (x ) Wt (cid:101)i (cid:101)i Wt (cid:101)i W(i) (cid:101)i W(i) (cid:101)i (cid:101)i Wt (cid:101)i W(i) (cid:101)i t t t (cid:16) (cid:17)2 (cid:16) (cid:17) (cid:16) (cid:17) = f (x ) − f (x ) + 2 f (x ) − y f (x ) − f (x ) . Wt (cid:101)i W(i) (cid:101)i W(i) (cid:101)i (cid:101)i Wt (cid:101)i W(i) (cid:101)i t t t We then have 1 (cid:88)n E (cid:104) (cid:96)(W , z ) − (cid:96)(W(i), z ) (cid:12) (cid:12) W , u(cid:105) n t (cid:101)i t (cid:101)i (cid:12) 0 i=1 1 (cid:88)n (cid:104)(cid:12)(cid:16) (cid:17)(cid:12) (cid:12)(cid:16) (cid:17)(cid:12) (cid:12) (cid:105) ≤ E (cid:12) f (x ) − y (cid:12) (cid:12) f (x ) − f (x ) (cid:12) (cid:12) W , u n (cid:12) W(i) (cid:101)i (cid:101)i (cid:12) (cid:12) Wt (cid:101)i W(i) (cid:101)i (cid:12) (cid:12) 0 t t i=1 1 (cid:88)n (cid:20)(cid:16) (cid:17)2 (cid:12) (cid:12) (cid:21) + 2n E f Wt(x (cid:101)i) − f W t(i)(x (cid:101)i) (cid:12) (cid:12) W 0, u i=1 (cid:118) (cid:118) (cid:117) (cid:117) 1 (cid:88)n (cid:20)(cid:16) (cid:17)2 (cid:12) (cid:12) (cid:21)(cid:117) (cid:117) 1 (cid:88)n (cid:20)(cid:16) (cid:17)2 (cid:12) (cid:12) (cid:21) ≤ (cid:116) n E f W t(i)(x (cid:101)i) − y (cid:101)i (cid:12) (cid:12) W 0, u (cid:116) n E f Wt(x (cid:101)i) − f W t(i)(x (cid:101)i) (cid:12) (cid:12) W 0, u i=1 i=1 1 (cid:88)n (cid:20)(cid:16) (cid:17)2 (cid:12) (cid:12) (cid:21) + 2n E f Wt(x (cid:101)i) − f W t(i)(x (cid:101)i) (cid:12) (cid:12) W 0, u i=1 where performing steps as in Eq. (15)-(16) we have (cid:16) (cid:17)2 f (x ) − f (x ) ≤ C2B2 (cid:107)W − W(i)(cid:107)2 . Wt (cid:101)i W(i) (cid:101)i x φ(cid:48) t t 2 t Plugging in this bound then yields the result. 18
C.2 Proof of Lemma 4: Bound on on-average parameter stability Throughout the proof empirical risk w.r.t. remove-one tuple S\i is denoted as 1 1 L (W) = L (W) − (cid:96)(W, z ) = L (W) − (cid:96)(W, z ) . S\i S n i Si n (cid:101)i Plugging in the gradient updates with the inequality (a + b)2 ≤ (1 + p)a2 + (1 + 1/p)b2 for p > 0 then yields (this technique having been applied within [Lei and Ying, 2020]) (cid:16) (cid:17) (cid:107)W − W(i) (cid:107)2 ≤ (1 + p) (cid:107)W − W(i) − η ∇L (W ) − ∇L (W(i)) (cid:107)2 t+1 t+1 2 t t S\i t S\i t 2 (cid:124) (cid:123)(cid:122) (cid:125) Expansiveness of the Gradient Update 2η2 (cid:16) (cid:17) + (1 + 1/p) · · (cid:107)∇(cid:96)(W , z )(cid:107)2 + (cid:107)∇(cid:96)(W(i)), z )(cid:107)2 . n2 t i 2 t (cid:101)i 2 We must now bound the expansiveness of the gradient update. Opening-up the squared norm we get (cid:107)W − W(i) − η(∇L (W ) − ∇L (W(i)))(cid:107)2 t t S\i t S\i t 2 = (cid:107)W − W(i)(cid:107)2 + η2(cid:107)∇L (W ) − ∇L (W(i))(cid:107)2 t t 2 S\i t S\i t 2 (cid:68) (cid:69) − 2η W − W(i), ∇L (W ) − ∇L (W(i)) t t S\i t S\i t For this purpose we will use the following key lemma shown in Appendix C.3.1: Lemma 5 (Almost Co-coercivity of the Gradient Operator). Consider the assumptions of Lemma 4. Then for t ≥ 1 (cid:16) ηρ (cid:17) (cid:104)W − W(i), ∇L (W ) − ∇L (W(i))(cid:105) ≥ 2η 1 − (cid:107)∇L (W ) − ∇L (W(i))(cid:107)2 t t S\i t S\i t 2 S\i t S\i t 2 (cid:13) (cid:16) (cid:17)(cid:13)2 − (cid:15) (cid:13)W − W(i) − η ∇L (W ) − ∇L (W(i)) (cid:13) (cid:13) t t S\i t S\i t (cid:13) 2 where (cid:18) (cid:19) B C ρ = C2 B2 + B B + √φ(cid:48)(cid:48) y , x φ(cid:48) φ(cid:48)(cid:48) φ m √ C2 C B (cid:16) √ √ (cid:17) (cid:15) = 2 · x √ 0 φ(cid:48)(cid:48) 4B C ηt + 2 . m φ(cid:48) x Thus by Lemma 5 we get (cid:16) (cid:17) (cid:107)W − W(i) − η ∇L (W ) − ∇L (W(i)) (cid:107)2 t t S\i t S\i t 2 (cid:13) (cid:13)2 ≤ (cid:107)W − W(i)(cid:107)2 + η2(2ηρ − 3) (cid:13)∇L (W ) − ∇L (W(i))(cid:13) t t 2 (cid:13) S\i t S\i t (cid:13) 2 (cid:13) (cid:16) (cid:17)(cid:13)2 + 2η(cid:15) (cid:13)W − W(i) − η ∇L (W ) − ∇L (W(i)) (cid:13) . (cid:13) t t S\i t S\i t (cid:13) 2 Rearranging and using that ηρ ≤ 1/2 we then arrive at the recursion 1 + p (cid:107)W − W(i) (cid:107)2 ≤ · (cid:107)W − W(i)(cid:107)2 t+1 t+1 F 1 − 2η(cid:15) t t F (cid:18) 1 (cid:19) 2η2 (cid:16) (cid:17) + 1 + · (cid:107)∇(cid:96)(W , z )(cid:107)2 + (cid:107)∇(cid:96)(W(i)), z )(cid:107)2 p n2 t i 2 t (cid:101)i 2 ≤ (cid:18) 1 + 1 (cid:19) · 2η2 (cid:18) 1 + p (cid:19)t (cid:88)t (cid:16) (cid:107)∇(cid:96)(W , z )(cid:107)2 + (cid:107)∇(cid:96)(W(i)), z )(cid:107)2(cid:17) . p n2 1 − 2η(cid:15) j i 2 j (cid:101)i 2 j=0 Taking expectation and summing we then get 1 (cid:88)n E (cid:104) (cid:107)W − W(i) (cid:107)2 (cid:12) (cid:12) W , u(cid:105) n t+1 t+1 F (cid:12) 0 i=1 ≤ 4(1 + 1/p) 2 nη 22 (cid:18) 1(1 −+ 2p η) (cid:15) (cid:19)t (cid:88)t E (cid:2) (cid:107)∇(cid:96)(W j, z i)(cid:107)2 2 (cid:12) (cid:12) W 0, u(cid:3) j=0 19
where we note that E (cid:2) (cid:107)∇(cid:96)(W j, z i)(cid:107)2 2 (cid:12) (cid:12) W 0, u(cid:3) = E (cid:104) (cid:107)∇(cid:96)(W j(i), z (cid:101)i)(cid:107)2 2 (cid:12) (cid:12) (cid:12) W 0, u(cid:105) since z i and z (cid:101)i are identically distributed. Picking p = 1/t and noting that (1 + p)t = (1 + 1/t)t ≤ e yields the bound. C.3 Proof of Lemma 5: Almost-co-coercivity of the Gradient Operator In this section we show Lemma 5 which says that a gradient operator of an overparameterised shallow network is almost-co-coercive. The proof of this lemma will require two auxiliary lemmas. Lemma 6. Consider Assumptions 1 and 2 and assume that η ≤ 1/(2ρ). Then for any t ≥ 0, i ∈ [n], (cid:112) (cid:107)W − W (cid:107) ≤ 2ηtL (W ) , t 0 F S 0 (cid:107)W(i) − W (cid:107) ≤ (cid:112) 2ηtL (W ) . t 0 F S(i) 0 Proof. The proof is given in Appendix C.3.2. We also need the following Lemma (whose proof is very similar to Lemma 1). Lemma 7. Consider Assumptions 1 and 2. Fix s ≥ 0, i ∈ [n]. For any α ∈ [0, 1] denote (cid:16) (cid:16) (cid:17)(cid:17) W(α) d =ef W(i) + α W − W(i) − η ∇L (W ) − ∇L (W(i)) , s s s S\i s S\i s (cid:16) (cid:16) (cid:17)(cid:17) W(cid:102) (α) d =ef W s + α W s(i) − W s − η ∇L S\i(W s(i)) − ∇L S\i(W s) . If η ≤ 1/(2ρ), then min λ (cid:0) ∇2L (W(α))(cid:1) ≥ −(cid:15) , min S\i (cid:101) α∈[0,1] (cid:16) (cid:16) (cid:17)(cid:17) min λ min ∇2L S\i W(cid:102) (α) ≥ − (cid:101)(cid:15) . α∈[0,1] with (cid:15) = C √x2B φ(cid:48)(cid:48) (cid:16) 4B C√ ηs (cid:16)(cid:112) L (W ) + (cid:112) L (W )(cid:17) + (cid:112) 2L (W ) + (cid:112) 2L (W )(cid:17) . (cid:101) m φ(cid:48) S 0 Si 0 Si 0 S 0 Proof. The proof is given in Appendix C.3.3. C.3.1 Proof of Lemma 5 The proof of this Lemma follows by arguing that the operator w (cid:55)→ ∇L (w) is almost-co-coercive: S\i Recall that the operator F : X → X is co-coercive whenever (cid:104)∇F (x) − ∇F (y), x − y(cid:105) ≥ α(cid:107)∇F (x) − ∇F (y)(cid:107)2 holds for any x, y ∈ X with parameter α > 0. In our case, right side of the inequality will be replaced by α(cid:107)∇F (x) − ∇F (y)(cid:107)2 − ε, where ε is a small. Let us begin by defining the following two functions ψ(W) = L (W) − (cid:104)∇L (W(i)), W(cid:105) , ψ(cid:63)(W) = L (W) − (cid:104)∇L (W ), W(cid:105) . S\i S\i t S\i S\i t Observe that ψ(W ) − ψ(W(i)) + ψ(cid:63)(W(i)) − ψ(cid:63)(W ) (18) t t t t = L (W ) − (cid:104)∇L (W(i)), W (cid:105) − L (W(i)) + (cid:104)∇L (W(i)), W(i)(cid:105) S\i t S\i t t S\i t S\i t t + L (W(i)) − (cid:104)∇L (W ), W(i)(cid:105) − L (W ) + (cid:104)∇L (W ), W (cid:105) S\i t S\i t t S\i t S\i t t = (cid:104)W − W(i), ∇L (W ) − ∇L (W(i))(cid:105) , t t S\i t S\i t from which follows that we are interesting in giving lower bounds on ψ(W ) − ψ(W(i)) and t t ψ(cid:63)(W(i)) − ψ(cid:63)(W ). t t 20
(cid:16) (cid:17) From Lemma 1 we know the loss is ρ-smooth with ρ = C x2 B φ2 (cid:48) + B φ(cid:48)(cid:48)B φ + Cy√B mφ(cid:48)(cid:48) , and thus, for any i ∈ [n], we immediately have the upper bounds (cid:16) ηρ (cid:17) ψ(W − ∇ψ(W )) ≤ ψ(W ) − η 1 − (cid:107)∇ψ(W )(cid:107)2 (19) t t t 2 t 2 (cid:16) ηρ (cid:17) ψ(cid:63)(W(i) − η∇ψ(cid:63)(W(i))) ≤ ψ(cid:63)(W(i)) − η 1 − (cid:107)∇ψ(cid:63)(W(i))(cid:107)2 (20) t t t 2 t 2 Now, in the smooth and convex case [Nesterov, 2003], convexity would be used here to lower bound the left side of each of the inequalities by ψ(W(i)) and ψ(cid:63)(W ) respectively. In our case, while the t t functions are not convex, we can get an “approximate” lower bound by leveraging that the minimum Eigenvalue evaluated at the points W , W(i) is not too small. More precisely, we have the following t t lower bounds by applying Lemma 7, which will be shown shortly: (cid:15) ψ(W − η∇ψ(W )) ≥ ψ(W(i)) − (cid:107)W − W(i) − η∇ψ(W )(cid:107)2 , (21) t t t 2 t t t 2 (cid:15) ψ(cid:63)(W(i) − η∇ψ(cid:63)(W(i))) ≥ ψ(cid:63)(W ) − (cid:107)W(i) − W − η∇ψ(cid:63)(W(i))(cid:107)2. (22) t t t 2 t t t 2 Combining this with Eq. (19), (20), and rearranging we get: (cid:16) ηρ (cid:17) (cid:15) ψ(W ) − ψ(W(i)) ≥ η 1 − (cid:107)∇ψ(W )(cid:107)2 − (cid:107)W − W(i) − η∇ψ(W )(cid:107)2 , (23) t t 2 t 2 2 t t t 2 (cid:16) ηρ (cid:17) (cid:15) ψ(cid:63)(W(i)) − ψ(cid:63)(W ) ≥ η 1 − (cid:107)∇ψ(cid:63)(W(i))(cid:107)2 − (cid:107)W(i) − W − η∇ψ(cid:63)(W(i))(cid:107)2. (24) t t 2 t 2 2 t t t 2 Adding together the two bounds and plugging into Eq. (18) completes the proof. Proof of Eq. (21) and Eq. (22). All that is left to do, is to prove Eq. (21) and (22). To do that, we will use Lemma 7 while recalling the definition of W(α) and W(cid:102) (α) given in the Lemma. That said, let us then define the following two functions: g(α) d =ef ψ(W(α)) + (cid:101)(cid:15)α2 (cid:107)W − W(i) − η(cid:0) ∇L (W ) − ∇L (W(i))(cid:1) (cid:107)2 , 2 t t S\i t S\i t 2 g (cid:101)(α) d =ef ψ(cid:63)(W(cid:102) (α)) + (cid:101)(cid:15)α 22 (cid:107)W t − W t(i) − η(cid:0) ∇L S\i(W t) − ∇L S\i(W t(i))(cid:1) (cid:107)2 2 . Note that from Lemma 7 we have that g(cid:48)(cid:48)(α), g(cid:48)(cid:48)(α) ≥ 0 for α ∈ [0, 1]. Indeed, we have with (cid:101) ∆ d =ef W − W(i) − η(cid:0) ∇L (W ) − ∇L (W(i))(cid:1) : t t S\i t S\i t g(cid:48)(cid:48)(α) = (cid:10) ∆, ∇2L (W(α))∆(cid:11) + (cid:15)(cid:107)∆(cid:107)2 ≥ 0 S\i (cid:101) 2 and similarly for g(α). Therefore both g(·) and g(·) are convex on [0, 1]. The first inequality then (cid:101) (cid:101) arises from noting the follow three points. Since g is convex we have g(1) − g(0) ≥ g(cid:48)(0) with g(cid:48)(0) = (cid:104)∇ψ(W(i)), ∆(cid:105) = 0 since ∇ψ(W(i)) = 0. This yields t t 0 ≤ g(1) − g(0) = ψ(W − η∇ψ(W )) + (cid:101)(cid:15) (cid:107)W − W(i) − η(cid:0) ∇L (W ) − ∇L (W(i))(cid:1) (cid:107)2 − ψ(W(i)) t t 2 t t S\i t S\i t 2 t which is almost Eq. (21): The missing step is showing that (cid:15) ≤ (cid:15). This comes by the uniform (cid:101) boundedness of the loss, that is, having (cid:96)(W , z) ≤ C a.s. we can upper-bound 0 0 √ (cid:15) ≤ 2 · C x2 √C 0B φ(cid:48)(cid:48) (cid:0) 4B C √ ηs + √ 2(cid:1) = (cid:15) (cid:101) m φ(cid:48) x This proves Eq. (21), while Eq. (22) comes by following similar steps and considering g(1) − g(0) ≥ (cid:101) (cid:101) g(cid:48)(0). (cid:101) C.3.2 Proof of Lemma 6 Recalling the Hessian (11) we have for any parameter W and data point z = (x, y), (cid:107)∇2(cid:96)(W, z)(cid:107) ≤ (cid:107)∇f (x)(cid:107)2 + (cid:107)∇2f (x)(cid:107) |f (x) − y| 2 W 2 W 2 W (cid:18) (cid:19) B C ≤ C2 B2 + B B + √φ(cid:48)(cid:48) y x φ(cid:48) φ(cid:48)(cid:48) φ m 21
That is we have from (12) the bound (cid:107)∇2f W(x)(cid:107) 2 ≤ √C mx2 B φ(cid:48)(cid:48), meanwhile we can trivially bound m 1 (cid:88) |f (x) − y| ≤ √ |φ((cid:104)(W) , x(cid:105))| + C W m j y j=1 √ ≤ mB + C . φ y and m 1 (cid:88) (cid:107)∇f (x)(cid:107)2 = (cid:107)x(cid:107)2 · φ(cid:48)((cid:104)(W) , x(cid:105)) W 2 2 m j j=1 ≤ C2B2 . x φ(cid:48) (cid:16) (cid:17) The loss is therefore ρ-smooth with ρ = C x2 B φ2 (cid:48) + B φB φ(cid:48)(cid:48) + Cy√B mφ(cid:48)(cid:48) . Following standard argu- ments we then have for j ∈ N 0 (cid:16) ηρ (cid:17) L (W ) ≤ L (W ) − η 1 − (cid:107)∇L (W )(cid:107)2 S j+1 S j 2 S j F which when rearranged and summed over j yields t t η(cid:0) 1 − ηρ (cid:1) (cid:88) (cid:107)∇L (W )(cid:107)2 ≤ (cid:88) L (W ) − L (W ) = L (W ) − L (W ) 2 S j F S j S j+1 S 0 S t+1 j=0 j=0 We also note that t (cid:88) W − W = −η ∇L (W ) t+1 0 S s s=0 and therefore by convexity of the squared norm we have (cid:107)W − W (cid:107)2 = t+1 0 F η2(cid:107) (cid:80)t ∇L (W )(cid:107)2 ≤ η2t (cid:80)t (cid:107)∇L (W )(cid:107)2 . Plugging this in we get when ηρ ≤ 1/2 s=0 S s F s=0 S s F 3 1 · (cid:107)W − W (cid:107)2 ≤ L (W ) 4 ηt t+1 0 F S 0 Rearranging then yields the inequality. An identical set of steps can be performed for the cases W(i) t for i ∈ [n]. C.3.3 Proof of Lemma 7 Looking at (11) we note the first matrix is positive semi-definite and therefore for any W ∈ Rdm:   λ min(∇2L S\i(W)) ≥ −λ max  n1 (cid:88) ∇2f W(x i)(cid:0) f W(x j) − y j(cid:1)  j∈[n]:j(cid:54)=i ≥ − C √x2B φ(cid:48)(cid:48) · 1 (cid:88) |f (x ) − y | m n W j j j∈[n]:j(cid:54)=i where we have used the operator norm of the Hessian (cid:107)∇2f (x)(cid:107) bound (12). We now choose W 2 W = W(α) and thus need to bound 1 (cid:80) |f (x ) − y | and 1 (cid:80) |f (x ) − n j∈[n]:j(cid:54)=i W(α) j i n j∈[n]:j(cid:54)=i W(cid:102)(α) i y |. Note that we then have for any iterate W with t ∈ N , i t 0 1 (cid:88) 1 (cid:88) 1 (cid:88) |f (x ) − y | ≤ |f (x ) − f (x )| + |f (x ) − y | n W(α) i i n W(α) i W(i) i n W(i) i i t t j∈[n]:j(cid:54)=i j∈[n]:j(cid:54)=i j∈[n]:j(cid:54)=i (cid:113) ≤ B C (cid:107)W(α) − W(i)(cid:107) + 2L (W(i)) φ(cid:48) x t F S\i t where the first term on the r.h.s. is bounded using Cauchy-Schwarz inequality as in Eq. (15)-(16), and the second term is bounded by Jensen’s inequality. A similar calculation yields n1 (cid:88)n |f W(cid:102)(α)(x i) − y i| ≤ B φ(cid:48)C x(cid:107)W(cid:102) (α) − W t(cid:107) F + (cid:113) 2L S\i(W t(i)) . j=1,j(cid:54)=i 22
Since the loss is ρ-smooth by Lemma 1 we then have (cid:107)W(α) − W(i)(cid:107) ≤ α(cid:0) (cid:107)W − W(i)(cid:107) + η(cid:107)∇L (W ) − ∇L (W(i))(cid:107) (cid:1) t F t t F S\i t S\i t F ≤ (1 + ηρ)(cid:107)W − W(i)(cid:107) t t F ≤ 3 (cid:0) (cid:107)W − W (cid:107) + (cid:107)W − W(i)(cid:107) (cid:1) 2 t 0 F 0 t F 3 (cid:112) (cid:0)(cid:112) (cid:112) (cid:1) ≤ 2ηs L (W ) + L (W ) 2 S 0 S(i) 0 where at the end we used Lemma 6. A similar calculation yields the same bound for (cid:107)W(cid:102) (α) − W t(cid:107) F . Bringing together we get λ (∇2L (W(α))) ≥ − C √x2B φ(cid:48)(cid:48) (cid:18) 4B C √ ηs(cid:0)(cid:112) L (W ) + (cid:112) L (W )(cid:1) + (cid:113) 2L (W(i))(cid:19) min S\i m φ(cid:48) x S 0 S(i) 0 S\i t λ min(∇2L S\i(W(cid:102) (α))) ≥ − C √x2B mφ(cid:48)(cid:48) (cid:16) 4B φ(cid:48)C x√ ηs(cid:0)(cid:112) L S(W 0) + (cid:112) L S(i)(W 0)(cid:1) + (cid:112) 2L S\i(W t)(cid:17) The final bound arises from noting that L (W ) ≤ L (W ) ≤ L (W ) and L (W(i)) ≤ S\i t S t S 0 S\i t L (W(i)) ≤ L (W ). S(i) t S(i) 0 D Connection between ∆oracle and NTK S This section is dedicated to the proof of Theorem 2. We will first need the following standard facts about the NTK. Lemma 8 (NTK Lemma). For any W, W(cid:102) ∈ Rd×m and any x ∈ Rd, m (cid:88) (cid:16)(cid:68) (cid:69)(cid:17) (cid:68) (cid:69) f W(x) = f W(cid:102) (x) + u kφ(cid:48) x, W(cid:102)k W k − W(cid:102)k, x + (cid:15)(x) k=1 where 1 (cid:88)m (cid:18)(cid:90) 1 (cid:16) (cid:68) (cid:69)(cid:17) (cid:19) (cid:68) (cid:69)2 (cid:15)(x) = 2 u k φ(cid:48)(cid:48) τ (cid:104)x, W k(cid:105) + (1 − τ ) x, W(cid:102)k dτ x, W k − W(cid:102)k . k=1 0 Note that B (cid:107)x(cid:107) |(cid:15)(x)| ≤ φ√(cid:48)(cid:48) · (cid:107)W − W(cid:102) (cid:107)2 . 2 m F Proof. By Taylor theorem, (cid:88) (cid:16)(cid:68) (cid:69)(cid:17) (cid:68) (cid:69) f W(x) = f W(cid:102) (x) + u kφ(cid:48) x, W(cid:102)k x, W k − W(cid:102)k k 1 (cid:88) (cid:18)(cid:90) 1 (cid:16) (cid:68) (cid:69)(cid:17) (cid:19) (cid:68) (cid:69)2 + 2 u k φ(cid:48)(cid:48) τ (cid:104)x, W k(cid:105) + (1 − τ ) x, W(cid:102)k dτ x, W k − W(cid:102)k . k 0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:15)(x) Cauchy-Schwarz inequality gives us B (cid:107)x(cid:107) |(cid:15)(x)| ≤ φ√(cid:48)(cid:48) · (cid:107)W − W(cid:102) (cid:107)2 . 2 m F We will use the following proposition [Du et al., 2018, Arora et al., 2019]: 23
Proposition 1 (Concentration of NTK gram matrix). With probability at least 1 − δ over W , 0 (cid:115) ln (cid:0) 2n (cid:1) (cid:107)Kˆ − K(cid:107) ≤ B δ . 2 φ(cid:48) 2m Proof. Since each entry is independent, by Hoeffding’s inequality we have for any t ≥ 0, (cid:16) (cid:17) P n|(Kˆ ) i,j − (K) i,j| ≥ t ≤ 2e−2nt2/B φ2 (cid:48) , and applying the union bound B2 ln (cid:0) 2n (cid:1) (cid:107)Kˆ − K(cid:107)2 ≤ φ(cid:48) δ . F 2m Now we are ready to prove the main Theorem of this section (in the main text we only report the second result). Theorem 2 (restated). Denote  u X diag (cid:0) φ(cid:48)(X(cid:62)W )(cid:1)  1 0,1 Φ d =ef  . .  0  .  u X diag (cid:0) φ(cid:48)(X(cid:62)W )(cid:1) m 0,m and Kˆ d =ef 1 Φ(cid:62)Φ . Assume that m (cid:38) (ηT )5. Then, n 0 0 (cid:18) 1 (cid:68) (cid:69)(cid:19) ∆oracle = O y, (nKˆ )−1y as ηT → ∞ . S ηT Consider Assumption 1 and that ηT = n. Moreover, assume that entries of W are i.i.d., K = √ 0 E[Kˆ | S, u] with λ (K) (cid:38) 1/n, and assume that u ∼ unif ({±1/ m})m independently from all min sources of randomness. Then, with probability least 1 − δ over (W , u), 0 (cid:18) (cid:19) ∆oracle = O˜ 1 (cid:10) y, (nK)−1y(cid:11) as n → ∞ . S P n Proof. The proof of the first inequality will follow by relaxation of the oracle R-ERM ∆oracle to the S Moore-Penrose pseudo-inverse solution to a linearised problem given by Lemma 8. The proof of the second inequality will build on the same idea, in addition making use of the concentration of entries of Kˆ around K. Define m f lin(x) d =ef (cid:88) u φ(cid:48) ((cid:104)x, W (cid:105)) (cid:104)W − W , x(cid:105) , W k 0,k k 0,k k=1 n Llin(W) d =ef 1 (cid:88) (cid:0) y − f lin(x)(cid:1)2 . S 2 i W i=1 Then for the square loss we have (f (x ) − y )2 = (cid:0) f (x ) + f lin(x ) + (cid:15)(x ) − y (cid:1)2 W i i W0 i W i i i ≤ 2 (cid:0) f lin(x ) − (y − f (x ))(cid:1)2 + 2(cid:15)(x )2 W i i W0 i i and so, B2 L(W) ≤ Llin(W) + φ(cid:48)(cid:48) · (cid:107)W − W (cid:107)4 m 0 F 24
where we observe that 1 Llin(W) = (cid:107)Φ(cid:62)(W − W ) − (y − yˆ )(cid:107)2 n 0 0 0 and with Φ , the matrix of NTK features, defined in the statement. Solving the above undetermined 0 least-squares problem using the Moore-Penrose pseudo-inverse we get Wpinv − W = (cid:0) Φ Φ(cid:62)(cid:1)† Φ (y − yˆ ) , 0 0 0 0 0 and so (cid:107)Wpinv − W (cid:107)2 = (y − yˆ )(cid:62)Φ(cid:62) (cid:0) Φ Φ(cid:62)(cid:1)†2 Φ (y − yˆ ) 0 F 0 0 0 0 0 0 = (y − yˆ )(cid:62) (cid:0) Φ(cid:62)Φ (cid:1)−1 (y − yˆ ) 0 0 0 0 = (y − yˆ )(cid:62)(nKˆ )−1(y − yˆ ) 0 0 where the final step can be observed by Singular Value Decomposition (SVD) of Φ . Since 0 L (Wpinv) = 0, S (cid:18) 1 (cid:68) (cid:69)(cid:19) ∆oracle = O (y − yˆ ), (nKˆ )−1(y − yˆ ) as ηT → ∞ . S ηT 0 0 This proves the first result. Now we prove the second result involving K. We will first handle the empirical risk by concentration between Kˆ and K. For α ∈ Rn define W = Φ α + W . Then, α 0 0 1 Llin(W ) = (cid:107)Φ(cid:62)Φ α − (y − yˆ )(cid:107)2 α n 0 0 0 1 = (cid:107)n(Kˆ − K)α + nKα − (y − yˆ )(cid:107)2 n 0 2 2 ≤ (cid:107)n(Kˆ − K)α(cid:107)2 + (cid:107)nKα − (y − yˆ )(cid:107)2 n n 0 2 ≤ 2n(cid:107)Kˆ − K(cid:107)2(cid:107)α(cid:107)2 + (cid:107)nKα − (y − yˆ )(cid:107)2 2 2 n 0 Plug into the above αˆ = (nK)−1(y − yˆ ) (note that K is full-rank by assumption) 0 Llin(W ) ≤ 2n(cid:107)Kˆ − K(cid:107)2(cid:107)αˆ (cid:107)2 αˆ 2 2 B2 ln (cid:0) 2n (cid:1) ≤ n · φ(cid:48) δ · (cid:0) (y − yˆ )(cid:62)(nK)−2(y − yˆ )(cid:1) m 0 0 B2 ln (cid:0) 2n (cid:1) 1 ≤ (cid:107)y − yˆ (cid:107)2 · φ(cid:48) δ · 0 m nλ (K)2 min B2 ln (cid:0) 2n (cid:1) 1 = 2L (W ) · φ(cid:48) δ · S 0 m λ (K)2 min where the last inequality hold w.p. at least 1 − δ by Proposition 1. Now we pay attention to the quadratic term within ∆oracle: S (cid:107)W − W (cid:107)2 = (cid:107)Φ αˆ (cid:107)2 αˆ 0 2 0 2 = (cid:107)Φ (nK)−1(y − yˆ )(cid:107)2 0 0 2 = (y − yˆ )(cid:62)(nK)−1(nKˆ )(nK)−1(y − yˆ ) 0 0 = (y − yˆ )(cid:62)(nK)−1(nKˆ − nK)(nK)−1(y − yˆ ) + (y − yˆ )(cid:62)(nK)−1(y − yˆ ) . 0 0 0 0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (i) (ii) 25
We will show that (i) is “small”: (y − yˆ )(cid:62)(nK)−1(nKˆ − nK)(nK)−1(y − yˆ ) 0 0 ≤ (cid:107)y − yˆ (cid:107)2(cid:107)(nK)−2(cid:107)n(cid:107)Kˆ − K(cid:107) 0 2 (cid:115) ln (cid:0) 2n (cid:1) ≤ (cid:107)y − yˆ (cid:107)2(cid:107)(nK)−2(cid:107) · nB δ 0 φ(cid:48) 2m (cid:115) 1 ln (cid:0) 2n (cid:1) ≤ 2L (W ) · · B δ S 0 λ (K)2 φ(cid:48) 2m min where we used Proposition 1 once again. Putting all together w.p. at least 1 − δ over W we have 0 (cid:32) ∆oracle = O 1 (cid:10) (y − yˆ ), (nK)−1(y − yˆ )(cid:11) S P ηT 0 0 2L (W ) B2 ln (cid:0) 2n (cid:1) 1 2L (W ) (cid:115) ln (cid:0) 2n (cid:1) (cid:33) + S 0 · φ(cid:48) δ + · S 0 · B δ as ηT → ∞ . λ (K)2 m ηT λ (K)2 φ(cid:48) 2m min min Moreover, assuming that λ (K) (cid:38) 1/n and ηT = n, the above turns into min (cid:18) (cid:19) ∆oracle = O˜ 1 (cid:10) (y − yˆ ), (nK)−1(y − yˆ )(cid:11) as n → ∞ . S P n 0 0 The final bit is to note that (cid:10) yˆ , (nK)−1yˆ (cid:11) ≤ (cid:107)yˆ 0(cid:107)2 2 (cid:46) (cid:107)yˆ (cid:107)2 0 0 nλ (K) 0 2 min √ m can be bounded w.h.p. by randomising u ∼ unif ({±1/ m}) : For any i ∈ [n] and δ ∈ (0, 1) by Hoeffding’s inequality we have:  (cid:115)   (cid:118)  P f W0(x i) ≥ B φ ln (cid:0) 21 δ (cid:1)  ≥ P f W0(x i) ≥ (cid:117) (cid:117) (cid:116) ln (cid:0) 21 δ (cid:1) m1 (cid:88)m φ ((cid:104)(W 0) k, x(cid:105))2  k=1 ≥ 1 − δ . Taking a union bound over i ∈ [n] completes the proof of the second result. 26
E Additional Proofs Corollary 2 (restated). Assume the same as in Theorem 1 and Lemma 2. Then, (cid:18) (cid:18) (cid:19)(cid:19) E [L(W T ) | W 0, u] ≤ 1 + C · η nT 1 + η nT E (cid:2) ∆o Sracle (cid:12) (cid:12) W 0, u(cid:3) . Proof. Considering Theorem 1 with t = T − 1, and noting that L (W ) ≤ 1 (cid:80)T L (W ) then S T T j=0 S j yields (cid:18) (cid:18) ηT η2T 2 (cid:19)(cid:19) 1 (cid:88)T E[L(W ) | W , u] ≤ 1 + b + E[L (W ) | W , u] T 0 n n2 T S j 0 j=0 (cid:18) (cid:18) ηT η2T 2 (cid:19)(cid:19) ≤ 1+b + n n2  (cid:12)  · E  W∈m Ri dn ×m(cid:110) L S(W)+ (cid:107)W − ηW t 0(cid:107)2 F + √(cid:101)b m · T1 (cid:88)T (1 ∨ (cid:107)W−W j(cid:107) F )3 (cid:111) (cid:12) (cid:12) (cid:12) (cid:12) W 0, u j=0 (cid:12) where at the end we applied Lemma 2 to bound T1 (cid:80)T j=0 E[L S(W j)|W 0]. The constants b, (cid:101)b are then defined in Theorem 1 and Lemma 2. Note from smoothness of the loss we have (cid:107)W − W (cid:107)3 ≤ 23/2(cid:0) (cid:107)W − W (cid:107)3 + (cid:107)W − W (cid:107)3 (cid:1) ≤ 23/2(cid:0) (cid:107)W − W (cid:107)3 + (ηjC )3/2(cid:1) , j F 0 F 0 j F 0 F 0 in particular from the properties of graident descent (cid:107)W − W (cid:107)2 ≤ ηjL (W ) for j ∈ [T ]. 0 j F S 0 Plugging in then yields the final bound. 27
