2202 beF 5 ]GL.sc[ 3v33550.8012:viXra Efficient Local Planning with Linear Function Approximation Dong Yin1, Botao Hao1, Yasin Abbasi-Yadkori1, Nevena Lazic´1, and Csaba Szepesva´ri1,2 1DeepMind * 2University of Alberta February 8, 2022 Abstract We study query and computationally efficient planning algorithms for discounted Markov de- cision processes (MDPs) with linear function approximation and a simulator. We assume that the agent has local access to the simulator, meaning that the simulator can be queried only for states that have been encountered in previous simulation steps. This is a more practical set- ting than the so-called random-access (or, generative) setting, where the agent has a complete description of the state space and features and is allowed to query the simulator at any state of its choice. We propose two new algorithms for this setting, which we call confident Monte Carlo least-squares policy iteration (CONFIDENT MC-LSPI), and confident Monte Carlo Po- litex (CONFIDENT MC-POLITEX), respectively. The main novelty in our algorithms is that it gradually builds a set of state-action pairs (“core set”) with which it can control the extrapolation errors. Under the assumption that the action-value functions of all policies are linearly realizable with given features, we show that our algorithm has polynomial query and computational cost in the dimension of the features, the effective planning horizon and the targeted sub-optimality, while the cost remains independent of the size of the state space. Our result strengthens previ- ous works by broadening their scope, either by weakening the assumption made on the power of the function approximator, or by weakening the requirement on the simulator and removing the need for being given an appropriate core set of states. An interesting technical contribution of our work is the introduction of a novel proof technique that makes use of a virtual policy it- eration algorithm. We use this method to leverage existing results on ℓ∞-bounded approximate policy iteration to show that our algorithm can learn the optimal policy for the given initial state even only with local access to the simulator. We believe that this technique can be extended to broader settings beyond this work. 1 Introduction Efficient planning lies at the heart of modern supervised learning (RL). In the simulation-based RL, the agent has access to a simulator which it uses to query a state-action pair to obtain the reward of the *Emails: {dongyin, bhao, yadkori, nevena, szepi}@google.com 1
queried pair and the next state. When planning with large state spaces in the presence of features, the agent can also compute the feature vector associated with a state or a state-action pair. Planning efficiency is measured in two ways: using query cost, the number of calls to the simulator, and using computation cost, the total number of logical and arithmetic operations that the agent uses. In Markov decision processes (MDPs) with a large state space, we call a planning algorithm query-efficient (computationally-efficient) if its query (respectively, computational) cost is independent of the size of the state space and polynomial in other parameters of the problem such as the dimension of the feature space, the effective planning horizon, the number of actions and the targeted sub-optimality. Prior works on planning in MDPs often assume that the agent has access to a generative model which allows the agent to query the simulator with any arbitrary state-action pair [Kakade, 2003, Sidford et al., 2018, Yang and Wang, 2019, Lattimore et al., 2020]. In what follows, we will call this the random access model. The random access model is often difficult to support. To illustrate this, consider a problem where the goal is to move the joints of a robot arm so that it moves objects around. The simulation state in this scenario is then completely described by the position, orientation and associated velocities of the various rigid objects involved. To access a state, a planner can then try to choose some values for each of the variables involved. Unfortunately, given only box constraints on the variable values (as is typically the case), a generic planner will often choose value combinations that are invalid based on physics, for example with objects penetrating each other in space. This problem is not specific to robotic applications but also arises in MDPs corresponding to combinatorial search, just to mention a second example. To address this challenge, we replace the random access model with a local access model, where the only states at which the agent can query the simulator are the initial states provided to the agent, or states returned in response to previously issued queries. This access model can be implemented with any simulator that supports resetting its internal state to a previously stored such state. This type of checkpointing is widely supported, and if a simulator does not support it, there are general techniques that can be applied to achieve this functionality. As such, this access model significantly expands the scope of planners. Definition 1.1 (Local access to the simulator). We say the agent has local access to the simulator if the agent is allowed to query the simulator with a state that the agent has previously seen paired with an arbitrary action. Our work relies on linear function approximation. Very recently, Weisz et al. [2021b] showed that linear realizability assumption of the optimal state-action value function (Q -realizability) alone is not ∗ sufficient to develop a query-efficient planner. In this paper, we assume linear realizability of all poli- cies (Q -realizability). We discuss several drawbacks of previous works [Lattimore et al., 2020, Du et al., π 2020] under the same realizability assumption. First, these works require the knowledge of the features of all state-action pairs; otherwise, the agent has to spend ( ) query cost to extract the features O |S||A| of all possible state-action pairs, where and are the sizes of the state space and action space, re- |S| |A| spectively. Second, these algorithms require the computation of either an approximation of the global optimal design [Lattimore et al., 2020] or a barycentric spanner [Du et al., 2020] of the matrix of all fea- tures. Although there exists algorithms to approximate the optimal design [Todd, 2016] or barycentric span- ner [Awerbuch and Kleinberg, 2008], the computational complexities for these algorithms are polynomial in the total number of all possible feature vectors, i.e., , which is impractical for large MDPs. |S||A| We summarize our contributions as follows: 2
• With local access to the simulator, we propose two policy optimization algorithms—confident Monte Carlo least-squares policy iteration (CONFIDENT MC-LSPI), and its regularized (see e.g. Even-Dar et al. [2009], Abbasi-Yadkori et al. [2019]) version confident Monte Carlo Politex (CONFIDENT MC-POLITEX). Both of our algorithms maintain a core set of state-action pairs and run Monte Carlo rollouts from these pairs using the simulator. The algorithms then use the rollout results to estimate the Q-function values and then apply policy improvement. During each rollout procedure, whenever the algorithm observes a state-action pair that it is less confident about (with large uncertainty), the algorithm adds this pair to the core set and restarts. Compared to several prior works that use additive bonus [Jin et al., 2020, Cai et al., 2020], our algorithm design demonstrates that in the local access setting, core-set- based exploration is an effective approach. • Under the Q π-realizability assumption, we prove that both CONFIDENT MC-LSPI and CONFIDENT MC-POLITEX can learn a κ-optimal policy with query cost of poly(d, 1 , 1 , log( 1 ), log(b)) and 1 γ κ δ computational costs of poly(d, 1 , 1 , , log( 1 ), log(b)), where d is the−dimension of the feature 1 γ κ |A| δ of state-action pairs, γ is the dis−count factor, δ is the error probability, and b is the bound on the ℓ norm of the linear coefficients for the Q-functions. In the presence of a model misspecification 2 error ǫ, we show that CONFIDENT MC-LSPI achieves a final sub-optimality of ( ǫ√d ), whereas O (1 γ)2 − CONFIDENT MC-POLITEX can improve the sub-optimality to ( ǫ√d ) with a higher query cost. O 1 γ e − • We develop a novel proof technique that makes use of a virtual policy iteration algorithm. We use this e method to leverage existing results on approximate policy iteration which assumes that in each iter- ation, the approximation of the Q-function has a bounded ℓ error [Munos, 2003, Farahmand et al., ∞ 2010] (see Section 5 for details). 2 Related work Simulators or generative models have been considered in early studies of supervised learning [Kearns and Singh, 1999, Kakade, 2003]. Recently, it has been shown empirically that in the local access setting, core-set-based exploration has strong performance in hard-exploration problems [Ecoffet et al., 2019]. In this section, we mostly focus on related theoretical works. We distinguish among random access, local access, and online access. • Random access means that the agent is given a list of all possible state action pairs and can query any of them to get the reward and a sample of the next state. • Local access means that the agent can access previously encountered states, which can be imple- mented with checkpointing. The local access model that we consider in this paper is a more practical version of planning with a simulator. • Online access means that the simulation state can only be reset to the initial state (or distribution) or moved to a next random state given an action. The online access setting is more restrictive compared to local access, since the agent can only follow the MDP dynamics during the learning process. 3
We also distinguish between offline and online planning. In the offline planning problem, the agent only has access to the simulator during the training phase, and once the training is finished, the agent outputs a policy and executes the policy in the environment without access to a simulator. This is the setting that we consider in this paper. On the other hand, in the online planning problem, the agent can use the simulator during both the training and inference phases, meaning that the agent can use the simulator to choose the action when executing the policy. Usually, online RL algorithms with sublinear regret can be converted to an offline plan- ning algorithm under the online access model with standard online-to-batch conversion [Cesa-Bianchi et al., 2004]. While most of the prior works that we discuss in this section are for the offline planning problem, the TENSORPLAN algorithm [Weisz et al., 2021a] considers online planning. In terms of notation, some works considers finite-horizon MDPs, in which case we use H to denote the episode length (similar to the effective planning horizon (1 γ) 1 in infinite-horizon discounted MDPs). − − Our discussion mainly focuses on the results with linear function approximation. We summarize some of the recent advances on efficient planning in large MDPs in Table 1. Table 1: Recent advances on RL algorithms with linear function approximation under different assumptions. Positive results mean query cost depends only polynomially on the relative parameter while negative results refer an exponential lower bound on the query complexity. CE stands for computational efficiency and “no” for CE means no computational efficient algorithm is provided. : The algorithms in these works are not query or computationally efficient unless the agent is provided with † an approximate optimal design [Lattimore et al., 2020] or barycentric spanner [Du et al., 2020] or “core states” [Shariff and Szepesva´ri, 2020] for free. : Weisz et al. [2021a] consider the online planning problem whereas other works in this table consider (or ‡ can be converted to) the offline planning problem. Positive Results Assumption CE Access Model Yang and Wang [2019] linear MDP yes random access Lattimore et al. [2020], Du et al. [2020] Q -realizability no random access π † Shariff and Szepesva´ri [2020] V -realizability no random access ∗ † This work Q -realizability yes local access π Weisz et al. [2021a] V -realizability, (1) actions no local access ∗ ‡ O Li et al. [2021] Q -realizability, constant gap yes local access ∗ Jiang et al. [2017] low Bellman rank no online access Zanette et al. [2020] low inherent Bellman error no online access Du et al. [2021] bilinear class no online access Lazic et al. [2021], Wei et al. [2021] Q -realizability, feature excitation yes online access π Jin et al. [2020], Agarwal et al. [2020a] linear MDP yes online access Zhou et al. [2020], Cai et al. [2020] linear mixture MDP ? online access Negative Results Assumption CE Access Model Du et al. [2020] Q -realizability, ǫ = Ω( H/d) N/A random access π Weisz et al. [2021b] Q -realizability, exp(d) actions N/A random access ∗ p Wang et al. [2021] Q -realizability, constant gap N/A online access ∗ 4
Random access Theoretical guarantees for the random access model have been obtained for the tab- ular setting [Sidford et al., 2018, Agarwal et al., 2020b, Li et al., 2020, Azar et al., 2013]. As for linear function approximation, different assumptions have been made for theoretical analysis. Under the linear MDP assumption, Yang and Wang [2019] derived an optimal (dκ 2(1 γ) 3) query complexity bound − − O − by a variance-reduction Q-learning type algorithm. Under the Q -realizability of all determinstic poli- π cies (a strictly weaker assumption than linear MDP [Zanette et al., 2020]), Du et al. [2020] showed a neg- ative result for the settings with model misspecification error ǫ = Ω( H/d) (see also Van Roy and Dong [2019], Lattimore et al. [2020]). When ǫ = o((1 γ)2/√d), assuming the access to the full feature matrix, − p Lattimore et al. [2020] proposed algorithms with polynomial query costs, and Du et al. [2020] proposed similar algorithm for the exact Q realizability setting. Since these works need to find a globally opti- π mal design or barycentric spanner, their computational costs depend polynomially on the size of the state space. Under the V -realizability assumption (i.e., the optimal value function is linear in some feature map), ∗ Shariff and Szepesva´ri [2020] proposed a planning algorithm assuming the availability of a set of core states but obtaining such core states can still be computationally inefficient. Zanette et al. [2019] proposed an algorithm that uses a similar concept named anchor points but only provided a greedy heuristic to gener- ate these points. A notable negative result is established in Weisz et al. [2021b] that shows that with only Q -realizability, any agent requires min(exp(Ω(d)), exp(Ω(H))) queries to learn an optimal policy. ∗ Local access Many prior studies have used simulators in tree-search style algorithms [Kearns et al., 2002, Munos, 2014]. Under this setting, for the online planning problem, recently Weisz et al. [2021a] established an ((dH/κ) |A|) query cost bound to learn an κ-optimal policy by the TENSORPLAN algorithm assuming O the V ∗-realizability. Whenever the action set is small, TENSORPLAN is query efficient, but its computa- tional efficiency is left as an open problem. Under Q -realizability and constant sub-optimality gap, for ∗ the offline planning problem, Li et al. [2021] proposed an algorithm with poly(d, H, κ 1, ∆ 1 ) query and − −gap computational costs. Online access As mentioned, many online RL algorithms can be converted to a policy optimization algo- rithm under the online access model using online-to-batch conversion. There is a large body of literature on online RL with linear function approximation and here we discuss a non-exhaustive list of prior works. Under the Q -realizability assumption, assuming that the probability transition of the MDP is deterministic, ∗ Wen and Van Roy [2013] proposed a sample and computationally efficient algorithm via the eluder dimen- sion [Russo and Van Roy, 2013]. Assuming the MDP has low Bellman rank, Jiang et al. [2017] proposed an algorithm that is sample efficient but computationally inefficient, and similar issues arise in Zanette et al. [2020] under the low inherent Bellman error assumption. Du et al. [2021] proposed a more general MDP class named bilinear class and provided a sample efficient algorithm, but the computational efficiency is unclear. Under Q π-realizability, several algorithms, such as POLITEX [Abbasi-Yadkori et al., 2019, Lazic et al., 2021], AAPI [Hao et al., 2021], and MDP-EXP2 [Wei et al., 2021] achieved sublinear regret in the infinite horizon average reward setting and are also computationally efficient. However, the corresponding analysis avoids the exploration issue by imposing a feature excitation assumption which may not be satisfied in many problems. Under the linear MDP assumption, Jin et al. [2020] established a (√d3H3T ) regret bound for O 5
an optimistic least-square value iteration algorithm. Agarwal et al. [2020a] derived a poly(d, H, κ 1) sam- − ple cost bound for the policy cover-policy gradient algorithm, which can also be applied in the state aggre- gation setting; the algorithm and sample cost were subsequently improved in Zanette et al. [2021]. Under the linear mixture MDP assumption [Yang and Wang, 2020, Zhou et al., 2020], Cai et al. [2020] proved an (√d3H3T ) regret bound for an optimistic least square policy iteration (LSPI) type algorithm. A notable O negative result for the online RL setting by Wang et al. [2021] shows that an exponentially large number of samples are needed if we only assume Q -realizability and constant sub-optimality gap. Other related works ∗ include Ayoub et al. [2020], Jin et al. [2021], Du et al. [2019], Wang et al. [2019], and references therein. 3 Preliminaries We use ∆ to denote the set of probability distributions defined on the set . Consider an infinite-horizon S S discounted MDP that is specified by a tuple ( , , r, P, ρ, γ), where is the state space, is the finite S A S A action space, r : [0, 1] is the reward function, P : ∆ is the probability transition S × A → S × A → S kernel, ρ is the initial state, and γ (0, 1) is the discount factor. For simplicity, in the main sections ∈ S ∈ of this paper, we assume that the initial state ρ is deterministic and known to the agent. Our algorithm can also be extended to the setting where the initial state is random and the agent is allowed to sample from the initial state distribution. We discuss this extension in Appendix E. Throughout this paper, we write [N ] := 1, 2, . . . , N for any positive integer N and use log( ) to denote natural logarithm. { } · A policy π : ∆ is a mapping from a state to a distribution over actions. We only consider S → A stationary policies, i.e., they do not change according to the time step. The value function V (s) of a policy π is the expected return when we start running the policy π from state s, i.e., ∞ V (s) = E γtr(s , a ) s = s , π at ∼π( ·|st),st+1 ∼P ( ·|st,at) " t t | 0 # t=0 X and the state-action value function Q (s, a), also known as the Q-function, is the expected return following π policy π conditioned on s = s, a = a, i.e., 0 0 ∞ Q (s, a) = E γtr(s , a ) s = s, a = a . π st+1 ∼P ( ·|st,at),at+1 ∼π( ·|st+1) " t t | 0 0 # t=0 X We assume that the agent interacts with a simulator using the local access protocol defined in Definition 1.1, i.e, for any state s that the agent has visited and any action a , the agent can query the simulator and ∈ A obtain a sample s P ( s, a) and the reward r(s, a). ′ ∼ ·| Our general goal is to find a policy that maximizes the expected return starting from the initial state ρ, i.e., max π V π(ρ). We let π ∗ be the optimal policy, V ∗( ) := V π∗( ), and Q ∗( , ) := Q π∗( , ). We also aim · · · · · · to learn a good policy efficiently, i.e., the query and computational costs should not depend on the size of the state space , which can be large in many problems. S Linear function approximation Let φ : Rd be a feature map which assigns to each state- S × A → action pair a d-dimensional feature vector. For any (s, a) , the agent can obtain φ(s, a) with a ∈ S × A computational cost of poly(d). Here, we emphasize that the computation of the feature vectors does not lead to a query cost. Without loss of generality, we impose the following bounded features assumption. 6
Assumption 3.1 (Bounded features). We assume that φ(s, a) 1 for all (s, a) . 2 k k ≤ ∈ S × A We consider the following two different assumptions on the linear realizability of the Q-functions: Assumption 3.2 (Q -realizability). There exists b > 0 such that for every policy π, there exists a weight π vector w Rd, w b, that ensures Q (s, a) = w φ(s, a) for all (s, a) . π ∈ k π k2 ≤ π π⊤ ∈ S × A Assumption 3.3 (Approximate Q -realizability). There exists b > 0 and model misspecification error ǫ > 0 π such that for every policy π, there exists a weight vector w Rd, w b, that ensures Q (s, a) π π 2 π ∈ k k ≤ | − w φ(s, a) ǫ for all (s, a) . π⊤ | ≤ ∈ S × A 4 Algorithm We first introduce some basic concepts used in our algorithms. Core set We use a concept called core set. A core set is a set of tuples z = (s, a, φ(s, a), q) C ∈ S × A × Rd (R none ). The first three elements in the tuple denote a state, an action, and the feature vector × ∪ { } corresponding to the state-action pair, respectively. The last element q R in the tuple denotes an estimate ∈ of Q (s, a) for a policy π. During the algorithm, we may not always have such an estimate, in which case π we write q = none. For a tuple z, we use z , z , z , and z to denote the s, a, φ, and q coordinates of s a φ q z, respectively. We note that in prior works, the core set usually consists of the state-action pairs and their features [Lattimore et al., 2020, Du et al., 2020, Shariff and Szepesva´ri, 2020]; whereas in this paper, for the convenience of notation, we also have the target values (Q-function estimates) in the core set elements. We denote by Φ R d the feature matrix of all the elements in , i.e., each row of Φ is the feature vector |C|× of an elemenC t ∈ in . Similarly, we define q R as the vector fC or the Q estimate ofC all the tuples in . |C| π C C ∈ C Good set It is also useful to introduce a notion of good set. Definition 4.1. Given λ, τ > 0, and feature matrix Φ R d, the good set Rd is defined as |C|× C ∈ H ⊂ := φ Rd : φ (Φ Φ + λI) 1φ τ . ⊤ ⊤ − H { ∈ C C ≤ } Intuitively, the good set is a set of vectors that are well-covered by the rows of Φ ; in other words, these C vectors are not closely aligned with the eigenvectors associated with the small eigenvalues of the covariance matrix of all the features in the core set. As an overview, our algorithm CONFIDENT MC-LSPI works as follows. First, we initialize the core set using the initial state ρ paired with all actions. Then, the algorithm runs least-squares policy itera- tion [Munos, 2003] to optimize the policy. This means that in each iteration, we estimate the Q-function value for every state-action pair in using Monte Carlo rollout with the simulator, and learn a linear function C to approximate the Q-function of the rollout policy, and the next policy is chosen to be greedy with respect to this linear function. Our second algorithm CONFIDENT MC-POLITEX works similarly, with the only dif- ference being that instead of using the greedy policy iteration update rule, we use the mirror descent update rule with KL regularization between adjacent rollout policies [Even-Dar et al., 2009, Abbasi-Yadkori et al., 2019]. Moreover, in both algorithms, whenever we observe a state-action pair whose feature is not in the good set during Monte Carlo rollout, we add the pair to the core set and restart the policy iteration process. We name the rollout subroutine CONFIDENTROLLOUT. We discuss details in the following. 7
4.1 Subroutine: CONFIDENTROLLOUT We first introduce the CONFIDENTROLLOUT subroutine, whose purpose is to estimate Q π(s 0, a 0) for a given state-action pair (s , a ) using Monte Carlo rollouts. During a rollout, for each state s that we en- 0 0 counter and all actions a , the subroutine checks whether the feature vector φ(s, a) is in the good set. If ∈ A not, we know that we have discovered a new feature direction, i.e. a direction which is not well aligned with eigenvectors corresponding to the the largest eigenvalues of the covariance matrix of the core features. In this case the subroutine terminates and returns the tuple (s, a, φ(s, a), none) along with the uncertain status. If the algorithm does not discover a new direction, it returns an estimate q of the desired value Q (s , a ) π 0 0 and the done status. This subroutine is formally presented in Algorithm 1. Algorithm 1 CONFIDENTROLLOUT 1: Input: number of rollouts m, length of rollout n, rollout policy π, discount γ, initial state s 0, initial action a , feature matrix Φ , regularization coefficient λ, threshold τ . 0 C 2: for i = 1, . . . , m do 3: s i,0 s 0, a i,0 a 0, query the simulator, obtain reward r i,0 r(s i,0, a i,0), and next state s i,1. ← ← ← 4: for t = 1, . . . , n do 5: for a do ∈ A 6: Compute feature φ(s i,t, a). 7: if φ(s i,t, a) ⊤(Φ ⊤Φ + λI) −1φ(s i,t, a) > τ then 8: status uncCertCain, result (s i,t, a, φ(s i,t, a), none) ← ← 9: return status, result 10: end if 11: end for 12: Sample a i,t π( s i,t). ∼ ·| 13: Query the simulator with s i,t, a i,t, obtain reward r i,t r(s i,t, a i,t), and next state s i,t+1. ← 14: end for 15: end for 16: status ← done, result ← m1 m i=1 n t=0 γtr i,t 17: return status, result P P 4.2 Policy iteration With the subroutine, now we are ready to present our main algorithms. Both of our algorithms maintain a core set . We first initialize the core set using the initial state ρ and all actions a . More specifically, C ∈ A we check all the feature vectors φ(ρ, a), a . If the feature vector is not in the good set of the current core ∈ A set, we add the tuple (ρ, a, φ(ρ, a), none) to the core set. Then we start the policy iteration process. Both { } algorithms start with an arbitrary initial policy π and run K iterations. Let π be the rollout policy in 0 k 1 − the k-th iteration. We try to estimate the state-action values for the state-action pairs in under the current C policy π k −1, i.e., Q πk−1(z s, z a) for z ∈ C, using CONFIDENTROLLOUT. In this Q-function estimation procedure, we may encounter two scenarios: (a) If the rollout subroutine always returns the done status with an estimate of the state-action value, once 8
we finish the estimation for all the state-action pairs in , we can estimate the Q-function of π k 1 C − using least squares with input features Φ and targets q and regularization coefficient λ. Let w be k C C the solution to the least squares problem, i.e., w = (Φ Φ + λI) 1Φ q . (4.1) k ⊤ − ⊤ C C C C Then, for CONFIDENT MC-LSPI, we choose the rollout policy of the next iteration, i.e., π k, as the greedy policy with respect to the linear function w φ(s, a): k⊤ π (a s) = 1 a = arg max w φ(s, a ) . (4.2) k | a′ k⊤ ′ ∈A (cid:0) (cid:1) For CONFIDENT MC-POLITEX, we construct a truncated Q-function Q k 1 : [0, (1 γ) −1] − S × A 7→ − using linear function with clipping: Q k 1(s, a) := Π [0,(1 γ)−1](w k⊤φ(s, a)), (4.3) − − where Π (x) := min max x, a , b . The rollout policy of the next iteration is then [a,b] { { } } k 1 − π (a s) exp α Q (s, a) , (4.4) k j | ∝ j=1 (cid:0) X (cid:1) where α > 0 is an algorithm parameter. (b) It could also happen that the CONFIDENTROLLOUT subroutine returns the uncertain status. In this case, we add the state-action pair with new feature direction found by the subroutine to the core set and restart the policy iteration process with the latest core set. As a final note, for CONFIDENT MC-LSPI, we output the rollout policy of the last iteration π K 1, − whereas for CONFIDENT MC-POLITEX, we output a mixture policy π K , which is a policy chosen from {π k }kK =−01 uniformly at random. The reason that this algorithm needs to output a mixture policy is that PO- LITEX [Szepesva´ri, 2021] uses the regret analysis of expert learning [Cesa-Bianchi and Lugosi, 2006], and to obtain a single output policy, we need to use the standard online-to-batch conversion argument [Cesa-Bianchi et al., 2004]. Our algorithms are formally presented in Algorithm 2. In the next section, we present theoretical guarantees for our algorithms. 5 Theoretical guarantees In this section, we present theoretical guarantees for our algorithms. First, we have the following main result for CONFIDENT MC-LSPI. Theorem 5.1 (Main result for CONFIDENT MC-LSPI). If Assumption 3.2 holds, then for an arbitrarily small κ > 0, by choosing τ = 1, λ = κ2 1( 01 2− 4bγ 2)4 , n = 1 3 γ log( 4(1+lo κg (1(1+ γλ )−1)d) ), K = 2 + 1 2 γ log( κ(13 γ) ), m = 4096 d(1+log(1+λ−1)) log( 8Kd(1+log(1+λ−1)) ), we− have with proba− bility at least 1 δ, t− he policy π− that CONFIDEκ N2 T(1 M−γ C)6 -LSPI outputs saδ tisfies − K −1 V (ρ) V (ρ) κ. ∗ − πK−1 ≤ 9
Algorithm 2 CONFIDENT MC-LSPI / POLITEX 1: Input: initial state ρ, initial policy π 0, number of iterations K, regularization coefficient λ, threshold τ , discount γ, number of rollouts m, length of rollout n, POLITEX parameter α. 2: // Initialize core set. C ← ∅ 3: for a do ∈ A 4: if = or φ(ρ, a) ⊤(Φ ⊤Φ + λI) −1φ(ρ, a) > τ then 5: C ∅ (ρ, a, φ(ρ,C a),Cnone) C ← C ∪ { } 6: end if 7: end for 8: z q none, z // Policy iteration starts. ( ) ← ∀ ∈ C ∗ 9: for k = 1, . . . , K do 10: for z do ∈ C 11: status, result CONFIDENTROLLOUT(m, n, π k 1, γ, z s, z a, Φ , λ, τ ) 12: if status = do← ne, then z q result; else − result and gC oto line ( ) ← C ← C ∪ { } ∗ 13: end for 14: w k ← (Φ ⊤ C Φ C + λI) −1Φ ⊤ C q C; Q k −1(s, a) ← Π [0,(1 −γ)−1](w k⊤φ(s, a)) (POLITEX only) 15: π k(a s) 1 a = arg max a′ ∈A w k⊤φ(s, a ′) , LSPI | ← (ex(cid:0)p α jk =−11 Q j(s, a) / a′ (cid:1)exp α jk =−11 Q j(s, a ′) POLITEX 16: end for ∈A (cid:0) P (cid:1) P (cid:0) P (cid:1) 17: return w K −1 for LSPI, or π K ∼ Unif {π k }K k=−01 for POLITEX. Moreover, the query and computational costs for the algorithm are poly(d, 1 , 1 , log( 1 ), log(b)) and 1 γ κ δ poly(d, 1 , 1 , , log( 1 ), log(b)), respectively. − 1 γ κ |A| δ Altern− atively, if Assumption 3.3 holds, then by choosing τ = 1, λ = ǫ2d , n = 1 log( 1 ), K = b2 1 γ ǫ(1 γ) 2 + 1 log 1 , m = 1 log( 8Kd(1+log(1+λ−1)) ), we have with probability at − least 1 δ− , the policy 1 γ ǫ√d ǫ2(1 γ)2 δ − π K 1−that C (cid:0)ONFI (cid:1)DENT MC-−LSPI outputs satisfies − 74ǫ√d V (ρ) V (ρ) (1 + log(1 + b2ǫ 2d 1)). ∗ − πK−1 ≤ (1 γ)2 − − − Moreover, the query and computational costs for the algorithm are poly(d, 1 , 1 , log( 1 ), log(b)) and 1 γ ǫ δ poly(d, 1 , 1 , , log( 1 ), log(b)), respectively. − 1 γ ǫ |A| δ − We prove Theorem 5.1 in Appendix B. For CONFIDENT MC-POLITEX, since we output a mixture policy, we prove guarantees for the expected value of the mixture policy, i.e., V πK := K1 K k=−01 V πk . We have the following result. P Theorem 5.2 (Main result for CONFIDENT MC-POLITEX). If Assumption 3.2 holds, then for an arbi- trarily small κ > 0, by choosing τ = 1, α = (1 − γ) 2 log K( |A|) , λ = κ2 2( 51 6− bγ 2)2 , K = 3 κ2 2l (o 1g( | γA )4|) , n = 1 log( 32√d(1+log(1+λ−1)) ), and m = 1024 d(1+log(1q+λ−1)) log( 8Kd(1+log(1+λ−1)) ), we have− with 1 γ (1 γ)2κ κ2(1 γ)4 δ probabi−lity at least 1 −δ, the mixture policy π K that CONFI− DENT MC-POLITEX outputs satisfies − V (ρ) V (ρ) κ. ∗ − πK ≤ 10
Moreover, the query and computational costs for the algorithm are poly(d, 1 , 1 , log( 1 ), log(b)) and 1 γ κ δ poly(d, 1 , 1 , , log( 1 ), log(b)), respectively. − 1 γ κ |A| δ − Alternatively, if Assumption 3.3 holds, then by choosing τ = 1, α = (1 − γ) 2 log K( |A|) , λ = ǫ b2 2d , K = ǫ2 2l do (g 1( |A γ)|) 2 , n = 1 1 γ log( ǫ(11 γ) ), and m = ǫ2(11 γ)2 log( 8Kd(1+lo δg(1+λ−1)) ), weqhave with probability at least 1 −δ, the mixtu−re policy − π K that CONFIDEN− T MC-POLITEX outputs satisfies − 42ǫ√d V (ρ) V (ρ) (1 + log(1 + b2ǫ 2b 1)). ∗ − πK ≤ 1 γ − − − Moreover, the query and computational costs for the algorithm are poly(d, 1 , 1 , log( 1 ), log(b)) and 1 γ ǫ δ poly(d, 1 , 1 , , log( 1 ), log(b)), respectively. − 1 γ ǫ |A| δ − We prove Theorem 5.2 in Appendix D. Here, we first discuss the query and computational costs of both algorithms and then provide a sketch of our proof. Query and computational costs In our analysis, we say that we start a new loop whenever we start (or restart) the policy iteration process, i.e., going to line ( ) in Algorithm 2. By definition, when we start a ∗ new loop, the size of the core set is increased by 1. First, in Lemma 5.1 below, we show that the size C of the core set will never exceed C = (d). Therefore, the total number of loops is at most C . In max max O each loop, we run K policy iterations; in each iteration, we run Algorithm 1 from at most C points from max the core set; and each time when we run Alegorithm 1, we query the simulator at most (mn) times. Thus, O for both algorithms, the total number of queries that we make is at most C2 Kmn. Therefore, using the max parameter choice in Theorems 5.1 and 5.2 and omitting logarithmic factors, we can obtain the query costs of CONFIDENT MC-LSPI and POLITEX in Table 2. As we can see, when ǫ = 0 or ǫ = 0 but ǫ = o(1/√d) (the 6 regime we care about in this paper), the query cost of CONFIDENT MC-LSPI is lower than POLITEX. As for computational cost, since our policy improvement steps only involve matrix multiplication and matrix inversion, the computational cost is also polynomial in the aforementioned factors. One thing to notice is that during the rollout process, in each step, the agent needs to compute the features of a state paired with all actions, and thus the computational cost linearly depends on ; on the contrary the query cost does |A| not depend on since in each step the agent only needs to query the simulator with the action sampled |A| according to the policy. Sub-optimality We also note that when Assumption 3.3 holds, i.e., ǫ = 0, the sub-optimality of the 6 output policy is ( ǫ√d ) for LSPI and ( ǫ√d ) for POLITEX. Therefore, in the presence of a model mis- O (1 γ)2 O 1 γ specification error, C− ONFIDENT MC-POLITE− X can achieve a better final sub-optimality than CONFIDENT MC-LSPI, althoeugh it’s query cost is higheer. Table 2: Comparison of CONFIDENT MC-LSPI and POLITEX Query (ǫ = 0) Query (ǫ = 0) Sub-optimality (ǫ = 0) 6 6 LSPI d3 d2 ǫ√d O κ2(1 γ)8 O ǫ2(1 γ)4 O (1 γ)2 POLITEX Oe(cid:0) κ4(1d− −3 γ)9 (cid:1) Oe(cid:0) ǫ4(1d− −γ)5 (cid:1) eO(cid:0) 1ǫ− √ −d γ (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) e e e 11
Proof sketch We now discuss our proof strategy, focusing on LSPI for simplicity. Step 1: Bound the size of the core set The first step is to show that our algorithm will terminate. This is equivalent to showing that the size of the core set will not exceed certain finite quantity, since whenever C we receive the uncertain status from CONFIDENTROLLOUT, we increase the size of the core set by 1, go back to line ( ) in Algorithm 2, and start a new loop. The following lemma shows that the size of the core ∗ set is always bounded, and thus the algorithm will always terminate. Lemma 5.1. Under Assumption 3.1, the size of the core set will not exceed C e 1 + τ 1 1 C := d log(1 + ) + log(1 + ) . max e 1 τ τ λ − (cid:18) (cid:19) This result first appears in Russo and Van Roy [2013] as the eluder dimension of linear function class. We present the proof of this lemma in Appendix A for completeness. Step 2: Virtual policy iteration The next step is to analyze the gap between the value of the optimal policy and the policy π parameterized by the vector w that the algorithm outputs in the final loop, i.e., K 1 − V (ρ) V (ρ). For ease of exposition, here we only consider the case of deterministic probability ∗ − πK−1 transition kernel P . Our full proof in Appendix B considers general stochastic dynamics. To analyze our algorithm, we note that for approximate policy iteration (API) algorithms, if in every iteration (say the k-th iteration), we have an approximate Q-function that is close to the true Q-function of the rollout policy (say π ) in ℓ norm, i.e., Q Q η, then existing results [Munos, k −1 ∞ k k −1 − πk−1k∞ ≤ 2003, Farahmand et al., 2010] ensure that we can learn a good policy if in every iteration we choose the new policy to be greedy with respect to the approximate Q-function. However, since we only have local access to the simulator, we cannot have such ℓ guarantee. In fact, as we show in the proof, we can ∞ only ensure that when φ(s, a) is in the good set , our linear function approximation is accurate, i.e., H Q (s, a) Q (s, a) η where Q (s, a) = w φ(s, a). To overcome the lack of ℓ guarantee, | k −1 − πk−1 | ≤ k −1 k⊤ ∞ we introduce the notion of virtual policy iteration algorithm. In the virtual algorithm, we start with the same initial policy π = π . In the k-th iteration of the virtual algorithm, we assume that we have access to the 0 0 true Q-function of the rollout policy π when φ(s, a) / , and construct k 1 − ∈ H e Q (e s, a) = w k⊤φ(s, a) if φ(s, a) ∈ H k 1 − (Q πe k−1(s, a) otherwise, e e where w is the linear coefficient that we learn in the virtual algorithm in the same way as in Eq. (4.1). Then k π is chosen to be greedy with respect to Q (s, a). In this way, we can ensure that Q (s, a) is close to k k 1 k 1 − − the trueeQ-function Q πe k−1(s, a) in ℓ norm and thus the output policy, say π K 1, of the virtual algorithm ies good in the sense that V ∗(ρ) − V πe∞ K−1 (ρe) is small. − e To connect the output policy of the virtual algorithm and our actual algoritehm, we note that by definition, in the final loop of our algorithm, in any iteration, for any state s that the agent visits in CONFIDENTROLL- OUT, and any action a , we have that φ(s, a) since the subroutine never returns uncertain status. ∈ A ∈ H Further, because the initial state, probability transition kernel, and the policies are all deterministic, we know 12
that the rollout trajectories of the virtual algorithm and our actual algorithm are always the same in the final loop (the virtual algorithm does not get a chance to use the true Q-function Q πe k−1). With rollout length n, we know that when we start with state ρ, the output of the virtual algorithm π and our actual algorithm K 1 tπ hK at− V1 ta (k ρe ) exa Vctly th (ρe )s ia sm se ma ac llt .io Tn os ef xo tr enn ds tt hep iss, ara gn ud mt eh nu ts t| oV tπ hK e− s1 e( tρ ti) ng− wV iπe thK e− st1 o(− cρ h) a| s≤ tic γ t1 rn −a+ γ n1 s, itw ioh ni sc ,h wim e np eli ee ds ∗ − πK−1 to use a coupling argument which we elaborate in the Appendix. 6 Conclusion We propose the CONFIDENT MC-LSPI and CONFIDENT MC-POLITEX algorithms, for local planning with linear function approximation. Under the assumption that the Q-functions of all policies are linear in some features of the state-action pairs, we show that our algorithm is query and computationally efficient. We introduce a novel analysis technique based on a virtual policy iteration algorithm, which can be used to leverage existing guarantees on approximate policy iteration with ℓ -bounded evaluation error. We use this ∞ technique to show that our algorithm can learn the optimal policy for the given initial state even only with local access to the simulator. Future directions include extending our analysis technique to broader settings. Acknowledgement The authors would like to thank Gelle´rt Weisz for helpful comments. References Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba Szepesvari, and Gelle´rt Weisz. Politex: Regret bounds for policy iteration using expert prediction. In International Conference on Machine Learning, pages 3692–3702. PMLR, 2019. Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. PC-PG: Policy cover directed exploration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020a. Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based supervised learning with a generative model is minimax optimal. In Conference on Learning Theory, pages 67–83. PMLR, 2020b. Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. Journal of Computer and System Sciences, 74(1):97–114, 2008. Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based supervised learning with value-targeted regression. In International Conference on Machine Learning, pages 463–474. PMLR, 2020. Mohammad Gheshlaghi Azar, Re´mi Munos, and Hilbert J Kappen. Minimax PAC bounds on the sample complexity of supervised learning with a generative model. Machine learning, 91(3):325–349, 2013. Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization. In Interna- tional Conference on Machine Learning, pages 1283–1294. PMLR, 2020. Nicolo Cesa-Bianchi and Ga´bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006. 13
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050–2057, 2004. Simon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably efficient Q-learning with function approxima- tion via distribution shift error checking oracle. arXiv preprint arXiv:1906.06321, 2019. Simon S Du, Sham M Kakade, Ruosong Wang, and Lin Yang. Is a good representation sufficient for sample efficient supervised learning? In International Conference on Learning Representations, 2020. Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in RL. arXiv preprint arXiv:2103.10897, 2021. Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019. Eyal Even-Dar, Sham M Kakade, and Yishay Mansour. Online Markov decision processes. Mathematics of Operations Research, 34(3):726–736, 2009. Amir Massoud Farahmand, Re´mi Munos, and Csaba Szepesva´ri. Error propagation for approximate policy and value iteration. In Advances in Neural Information Processing Systems, 2010. Botao Hao, Nevena Lazic, Yasin Abbasi-Yadkori, Pooria Joulani, and Csaba Szepesva´ri. Adaptive approximate policy iteration. In International Conference on Artificial Intelligence and Statistics, pages 523–531. PMLR, 2021. David A Harville. Matrix algebra from a statistician’s perspective, 1998. Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, pages 1704–1713. PMLR, 2017. Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient supervised learning with linear function approximation. In Conference on Learning Theory, pages 2137–2143. PMLR, 2020. Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. arXiv preprint arXiv:2102.00815, 2021. Sham Machandranath Kakade. On the sample complexity of supervised learning. University of London, University College London (United Kingdom), 2003. Michael Kearns and Satinder Singh. Finite-sample convergence rates for Q-learning and indirect algorithms. Advances in Neural Information Processing Systems, pages 996–1002, 1999. Michael Kearns, Yishay Mansour, and Andrew Y Ng. A sparse sampling algorithm for near-optimal planning in large Markov decision processes. Machine learning, 49(2):193–208, 2002. Tor Lattimore, Csaba Szepesvari, and Gellert Weisz. Learning with good feature representations in bandits and in RL with a generative model. In International Conference on Machine Learning, pages 5662–5670. PMLR, 2020. Nevena Lazic, Dong Yin, Yasin Abbasi-Yadkori, and Csaba Szepesvari. Improved regret bound and experience replay in regularized policy iteration. arXiv preprint arXiv:2102.12611, 2021. Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the sample size barrier in model-based supervised learning with a generative model. Advances in Neural Information Processing Systems, 33, 2020. 14
Gen Li, Yuxin Chen, Yuejie Chi, Yuantao Gu, and Yuting Wei. Sample-efficient supervised learning is feasible for linearly realizable MDPs with limited revisiting. arXiv preprint arXiv:2105.08024, 2021. Re´mi Munos. Error bounds for approximate policy iteration. In International Conference on Machine Learning, volume 3, pages 560–567, 2003. Re´mi Munos. From bandits to Monte-Carlo tree search: The optimistic principle applied to optimization and planning. Foundations and Trends in Machine Learning, 2014. Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In Advances in Neural Information Processing Systems, pages 2256–2264. Citeseer, 2013. Roshan Shariff and Csaba Szepesva´ri. Efficient planning in large MDPs with weak linear function approximation. arXiv preprint arXiv:2007.06184, 2020. Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-optimal time and sample complexities for solving Markov decision processes with a generative model. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 5192–5202, 2018. Satinder P Singh and Richard C Yee. An upper bound on the loss from approximate optimal-value functions. Machine Learning, 16(3):227–233, 1994. Csaba Szepesva´ri. RL Theory lecture notes: POLITEX. https://rltheory.github.io/lecture-notes/planning-in-mdps/lec14/, 2021. Michael J Todd. Minimum-volume ellipsoids: Theory and algorithms. SIAM, 2016. Benjamin Van Roy and Shi Dong. Comments on the Du-Kakade-Wang-Yang lower bounds. arXiv preprint arXiv:1911.07910, 2019. Yining Wang, Ruosong Wang, Simon S Du, and Akshay Krishnamurthy. Optimism in supervised learning with generalized linear function approximation. arXiv preprint arXiv:1912.04136, 2019. Yuanhao Wang, Ruosong Wang, and Sham M Kakade. An exponential lower bound for linearly-realizable MDPs with constant suboptimality gap. arXiv preprint arXiv:2103.12690, 2021. Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, and Rahul Jain. Learning infinite-horizon average-reward MDPs with linear function approximation. In International Conference on Artificial Intelligence and Statistics, pages 3007–3015. PMLR, 2021. Gellert Weisz, Philip Amortila, Barnaba´s Janzer, Yasin Abbasi-Yadkori, Nan Jiang, and Csaba Szepesva´ri. On query-efficient planning in MDPs under linear realizability of the optimal state-value function. arXiv preprint arXiv:2102.02049, 2021a. Gelle´rt Weisz, Philip Amortila, and Csaba Szepesva´ri. Exponential lower bounds for planning in MDPs with linearly- realizable optimal action-value functions. In Algorithmic Learning Theory, pages 1237–1264. PMLR, 2021b. Zheng Wen and Benjamin Van Roy. Efficient exploration and value function generalization in deterministic systems. Advances in Neural Information Processing Systems, 26, 2013. Lin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features. In International Conference on Machine Learning, pages 6995–7004. PMLR, 2019. 15
Lin Yang and Mengdi Wang. Reinforcement leaning in feature space: Matrix bandit, kernels, and regret bound. International Conference on Machine Learning, 2020. Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. Limiting extrapolation in linear approximate value iteration. Advances in Neural Information Processing Systems, 32:5615–5624, 2019. Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent Bellman error. In International Conference on Machine Learning, pages 10978–10989. PMLR, 2020. Andrea Zanette, Ching-An Cheng, and Alekh Agarwal. Cautiously optimistic policy optimization and exploration with linear function approximation. In Conference on Learning Theory (COLT), 2021. Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient supervised learning for discounted MDPs with feature mapping. arXiv preprint arXiv:2006.13165, 2020. Appendix A Proof of Lemma 5.1 This proof essentially follows the proof of the upper bound for the eluder dimension of a linear function class in Russo and Van Roy [2013]. We present the proof here for completeness. We restate the core set construction process in the following way with slightly different notation. We begin with Φ = 0. In the t-th step, we have a core set with feature matrix Φ R(t 1) d. Suppose that 0 t 1 − × we can find φ Rd, φ 1, such that − ∈ t t 2 ∈ k k ≤ φ (Φ Φ + λI) 1φ > τ, (A.1) ⊤t ⊤t 1 t 1 − t − − then we let Φ := [Φ φ ] Rt d, i.e., we add a row at the bottom of Φ . If we cannot find such φ , we t ⊤t −1 t ⊤ ∈ × t −1 t terminate this process. We define Σ := Φ Φ + λI. It is easy to see that Σ = λI and Σ = Σ + φ φ . t ⊤t t 0 t t 1 t ⊤t − According to matrix determinant lemma [Harville, 1998], we have det(Σ ) = (1 + φ Σ 1 φ ) det(Σ ) > (1 + τ ) det(Σ ) t ⊤t −t 1 t t 1 t 1 − − − > > (1 + τ )t det(Σ ) = (1 + τ )tλd, (A.2) 0 · · · where the inequality is due to (A.1). Since det(Σ ) is the product of all the eigenvalues of Σ , according to t t the AM-GM inequality, we have d tr(Σ ) d tr( t φ φ ) + tr(λI) t det(Σ ) t = i=1 i ⊤i ( + λ)d, (A.3) t ≤ d d ≤ d ! (cid:18) (cid:19) P where in the second inequality we use the fact that φ 1. Combining (A.2) and (A.3), we know that t i 2 k k ≤ must satisfy t (1 + τ )tλd < ( + λ)d, d 16
which is equivalent to t t (1 + τ ) d < + 1. (A.4) λd We note that if t d, the result of the size of the core set in Lemma 5.1 automatically holds. Thus, we only ≤ consider the situation here t > d. In this case, the condition (A.4) implies t t t 1 t 1 log(1 + τ ) < log(1 + ) < log( (1 + )) = log( ) + log(1 + ) d λd d λ d λ tτ 1 + τ 1 = log + log( ) + log(1 + ). (A.5) d(1 + τ ) τ λ (cid:18) (cid:19) Using the fact that for any x > 0, log(1 + x) > x , and that for any x > 0, log(x) x , we obtain 1+x ≤ e tτ tτ 1 + τ 1 < + log( ) + log(1 + ), (A.6) d(1 + τ ) ed(1 + τ ) τ λ which implies e 1 + τ 1 1 t < d log(1 + ) + log(1 + ) . e 1 τ τ λ − (cid:18) (cid:19) B Proof of Theorem 5.1 In this proof, we say that we start a new loop whenever we start (or restart) the policy iteration process, i.e., going to line ( ) in Algorithm 2. In each loop, we have at most K iterations of policy iteration steps. By ∗ definition, we also know that when we start a new loop, the size of the core set increases by 1 compared C with the previous loop. We first introduce the notion of virtual policy iteration algorithm. This virtual algorithm is designed to leverage the existing results on approximate policy iteration with ℓ bounded error ∞ in the approximate Q-functions [Munos, 2003, Farahmand et al., 2010]. We first present the details of the virtual algorithm, and then provide performance guarantees for the main algorithm. B.1 Virtual approximate policy iteration with coupling The virtual policy iteration algorithm is a virtual algorithm that we use for the purpose of proof. It is a version of approximate policy iteration (API) with a simulator. An important factor is that the simulators of the virtual algorithm and the main algorithm need to be coupled, which we explain in this section. The virtual algorithm is defined as follows. Unlike the main algorithm, the virtual algorithm runs exactly C loops, where C is the upper bound for the size of the core set defined in Lemma 5.1. In the virtual max max algorithm, we let the initial policy be the same as the main algorithm, i.e., π = π . Unlike the main 0 0 algorithm, the virtual algorithm runs exactly K iterations of policy iteration. In the k-th iteration (k 1), ≥ the virtual algorithm runs rollouts from each element in the core set (we we ill discuss how the virtual C algorithm constructs the core set later) with π with a simulator where π is in the form of Eq. (B.3) k 1 k 1 − − (Q will be defined once we present the details of the virtual algorithm). k 1 − We now describe the rollout process of thee virtual algorithm. We still uese a subroutine similar to CON- FIe DENTROLLOUT. The simulator of the virtual algorithm can still generate samples of next state given a 17
state-action pair according to the probability transition kernel P . The major difference from the main algo- rithm is that during the rollout process, when we find a state-action pair whose feature is outside of the good set (defined in Definition 4.1), i.e., (s, a) such that φ(s, a) (Φ Φ + λI)φ(s, a) > τ , we do not termi- ⊤ ⊤ H C C nate the subroutine, instead we record this state-action pair along with its feature (we call it the recorded element), and then keep running the rollout process using π . Two situations can occur at the end of each k 1 − loop: 1) We did not record any element, in which case we use the same core set in the next loop, and C 2) We have at least one recorded element in a particulareloop, in which case we add the first element to the core set and discard any other recorded elements. In other words, in each loop of the virtual algorithm, we find the first state-action pair (if any) whose feature is outside of the good set and add this pair to the core set. Another difference from the main algorithm is that in the virtual algorithm, we do not end the rollout subroutine when we identify an uncertain state-action pair, and as a result, the rollout subroutine in the virtual algorithm always returns an estimation of the Q-function. We now proceed to present the virtual policy iteration process. In the k-th iteration, the virtual algorithm runs m trajectories of n-step rollout using the policy π from each element z , obtains the empirical k 1 − ∈ C average of the discounted return z in the same way as in Algorithm 1. Then we concatenate them, obtain q the vector q , and compute e C w = (Φ Φ + λI) 1Φ q . (B.1) e k ⊤ − ⊤ C C C C We use the notion of good set defined in Definition 4.1, and define the virtual Q-function as follows: e e H w φ(s, a), φ(s, a) , Q (s, a) := k⊤ ∈ H (B.2) k 1 − (Q πe k−1(s, a), φ(s, a) ∈/ H, e e by assuming the access to the true Q-function Q πe k−1(s, a). The next policy π k is defined as the greedy policy with respect to Q (s, a), i.e., k 1 − e e π (a s) = 1 a = arg max Q (s, a )) . (B.3) k k 1 ′ | a′ − (cid:18) ∈A (cid:19) Recall that for the main algorithem, once we learn the parameteer vector w , the next policy π is greedy with k k respect to the linear function w φ(s, a), i.e., k⊤ π (a s) = 1 a = arg max w φ(s, a )) . k | a′ k⊤ ′ (cid:18) ∈A (cid:19) For comparison, the key difference is that when we observe a feature vector φ(s, a) that is not in the good set , our actual algorithm terminates the rollout and returns the state-action pair with the new direction, H whereas the virtual algorithm uses the true Q-function of the state-action pair. Coupling The major remaining issue now is how the main algorithm is connected to the virtual algorithm. We describe this connection with a coupling argument. In a particular loop, for any positive integer N , when the virtual algorithm makes its N -th query in the k-th iteration to the virtual simulator with a state- action pair, say (s , a ), if the main algorithm has not returned due to encountering an uncertain virtual virtual 18
state-action pair, we assume that at the same time the main algorithm also makes its N -th query to the simulator, with a state-action pair, say (s , a ). We let the two simulators be coupled: When they are main main queried with the same pair, i.e., (s , a ) = (s , a ), the next states that they return are also main main virtual virtual the same. In other words, the simulator for the main algorithm samples s P ( s , a ), and the ′main ∼ ·| main main virtual algorithm samples s P ( s , a ), and s and s satisfy the joint distribution ′virtual ∼ ·| virtual virtual ′main ′virtual such that P s = s = 1. In the cases where (s , a ) = (s , a ) or the main algorithm ′main ′virtual main main 6 virtual virtual has already returned due to the discovery of a new feature direction, the virtual algorithm samples from P (cid:0) (cid:1) independently from the main algorithm. Note that this setup guarantees that both the virtual algorithm and the main algorithm have valid simulators which can sample from the same probability transition kernel P . There are a few direct consequences of this coupling design. First, since the virtual and main algo- rithms start with the same initial core set elements (constructed using the initial state), we know that in any loop, when starting from the same core set element z, both algorithms will have exactly the same rollout trajectories until the main algorithm identifies an uncertain state-action pair and returns. This is due to the coupling of the simulators and the fact that within the good set , the policies for the main algorithm and H the virtual algorithm take the same action. Later, we will discuss this point more in Lemma B.5. Second, the core set elements that the virtual and main algorithms use are exactly the same for any loop. This is because when the main algorithm identifies an uncertain state-action pair, it adds it to the core set and start a new loop, and the virtual algorithm also only adds the first recorded element to the core set. Since the simulators are the coupled, the first uncertain state-action pair that they encounter will be the same, meaning that both algorithms always add the same element to the core set, until the main algorithm finishes its final loop. We note that the core set elements on our algorithm are stored as ordered list so the virtual and main algorithm always run rollouts with the same ordering of the core set elements. Another observation is that while the virtual algorithm has a deterministic number of loops C , the total number of loops that the max main algorithms may run is a random variable whose value cannot exceed C . max The next steps of the proof are the following: • We show that in each loop, with high probability, the virtual algorithm proceeds as an approximate policy iteration algorithm with a bounded ℓ error in the approximate Q-function. Thus the virtual ∞ algorithm produces a good policy at the end of each loop. Then, since by Lemma 5.1, we have at most e 1 + τ 1 1 C := d log(1 + ) + log(1 + ) (B.4) max e 1 τ τ λ − (cid:18) (cid:19) loops, with a union bound, we know that with high probability, the virtual algorithm produces a good policy in all the loops. • We show that due to the coupling argument, the output parameter vector in the main and the virtual algorithms, i.e., w and w in the final loop are the same. This leads to the conclusion that with K 1 K 1 − − the same initial state ρ, the value of the outputs of the main algorithm and the virtual algorithm are close, and thus the main algeorithm also outputs a good policy. We prove these two points in Sections B.2 and B.3, respectively. 19
B.2 Analysis of the virtual algorithm Throughout this section, we will consider a fixed loop of the virtual algorithm, say the ℓ-th loop. We assume that at the beginning of this loop, the virtual algorithm has a core set . Notice that is a random variable ℓ ℓ C C that only depends on the randomness of the first ℓ 1 loops. In this section, we will first condition on the − randomness of all the first ℓ 1 loops and only consider the randomness of the ℓ-th loop. Thus we will first − treat as a deterministic quantity. For simplicity, we write := . ℓ ℓ C C C Consider the k-th iteration of a particular loop of the virtual algorithm with core set . We would like to C bound kQ k −1 − Q πe k−1k∞. First, we have the following lemma for the F1-score of the Q-function for any element in the core set. To simplify notation, in this lemma, we omit the subscript and use π to denote a policy theat we run rollout with in an arbitrary iteration of the virtual algorithm. Lemma B.1. Let π be a policy that we run rollout with in an iteration of the virtual algorithm. Then, for any element z and any θ > 0, we have with probability at least 1 2 exp( 2θ2(1 γ)2m), ∈ C − − − γn+1 z Q (z , z ) + θ. (B.5) q π s a | − | ≤ 1 γ − Proof. By the definition of Q (z , z ): π s a ∞ Q (z , z ) = E γtr(s , a ) s = z , a = z , π s a st+1 ∼P ( ·|st,at),at+1 ∼π( ·|st+1) " t t | 0 s 0 a # t=0 X and define the n-step truncated Q-function: n Qn(z , z ) = E γtr(s , a ) s = z , a = z . π s a st+1 ∼P ( ·|st,at),at+1 ∼π( ·|st+1) " t t | 0 s 0 a # t=0 X Then we have Qn(s, a) Q (s, a) γn+1 . Moreover, the Q-function estimate z is an average of m | π − π | ≤ 1 γ q independent and unbiased estimates of Qn(− s, a), which are all bounded in [0, 1/(1 γ)]. By Hoeffding’s π − inequality we have with probability at least 1 2 exp( 2θ2(1 γ)2m), z Qn(s, a) θ, which completes − − − | q − π | ≤ the proof. By a union bound over the elements in the core set, we know that |C| γn+1 P ∀ z ∈ C, |z q − Q πe k−1(z s, z a) | ≤ 1 γ + θ ≥ 1 − 2C max exp( −2θ2(1 − γ)2m). (B.6) (cid:18) − (cid:19) The following lemma provides a bound on |Q k −1(s, a) − Q πe k−1(s, a) |, ∀ (s, a) such that φ(s, a) ∈ H. Lemma B.2. Suppose that Assumption 3.3 holds. Then, with probability at least e 1 2C exp( 2θ2(1 γ)2m), max − − − for any (s, a) pair such that φ(s, a) , we have ∈ H γn+1 |Q k −1(s, a) − Q πe k−1(s, a) | ≤ b√λτ + ǫ + 1 γ + θ τ C max + ǫ := η. (B.7) − (cid:0) (cid:1)p e 20
We prove this lemma in Appendix C. Since when φ(s, a) ∈/ H, Q k −1(s, a) = Q πe k−1(s, a), we know t ph ra ot bk aQ bik li− ty1( as t, la e) as− t Q πe k−1 (s, a) k∞ ≤ η. With another union bound ove er the K iterations, we know that with e 1 2KC exp( 2θ2(1 γ)2m), max − − − the virtual algorithm is an approximate policy iteration algorithm with ℓ bound η for the approximation ∞ error on the Q-functions. We use the following results for API, which is a direct consequence of the results in Munos [2003], Farahmand et al. [2010], and is also stated in Lattimore et al. [2020]. Lemma B.3. Suppose that we run K approximate policy iterations and generate a sequence of policies π , π , . . . , π . Suppose that for every k = 1, 2, . . . , K, in the k-th iteration, we obtain a function Q 0 1 K k 1 − such that, Q Q η, and choose π to be greedy with respect to Q . Then k k −1 − πk−1k∞ ≤ k k −1 e e 2η γK e Q Q + . k ∗ − πK k∞ ≤ 1 γ 1 γ − − According to Lemma B.3, 2η γK 2 − kQ ∗ − Q πe K−2k∞ ≤ 1 γ + 1 γ . (B.8) − − Then, since kQ πe K−2 − Q K −2 k∞ ≤ η, we know that e 3η γK −2 Q Q + . (B.9) ∗ K 2 k − − k∞ ≤ 1 γ 1 γ − − The following lemma translates the gap in Qe-functions to the gap in value. Lemma B.4. [Singh and Yee, 1994] Let π be greedy with respect to a function Q. Then for any state s, 2 V (s) V (s) Q Q . ∗ π ∗ − ≤ 1 γ k − k∞ − Since π is greedy with respect to Q , we know that K 1 K 2 − − e V ∗(ρ) − Ve πe K−1 (ρ) ≤ (1 6η γ)2 + (2 1γK γ− )2 2 . (B.10) − − We notice that this result is obtained by conditioning on all the previous ℓ 1 loops and only consider the − randomness of the ℓ-th loop. More specifically, given any core set at the beginning of the ℓ-th loop, we ℓ C have 6η 2γK 2 P V ∗(ρ) − V πe K−1 (ρ) ≤ (1 γ)2 + (1 γ− )2 | Cℓ ≥ 1 − 2KC max exp( −2θ2(1 − γ)2m). (cid:18) − − (cid:19) 21
By law of total probability we have 6η 2γK 2 P V ∗(ρ) − V πe K−1 (ρ) ≤ (1 γ)2 + (1 γ− )2 (cid:18) − − (cid:19) 6η 2γK 2 = P V ∗(ρ) − V πe K−1 (ρ) ≤ (1 γ)2 + (1 γ− )2 | Cℓ P ( Cℓ) XCℓ (cid:18) − − (cid:19) 1 2KC exp( 2θ2(1 γ)2m) P ( ) max ℓ ≥ − − − C XCℓ =1 2KC exp( 2θ2(1 γ)2m). max − − − With another union bound over the C loops of the virtual algorithm, we know that with probability at max least 1 2KC2 exp( 2θ2(1 γ)2m), (B.11) − max − − Eq. (B.10) holds for all the loops. We call this event in the following. 1 E B.3 Analysis of the main algorithm We now move to the analysis of the main algorithm. Throughout this section, when we mention the final loop, we mean the final loop of the main algorithm, which may not be the final loop of the virtual algorithm. We have the following result. Lemma B.5. In the final loop of the main algorithm, all the rollout trajectories in the virtual algorithm are exactly the same as those in the main algorithm, and therefore w = w for all 1 k K. k k ≤ ≤ Proof. We notice that since we only consider the final loop, in any iteration, for any state s in all the rollout e trajectories in the main algorithm, and all action a , φ(s, a) . In the first iteration, since π = π , 0 0 ∈ A ∈ H and the simulators are coupled, we know that all the rollout trajectories are the same between the main algorithm and the virtual algorithm, and as a result, all the Q-function estimates are the same, and theus w = w . If we have w = w , we know that by the definition in (B.2), the policies π and π always take 1 1 k k k k the same action given s if for all a , φ(s, a) . Again using the fact that the simulators are coupled, ∈ A ∈ H the rolleout trajectories by π eand π are also the same between the main algorithm and the viretual algorithm, k k and thus w = w . k+1 k+1 e Since φ(s, a) 1 for all s, a, we can verify that if we set τ 1, then after adding a state-action k ek2 ≤ ≥ pair s, a to the core set, then its feature vector φ(s, a) stays in the good set . Recall that in the core set H initialization stage of Algorithm 2, if for an action a , φ(ρ, a) is not in , we add ρ, a to . Thus, after ∈ A H C the core set initialization stage, we have φ(ρ, a) for all a. Thus π (ρ) = π (ρ) := a . Moreover, K 1 K 1 ρ ∈ H − − according to Lemma B.2, we also know that when happens, 1 E e |V πe K−1 (ρ) − w K⊤ φ(ρ, a ρ) | = |Q πe K−1 (ρ, a ρ) − w K⊤ φ(ρ, a ρ) | ≤ η. (B.12) In the following, we bound the difference of the values of the output policy of the main algorithm π and e e K 1 − the output policy of the virtual algorithm π in the final loop of the main algorithm, i.e., V (ρ) K −1 | πK−1 − e 22
V πe K−1 (ρ) |. To do this, we use another auxiliary virtual policy iteration algorithm, which we call virtual- 2 in the following. Virtual-2 is similar to the virtual policy iteration algorithm in Appendix B.1. The simulator of virtual-2 is coupled with the virtual algorithm, and virtual-2 also uses the same initial policy π := π as the main algorithm. Virtual-2 also uses Monte-Carlo rollouts with the simulator and obtains the 0 0 estimated Q-function values q , and the linear regression coefficients are computed in the same way as (B.1), C ib.e., w = (Φ Φ + λI) 1Φ q . The virtual-2 algorithm also conducts uncertainty check in the rollout k ⊤ − ⊤ C C C C subroutine. Similar to the vibrtual algorithm, when it identifies an uncertain state-action pair, it records the pair abnd keeps running the rolloubt process. At the end of each loop, the virtual-2 algorithm still adds the first recorded element to the core set and discard other recorded elements. The only difference is that in virtual-2, we choose the virtual Q-function to be Q (s, a) := w φ(s, a) for all (s, a) . Using the same k −1 k⊤ ∈ S × A arguments in Appendix B.2, we know that with probability at least 1 2KC2 exp( 2θ2(1 γ)2m), for − max − − all the loops and all the policy iteration sbteps in every loobp, we have |Q k −1(s, a) − Q πb k−1(s, a) | ≤ η for all (s, a) such that φ(s, a) . We call this event . Since the simulators of virtual-2 is also coupled with 2 ∈ H E that of the main algorithm, by the same argument as in Lemma B.5, wbe know that in the last iteration of the final loop of the main algorithm, we have π = π and w = w . We also know that when event K 1 K 1 K K 2 − − E happens, in the last iteration of the all the loops of virtual-2, b b |V πb K−1 (ρ) − w K⊤ φ(ρ, a ρ) | ≤ η. (B.13) Therefore, when both events and happen, combining (B.12) and (B.13), and using the fact that w = 1 2 K E E b w = w , we know that K K e b |V πK−1 (ρ) − V πe K−1 (ρ) | = |V πb K−1 (ρ) − V πe K−1 (ρ) | ≤|V πb K−1 (ρ) − w K⊤ φ(ρ, a ρ) | + |w K⊤ φ(ρ, a ρ) − w K⊤ φ(ρ, a ρ) | + |w K⊤ φ(ρ, a ρ) − V πe K−1 (ρ) | η + 0 + η = 2η. ≤ b b e e Combining this fact with (B.10) and using union bound, we know that with probability at least 1 4KC2 exp( 2θ2(1 γ)2m), (B.14) − max − − with C defined as in (B.4), we have max 8η 2γK 2 − V (ρ) V (ρ) + . (B.15) ∗ − πK−1 ≤ (1 γ)2 (1 γ)2 − − Finally, we choose the appropriate parameters. Note that we would like to ensure that the success probability in Eq. (B.14) is at least 1 δ and at the same time, the sub-optimality (right hand side of − Eq. (B.15)) to be as small as possible. Suppose that Assumption 3.2 holds, i.e, ǫ = 0 in (B.7). It can be verified that by choosing τ = 1, λ = κ2 1( 01 2− 4bγ 2)4 , n = 1 −3 γ log( 4(1+lo κg (( 11 −+ γλ )−1)d) ), θ = 64√d(1κ +(1 l− ogγ () 12 +λ−1)) , K = 2 + 2 log( 3 ), m = 4096 d(1+log(1+λ−1)) log( 8Kd(1+log(1+λ−1)) ), we can ensure that the error 1 γ κ(1 γ) κ2(1 γ)6 δ probability −is at most− 1 δ and V (ρ) V − (ρ) κ. Suppose that Assumption 3.3 holds. It can − ∗ − πK−1 ≤ be verified that by choosing τ = 1, λ = ǫ2d , n = 1 log( 1 ), θ = ǫ, K = 2 + 1 log 1 , b2 1 γ ǫ(1 γ) 1 γ ǫ√d m = 1 log( 8Kd(1+log(1+λ−1)) ), we can ensure tha− t with pro− bability at least 1 δ, − (cid:0) (cid:1) ǫ2(1 γ)2 δ − − 74ǫ√d V (ρ) V (ρ) (1 + log(1 + λ 1)). ∗ − πK−1 ≤ (1 γ)2 − − 23
C Proof of Lemma B.2 To simplify notation, we write π := π , Q( , ) := Q ( , ), and w = w in this proof. According to k 1 k 1 k − · · − · · Eq. (B.6), with probability at least 1 2C exp( 2θ2(1 γ)2m), max − − − e e e e e γn+1 z Q (z , z ) + θ q π s a | − | ≤ 1 γ − holds for all z . We condition on this event in the following derivation. Suppose that Assumption 3.3 ∈ C holds. We know that there exists w Rd with w b such that for any s, a, π π 2 ∈ k k ≤ Q (s, a) w φ(s, a) ǫ. | π − π⊤ | ≤ Let ξ := q Φ w . Then we have π C − C γn+1 e ξ ǫ + + θ. (C.1) k k∞ ≤ 1 γ − Suppose that for a state-action pair s, a, the feature vector φ := φ(s, a) , with defined in Defini- ∈ H H tion 4.1. Then we have Q(s, a) Q (s, a) φ w φ w + ǫ π ⊤ ⊤ π | − | ≤ | − | = φ (Φ Φ + λI) 1Φ (Φ w + ξ) φ w + ǫ ⊤ ⊤ − ⊤ π ⊤ π e | e C C C C − | φ I (Φ Φ + λI) 1Φ Φ w + φ (Φ Φ + λI) 1Φ ξ +ǫ. (C.2) ⊤ ⊤ − ⊤ π ⊤ ⊤ − ⊤ ≤ | − C C C C | | C C C | (cid:0) E1 (cid:1) E2 | {z } | {z } We then bound E and E in (C.2). Similar to Appendix A, let Φ Φ + λI := V ΛV be the eigendecom- 1 2 ⊤ ⊤ C C position of Φ Φ + λI with Λ = diag(λ , . . . , λ ) and V being an orthonormal matrix. Notice that for all ⊤ 1 d C C i, λ λ. Let α = V φ. Then for E , we have i ⊤ 1 ≥ E = φ V I Λ 1(Λ λI) V w = λ φ V Λ 1V w 1 ⊤ − ⊤ π ⊤ − ⊤ π | − − | | | (cid:0) d (cid:1) α2 λb α Λ 1 = λb i ≤ k ⊤ − k2 v λ2 u i=1 i uX t d α2 b√λ i , (C.3) ≤ v λ u i=1 i uX t where for the first inequality we use Cauchy-Schwarz inequality and the assumption that w b, and k πk−1k2 ≤ for the second inequality we use the fact that λ λ. On the other hand, since we know that φ , we i ≥ ∈ H know that α Λ 1α τ , i.e., d α2λ 1 τ . Combining this fact with (C.3), we obtain ⊤ − ≤ i=1 i −i ≤ P E b√λτ . (C.4) 1 ≤ 24
We now bound E . According to Ho¨lder’s inequality, we have 2 E φ (Φ Φ + λI) 1Φ ξ 2 ⊤ ⊤ − ⊤ 1 ≤ k C C C k k k∞ φ (Φ Φ + λI) 1Φ ξ ⊤ ⊤ − ⊤ 2 ≤ k C C C k k k∞ |C| = φ (Φ Φ + λI) 1Φ Φ (Φ Φp + λI) 1φ 1/2 ξ ⊤ ⊤ − ⊤ ⊤ − C C C C C C k k∞ |C| = (cid:0)α Λ 1(Λ λI)Λ 1α 1/2 ξ (cid:1) p ⊤ − − − k k∞ |C| (cid:0) d (cid:1) p λ λ = α2 i − ξ v u i=1 i λ2 i k k∞ |C| uX p t γn+1 (ǫ + + θ) τ C , (C.5) max ≤ 1 γ − p where in the last inequality we use the facts that d α2λ 1 τ , Eq. (C.1), and Lemma 5.1. We can then i=1 i −i ≤ complete the proof by combining (C.4) and (C.5). P D Proof of Theorem 5.2 First, we state a general result in Szepesva´ri [2021] on POLITEX. Notice that in this result, we consider an arbitrary sequence of approximate Q-functions Q , k = 0, . . . , K 1, which do not have to take the form k − of (4.3). Lemma D.1 (Szepesva´ri [2021]). Given an initial policy π and a sequence of functions Q : 0 k S × A 7→ [0, (1 γ) 1], k = 0, . . . , K 1, construct a sequence of policies π , . . . , π according to (4.4) with − 1 K 1 − − − α = (1 − γ) 2 log K( |A|) , then, for any s ∈ S, the mixture policy π K satisfies q 1 2 log( ) 2 max Q Q V ∗(s) − V πK (s) ≤ (1 γ)2 K|A| + 0 ≤k ≤K 1−1 k γ k − πkk∞ . r − − We then consider a virtual POLITEX algorithm. Similar to the vanilla policy iteration algorithm, in the virtual POLITEX algorithm, we begin with π 0 := π 0. In the k-th iteration, we run Monte Carlo rollout with policy π , and obtain the estimates of the Q-function values q . We then compute the weight vector k 1 − C e w = (Φ Φ + λI) 1Φ q , e k ⊤ − e⊤ C C C C and according to Lemma B.2, for any θ > 0, with probability at least 1 2C exp( 2θ2(1 γ)2m), for e e − max − − all (s, a) such that φ(s, a) , ∈ H γn+1 |w k⊤φ(s, a) − Q πe k−1(s, a) | ≤ b√λτ + ǫ + 1 γ + θ τ C max + ǫ := η. (D.1) − (cid:0) (cid:1)p Then we define thee virtual Q-function as Q k 1(s, a) := Π [0,(1 −γ)−1](w k⊤φ(s, a)), φ(s, a) ∈ H, − (Q πe k−1(s, a), φ(s, a) ∈/ H, e e 25
assuming we have access to the true Q-function Q πe k−1(s, a) when φ(s, a) ∈/ H. We let the policy of the (k + 1)-th iteration π be k k 1 e − π (a s) exp α Q (s, a) . (D.2) k k 1 | ∝  −  j=1 X e  e  Since we always have Q πe k−1(s, a) ∈ [0, (1 − γ) −1], the clipping at 0 and (1 − γ) −1 can only improve the F1-score of the estimation of the Q-function. Therefore, we know that with probability at least 1 − 2C max exp( −2θ2(1 −γ)2m), we have kQ k −1 −Q πe k−1k∞ ≤ η. Then, by taking a union bound over the K it- erations and using the result in Lemma D.1, we know that with probability at least 1 2KC exp( 2θ2(1 max − − − γ)2m), for any s , the virtual POLITe EX algorithm satisfies ∈ S 1 2 log( ) 2η V ∗(s) − V πe K (s) ≤ (1 γ)2 K|A| + 1 γ , (D.3) r − − where π is the mixture policy of π , . . . , π . Using another union bound over the C loops, we know K 0 K 1 max − that with probability at least 1 2KC2 exp( 2θ2(1 γ)2m), (D.3) holds for all the loops. We call this − max − − event e in the following. e e 1 E We then consider the virtual-2 POLITEX algorithm. Similar to LSPI, the virtual-2 algorithm begins with π := π . In the k-th iteration, we run Monte Carlo rollout with policy π , and obtain the estimates of the 0 0 k 1 − Q-function values q . We then compute the weight vector C b b w = (Φ Φ + λI) 1Φ q , b k ⊤ − ⊤ C C C C and according to Lemma B.2, for any θ > 0, with probability at least 1 2C exp( 2θ2(1 γ)2m), for b b max − − − all (s, a) such that φ(s, a) , ∈ H |w k⊤φ(s, a) − Q πb k−1(s, a) | ≤ η, (D.4) where η is defined as in (D.1). We also note that in the rollout process of the virtual-2 algorithm, we do not b conduct the uncertainty check, i.e., we do not check whether the features are in the good set . By union H bound, we know that with probability at least 1 2KC2 exp( 2θ2(1 γ)2m), (D.4) holds for all the K − max − − iterations of all the C loops. We call this event in the following. In the virtual-2 algorithm, we define max 2 E the approximate Q-function in the same way as the main algorithm, i.e., we define Q k 1(s, a) := Π [0,(1 γ)−1](w k⊤φ(s, a)), − − and we let the policy of the (k + 1b)-th iteration be b k 1 − π (a s) exp α Q (s, a) . (D.5) k k 1 | ∝  −  j=1 X b  b  We still let the simulators of all the algorithms be coupled in the same way described as in Appendix B.1. In addition, we also let the agent in the main algorithm be coupled with the virtual and virtual-2 algorithm. 26
Take the main algorithm and the virtual algorithm as an example. Recall that in the k-th iteration of a particular loop, the main algorithm and the virtual algorithm use rollout policies π and π , respectively. k 1 k 1 − − In the CONFIDENTROLLOUT subroutine, the agent needs to sample actions according to the policies given a state. Suppose that in the N -th time that the agent needs to take an action, the main alegorithm is at state s and the virtual algorithm is at state s . If the two states are the same, i.e., s = s and main virtual main virtual two distributions of actions given this state are also the same, i.e., π ( s ) = π ( s ), then k 1 main k 1 virtual − ·| − ·| the actions that the agent samples in the main algorithm and the virtual algorithm are also the same. This means that the main algorithm samples a π ( s ) and the virtual algorithme samples a main k 1 main virtual ∼ − ·| ∼ π ( s ), and with probability 1, a = a . Otherwise, when s = s or π ( s ) = k 1 virtual main virtual main virtual k 1 main − ·| 6 − ·| 6 π ( s ), the main algorithm and the virtual algorithm samples a new action independently. The main k 1 virtual − ·| algorithm and the virtual-2 algorithm are coupled in the same way. We note that using the same argument as ien Lemma B.5, for the final loop of the main algorithm, all the rollout trajectories of the main, virtual, and virtual-2 algorithms are the same, which implies that w = w = w for all 1 k K. This also implies k k k ≤ ≤ that in the final loop of the main algorithm, all the policies in the K iterations are the same between the main and the virtual-2 algorithm, i.e., π = π , 0 k K 1. Meoreovber, for any state s such that φ(s, a) k k ≤ ≤ − ∈ H for all a , we have π ( s) = π( s) = π ( s). Since the initial state ρ satisfies the condition that k k ∈ A ·| ·| ·| φ(ρ, a) for all a , we have π b( ρ) = π( ρ) = π ( ρ). k k ∈ H ∈ A ·| ·| ·| Let π be the policy that is unifoermly chosben from π , . . . , π in the virtual-2 algorithm in the final K 0 K 1 − loop of the main algorithm, and π be the poleicy that isb uniformly chosen from π , . . . , π in the final K 0 K 1 − loop of tbhe main algorithm. Then we have b b K 1 1 − |V πb K (ρ) − V πK (ρ) | = (cid:12) K (V πb k (ρ) − V πk (ρ)) (cid:12) = 0, (D.6) (cid:12) Xk=0 (cid:12) (cid:12) (cid:12) and when events and happen, (cid:12) (cid:12) 1 2 (cid:12) (cid:12) E E V (ρ) V (ρ) | πb K − πe K | K 1 1 − ≤ K |V πb k (ρ) − V πe k (ρ) | k=0 X K 1 1 − = K (cid:12) (π k(a |ρ)Q πb k (ρ, a) − π k(a |ρ)Q πe k (ρ, a)) (cid:12) Xk=0 (cid:12) (cid:12)a X∈A (cid:12) (cid:12) 1 K −1 (cid:12) b e (cid:12) ≤ K (cid:12) π k(a |ρ) Q πb k (ρ, a) − Q πe k (ρ, a)) (cid:12) k=0 a X X∈A (cid:12) (cid:12) K 1 (cid:12) (cid:12) 1 − ≤ K π k(a |ρ) Q πb k (ρ, a) − w k⊤φ(ρ, a) + w k⊤φ(ρ, a) − w k⊤φ(ρ, a) + w k⊤φ(ρ, a) − Q πe k (ρ, a)) Xk=0 a X∈A (cid:12) (cid:12) (cid:12) (cid:12) 1 K −1 (cid:12) b b e e (cid:12) ≤ K π k(a |ρ) |Q πb k (ρ, a) − w k⊤φ(ρ, a) | + |w k⊤φ(ρ, a) − w k⊤φ(ρ, a) | + |w k⊤φ(ρ, a) − Q πe k (ρ, a)) | Xk=0 a X∈A (cid:16) (cid:17) 1 K −1 b b e e π (a ρ)(η + 0 + η) = 2η. (D.7) k ≤ K | k=0 a X X∈A 27
By combining (D.3), (D.6), and (D.7), and using a union bound, we obtain that with probability at least 1 4KC2 exp( 2θ2(1 γ)2m), − max − − 1 2 log( ) 4η V ∗(ρ) − V πK (ρ) ≤ (1 γ)2 K|A| + 1 γ . (D.8) r − − Now we choose appropriate parameters to obtain the final result. When Assumption 3.2 holds, i.e., ǫ = 0, one can verify that when we choose τ = 1, λ = (1 2− 5γ 6) b2 2κ2 , K = 3 κ2 2l (o 1g( | γA )4|) , n = 1 1 γ log( 32√d(1 (+ 1 lo γg )( 21 κ+λ−1)) ), θ = 32√d(1( +1 − loγ g) (κ 1+λ−1)) , and m = 1024d( (1 1+l γog )4(1 κ+ 2 λ−1)) log( 8Kd(1− +lo δg(1+λ−1))− ), we can ensu− re that with − probability at least 1 δ, V (ρ) V (s) κ. When Assumption 3.3 holds, one can verify that when we choose τ = 1, λ = ǫ b2 2− d , K =∗ ǫ2 2l do (g− 1( |A γ)|π ) 2K , θ =≤ ǫ, n = 1 1 γ log( ǫ(11 γ) ), and m = ǫ2(11 γ)2 log( 8Kd(1+lo δg(1+λ−1)) ), we can ensure that with probability− at least 1 δ, − − − − 42ǫ√d V (ρ) V (ρ) (1 + log(1 + λ 1)). ∗ − πK ≤ 1 γ − − E Random initial state We have shown that with a deterministic initial state ρ, our algorithm can learn a good policy. In fact, if the initial state is random, and the agent is allowed to sample from a distribution of the initial state, denoted by ρ in this section, then we can use a simple reduction to show that our algorithm can still learn a good policy. In this case, the optimality gap is defined as the difference between the expected value of the optimal policy and the learned policy, where the expectation is taken over the initial state distribution, i.e., we hope to guarantee that E [V (s) V (s)] is small. s ρ ∗ π ∼ − The reduction argument works as follows. First, we add an auxiliary state s to the state space init S and assume that the algorithm starts from s . From s and any action a , we let the distribution of init init ∈ A the next state be ρ ∆ , i.e., P ( s , a) = ρ. We also let r(s , a) = 0. Then, for any policy π, we init init have E [V (s)] =∈ 1 VS (s ). A· s| for the features, for any (s, a) , we add an extra 0 as the last dimenss io∼ nρ ofπ the featuγ reπ vei cn tit or, i.e., we use φ+(s, a) = [φ(s, a) ∈ 0]S × A Rd+1. For any a , we let ⊤ ⊤ ∈ ∈ A φ+(s , a) = [0 0 1] Rd+1. Note that this does not affect linear realizability except a change in the init ⊤ · · · ∈ upper bound on the ℓ norm of the linear coefficients. Suppose that Asumption 3.2 holds. Suppose that in the 2 original MDP, we have Q (s, a) = w φ(s, a) with w Rd. Let us define w+ = [w V (s )] Rd+1. π π⊤ π ∈ π π⊤ π init ⊤ ∈ Then, for any s = s , we still have Q (s, a) = (w+) φ+(s, a) since the last coordinate of φ+(s, a) is 6 init π π ⊤ zero. For s , we have Q (s , a) = V (s ) = (w+) φ+(s, a). The only difference is that we now have init π init π init π ⊤ w+ b2 + ( γ )2 since we always have 0 V (s ) γ . k π k2 ≤ 1 γ ≤ π init ≤ 1 γ − − Then thqe problem reduces to the deterministic initial state case with initial state s init. In the first step of the algorithm, we let = (s , a, φ+(s , a), none) . During the algorithm, to run rollout from any core init init C { } set element z with z , we can use the current version of Algorithm 1. To run rollout from (s , a), s init ∈ S we simply sample from ρ as the first “next state” and then use the simulator to keep running the following trajectory of the rollout process. 28
