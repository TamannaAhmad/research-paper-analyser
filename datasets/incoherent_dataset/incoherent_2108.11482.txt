ETA Prediction with Graph Neural Networks in Google Maps Austin Derrow-Pinion1, Jennifer She1, David Wong2∗, Oliver Lange3, Todd Hester4∗, Luis Perez5∗, Marc Nunkesser3, Seongjae Lee3, Xueying Guo3, Brett Wiltshire1, Peter W. Battaglia1, Vishal Gupta1, Ang Li1, Zhongwen Xu6∗, Alvaro Sanchez-Gonzalez1, Yujia Li1 and Petar Veličković1 1DeepMind 2Waymo 3Google 4Amazon 5Facebook AI 6Sea AI Lab ∗work done while at DeepMind {derrowap,jenshe,wongda,petarv}@google.com ABSTRACT Travel-time prediction constitutes a task of high importance in transportation networks, with web mapping services like Google Maps regularly serving vast quantities of travel time queries from users and enterprises alike. Further, such a task requires accounting for complex spatiotemporal interactions (modelling both the topo- logical properties of the road network and anticipating events—such as rush hours—that may occur in the future). Hence, it is an ideal target for graph representation learning at scale. Here we present a graph neural network estimator for estimated time of arrival (ETA) which we have deployed in production at Google Maps. While our main architecture consists of standard GNN building blocks, we further detail the usage of training schedule methods such as Meta- Figure 1: Google Maps estimated time-of-arrival (ETA) pre- Gradients in order to make our model robust and production-ready. diction improvements for several world regions, when using We also provide prescriptive studies: ablating on various architec- our deployed graph neural network-based estimator. Num- tural decisions and training regimes, and qualitative analyses on bers represent relative reduction in negative ETA outcomes real-world situations where our model provides a competitive edge. compared to the prior approach used in production. A neg- Our GNN proved powerful when deployed, significantly reducing ative ETA outcome occurs when the ETA error from the ob- negative ETA outcomes in several regions compared to the previous served travel duration is over some threshold and acts as a production baseline (40+% in cities like Sydney). measure of F1-score. CCS CONCEPTS are used not only by everyday users finding the (optimal) routes • Applied computing → Transportation. between waypoints, but also by enterprises which rely on accurate mapping features (such as food delivery services, which may seek KEYWORDS to provide accurate and optimised delivery time estimates). One Graph neural networks, MetaGradients, Google Maps critical task in this context is expected time of arrival (ETA) pre- diction: given current conditions on the road network, and for a ACM Reference Format: Austin Derrow-Pinion1, Jennifer She1, David Wong2∗, Oliver Lange3, Todd particular candidate route between waypoints, predict the expected Hester4∗, Luis Perez5∗, Marc Nunkesser3, Seongjae Lee3, Xueying Guo3, time of travel along this route. Brett Wiltshire1, Peter W. Battaglia1, Vishal Gupta1, Ang Li1, Zhongwen The significance of such a predictor is clear: accurate ETA pre- Xu6∗, Alvaro Sanchez-Gonzalez1, Yujia Li1 and Petar Veličković1. 2021. ETA dictions allow traffic participants to make more informed decisions, Prediction with Graph Neural Networks in Google Maps. In Proceedings potentially avoiding congested areas and minimising overall time of the 30th ACM International Conference on Information and Knowledge spent in traffic. It is important to note that powerful ETA predictors Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. need to meaningfully reason about conditions which take place ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3481916 in the future, and may not be directly obvious from current road state. As a simple example, the number of traffic participants may 1 INTRODUCTION substantially increase during daily rush-hour periods, when most Web mapping services such as Google Maps are invaluable tools people commute to and from the workplace—but in many cases for many users for interactively navigating areas of the Earth— such situations may be more subtle. Further, there is a wealth of especially so in metropolitan areas. On a daily level, such products historical travel data available which may be used to support detec- Permission to make digital or hard copies of part or all of this work for personal or tion and exploitation on such subtleties. As such, this problem is classroom use is granted without fee provided that copies are not made or distributed substantially amenable to machine learning approaches. for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. As the road network is naturally modelled by a graph of road For all other uses, contact the owner/author(s). segments and intersections, ETA prediction is amenable to graph CIKM ’21, November 1–5, 2021, Virtual Event, QLD, Australia representation learning [1, 2, 10] approaches, particularly graph © AC2 M021 ISC Bo Npy 97ri 8g -h 1t -4h 5e 0l 3d -8b 4y 4t 6h -e 9/o 2w 1/n 1e 1r ./author(s). neural networks (GNNs) [8, 15, 25]. Here we present our graph https://doi.org/10.1145/3459637.3481916 neural network model for ETA prediction, which we deployed in 1202 guA 52 ]GL.sc[ 1v28411.8012:viXra
production at Google Maps, observing significant reductions in 2 RELATED WORK negative ETA outcomes across all trips worldwide—above 40% in We outline several related works that either directly inspire the cities like Sydney—compared to the previous production baseline. building blocks of our proposed GNN architecture and pipeline, or See Figure 1 for improvements in additional regions. A negative sit more broadly in the space of GNNs for traffic forecasting. ETA outcome occurs when the ETA error from the observed travel duration is over some threshold and acts as a measure of F1-score. Graph representation learning. Being a graph neural network, our The task of arrival time estimation can be described as providing work directly builds upon the previous years of progress in graph an estimated travel time given a user-provided starting location and representation learning. In particular, our model is fundamentally a route suggested by a prior system. This problem is difficult because based on the Graph Network framework [1] and follows the encode- it needs to take into account both spatial information (captured process-decode [11] paradigm. This allows us to align better with within the road network) as well as temporal information (the the iterative nature of traffic computations, as well as pathfinding evolution of traffic conditions over time). In particular, we need to algorithms. Improving such an alignment is known to help GNNs anticipate that travel times across road segments may be affected by generalise better to a diverse distribution of graphs [27]. traffic conditions that are further away, or not immediately on the user track—potentially also accounting for “background” signals Travel-time prediction. Given its wide applicability, travel-time prediction is a problem that has historically been closely studied such as traffic light configurations. We also need to anticipate that under various settings. Earlier work modifies convolutional neural traffic conditions may have changed by the time a user traverses to networks to be mindful of the spatial properties of the trajectory a road segment further along their route. [28] and employs graph neural networks coupled with recurrent mechanisms [13]. The concurrently developed work of Yuan et Contributions. We demonstrate how a powerful graph neural al. [35] makes an important observation in this space: travel time is often largely dependent on the historical travel times in the network estimator can be constructed for the ETA prediction task, given time-of-day—an observation that we extensively use. The while following simple design principles and carefully designed loss recent CurbGAN framework [38] may also be of interest, with its functions. Such a GNN then remains stable and performant when leveraging of generative adversarial networks for evaluating urban deployed in production, providing substantial gains against prior development plans in the context of estimated travel times. production baselines which did not use the road network graph reprsentations. Beyond this, our key contributions to this problem Spatiotemporal traffic forecasting. Using GNNs to forecast traffic are as follows: conditions is historically a very active direction: most influential papers in the area of spatiotemporal graph representation learning • Innovations on how the road network data is featurised rely on traffic datasets. The spatiotemporal setup typically assumes and presented to the GNN; a graph structure over which each node maintains a time series—in • Innovations in the GNN model design and operation. While the case of road networks, such time series typically correspond the GNN itself is constructed of standard building blocks, we to historical speeds measured at regular time intervals. While our found several benefits for the training regime (such as Meta- GNN model detailed here processes static inputs, and does not Gradients [31] and semi-supervised training [16, 26]) as well directly forecast future speeds, having some estimate about future as architectural ablations (e.g. carefully tuned combinations traffic flow behaviour is likely to be meaningful for ETA prediction. of aggregators [5]) that proved particularly important for One of the first influential papers in the area was the diffusion- maintaining stability of the GNN in this regime and making convolutional recurrent neural network (DCRNN) [17]. This paper it production-ready. illustrated how explicitly accounting for the graph structure pro- • Finally, we deploy the trained GNN model in production vides a significant reduction in forecasting error over several hori- within Google Maps, observing strong real-world bene- zons. This generic blueprint of a spatial GNN component combined fits. As displayed in Figure 1, we noticed clear quantitative with a temporal component yielded several subsequent proposals, improvements in negative ETA outcomes, but besides the including STGCN [34], GaAN [37], Graph WaveNet [29], ST-GRAT on-line performance, we performed several offline ablations [19], StemGNN [3], and GMAN [39]. We refer interested readers to on various architectural and setup decisions, as well as vi- [23] for a survey of interesting problems and methodologies in the sualisations of particular traffic situations where our model area. has a clear advantage over the prior baseline. Lastly, we find it important to mention that the assumption of the time-series being aligned across nodes does not necessarily hold in practice—we may instead expect data to be provided in an On one hand, our work represents another in a series of suc- asynchronous fashion. A recent line of research on dynamic graph cessful web-scale deployed applications of graph neural networks representation learning [20, 30] tries to explicitly account for such with real-time positive user experience impact [12, 18, 32, 33]. On situations, both on the featurisation and the topological side. another, we offer ablative studies and insights into the GNN model’s predictions which simultaneously (a) elucidate the kinds of real- Graph neural networks at scale. As graph neural network method- world traffic use cases where such a model may provide a significant ologies became more widely available and scalable [4, 9, 21], this edge; and (b) provide prescriptive advice for other practitioners in also enabled a wider range of web-scale applications of graph rep- the general area of user-facing traffic recommendation. resentation learning. Perhaps the most popularised applications
concerned recommender systems, which are very naturally repre- involves sequentially using the prediction time of an earlier su- sentable as a graph-structured task: with Pinterest being one of persegment to determine the relevant fixed horizons for the next the most early adopters [18, 33]. GNNs have also been deployed supersegment, and interpolating between the prediction times of for product recommendation at Amazon [12], E-commerce applica- the fixed horizons to arrive at a travel time for the next super- tions at Alibaba [32], engagement forecasting and friend ranking segment. This process enables us to provide accurate travel time in Snapchat [22, 24], and most relevantly, they are powering traffic estimates in a scalable way, taking into account both spatial and predictions within Baidu Maps [6]. temporal information. A supersegment, S, is defined as a graph S = (𝑆, 𝐸) where each Training regimes and components. While the core components node 𝑠 ∈ 𝑆 is a road segment, and an edge 𝑒 𝑖 𝑗 ∈ 𝐸 exists if two of our architecture correspond to the Graph Network paradigm, segments 𝑠 and 𝑠 are connected (see Figure 2 for an overview). 𝑖 𝑗 ETA prediction in production invites particularly unstable training The supersegment travel time prediction task is then: given a su- conditions across many batches of queries, particularly over routes persegment S𝑡 , predict the travel time across the supersegment at of different scales. We adapted the MetaGradients [31] methodology different horizons into the future 𝑦 , 𝑦 , 𝑦 , ..., 𝑦 , where from supervised learning, which allowed us to dynamically tune ℎ1, ℎ2, ...ℎ 𝑘 correspond to the differe𝑡 nt h𝑡+ oℎ r1 izo𝑡 n+ sℎ .2 𝑡+ℎ𝑘 the learning rate during training and stabilise it across many uneven query batches, enabling a production-ready GNN model. 3.1.1 Data. The world is divided into regions that confine similar Further, there exist many patterns inherent to the road network driving behaviors and capture most trips without splitting. Data topology that may be particularly pertinent to forecasting traffic is constructed for each of these regions in order to build region- flow. Similar motifs in the road network of the same region are likely specific GNN models. For training and evaluation, each example to correspond to similar traffic dynamics—as such, automatically was collected from a traversal event across a supersegment and its discovering and compressing them is likely to be useful. To enable underlying segments. Specifically, a unique supersegment may ap- such effects, we have directly adapted established unsupervised pear in multiple examples corresponding to multiple traversals over GNN methodologies such as graph auto-encoders [16] and deep that supersegment. The actual traversal times along segments and graph infomax [26]—for both of them, we demonstrate benefits to supersegments in seconds are used as node-level and graph-level more accurate ETA predictions. labels for prediction, and each unique supersegment can appear in Additionally, different kinds of heuristical computations over the training and evaluation data multiple times, corresponding to road networks may require different ways of aggregating informa- multiple traversals. The features, describing the traffic conditions tion. For example, shortest path-finding would require optimising of the supersegments prior to the traversal event, were collected the signals over a node’s neighbourhood, whereas flow estima- backwards in time by the duration of each fixed horizon. The road tion may require summing such signals. Inspired by the principal segments are on average 50 - 100 meters long, and the superseg- neighbourhood aggregation [5] architecture, we have investigated ments contain on average about 20 road segments. Horizons we use various combinations of GNN aggregation functions, showing them are 0 seconds, 600 seconds, 1200 seconds, 1800 seconds, and 3600 to be beneficial across many modelling scenarios. seconds. In Section 4, we also investigate the trade-off of expanding supersegment graphs to include neighbouring nodes, or segments that extend beyond an immediate route. 3 METHOD In this section, we describe our travel time prediction problem in 3.1.2 Features. For this task, we utilize both real-time information the context of arrival time estimation. We also provide the technical that captures traffic patterns at the time of prediction and historical details of the GNN models that we used for this problem, including data for traffic patterns specific to the time-of-day and day-of-week their architecture and training details. The key components of our when the prediction is made. On a segment (node) level, we pro- method are models that operate on networks of route segments vide the average real-time and historical segment travel speeds and in order to leverage their structure, and predict travel times for times, as well as segment length and segment priority (eg. road multiple time horizons into the future, in order to provide end users classifications such as highways) as features. On a supersegment with more accurate ETAs. Key technical details of our GNN models (graph) level, we additionally provide real-time supersegment travel are the various stabilizing methods that are used to reduce training times as a feature. Real-time travel speeds and times are provided variance, which have critical impact on user experience. Further, for 17 2-minute windows prior to horizon 0. Historical speeds and we will demonstrate that several changes to our launched model times are provided for five 8-minute windows prior to horizon 0, provide offline improvements under specific conditions. and seven 8-minute windows after horizon 0, with the values being an average across the past 17 weeks. We additionally provide learn- able segment- and supersegment-level embedding vectors (of sizes 3.1 Problem Setup 16 and 64, respectively). This enables sharing of information when- To achieve accurate travel time estimates, we model the road net- ever the same segment appears in different supersegments, and work using supersegments, which are sequences of connected whenever the same supersegment appears in different candidate road segments that follow typical traffic routes. For a given starting routes. time, we learn the travel time of each supersegment for different fixed time horizons into the future. At serving time, the sequence of 3.1.3 Baselines. Possible baselines for this problem include (a) non- supersegments that are in a proposed route are queried sequentially parametric methods, that compute travel times by averaging speeds for increasing horizons into the future. This process specifically across segments and supersegments. (b) segment-level models, that
Figure 2: An example road network with shared traffic volume, which is partitioned into segments of interest (left). Each segment is treated as a node (middle), with adjacent segments connected by edges, thus forming a supersegment (right). Note that for extended supersegments (discussed in Section 4.1.4), extra off-route nodes may be connected to the graph. bypass the graph structure induced by supersegments. Such models the individual segment predictions, for example, can lead to an predict travel times along every segment in isolation, adding them accumulation of error. together to predict across supersegments. Prior production models The update functions, 𝜙, are all simple multilayer perceptrons used segment-level linear regression models. (MLPs). Aggregation functions 𝜌 are summations. While this is com- mon practice, there exist benefits from using multiple aggregator 3.2 Model Architecture functions like summing and averaging and concatenating their re- sults together [5]. For certain regions and horizons in offline testing, Our GNN model leverages full Graph Network (GN) blocks exactly this was true in our case as well. We will discuss the comparative as described in [1]. Specifically, a GN block is defined with three benefits of various aggregators in Section 4. “update” functions 𝜙 corresponding to edge, node, and global or supersegment-level updates, and three “aggregation” functions, 𝜌: 3.3 Model Training e 𝑘′ = 𝜙𝑒 (cid:0) e𝑘, v𝑠𝑘 , v𝑡𝑘 , u(cid:1) e¯𝑖′ = 𝜌𝑒→𝑣 (cid:0)𝐸 𝑖′(cid:1) While training our GNN models, we found it very useful to use v𝑖′ = 𝜙𝑣 (cid:0) e¯𝑖′, v𝑖, u(cid:1) e¯′ = 𝜌𝑒→𝑢 (cid:0)𝐸′(cid:1) . (1) a combination of various loss functions. This resulted in strong u′ = 𝜙𝑢 (cid:0) e¯′, v¯′, u(cid:1) v¯′ = 𝜌𝑣→𝑢 (cid:0)𝑉 ′(cid:1) serving performance when deployed in production. The primary bottleneck to deploying was the high variability of models during For each edge 𝑘, 𝜙𝑒 takes in edge features 𝑒 , source and target 𝑘 training. Variance is an issue in application because models saved node features 𝑣 and 𝑣 , supersegment features 𝑢, and outputs an 𝑠𝑘 𝑡𝑘 at different points of training will result in different end user expe- edge representation 𝑒′ . For each node 𝑖, 𝜙𝑣 takes in node features 𝑘 riences. To address this, we use a MetaOptimizer that makes use of 𝑣 𝑖 , aggregate edge representations of node 𝑖’s edges 𝑒¯ 𝑖′, superseg- MetaGradients [31] for adapting the learning rate during training, ment features 𝑢, and outputs a node representation 𝑣 ′. Finally, 𝜙𝑢 as well as applying an exponential moving average over model takes in aggregate node representations 𝑣¯′, aggregate edge repre- parameters during evaluation and serving. sentations 𝑒¯′, supersegment features 𝑢, and outputs a supersegment representation 𝑢′. 3.3.1 Losses. Although our main objective is to predict supersegment- We compose 3 GN blocks into an encode-process-decode archi- level travel time, we found that using auxiliary losses for good tecture [11], and learn a separate model for each horizon ℎ. These segment-level travel time prediction helps the final performance. blocks are defined as GN(ℎ) , GN(ℎ) , GN(ℎ) . Our final loss for a specific horizon ℎ is 𝑒𝑛𝑐 𝑝𝑟𝑜𝑐 𝑑𝑒𝑐 The encoder is first applied to the supersegment S with the previously described raw features. It produces latent representa- 𝐿 = ℓss + 𝜆sℓs + 𝜆scℓsc, (2) tions of nodes, edges and the supersegment itself. These latents with additional weight decay, where 𝜆s = 1, 𝜆sc = 0.15 and are then passed to the processor, which is applied 2 times (with s reh pa rr ee sd ep na tara tim onet se ars r) et to rau np sd fa ot re mth edes ie nr te op are ps pe rn ot pa rt ii ao tn es. pF rein da icll ty io, nth se bs ye ℓss = ∑︁𝑁 (cid:32) max{𝑓 (1 𝑖), 1}0.75 (cid:33) · L400 (cid:0)𝑦 𝑡( +𝑖) ℎ, 𝑦ˆ 𝑡( +𝑖) ℎ(cid:1) (3) applying the decoder. 𝑖=0 segT mh ee nm t,o sd ee gl mpr ee nd ti ,c ati no dns cuar me u𝑦ˆ l𝑡( a+𝑖 t) ℎ iv, 𝑦 eˆ 𝑗( s,𝑖 𝑡) e+ gℎ m, 𝑦ˆ e𝑐( n,𝑖 𝑗) t,𝑡 p+ℎ re, dw ich ti ic oh na sr . e Os nup ther e- ℓs = ∑︁ 𝑖𝑁 =0 𝑚 ∑︁ 𝑗=(𝑖 0) (cid:32) max{𝑓 𝑗(1 𝑖), 1}0.75 (cid:33) · L400 (cid:0)𝑦 𝑗( ,𝑖 𝑡) +ℎ, 𝑦ˆ 𝑗( ,𝑖 𝑡) +ℎ(cid:1) (4) supersegment level, the models predict the estimated travel time f mo or dth ele s e pn rt ei dre ictsu thp eer es se tg imm ae tn et d, 𝑦 tˆ r𝑡( a+𝑖) vℎ e. lIn timad ed ait ci ro on s, sf to hr ee sv ee gr my en no td 𝑦e ˆ 𝑗, ( ,𝑖 𝑡t )h +ℎe ℓsc = ∑︁ 𝑖𝑁 =0 𝑚 ∑︁ 𝑗=(𝑖 0) (cid:32) max{(cid:205) 𝑘𝑗 =01 𝑓 𝑘(𝑖), 1}0.75 (cid:33) · L400 (cid:0) 𝑘∑︁ =𝑗 0 𝑦 𝑘(𝑖 ,𝑡) +ℎ, 𝑦ˆ 𝑐( ,𝑖 𝑗) ,𝑡+ℎ(cid:1). as well as the cumulative estimated travel time up to and including (5) that segment 𝑦ˆ(𝑖) , which we found are helpful for representa- 𝑐,𝑗,𝑡+ℎ The provided labels and weights are 𝑦 (𝑖) and 𝑦 (𝑖) , which are su- tion learning. In production, we use the supersegment-level output 𝑡+ℎ 𝑗,𝑡+ℎ 𝑦ˆ(𝑖) instead of any of the node-level outputs. Intuitively, summing persegment and segment traversal times, and 𝑓 (𝑖) and 𝑓 (𝑖) , which 𝑡+ℎ 𝑗
# (Unique) Training # (Unique) Testing Average # Segments Average # Segments Average Segment Dataset Supsersegments Supersegments per Supersegment per Extended Supersegment Length (m) LAX 661M (16k) 126M (16k) 21.79 204.60 81.02 NYC 502M (12k) 91M (12k) 20.75 147.67 109.03 SGP 257M (5k) 54M (5k) 18.80 96.31 106.29 TYO 115M (4k) 22M (4k) 24.52 236.54 57.94 Table 1: Information on datasets, including the number of total supersegments, and unique supersegments in each training and evaluation dataset, the average number of segments in a supersegment computed across a training dataset, and the average length of segments computed across a training dataset. are supersegment and segment free flow times, an estimate of tra- where 𝛼 = 0.99. EMAs were another key component in reducing versal times when little to no traffic is present. We predict 𝑦ˆ(𝑖) in variance in saved models within training runs. 𝑡+ℎ addition to segment-level travel times in order to avoid an accumu- lation of errors over segments. L𝛿 is a Huber loss function with 4 EXPERIMENTS delta value 𝛿, which is important in being less sensitive to outliers. 4.1 Experimental Setup The losses are summed up across the segments and supersegments, 4.1.1 Datasets. Although our models were launched across re- so losses such as the supersegment-level loss is more influenced gions world-wide, we evaluate our model specifically over: New by supersegment length whereas losses like the segment-level loss York (NYC), Los Angeles (LAX), Tokyo (TYO) and Singapore (SGP), is more independent from length. We exponentially scale the Hu- finding that the results generalize to other regions (see Figure 1). ber loss down for examples with greater free flow times to better For offline evaluation, we used training data collected during Jan- control the loss’s growth. uary 2020. For online evaluation, our data was collected during We also experimented with purely unsupervised auxiliary losses, November 2020. We note that November 2020 data may be suscepti- and will discuss their trade-offs in Section 4. ble to COVID-19 traffic patterns, and hence we attempted to select 3.3.2 MetaGradients. In order to reduce the variance of the recov- regions that minimize this deviation for evaluation in order to cap- ered GNN models across epochs, we resort to a careful treatment ture performance under regular traffic conditions. The distributions of the learning rate for each step of training. Our method of choice, of the datasets and their extended versions are described in Table 1. MetaGradients [31], follows the online cross-validation paradigm 4.1.2 Baselines. We evaluate our GNN models against nonpara- and jointly optimizes the hyper-parameter 𝜂 with the model pa- metric travel times computed using real-time speeds and historical rameters 𝜃 at each training iteration. Although the original work speeds, as well as models that do not leverage graph structure: targets at supervised learning, the approach can be generalized well to supervised learning applications. MetaGradients frames the • Real-time travel times: Summation of segment-level travel optimization problem as 𝐿(𝜏, 𝜃, 𝜂), where 𝜏 is a training example, times computed using segment speeds averaged over a two 𝜃 are the model parameters, 𝜂 is the hyper-parameter (e.g., the minute window prior to the time of prediction. These are learning rate) and 𝐿 is the original loss function additionally pa- numbers computed from data. rameterized by 𝜂. For a new training example, 𝜏 ′, an underlying • Historical travel times: Summation of segment-level travel update to the model parameters 𝜃 ′ = 𝜃 + 𝑓 (𝜏, 𝜃, 𝜂) with some update times computed using segment speeds specific to hour of day function 𝑓 (eg. a gradient step) additionally parameterized by 𝜂, and day of week at the time of prediction, averaged across and a reference meta-parameter value 𝜂′, MetaGradients computes last 17 weeks. These are numbers computed from data. the update for 𝜂 using the following derivative: • DeepSets: Transform node representations using a segment- level feed-forward network followed by a sum aggregation 𝜕𝐿(𝜏 ′, 𝜃 ′, 𝜂′) = 𝜕𝐿(𝜏 ′, 𝜃 ′, 𝜂′) 𝜕𝜃 ′ , (6) over the segment-level representations before using a final 𝜕𝜂 𝜕𝜃 ′ 𝜕𝜂 feed-forward network to make graph-level predictions [36]. where the gradient w.r.t. the model parameter is multiplied by a This effectively treats supersegments as a “bag-of-segments”, factor 𝜕𝜃 ′/𝜕𝜂. Xu et al. [31] has shown that, in practice, the second ignoring their graph structure. term can be approximated using an accumulative trace, i.e., 4.1.3 Ablations of Launched Models. We ablate over the specific 𝜕𝜃 ′ 𝜕𝜃 𝜕𝐿(𝜏, 𝜃, 𝜂) elements of our launched GNNs that contribute to performance = + . (7) 𝜕𝜂 𝜕𝜂 𝜕𝜂 improvements and variance reduction. This includes learnable seg- ment and supersegment embeddings. In our application, we have successfully combined MetaGradients with the Adam optimizer [14]. Embeddings. We ablated over the embeddings defined for unique segment and supersegment IDs. We use a dimension of 16 for 3.3.3 Exponential Moving Average (EMA) of Parameters. During segment-level embeddings and a dimension of 64 for supsersegment- evaluation and serving, we use an EMA of model parameters level embeddings across all models that use embeddings. The em- 𝜃 EMA = 𝛼 · 𝜃 EMA + (1 − 𝛼) · 𝜃, (8) bedding vocabularies are region-specific, covering 99.5% of segment
Real-Time Historical DeepSets GN Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 NYC 42.31 69.60 48.03 51.80 37.33 46.68 36.84 46.55 LAX 47.58 79.31 53.40 60.22 40.71 50.06 40.32 49.94 TYO 63.56 81.01 63.76 67.63 52.15 60.07 51.64 59.86 SGP 45.09 75.83 58.25 61.39 37.56 50.81 36.79 50.67 Table 2: Offline model performance against baseline methods reported in RMSE, averaged across five random seeds. The graph network (GN) significantly outperforms all reported baselines for each region and horizon (𝑝 < 0.01 using a 𝑡-test). IDs and supersegment IDs in the corresponding datasets, and assign- graphs. This, in turn, may simplify discovery of interesting motifs ing the rest to be out-of-vocabulary (OOV), spread out across 200 within the road network topology, which may implicitly drive traffic and 20 OOV buckets for segments and supersegments respectively. dynamics. In all experiments, these losses are combined with the supervised objective (Equation 2) to assist learning. MetaGradients & EMA. We directly compare our models trained We combine two main kinds of unsupervised loss functions. with and without MetaGradients. When trained with MetaGradi- Firstly, Deep graph infomax (DGI) [26] encourages the node em- ents, we set the meta-learning rate to 0.01 and the update frequency beddings to encode global structural properties, by carefully con- to every 100 iterations. trasting the road network against an artificially corrupted version. We compare our GNN models that use EMA (with a decay factor Conversely, the graph auto-encoder (GAE) [16] leverages a link of 𝛼 = 0.99) to models that do not. prediction-style loss and is hence mindful of local topology. 4.1.4 Notable Extensions. We describe and compare extensions to the launched GNN models that show offline improvements, and 4.2 Offline Evaluation can be useful in specific settings. This includes extended superseg- For offline evaluation, we use the root precision (RMSE) ment graph data, combinations of aggregators, and unsupervised between the predicted supersegment travel times and the actual auxiliary losses. traversal times across supersegments, in seconds: meE nx tste fn od re Gd NSu Npe mrs oe dg em lsen thts a. t W ope ei rn av tees ot nig ea xte tep ner df eo drm sua pn ec re si em gmpr eo nv te s- . RMSE ss = (cid:118)(cid:117)(cid:116) 𝑁1 ∑︁𝑁 (cid:0)𝑦 𝑡( +𝑖) ℎ − 𝑦ˆ 𝑡( +𝑖) ℎ(cid:1)2. (10) Extended supersegments are larger graphs that include nodes from 𝑖=0 neighbouring segments, in addition to the original segments. These Reported values are computed using the temporally held-out test include segments from nearby, possibly connected traffic. The pre- datasets of the same regions. Other metrics such as mean absolute diction task and labels remain the same. Specifically, the goal is still error (MAE) show similar results and are thus omitted. RMSE is to predict travel times across the original segments and superseg- used to make decisions for which architectures are used in online ments within the extended supersegments. evaluations and launches, so we focus on that metric here. This enables our models to make use of additional spatial context— such as traffic congestion on a connecting route—at the expense Baseline Comparison. Table 2 shows the offline evaluation results of data storage, slower training, and inference. We note that the of our GNN models compared to the baselines, averaged over five same GNN architecture is used for both supersegment and extended random seeds. Across all studied regions and horizons, our GNN supersegment experiments, making the model storage cost remain models outperform the baselines. Compared to the closest baseline, constant. Further, we experiment with binary features that indicate DeepSets, our GNN models show an improvement of 0.12 to 0.77 whether segments are on the main route in an extended graph. RMSE. One note is that these improvements are computed over individual supersegments, and they will accumulate over entire Combinations of Aggregators. We further investigated the ben- routes—hence our improvements may quickly become more impor- efits of swapping out the default sum aggregation function with tant on a whole-route level. Across all of the considered regions and combinations of different aggregators. We combine by concatenat- horizons, the improvements of the GNN are statistically signifi- ing; for example, the combination [Min, Max] has an aggregation cant: 𝑝 < 0.01 using a 𝑡-test on the individual run results, across function for nodes of: five runs. (cid:20) (cid:21) 𝜌 𝑖𝑠→𝑢 (𝑆) = min 𝑠 𝑗 || max 𝑠 𝑗 (9) Embeddings. Table 3 studies the effect of the various learnable 𝑗 ∈𝑁 (𝑖) 𝑗 ∈𝑁 (𝑖) input embeddings, by attempting to ablate them out. Across most where 𝑁 (𝑖) are the road segments that neighbour 𝑠 𝑖 . We ablate over regions and horizons, such embeddings are useful for improving all combinations of Max, Min, SqrtN, and Sum. In the interests of RMSE. brevity, only the best-performing ones are included in Table 6. MetaGradients & EMA. Figure 3 compares training with and with- Unsupervised Auxiliary Losses. Lastly, we experimented with out applying MetaGradients and EMA decay over several seeds. incorporating unsupervised losses, which are designed to produce For brevity, we only show results for two settings (TYO Horizon 0, representations that capture the structural information of input LAX Horizon 3600); the trends generalize across different regions
GN without GN without GN without Segment and GN Segment Embeddings Supersegment Embeddings Supersegment Embeddings Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 NYC 36.87 46.59 37.03 46.59 37.04 46.61 37.55 46.86 LAX 40.39 49.96 40.48 50.03 40.38 49.96 41.11 50.33 TYO 51.76 59.90 52.02 59.98 51.72 60.00 52.63 60.60 SGP 36.84 50.60 37.29 50.83 36.98 50.52 37.41 51.25 Table 3: An ablation on the segment and supersegment learnable embedding representations used as input to the Graph Net model. Performance for each reported in RMSE. Figure 3: Validation RMSE during training, with and without MetaGradients and EMA decay, aggregated across five seeds. Both methods contribute to variance reduction. Results shown for two metro/horizon setups – same trends hold elsewhere. and horizons. Both MetaGradients and EMA contribute to lowering this space, as the potential performance improvements were not within-run and across-run variance, which are critical for inference deemed significant enough for the additional costs incurred in a in production. Further, it was shown that MetaGradients consis- production environment. tently decays the learning rate over time, which ensures stable Combinations of Aggregators. Table 6 compares model perfor- training. mance when using different aggregation functions. When compared Extended Supersegments. Table 4 compares GNNs that operate on to our production default of sum, other variants may achieve an supersegments to variants that operate on extended supersegments. improvement of up to 0.34 RMSE depending on the region and Combining extended supersegments with additional binary features horizon. Since the optimal aggregation function differs between that indicate whether a segment or an edge is part of the original regions and horizons, it is likely difficult and expensive to tune supersegment is capable of improving RMSE (by 0.01 to 0.29). In a for all regions and horizons of the world. Using all five aggrega- scenario where data storage and inference cost increases are not tions at once is a reasonable preference, and it may be beneficial to core issues, the extended supersegments may hence be a useful investigate dynamic aggregators as a future line of work. data representation. 4.3 Online Evaluation Unsupervised Auxiliary Losses. Table 5 compares the performance We compare our GNNs against baselines in an online setting in order between different unsupervised losses in terms of RMSE. Augment- to show the effectiveness of the models during serving. Both real- ing the loss function with deep graph infomax (DGI) or graph time travel times and historical travel times are used in precursor auto-encoders (GAE) by a tuned weight parameter achieves an systems and fall-back systems. For online evaluation, we use: improvement of 0.05 to 0.23 RMSE. The optimal variant and hyper- Tpa hr ua sm , ie tte isrs liw kee lr ye ed xiff pee nre sn ivt efo tor e ta uc nh er se ug ci hon loa sn sd esp fr oe rdi ac lt li eo xn ish to inri gzo ren -. RMSE track = (cid:118)(cid:117)(cid:116) 𝑁1 ∑︁𝑁 (cid:0)𝑢 (𝑖) − 𝑢ˆ(𝑖) (cid:1)2 (11) gions and each horizon. However, GN+DGI would be a reasonable 𝑖=0 option as it showed some degree of improvement in all cases. Note which is the root precision between the predicted track that there are several important design decisions involved with travel times and actual travel times, computed over all tracks within deploying DGI: e.g. deciding the readout, corruption, and discrim- a week, specifically January 15th 6:00 PM - January 22nd 2021 6:00 inator functions. We, however, did not explore many options in PM. We note that the GN and DeepSets models additionally use
Extended Supersegments Supersegments Extended Supersegments + Extra Features Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 NYC 35.85 45.91 36.11 45.71 35.64 45.65 LAX 39.51 49.01 39.67 48.99 39.22 48.79 TYO 52.59 60.72 53.24 60.85 52.57 60.64 SGP 34.89 49.51 35.29 49.48 34.73 49.50 Table 4: A comparison between different input graph formats. Travel time predictions only include road segments in the traversed track. Same embedding vocabularies are used across variants. Performance for each reported in RMSE. NYC LAX TYO SGP Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 GN 36.88 46.56 40.30 49.95 51.59 59.82 36.91 50.77 GN+DGI 36.79 46.53 40.18 49.90 51.48 59.76 36.76 50.54 GN+GAE 36.76 46.38 40.23 49.91 51.43 59.82 36.85 50.57 Table 5: Compares using different unsupervised algorithms for augmenting the Graph Net as an auxiliary loss function. Per- formance for each reported in RMSE. NYC LAX TYO SGP Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 Horizon 0 Horizon 3600 Sum 36.88 46.56 40.30 49.95 51.59 59.82 36.91 50.77 SqrtN 37.06 46.60 40.39 49.95 51.75 59.96 36.93 50.66 Mean 36.92 46.60 40.45 49.98 51.92 60.01 36.78 50.66 Min 36.98 46.57 40.49 49.91 51.83 59.95 36.98 50.81 Max 37.01 46.57 40.48 49.95 51.84 59.96 36.97 50.94 All 36.83 46.53 40.33 49.88 51.60 59.90 36.95 50.43 Table 6: Performance in RMSE using various aggregation functions. Only the individual aggregation operations and the full combination are included for brevity due to similar or worse results. Note that the optimal aggregation combination differs between regions as well as horizons. Region Real-Time Historical DeepSets GN 4.4 Analysis NYC 130.05 154.83 119.76 118.63 In this section, we provide specific examples that provide insight LAX 154.19 171.25 139.02 138.10 into scenarios where our GNN models are able to provide more TYO 211.85 250.30 182.73 181.98 accurate estimated time of arrivals compared to baselines, by better SGP 127.54 138.59 109.39 108.78 making use of spatial and temporal information. Figure 4 (left) shows the supersegment (which is part of LAX) which we will use Table 7: Online model performance against baselines re- to illustrate these qualitative findings. ported in RMSE of track travel times. Figure 4 (above) compares the predicted travel times over the supersegment to the actual travel times, over a range of departure times. The upward slopes of the GNN predicted travel times match the upward slopes of the actual travel times better than the real- a series of fallback models for segments that do not appear in su- time travel times baseline. In this setting, our GNN model is able persegments, and segments that do not have real-time or historical to detect traffic congestions more accurately than the baseline by travel time input features, whereas the same is not done for real- making use of traffic information from a larger neighborhood. time and historical travel times. Figure 4 (below) compares predicted travel times over the super- Table 7 shows the results of our GNN models compared to the segment for various horizons into the future. The predictions of our baselines in the online setting. It shows a similar pattern com- GNN for ℎ = 3600 better match the actual travel times 60 minutes pared to Table 2 with GNNs outperforming the baseline methods. into the future, compared to the predictions of our GNN for ℎ = 0. Compared to DeepSets, the strongest baseline approach, GNNs This example illustrates the value of having multiple prediction outperform by 0.05 to 1.35 RMSE in this online setting. horizons in providing accurate ETA.
Figure 4: Qualitative analysis over a supersegment in LAX (shown on the left). Above: Travel times and predictions over dif- ferent departure times. Our GNN is able to detect traffic congestions more accurately than using real-time travel times. Below: Travel times and predictions for different horizons into the future, over different route departure times. Having multiple prediction horizons is important to providing accurate ETA. 5 ENGINEERING CHALLENGES 6 CONCLUSION Caching It is challenging to meet the latency requirements of We presented how a graph neural network can be engineered to Google Maps while keeping the cost of evaluating Graph Net mod- accurately predict travel time estimates along candidate routes. Ap- els low. The path in an ETA request usually includes multiple su- plying stabilising techniques such as MetaGradients and EMA was persegments and making predictions for these supersegments on a necessary addition to make the GNNs production-ready. We de- the fly is not practical nor scalable. Approaches like CompactETA ployed our GNN model for ETA prediction in Google Maps, where [7] learn high level representations that can be queried during in- it now serves user queries worldwide. We presented offline metrics, ference time for use in a very simple MLP. To resolve this issue, online evaluations, and user studies: all showing significant quanti- we instead created a shared lookup table caching fresh predictions tative improvements of using GNNs for ETA predictions. Further, for predefined supersegments for a fixed set of horizons, which through extensive ablations on various design and featurization is periodically updated. The server fetches necessary predictions choices, we provide prescriptive advice on deploying these kinds upon a request and interpolates the predictions of adjacent hori- of models in practice. We believe our findings will be beneficial to zons to compute the estimated arrival time for each supersegment. researchers applying GNNs to any kind of transportation network We have tested an update frequency of 15 seconds and found no analysis—which is certain to remain a very important application measurable regression up to a frequency of 2 minutes, indicating area for graph representation learning as a whole. staleness is not an issue. ACKNOWLEDGMENTS Turn speeds It is common to have a multimodal speed distribution We would like to thank Paulo Estriga and Adam Cain for designing for a busy road segment heading to a split of traffic, such as an in- several of the figures featured in this paper. tersection or a highway ramp. For such cases, we used the real time and historical speed for the specific turn which the supersegment REFERENCES follows. [1] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Coverages Currently we have about 1 million predefined super- Santoro, Ryan Faulkner, et al. 2018. Relational inductive biases, deep learning, segments that cover the most common routes taken by our users. and graph networks. arXiv preprint arXiv:1806.01261 (2018). [2] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Van- The effect is that most freeways, major arterials and some popular dergheynst. 2017. Geometric deep learning: going beyond euclidean data. IEEE short-cuts in metropolitan areas are selected, whereas less busy Signal Processing Magazine 34, 4 (2017), 18–42. surface streets will not be covered. Multiple supersegments may [3] Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. 2020. Spec- cover any given road segments to account for multiple common tral Temporal Graph Neural Network for Multivariate Time-series Forecast- routes that traverse through that road segment. For less frequently ing. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran As- visited road segments, we use simpler per-segment models. sociates, Inc., 17766–17778. https://proceedings.neurips.cc/paper/2020/file/ cdf6581cb7aca4b7e19ef136c6e601a5-Paper.pdf [4] Jie Chen, Tengfei Ma, and Cao Xiao. 2018. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247
(2018). [30] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. [5] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Veličković. 2020. Inductive Representation Learning on Temporal Graphs. arXiv preprint 2020. Principal neighbourhood aggregation for graph nets. arXiv preprint arXiv:2002.07962 (2020). arXiv:2004.05718 (2020). [31] Zhongwen Xu, Hado P van Hasselt, and David Silver. 2018. Meta-gradient [6] Xiaomin Fang, Jizhou Huang, Fan Wang, Lingke Zeng, Haijin Liang, and Haifeng supervised learning. Advances in neural information processing systems 31 Wang. 2020. ConSTGAT: Contextual Spatial-Temporal Graph Attention Network (2018), 2396–2407. for Travel Time Estimation at Baidu Maps. In Proceedings of the 26th ACM SIGKDD [32] Hongxia Yang. 2019. Aligraph: A comprehensive graph neural network platform. International Conference on Knowledge Discovery & Data Mining. 2697–2705. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge [7] Kun Fu, Fanlin Meng, Jieping Ye, and Zheng Wang. 2020. CompactETA: A Fast Discovery & Data Mining. 3165–3166. Inference System for Travel Time Prediction. Association for Computing Machinery, [33] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, New York, NY, USA, 3337–3345. https://doi.org/10.1145/3394486.3403386 and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale [8] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E recommender systems. In Proceedings of the 24th ACM SIGKDD International Dahl. 2017. Neural message passing for quantum chemistry. arXiv preprint Conference on Knowledge Discovery & Data Mining. 974–983. arXiv:1704.01212 (2017). [34] Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2017. Spatio-temporal graph con- [9] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation volutional networks: A deep learning framework for traffic forecasting. arXiv learning on large graphs. In Advances in neural information processing systems. preprint arXiv:1709.04875 (2017). 1024–1034. [35] Haitao Yuan, Guoliang Li, Zhifeng Bao, and Ling Feng. 2020. Effective Travel [10] William L Hamilton. 2020. Graph representation learning. Synthesis Lectures on Time Estimation: When Historical Trajectories over Road Networks Matter. In Artifical Intelligence and Machine Learning 14, 3 (2020), 1–159. Proceedings of the 2020 ACM SIGMOD International Conference on Management of [11] Jessica B Hamrick, Kelsey R Allen, Victor Bapst, Tina Zhu, Kevin R McKee, Data. 2135–2149. Joshua B Tenenbaum, and Peter W Battaglia. 2018. Relational inductive bias for [36] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R physical construction in humans and machines. arXiv preprint arXiv:1806.01203 Salakhutdinov, and Alexander J Smola. 2017. Deep Sets. In Advances in Neural (2018). Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, [12] Junheng Hao, Tong Zhao, Jin Li, Xin Luna Dong, Christos Faloutsos, Yizhou R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 3391– Sun, and Wei Wang. 2020. P-Companion: A Principled Framework for Diversi- 3401. http://papers.nips.cc/paper/6931-deep-sets.pdf fied Complementary Product Recommendation. In Proceedings of the 29th ACM [37] Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. International Conference on Information & Knowledge Management. 2517–2524. 2018. Gaan: Gated attention networks for learning on large and spatiotemporal [13] Jilin Hu, Bin Yang, Chenjuan Guo, Christian S Jensen, and Hui Xiong. 2020. graphs. arXiv preprint arXiv:1803.07294 (2018). Stochastic origin-destination matrix forecasting using dual-stage graph convolu- [38] Yingxue Zhang, Yanhua Li, Xun Zhou, Xiangnan Kong, and Jun Luo. 2020. Curb- tional, recurrent neural networks. In 2020 IEEE 36th International Conference on GAN: Conditional Urban Traffic Estimation through Spatio-Temporal Generative Data Engineering (ICDE). IEEE, 1417–1428. Adversarial Networks. In Proceedings of the 26th ACM SIGKDD International [14] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- Conference on Knowledge Discovery & Data Mining. 842–852. mization. arXiv preprint arXiv:1412.6980 (2014). [39] Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. GMAN: [15] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph A Graph Multi-Attention Network for Traffic Prediction. In The Thirty-Fourth convolutional networks. arXiv preprint arXiv:1609.02907 (2016). AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innova- [16] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. arXiv tive Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI preprint arXiv:1611.07308 (2016). Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New [17] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2017. Diffusion convolu- York, NY, USA, February 7-12, 2020. AAAI Press, 1234–1241. https://aaai.org/ojs/ tional recurrent neural network: Data-driven traffic forecasting. arXiv preprint index.php/AAAI/article/view/5477 arXiv:1707.01926 (2017). [18] Aditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles Rosenberg, and Jure Leskovec. 2020. PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2311–2320. [19] Cheonbok Park, Chunggi Lee, Hyojin Bahng, Yunwon Tae, Seungmin Jin, Kihwan Kim, Sungahn Ko, and Jaegul Choo. 2020. ST-GRAT: A Novel Spatio-temporal Graph Attention Networks for Accurately Forecasting Dynamically Changing Road Speed. In Proceedings of the 29th ACM International Conference on Informa- tion & Knowledge Management. 1215–1224. [20] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. 2020. Temporal graph networks for deep learning on dynamic graphs. arXiv preprint arXiv:2006.10637 (2020). [21] Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael Bronstein, and Federico Monti. 2020. SIGN: Scalable Inception Graph Neural Networks. arXiv preprint arXiv:2004.11198 (2020). [22] Aravind Sankar, Yozen Liu, Jun Yu, and Neil Shah. 2021. Graph Neural Networks for Friend Ranking in Large-scale Social Platforms. (2021). [23] Xingjian Shi and Dit-Yan Yeung. 2018. Machine learning for spatiotemporal sequence forecasting: A survey. arXiv preprint arXiv:1808.06865 (2018). [24] Xianfeng Tang, Yozen Liu, Neil Shah, Xiaolin Shi, Prasenjit Mitra, and Suhang Wang. 2020. Knowing your FATE: Friendship, Action and Temporal Explanations for User Engagement Prediction on Social Apps. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2269– 2279. [25] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017). [26] Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. 2019. Deep graph infomax. (2019). [27] Petar Veličković, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. 2019. Neural execution of graph algorithms. arXiv preprint arXiv:1910.10593 (2019). [28] Dong Wang, Junbo Zhang, Wei Cao, Jian Li, and Yu Zheng. 2018. When will you arrive? estimating travel time based on deep neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32. [29] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019. Graph wavenet for deep spatial-temporal graph modeling. arXiv preprint arXiv:1906.00121 (2019).
