Double Control Variates for Gradient Estimation in Discrete Latent Variable Models Michalis K. Titsias Jiaxin Shi DeepMind Microsoft Research New England Abstract et al., 2014; Titsias and L´azaro-Gredilla, 2014), but they are only applicable to continuous distributions. A more general class of gradient estimators based on Stochastic gradient-based optimization for the score function method or REINFORCE (Glynn, discrete latent variable models is challenging 1990; Williams, 1992) is applicable to both continuous due to the high variance of gradients. We and discrete distributions. However, score function es- introduce a variance reduction technique for timators suffer from high variance and reducing the score function estimators that makes use of variance remains an important open problem. double control variates. These control vari- ates act on top of a main control variate, and Variance reduction techniques for REINFORCE es- try to further reduce the variance of the over- timators range from simple baselines (Ranganath all estimator. We develop a double control et al., 2014; Mnih and Gregor, 2014) and Rao- variate for the REINFORCE leave-one-out blackwellization (Titsias and L´azaro-Gredilla, 2015; estimator using Taylor expansions. For train- Tokui and Sato, 2017) to more advanced gradient- ing discrete latent variable models, such as based control variates (Tucker et al., 2017; Grathwohl variational autoencoders with binary latent et al., 2018; Gu et al., 2016) and coupled sampling (Yin variables, our approach adds no extra com- and Zhou, 2019; Dong et al., 2020; Yin et al., 2020; putational cost compared to standard train- Dimitriev and Zhou, 2021). Another variance reduc- ing with the REINFORCE leave-one-out es- tion method that has become prominent recently is timator. We apply our method to challenging the REINFORCE leave-one-out estimator (Salimans high-dimensional toy examples and for train- and Knowles, 2014; Kool et al., 2019; Richter et al., ing variational autoencoders with binary la- 2020), that assumes K ≥ 2 samples and uses a leave- tent variables. We show that our estimator one-out procedure to define sample-specific stochastic can have lower variance compared to other control variates. Despite its simplicity, this estimator state-of-the-art estimators. performs very strongly for training discrete latent vari- ables models (Dong et al., 2020; Richter et al., 2020; Dong et al., 2021). Presumably this is because the 1 INTRODUCTION leave-one-out stochastic baselines can automatically adapt to the non-stationarity of the objective function which has trainable parameters itself, e.g., the param- Several problems in machine learning, such as varia- eters in the generative model. tional inference and supervised learning, require the optimization of an intractable expectation of an objec- In this work, our motivation is to take advantage of tive function under a distribution with tunable param- the compositional structure of control variate tech- eters. Since exact gradients with respect to the param- niques (Owen, 2013; Geffner and Domke, 2018), where eters of the distribution are intractable, optimization multiple control variates can be linearly combined, to must rely on unbiased stochastic estimates. Pathwise further reduce the variance of an existing estimator. or reparametrization gradients (Glasserman, 2003) Specifically, we focus on the REINFORCE leave-one- have been shown to be effective for machine learn- out (RLOO) estimator and enhance it by adding ex- ing problems (Kingma and Welling, 2014; Rezende tra control variates. We refer to the added baselines as double control variates since they co-exist with the Proceedings of the 25th International Conference on Artifi- main RLOO baseline, and are designed to have a com- cial Intelligence and Statistics (AISTATS) 2022, Valencia, plementary effect by reducing the variance of the initial Spain. PMLR: Volume 151. Copyright 2022 by the au- RLOO estimator. We design the double control vari- thor(s). 2202 nuJ 4 ]LM.tats[ 3v00350.1112:viXra
Double Control Variates for Gradient Estimation in Discrete Latent Variable Models ates by applying Taylor expansions and utilising gra- variant of this approach that avoids learning b is the dients of the objective function over the Monte Carlo REINFORCE leave-one-out (RLOO) estimator (Sali- samples. For training latent variable models with dis- mans and Knowles, 2014; Kool et al., 2019; Richter crete variables, these gradients add essentially no extra et al., 2020) that takes advantage of multiple evalua- computational cost since they can be obtained by the tions of f : same meta-optimization synthesis operation needed to collect the   gradients over model parameters. Therefore, training 1 (cid:88)K 1 (cid:88) by using our proposed estimator runs roughly at the K f (x k) − K − 1 f (x j) ∇ η log q η(x k), (3) same speed with the previous RLOO approach. k=1 j(cid:54)=k We apply our double control variate approach to where each leave-one-out average 1 (cid:80) f (x ) K−1 j(cid:54)=k j toy learning examples (see Fig. 1) and for train- acts as a sample-specific control variate that excludes ing variational autoencoders with binary latent vari- the current sample x , so that the whole estimator k ables. We show that our estimator outperforms other is unbiased. This estimator can also be re-written as methods including standard RLOO, DisARM (Dong an unbiased covariance estimator2, i.e. RLOO(η) = et al., 2020; Yin et al., 2020) and its improved version 1 (cid:80)K (cid:16) f (x ) − 1 (cid:80)K f (x )(cid:17) ∇ log q (x ), ARMS (Dimitriev and Zhou, 2021) when using K = 2 K−1 k=1 k K j=1 j η η k which could be more convenient in implementation or more samples. Although we focus on binary latent (Kool et al., 2019; Richter et al., 2020). variables in our experiments, our estimator is equally applicable to categorical latent variables. RLOO was shown to have strong empirical perfor- mance, especially for discrete variable problems (Dong 2 BACKRGOUND et al., 2020; Kool et al., 2019; Dong et al., 2021). It has the attractive property that the sample-specific control variates automatically adapt to the non-stationarity of Assume f (x) is a differentiable objective function, f (x). Specifically, the function f (x) := f (x) can of- where x is a D-dimensional vector. We want to maxi- θ mize the expectation E [f (x)] with respect to the ten contain additional model parameters θ updated at qη(x) each optimization step (for θ is straightforward to ob- parameters η of some distribution q (x). Since f (x) η tain low variance gradients), as for instance in vari- can have a complex non-linear form, the expectation ational autoencoders (VAEs) (Kingma and Welling, is generally intractable. For instance, such problems 2014; Rezende et al., 2014). Although θ is changing, arise in variational inference (Blei et al., 2017), where the sample-specific control variate 1 (cid:80) f (x ) f (x) is the instantaneous ELBO and q η(x) the vari- always remains an unbiased estimateK o− f 1 E j(cid:54)=k [f θ (x)j ]. ational distribution, and in supervised learning, qη(x) θ where f (x) is a reward function and q η(x) is the pol- However, RLOO is still limited in how much variance icy (Weaver and Tao, 2001). reduction it can achieve, as stated in the following proposition which is proved in the Appendix. To apply gradient-based optimization over η we need to compute the gradient Proposition 1 Consider the estimator R∗(η) = ∇ E [f (x)] = E [f (x)∇ log q (x)] , (1) 1 (cid:80)K (f (x ) − Ef ) ∇ log q (x ), where Ef = η qη(x) qη(x) η η K k=1 k η η k E [f (x)] is a constant baseline across all samples. where for simplicity we assume f (x) does not depend Tq hη e( nx) , V ar(RLOO) ≥ V ar(R∗). on η.1 Since this exact gradient is intractable, several techniques apply stochastic optimization (Robbins and Thus, the performance of RLOO is bounded by R∗ Monro, 1951) based on unbiased Monte Carlo gradi- which uses the mean Ef (intractable in practice) as a ents by sampling from q (x). A very general stochastic η constant baseline. However, an estimator with a con- gradient is the score function or REINFORCE estima- stant baseline, even an ideal one as R∗, can often have tor (Glynn, 1990; Williams, 1992; Carbonetto et al., substantial variance in practice. For instance, in the 2009; Paisley et al., 2012; Ranganath et al., 2014; Mnih toy example shown by Fig. 1, where R∗ is tractable, we and Gregor, 2014), compare our proposed double control variates method with both RLOO and R∗, and show that our technique K 1 (cid:88) (f (x ) − b) ∇ log q (x ), x ∼ q (x), (2) can outperform R∗ significantly. Our proposed estima- K k η η k k η tor in Section 3 tries to further reduce the variance of k=1 where b is called a baseline and is often learned to re- 2Because of the score function property E [∇ log q (x)] = 0 the exact gradient duce the variance. Given K ≥ 2 samples, a powerful qη(x) η can be re-written as Cov[f(x), ∇ log q (x)] = 1If there is dependence this adds a low variance gradient E qη(x) (cid:2)(cid:0) f(x) − E qη(x)[f(x)](cid:1) ∇ log q η(x)(cid:3) ;η seeη Salimans to any stochastic estimator; see, e.g., Dong et al. (2020). and Knowles (2014).
Michalis K. Titsias, Jiaxin Shi 1.75 1.50 1.25 1.00 0.75 0.50 0.25 0.00 500 1000 1500 2000 Step ecnairaV tneidarG 1e 9 0.2510 Double CV RLOO 0.2508 DisARM R* 0.2506 0.2504 0.2502 0.2500 500 1000 1500 2000 Step )x(f egarevA 1.0 Double CV RLOO 0.9 DisARM R* 0.8 Reinforce MuProp 0.7 0.6 0.5 500 1000 1500 2000 Step ) ( egarevA i Figure 1: Variance reduction for a toy 200-dimensional maximization problem, following Tucker et al. (2017), with binary variables and fitting probabilities σ(η ) (where σ(η ) = 1 is optimal); see Section 5.1. Left: Gradient i i variances for four different estimators. Middle: Objective function that we want to maximize. Right: Average of the estimated σ(η )s. In the latter two panels two additional estimators are shown. The proposed double control i variate estimator (Double CV) is the most effective one. RLOO and can be considered as using more general 3 DOUBLE CONTROL VARIATES sample-specific baselines, as outlined in Section 2.1. FOR REINFORCE LOO 2.1 More General Sample-Specific Control In RLOO the sample-specific baseline 1 (cid:80) f (x ) K−1 j(cid:54)=k j Variates is constant with respect to x . Also it is stochastic k and as shown by Proposition 1 its variance is lower Let’s denote by x 1:K all K samples in the estima- bounded by the estimator with Ef as the baseline. tor. We say a baseline γ (x ) is sample-specific if k 1:K Therefore, there is scope to further reduce the variance it varies with the sample index k in the Monte Carlo of this estimator, and the approach we follow is to sum, i.e. γ (x ) (cid:54)= γ (x ) for k (cid:54)= j. Note that k 1:K j 1:K consider additional control variates. We refer to these each γ (x ) can depend on all samples including also k 1:K control variates as double since they act on top of the the “current” sample x . A general estimator with k main RLOO baseline. We construct these new control sample-specific control variates is written as variates along two directions: K 1 (cid:88) {(f (x ) − γ (x )) ∇ log q (x )} K k k 1:K η η k (a) We want to add a different type of control variate k=1 that depends on x which may have a complemen- k 1 (cid:88)K tary effect to the main RLOO baseline. + E [γ (x )∇ log q (x )]. (4) K qη(xk) k 1:K η η k k=1 (b) Since the main baseline 1 (cid:80) f (x ) is K−1 j(cid:54)=k j The added sample-specific correction term stochastic and thus has variance, we can try to E [γ (x )∇ log q (x )] must be analyti- reduce the variance by adding a separate control qη(xk) k 1:K η η k cally tractable and ensures that the gradient is variate for each stochastic random term f (x j). unbiased. In some cases the correction term can be dropped, since it will have overall zero expectation, In the remaining of Section 3 we simplify notation by as stated next. using s(x) := ∇ log q (x) to denote the score func- η η tion. To accomplish both (a) and (b) simultaneously Proposition 2 Let x denote all samples 1:k−1,k+1:K we start with the unbiased estimator excluding x . If E [γ (x )] = const, k qη(x1:k−1,k+1:K) k 1:K then E qη(x1:K)[γ k(x 1:K)∇ η log q η(x k)] = 0. 1 (cid:88)K [f (x ) + αb(x )] s(x ) − αE [b(x)s(x)], (5) See Appendix for the proof. A special case of K k k k qη(x) k=1 this arises in REINFORCE LOO where the baseline γ (x ) does not depend on the current sam- where we introduced a control variate b(x ), that de- k 1:k−1,k+1:K k ple x . However, more effective estimators can have a pends on the current sample x and has analytic global k k γ depending on the current sample x as well. For correction E [b(x)s(x)]. Then, to create a double k k qη(x) instance, such an estimator is the second variant (see control variate estimator we treat f (x) + αb(x) as the equation (11)) of the double control variates approach “new effective objective function” and apply the leave- presented next. one-out procedure to it. This leads to the following
Double Control Variates for Gradient Estimation in Discrete Latent Variable Models unbiased estimator {0, 1}d and a factorised Bernoulli distribution   K d 1 (cid:88) 1 (cid:88) (cid:89) K f (x k)+αb(x k)− K−1 (f (x j)+αb(x j))s(x k) q η(x) = µx i i(1 − µ i)1−xi, µ i = σ(η i), (9) k=1 j(cid:54)=k i=1 − αE qη(x)[b(x)s(x)]. (6) E qη(x)[s(x)(x − µ)(cid:62)] = diag(µ ◦ (1 − µ)) and the global correction term simplifies to −αµ ◦ (1 − µ) ◦ ∇f (µ), The scalar α is a regression coefficient that can be fur- where ◦ denotes element-wise vector product. ther optimized to reduce the variance; see Section 3.3. In the above estimator we have highlighted with blue 3.2 An Estimator without Extra Gradient the first appearance b(x ), that can be thought of as a k Evaluations baseline paired with the value f (x ), and with red the k second appearances b(x ) paired with the remaining j The estimator in Eq. (8) requires a backpropaga- values f (x ) of the main RLOO baseline. Intuitively, j tion operation to compute the gradient ∇f (µ), which b(x ) can be considered as targeting to reduce the vari- k adds extra computational cost compared to standard ance of f (x ) and b(x ) the variance of f (x ). k j j RLOO. Next, we wish to develop an alternative es- In Sections 3.1 and 3.2 we describe two ways to spec- timator that avoids this extra cost for certain prob- ify b(x). For training latent variable models, such as lems. For many applications, such as VAEs, the func- VAEs, the second one will be the most practical since tion f (x) depends on model parameters θ (typically it adds no extra cost. The first method helps to intro- different than η) that we update at each optimization duce the idea and it is based on a mean field argument. iteration by computing the gradients {∇ θf (x j)}K j=1. Then, from the same meta-optimization synthesis operations it is easy to also return the gradients with respect to the 3.1 Mean Field Approach latent vectors, i.e. to compute {∇ f (x )}K . To xj j j=1 The optimal choice of b(x) is to become an exact sur- simplify notation we will write ∇f (x) := ∇ f (x). We x rogate of f (x).3 This motivates to construct b(x) by would like to utilize these latter gradients to define the applying some tractable approximation to f (x). While double control variate b(x). any surrogate of f (x) with a tractable global correc- Starting from (7) we first want to modify b(x ) by re- tion could work, next we focus on the case when f (x) k placing ∇f (µ) with some new gradient computed from is differentiable w.r.t. the input x. Specifically, we {∇f (x )}K . We cannot use the full average because assume the target function f is implemented as dif- j j=1 this will lead to ( 1 (cid:80)K ∇f (x ))(cid:62)(x − µ) which has ferentiable function of real-valued inputs, but is re- K j=1 j k an intractable global correction due to the intractable stricted on a discrete subset of its domain. Then, term E [∇f (x )(cid:62)(x −µ)∇ log q (x )]. However, we construct b(x) from a first order Taylor approx- qη(xk) k k η η k imation around the mean µ = E [x], so that we can use the leave-one-out gradient, i.e. by leaving qη(x) out ∇f (x ), which gives f (x) ≈ f (µ) + ∇f (µ)(cid:62)(x − µ) = b(x). Furthermore, k observe that any constant term in b(x) can be ignored  (cid:62) because it cancels out in (6). Thus, the constant f (µ) 1 (cid:88) in the Taylor approximation can be dropped, yielding b k(x 1:K) =  K − 1 ∇f (x j) (x k − µ), (10) j(cid:54)=k the double control variate where we used the index k in b to emphasize that b(x) = ∇f (µ)(cid:62)(x − µ). (7) k this now becomes a sample-specific control variate that varies with sample index; see Section 2.1. This has By substituting this in (6) we obtain the estimator a tractable correction term E [b (x )s(x )] and qη(xk) k 1:K k K (cid:20) also satisfies E [b (x ] = 0. Having specified K1 (cid:88) f (x k) + α∇f (µ)(cid:62)(x k − µ) the double contq rη o( lxk v) arik ate1 w:K e express the unbiased es- k=1 timator as stated below. (cid:21) − 1 (cid:88) (cid:0) f (x ) + α∇f (µ)(cid:62)(x − µ)(cid:1) s(x ) K −1 j j k Proposition 3 For b k(x 1:K) from (10) we obtain the j(cid:54)=k following unbiased gradient estimator − αE [s(x)(x − µ)(cid:62)]∇f (µ), (8) qη(x) K (cid:20) (cid:21) 1 (cid:88) 1 (cid:88) w trh ae cr te abE leqη .(x F) o[s r( ix n) s( tx a− ncµ e) ,(cid:62) fo] rw bil il nt ay rp yic laa tll ey nb t e vaa rn ia al by lt ei sca xll ∈y K k=1 f (x k)+αb k(x 1:K)− K−1 j(cid:54)=k(f (x j)+αb j(x 1:K)) (cid:32) K (cid:33) 3In the estimator (6) this leads to zero variance when ×s(x )−αE [s(x)(x−µ)(cid:62)] 1 (cid:88) ∇f (x ) . (11) α = −1. k qη(x) K k k=1
Michalis K. Titsias, Jiaxin Shi Algorithm 1 Optimization with double control vari- 4 RELATED WORK ate gradients input: loss f (x), distribution q (x). θ η Our proposed gradient estimators follow the general Initialise θ, η, α = 0. form of unbiased REINFORCE estimators (Williams, for t = 1, 2, 3, . . . , do 1992; Glynn, 1990; Carbonetto et al., 2009; Paisley 1: Draw K samples x , x ∼ q (x). 1:K k η et al., 2012; Ranganath et al., 2014; Mnih and Gre- 2: Compute µ = E [x]. qη(x) gor, 2014), which unlike reparametrization or pathwise 3: [f (x ),∇ f (x ),∇ f (x )]K ← grad(f , q , x ). k θ k xk k k=1 θ η 1:k gradients (Kingma and Welling, 2014; Rezende et al., 4: Compute double control variates b (x ) from k 1:K 2014; Titsias and L´azaro-Gredilla, 2014), are applica- Eq. (10). ble also to discrete latent variables. The double control 5: Compute double control variates gradient g(η; α) variates we develop build on top of the RLOO estima- from Eq. (11). tor (Kool et al., 2019; Salimans and Knowles, 2014; 6: Adapt η: η ← η − ρ × g(η; α). t Richter et al., 2020); see also the VIMCO method of 7: Adapt θ: θ ← θ − ρˆ × 1 (cid:80)K ∇ f (x ). t K k=1 θ θ k Mnih and Rezende (2016) who also used a leave-one- 8: Adapt regression scalar α by applying a gradient out procedure. RLOO was shown to be a competi- step to minimize ||g(η; α)||2. tive estimator for challenging problems such as train- end for ing VAEs with binary or categorical latent variables (Dong et al., 2020; Richter et al., 2020; Dong et al., 2021). As shown by our experiments, our enhance- The proof of unbiasedness is given in the Appendix. ment of RLOO with double control variates leads to Notably, the above estimator follows the general struc- further variance reduction, and without increasing the ture from Eq. (4) for a certain choice of the sample- computational cost when training VAEs. specific control variate. In our current framework, the double control variates are constructed by using the gradients of the objective 3.3 Further Details and Algorithmic function f θ(x). These gradients are also used by other Summary unbiased gradient techniques based on control vari- ates, such as the MuProp estimator (Gu et al., 2016), To apply the estimator in (11) we need to specify REBAR (Tucker et al., 2017) and RELAX (Grath- the regression coefficient α by minimizing the vari- wohl et al., 2018). Our method differs significantly ance. If g(η; α) denotes the stochastic gradient and since our control variates act on top of the sample- g¯ = E[g(η; α)] the exact gradient where the latter does specific RLOO baseline K1 −1 (cid:80) j(cid:54)=k f θ(x j), i.e., they not depend on α, the total variance is Tr[E(g(η; α) − try to have complementary effect to this existing con- g¯)(g(η; α) − g¯)(cid:62)] = E[||g(η; α)||2] + const. Thus, in trol variate. This means that our estimators preserve practice at each optimization iteration we can perform RLOO’s property of capturing the non-stationarity of a gradient step towards minimizing the empirical vari- f θ(x), since the leave-one-out baseline always tracks ance ||g(η; α)||2. There also exists an analytic formula the expected value E[f θ(x)] as θ evolves. In con- (but requiring intractable expectations) for the opti- trast, previous gradient-based estimators use stand- mal value of α that can inspire different types of learn- alone global control variates. For instance, the base- ing rules; see Appendix for further details. The whole line in MuProp (Gu et al., 2016) is constructed using algorithm that also deals with a non-stationary f θ(x), only f θ(µ) and x k, which can be a poor tracker of i.e., that includes θ updates at each iteration, is out- the expected value E[f θ(x)]. Unlike MuProp, REBAR lined in Algorithm 1. For the special case where K = 2 (Tucker et al., 2017) is much more effective, however the estimator (11) simplifies as it is more expensive than our method — it requires differentiating f three times, while our method can θ ∇ log q (x )−∇ log q (x ) work with just two, and it is less generally applicable ∆(x 1, x 2, α) η η 1 2 η η 2 since they assume a continuous reparameterization for q. RELAX (Grathwohl et al., 2018) suffers from the ∇f (x ) + ∇f (x ) − αE [s(x)(x − µ)(cid:62)] 1 2 , (12) same problem as its strong performance relies on the qη(x) 2 REBAR control variate (Richter et al., 2020). where ∆(x , x , α) = f (x ) − f (x ) + α[∇f (x )(cid:62)(x − Other recent REINFORCE type of estimators for dis- 1 2 1 2 2 1 µ)−∇f (x )(cid:62)(x −µ)]. In the experiments we compare crete latent variables are based on coupled sampling 1 2 this estimator with the DisARM method (Dong et al., (Owen, 2013), such as antithetic sampling (Yin and 2020; Yin et al., 2020) that uses K = 2 antithetic Zhou, 2019; Dong et al., 2020; Yin et al., 2020; Dim- samples, and also with RLOO with K = 2 samples. itriev and Zhou, 2021). For instance, the recent Dis-
Double Control Variates for Gradient Estimation in Discrete Latent Variable Models Table 1: Training nonlinear binary latent VAEs with K = 2 (except RELAX which needs 3 evaluations of f ) on MNIST, Fashion-MNIST, and Omniglot. We report the average training ELBO over 5 independent runs. Bernoulli Likelihoods Gaussian Likelihoods MNIST Fashion-MNIST Omniglot MNIST Fashion-MNIST Omniglot RLOO −103.11 ± 0.16 −241.53 ± 0.24 −116.83 ± 0.05 668.07 ± 0.40 179.52 ± 0.23 443.51 ± 0.93 Double CV −102.45 ± 0.13 −240.96 ± 0.17 −116.22 ± 0.08 676.87 ± 1.18 186.35 ± 0.64 446.95 ± 0.63 DisARM −102.56 ± 0.09 −241.02 ± 0.20 −116.36 ± 0.05 668.03 ± 0.61 182.65 ± 0.47 446.22 ± 1.38 RELAX (3 evals) −101.86 ± 0.11 −240.63 ± 0.16 −115.79 ± 0.06 688.58 ± 0.52 196.38 ± 0.66 462.30 ± 0.91 0.0225 0.0200 0.0175 0.0150 0.0125 0.0100 0.0075 200K 400K 600K 800K 1000K Training Step ecnairaV tneidarG MNIST Fashion-MNIST Omniglot 0.0250 0.0225 0.010 0.0200 0.008 0.0175 0.0150 0.006 Double CV 0.0125 RLOO 0.0100 0.004 DisARM RELAX (3 evals) 0.0075 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step 102 104 106 108 110 200K 400K 600K 800K 1000K Training Step OBLE gniniarT 240 116 241 242 118 243 120 Double CV 244 RLOO 122 DisARM 245 RELAX (3 evals) 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step Figure 2: Training nonlinear binary latent VAEs with Bernoulli likelihoods with K = 2 (except RELAX which needs 3 evaluations of f ) on dynamically binarized MNIST, Fashion-MNIST, and Omniglot. Top: Variance of gradient estimates. Bottom: Average ELBO on training examples. ARM estimator independently proposed by Dong et al. our proposed double control variates estimator (Dou- (2020) and Yin et al. (2020) was shown to give state-of- ble CV) from Eq. (11). We use K = 2 samples for the-art results for binary latent-variable models with all methods (note that Double CV in this case simpli- K = 2 antithetic samples. fies as in (12)). Also we include in the comparison R∗ which is tractable in this toy example. Fig. 1 compares 5 EXPERIMENTS the methods in terms of variance, the objective func- tion and the average value of the D probabilities σ(η ). i Fig. 6 in the Appendix shows further comparison for Code for reproducing all experiments is available at the D = 1 case, as in Tucker et al. (2017). We observe https://github.com/thjashin/double-cv. that Double CV gradients have smaller variance which results in much faster optimization convergence. 5.1 Toy Learning Problem We consider a generalization of the artificial problem 5.2 Variational Autoencoders with Binary considered by Tucker et al. (2017). The goal is to Latent Variables maximize E(η) = E [D−1 (cid:80)D (x − p )2], where qη(x) i=1 i 0 q η(x) = (cid:81)D i=1 σ(η i)xi(1 − σ(η i))1−xi, p 0 = 0.499 and Experimental setup We consider training vari- the optimal solution is σ(η ) = 1 for all i = 1, . . . , D. ational autoencoders (Kingma and Welling, 2014; i While Tucker et al. (2017) considered D = 1, here we Rezende et al., 2014) with binary latent variables. We additionally consider a more difficult high-dimensional conduct separate experiments for binary output data case with D = 200. We compare five methods: RLOO, y ∈ {0, 1}d and continuous data y ∈ Rd. For binary DisARM, MuProp, Reinforce (with no baselines) and data we use the standard Bernoulli likelihood. For
Michalis K. Titsias, Jiaxin Shi 5 4 3 2 1 200K 400K 600K 800K 1000K Training Step ecnairaV tneidarG MNIST Fashion-MNIST Omniglot 5 2.5 4 2.0 3 1.5 2 1.0 Double CV DisARM 1 RLOO RELAX (3 evals) 0.5 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step 675 650 625 600 575 550 525 200K 400K 600K 800K 1000K Training Step OBLE gniniarT 200 450 180 160 400 140 350 120 100 300 Double CV DisARM 80 RLOO RELAX (3 evals) 250 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step Figure 3: Training nonlinear binary latent VAEs with Gaussian likelihoods with K = 2 (except RELAX which needs 3 evaluations of f ) on non-binarized MNIST, Fashion-MNIST, and Omniglot. Top: Variance of gradient estimates. Bottom: Average ELBO on training examples. continuous data we centered data between [−1, 1] and we include in the comparison the RELAX estimator consider a Gaussian likelihood of the form p (y|x) = that combines concrete relaxation (Tucker et al., 2017) θ N (y|m (x), Σ), where m (x) is a decoder mean func- with a learned control variate (Grathwohl et al., 2018). θ θ tion that depends on the latent variable x and Σ is a We point out that RLOO, DisARM, Double CV, and learnable diagonal covariance matrix. We consider the ARMS (when K = 4) have roughly the same running datasets MNIST, Fashion-MNIST and Omniglot. For time on a P100 GPU while RELAX is computationally all three datasets we use both the dynamically bina- more expensive and is roughly twice slower than the rized versions and their original continuous versions. other four estimators with K = 4 (see Table 3 in the Appendix). Also note that RELAX is less generally We consider the nonlinear VAE models used in Yin applicable since it assumes the existence of a continu- and Zhou (2019); Dong et al. (2020); results for lin- ous relaxation for x. ear VAEs are included in the Appendix. The VAE model uses fully connected neural networks with two hidden layers of 200 LeakyReLU activation units with Results Table 1 shows the training ELBO for bina- the coefficient 0.3. All models are trained using Adam rized and continuous datasets when training the VAE (Kingma and Ba, 2014) with learning rate 10−3 for by different estimators with K = 2. We can observe the binarized data, while for the continuous data we that Double CV consistently outperforms RLOO in used smaller learning rate 10−4. In all experiments all experiments, while having approximately the same the regression coefficient α of the double control vari- running time. Double CV also outperforms DisARM ates was also trained (see Section 3.3) with Adam and in all cases for both Bernoulli and Gaussian likeli- with learning rate 10−3. For all experiments we use a hoods. Furthermore, Fig. 2 plots the gradient variance uniform factorized Bernoulli prior over the D = 200 di- and the training ELBO for the binarized datasets as a mensional latent variable x. The model was trained by function of the training steps. Similarly, Fig. 3 shows maximizing the ELBO using an amortised factorised the corresponding results for the non-binarized (con- variational Bernoulli distribution. tinuous) datasets where a Gaussian likelihood is used. We compared the following estimators: RLOO, Dis- We observe that the Double CV estimator can have ARM and the proposed Double CV method where all lower variance than RLOO and DisARM. Also, while three estimators use K samples. We experimented RELAX performs better than the other methods it is with K = 2 and K = 4. For K = 4 we also com- less generally applicable and more expensive. pare to the state-of-the-art ARMS estimator recently For K = 4, the final training ELBO values are re- proposed by Dimitriev and Zhou (2021). Besides, ported in Table 2 and the variances of the different
Double Control Variates for Gradient Estimation in Discrete Latent Variable Models Table 2: Training a nonlinear binary latent VAE with K = 4 (except RELAX which needs 3 evaluations of f ) on MNIST, Fashion-MNIST, and Omniglot. We report the average training ELBO over 5 independent runs. Bernoulli Likelihoods Gaussian Likelihoods MNIST Fashion-MNIST Omniglot MNIST Fashion-MNIST Omniglot RLOO −100.50 ± 0.22 −239.03 ± 0.15 −114.75 ± 0.07 687.83 ± 0.50 195.27 ± 0.24 460.23 ± 1.42 Double CV −99.89 ± 0.12 −238.98 ± 0.18 −114.56 ± 0.06 691.51 ± 0.75 199.01 ± 0.60 463.03 ± 0.94 DisARM −100.67 ± 0.07 −239.20 ± 0.15 −115.05 ± 0.07 683.28 ± 0.89 192.96 ± 0.29 458.38 ± 0.88 ARMS −100.07 ± 0.08 −238.50 ± 0.13 −114.57 ± 0.06 687.26 ± 1.21 197.25 ± 0.48 463.30 ± 0.86 RELAX (3 evals) −101.86 ± 0.11 −240.63 ± 0.16 −115.79 ± 0.06 688.58 ± 0.52 196.38 ± 0.66 462.30 ± 0.91 0.011 0.010 0.009 0.008 0.007 0.006 0.005 200K 400K 600K 800K 1000K Training Step ecnairaV tneidarG MNIST Fashion-MNIST Omniglot 0.014 Double CV RELAX (3 evals) 0.007 RLOO ARMS 0.012 DisARM 0.006 0.010 0.005 0.008 0.004 0.006 0.003 0.004 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step 3.0 2.5 2.0 1.5 1.0 200K 400K 600K 800K 1000K Training Step ecnairaV tneidarG MNIST Fashion-MNIST Omniglot 1.6 2.50 2.25 1.4 2.00 1.75 1.2 1.50 1.0 1.25 Double CV RELAX (3 evals) RLOO ARMS 1.00 DisARM 0.8 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step Figure 4: Variance of gradient estimates in training nonlinear binary latent variational autoencoders with K = 4 (except RELAX which needs 3 evaluations of f ) on MNIST, Fashion-MNIST, and Omniglot. Top: Using Bernoulli likelihoods and dynamically binarized datasets. Bottom: Using Gaussian likelihoods and non-binarized datasets. estimators are plotted in Fig. 4. We can observe that Finally, the use of double control variates can be or- Double CV consistently has lower variance than other thogonal to other techniques for variance reduction estimators and it outperforms ARMS in terms of train- such as coupled sampling (Yin and Zhou, 2019; Dong ing ELBO in most cases. It also significantly outper- et al., 2020, 2021; Dimitriev and Zhou, 2021) and con- forms RELAX. Note that, even with K = 4, Double crete relaxations (Tucker et al., 2017; Grathwohl et al., CV is still nearly twice faster than RELAX. 2018). This could lead to various combinations of our approach with these techniques. For instance, if we start from our initial estimator in (5) where we sim- 6 CONCLUSION ply replace the initial objective function f (x) with the new effective objective f (x) + αb(x), a combination We presented a new variance reduction technique with coupled sampling is possible, e.g. certainly this called double control variates for gradient estimation holds for the mean field choice b(x) = ∇f (µ)(cid:62)(x − µ). of discrete latent variable models. We achieved sub- Also if we relax the restriction of the global correction stantial variance reduction by constructing control E qη(x)[b(x)∇ η log q η(x)] to be analytic but instead al- variates on top of existing leave-one-out baselines in low to be reparametrizable, then our method could REINFORCE estimators. The proposed estimator is be combined with the concrete relaxation methods. unbiased and adds no extra computational cost to the The investigation of such combinations is an interest- standard meta-optimization synthesis cost needed for obtaining ing topic for future research. gradients over model parameters.
Michalis K. Titsias, Jiaxin Shi References ternational Conference on Machine Learning, pages 1791–1799. Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. (2017). Variational inference: A review for statis- Mnih, A. and Rezende, D. J. (2016). Variational in- ticians. Journal of the American statistical Associ- ference for Monte Carlo objectives. In International ation, 112(518):859–877. Conference on Machine Learning. Carbonetto, P., King, M., and Hamze, F. (2009). A Owen, A. B. (2013). Monte Carlo theory, methods and stochastic approximation method for inference in examples. probabilistic graphical models. In Advances in Neu- Paisley, J. W., Blei, D. M., and Jordan, M. I. ral Information Processing Systems, volume 22. (2012). Variational Bayesian inference with stochas- Dimitriev, A. and Zhou, M. (2021). ARMS: antithetic- tic search. In International Conference on Machine reinforce-multi-sample gradient for binary variables. Learning. In International Conference on Machine Learning, Ranganath, R., Gerrish, S., and Blei, D. (2014). Black volume 139, pages 2717–2727. box variational inference. In International Confer- Dong, Z., Mnih, A., and Tucker, G. (2020). Dis- ence on Artificial Intelligence and Statistics, page ARM: An antithetic gradient estimator for binary 814–822. latent variables. In Advances in Neural Information Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Processing Systems, volume 33, pages 18637–18647. Stochastic meta-optimization synthesis and approximate infer- Curran Associates, Inc. ence in deep generative models. In International Dong, Z., Mnih, A., and Tucker, G. (2021). Cou- Conference on Machine Learning. pled gradient estimators for discrete latent variables. Richter, L., Boustati, A., Nu¨sken, N., Ruiz, F., and arXiv preprint arXiv:2106.08056. Akyildiz, O. D. (2020). VarGrad: A low-variance Geffner, T. and Domke, J. (2018). Using large en- gradient estimator for variational inference. In Ad- sembles of control variates for variational inference. vances in Neural Information Processing Systems, In Advances in Neural Information Processing Sys- volume 33, pages 13481–13492. Curran Associates, tems, volume 31. Inc. Glasserman, P. (2003). Monte Carlo methods in fi- Robbins, H. and Monro, S. (1951). A Stochastic Ap- nancial engineering, volume 53. Springer Science & proximation Method. The Annals of Mathematical Business Media. Statistics, 22(3):400–407. Glynn, P. W. (1990). Likelihood ratio gradient es- Salimans, T. and Knowles, D. A. (2014). On using timation for stochastic systems. Commun. ACM, control variates with stochastic approximation for 33(10):75–84. variational Bayes and its connection to stochastic Grathwohl, W., Choi, D., Wu, Y., Roeder, G., and linear regression. arXiv preprint arXiv:1401.1022. Duvenaud, D. (2018). meta-optimization synthesis through the Titsias, M. K. and L´azaro-Gredilla, M. (2014). Dou- void: Optimizing control variates for black-box gra- bly stochastic variational Bayes for non-conjugate dient estimation. In International Conference on inference. In International Conference on Machine Learning Representations. Learning. Gu, S., Levine, S., Sutskever, I., and Mnih, A. (2016). Titsias, M. K. and L´azaro-Gredilla, M. (2015). Local MuProp: Unbiased meta-optimization synthesis for stochastic expectation gradients for black box variational in- neural networks. In International Conference on ference. Advances in Neural Information Processing Learning Representations. Systems, 28:2638–2646. Kingma, D. P. and Ba, J. (2014). Adam: A method Tokui, S. and Sato, I. (2017). Evaluating the vari- for stochastic optimization. International Confer- ance of likelihood-ratio gradient estimators. In In- ence for Learning Representations. ternational Conference on Machine Learning, pages Kingma, D. P. and Welling, M. (2014). Auto-encoding 3414–3423. variational Bayes. In International Conference on Tucker, G., Mnih, A., Maddison, C. J., and Sohl- Learning Representations. Dickstein, J. (2017). REBAR: low-variance, unbi- Kool, W., Hoof, H. V., and Welling, M. (2019). Buy 4 ased gradient estimates for discrete latent variable reinforce samples, get a baseline for free! In Deep- models. In International Conference on Learning RLStructPred@ICLR. Representations. Mnih, A. and Gregor, K. (2014). Neural variational Weaver, L. and Tao, N. (2001). The optimal reward inference and learning in belief networks. In In- baseline for gradient-based supervised learning.
Double Control Variates for Gradient Estimation in Discrete Latent Variable Models In Uncertainty in Artificial Intelligence, pages 538– 545. Williams, R. J. (1992). Simple statistical gradient- following algorithms for connectionist supervised learning. Machine Learning, 8(3-4):229–256. Yin, M., Ho, N., Yan, B., Qian, X., and Zhou, M. (2020). Probabilistic best subset selection via gradient-based optimization. Yin, M. and Zhou, M. (2019). ARM: Augment- REINFORCE-merge gradient for stochastic binary networks. In International Conference on Learning Representations.
Michalis K. Titsias, Jiaxin Shi A Proofs A.1 Proof of Proposition 1 The RLOO estimator can be written as   K K 1 (cid:88) 1 (cid:88) 1 (cid:88) K (f (x k) − Ef ) ∇ η log q η(x k) + K Ef − K − 1 f (x j) ∇ η log q η(x k) (13) k=1 k=1 j(cid:54)=k (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) R∗ E where R∗ is the REINFORCE estimator with baseline Ef and E is a residual term of zero mean. To prove the Proposition we will use V ar(RLOO) = V ar(R∗ + E) = V ar(R∗) + V ar(E) + 2Cov(R∗, E). Then, it suffices to show that Cov(R∗, E) = 0. We have K K Cov(R∗, E) = 1 (cid:88) (cid:88) E (cid:2) (f (x ) − Ef )(Ef − f )∇ log q (x )∇ log q (x )(cid:62)(cid:3) K2 k −k(cid:48) η η k η η k(cid:48) k=1 k(cid:48)=1 where we used f = 1 (cid:80) f (x ) for short. For all terms in the double sum such that k = k(cid:48) the −k(cid:48) K−1 j(cid:54)=k(cid:48) j expectation E (cid:2) (f (x ) − Ef )(Ef − f )∇ log q (x )∇ log q (x )(cid:62)(cid:3) = 0 k −k η η k η η k because the zero-mean random variable Ef − f is independent from the remaining product (since it does not −k contain the sample x ). For all cross terms k (cid:54)= k(cid:48) the whole product (f (x ) − Ef )(Ef − f )∇ log q (x ) does k k −k(cid:48) η η k not contain the sample x . Therefore this product is independent from ∇ log q (x ) and thus each cross term k(cid:48) η η k(cid:48) is zero because of the score function property E[∇ log q (x )] = 0. This shows that Cov(R∗, E) = 0 which η η k(cid:48) completes the proof. A.2 Proof of Proposition 2 It holds E [γ (x )∇ log q (x )] qη(x1:K) k 1:K η η k (cid:20) (cid:21) = E E [γ (x )]∇ log q (x ) qη(xk) qη(x1:k−1,xk+1:K) k 1:K η η k (cid:20) (cid:21) = E const∇ log q (x ) = 0. (14) qη(xk) η η k where the last line is just a consequence of the score function property since const does not depend on x . k A.3 Proof of Proposition 3 The estimator can be written as   K 1 (cid:88) 1 (cid:88) K f (x k) − K − 1 f (x j) ∇ η log q η(x k) k=1 j(cid:54)=k   K 1 (cid:88) 1 (cid:88) + α K b k(x 1:K) − K − 1 b j(x 1:K) ∇ η log q η(x k) k=1 j(cid:54)=k (cid:32) K (cid:33) 1 (cid:88) − αE [∇ log q (x) × (x − µ)(cid:62)] ∇f (x ) , (15) q(x) η η K k k=1 (cid:16) (cid:17)(cid:62) (cid:16) (cid:17)(cid:62) where b (x ) = 1 (cid:80) ∇f (x ) (x − µ) and b (x ) = 1 (cid:80) ∇f (x ) (x − µ). It suffices k 1:K K−1 j(cid:54)=k j k j 1:K K−1 m(cid:54)=j m j to show that the expectation of the second line is minus the correction term at the third line. The expectation
Double Control Variates for Gradient Estimation in Discrete Latent Variable Models of each term b (x )∇ log q (x ) for j (cid:54)= k is zero because the zero-mean term x − µ is always independent j 1:K η η k j from the rest of the terms in the product. Then, we need to examine only the expectation of K K 1 (cid:88) 1 (cid:88) (cid:88) b (x )∇ log q (x ) = ∇ log q (x )(x − µ)(cid:62) ∇f (x ). K k 1:K η η k K(K − 1) η η k k j k=1 k=1 j(cid:54)=k Then observe that the expectation of ∇ log q (x ) × (x − µ)(cid:62) is the same for every sample x , so the above η η k k k reduces to K 1 (cid:88) (cid:88) E [∇ log q (x) × (x − µ)(cid:62)] ∇f (x ) qη(x) η η K(K − 1) j k=1 j(cid:54)=k from which the result follows since (cid:80)K (cid:80) ∇f (x ) = (K − 1) (cid:80)K ∇f (x ). k=1 j(cid:54)=k j k=1 k A.4 The Optimal Value of α for K = 2 The gradient for K = 2 can be written as 1 [f(x ) − f(x )](∇ log q (x ) − ∇ log q (x )) 2 1 2 η η 1 η η 2 1 (cid:16) (cid:17) − α M(∇f(x ) + ∇f(x )) − [∇f(x )(cid:62)(x − µ) − ∇f(x )(cid:62)(x − µ)](∇ log q (x ) − ∇ log q (x )) (16) 2 1 2 2 1 1 2 η η 1 η η 2 where M = E [∇ log q (x) × (x − µ)(cid:62)]. If we denote qη(x) η η g(x , x ) = [f (x ) − f (x )](∇ log q (x ) − ∇ log q (x )) 1 2 1 2 η η 1 η η 2 and h(x , x ) = M(∇f(x ) + ∇f(x )) − [∇f(x )(cid:62)(x − µ) − ∇f(x )(cid:62)(x − µ)](∇ log q (x ) − ∇ log q (x )) 1 2 1 2 2 1 1 2 η η 1 η η 2 the gradient can be written as 1 (g(x , x ) − αh(x , x )) . 2 1 2 1 2 Then the optimal α that minimizes the variance is given by E[g(x , x )(cid:62)h(x , x )] α = 1 2 1 2 E[h(x , x )(cid:62)h(x , x )] 1 2 1 2 Similarly we can construct the optimal value of α for any K > 2. A.5 The “half ” Double Control Variate Estimators One question is whether we need both b(x ) and b(x ) or we could keep one of them, i.e. to use an “b(x ) only” or k j k “b(x ) only” estimator. It is straightforward to express these latter unbiased estimators, as follows. The “b(x ) j k only” estimator is given by   K 1 (cid:88) 1 (cid:88) K f (x k) + αb(x k)− K −1 f (x j)∇ η log q η(x k) − αE qη(x)[b(x)∇ ηlog q η(x)]. (17) k=1 j(cid:54)=k and the “b(x ) only” by j   K 1 (cid:88) 1 (cid:88) K f (x k)− K −1 (f (x j) + αb(x j))∇ η log q η(x k). (18) k=1 j(cid:54)=k It is easy to show that both estimators are unbiased. However, in practice these estimators can be much less effective in terms of variance reduction than their Double CV combination. In Fig. 5 we apply these two estimators to the toy learning problem with D = 10. Both estimators are significantly outperformed by the full Double CV estimator. Notably, the “b(x ) only” estimator could outperform R∗ since it uses a baseline that k depends on the current sample x , while “b(x ) only” reduces the variance of the RLOO control variate but k j remains bounded by R∗.
Michalis K. Titsias, Jiaxin Shi 1e 8 Variance 1e 8 Variance 2.5 RLOO 2.5 RLOO Double CV Double CV 2.0 2.0 DisARM DisARM R* R* 1.5 1.5 Only b(xk) Only b(xj) 1.0 1.0 0.5 0.5 0.0 0.0 500 1000 1500 2000 500 1000 1500 2000 Training steps Training steps Figure 5: Left: Variance of the “only b(x )” estimator where only the half part of the double control variate k is used. Right: The corresponding plot for the “only b(x )” estimator where the other half part of the double j control variate is used. The full double control variate estimator (Double CV), RLOO, DisARM and R∗ are included for comparison. The experiment corresponds to the toy problem with D = 10 and b(x) was chosen according to Eq. (10), i.e. the full Double CV estimator is from (11). B Additional Results B.1 Toy Experiment with D = 1 For completeness, we include the results of a simpler version of the toy experiment described in Section 5.1, where we set D = 1. This is the setting used in several previous works (Tucker et al., 2017; Grathwohl et al., 2018; Yin and Zhou, 2019; Dong et al., 2020). The variances of the gradient estimators and the training curves of σ(η) are plotted in Fig. 6. Fig. 7 shows the evolution of the estimated regression coefficient α. 2.5 2.0 1.5 1.0 0.5 0.0 500 1000 1500 2000 Step ecnairaV tneidarG 1e 7 1.00 RLOO Double CV DisARM 0.95 R* 0.90 0.85 0.80 500 1000 1500 2000 Step ) ( Figure 6: Left: Variance of the gradient estimators for the toy problem with D = 1. Right: The estimated value σ(η) across iterations (optimal value is 1). B.2 Training Binary Latent VAEs B.2.1 Time comparison In Fig. 3 we report the per-step running time of RLOO, Double CV, DisARM, ARMS estimators when K = 4 and compare to RELAX. RELAX is almost twice slower. RLOO Double CV DisARM ARMS RELAX Time (sec/step) 0.0035 0.0036 0.0031 0.0037 0.0080 Table 3: Time per step when training a Bernoulli VAE with K = 4 (except RELAX which needs 3 evaluations of f ) on dynamically binarized Fashion-MNIST.
Double Control Variates for Gradient Estimation in Discrete Latent Variable Models 0.0005 0.0005 0.002 0.0010 0.0010 0.004 0.0015 0.006 0.0020 0.0015 0.0025 0.008 0.0030 0.0020 0.010 0.0035 500 1000 1500 2000 500 1000 1500 2000 500 1000 1500 2000 Training steps Training steps Training steps D = 1 D = 10 D = 200 Figure 7: The evolution of the estimated regression coefficient α during optimization for the toy learning problem. RLOO Double CV DisARM RELAX (3 evals) MNIST : Linear −113.06 ± 0.05 −112.82 ± 0.07 −112.72 ± 0.07 −112.18 ± 0.07 Nonlinear −103.11 ± 0.16 −102.45 ± 0.13 −102.56 ± 0.09 −101.86 ± 0.11 Fashion-MNIST : Linear −257.38 ± 0.17 −256.21 ± 0.17 −257.01 ± 0.06 −255.16 ± 0.17 Nonlinear −241.53 ± 0.24 −240.96 ± 0.17 −241.02 ± 0.20 −240.63 ± 0.16 Omniglot: Linear −119.63 ± 0.05 −119.52 ± 0.02 −119.42 ± 0.03 −119.16 ± 0.02 Nonlinear −116.83 ± 0.05 −116.22 ± 0.08 −116.36 ± 0.05 −115.79 ± 0.06 Table 4: Training binary latent VAEs with K = 2 (except RELAX which needs 3 evaluations of f ) on dynamically binarized MNIST, Fashion-MNIST, and Omniglot. We report the average ELBO on the training set over 5 independent runs. B.2.2 Full results of training ELBOs Here we include the full results of final training ELBOs from the experiment in Section 5.2. Table 4 and Table 5 extend Table 1 to include the linear VAE results trained under the same setting. Table 6 and Table 7 extend Table 2 to include the linear VAE results trained under the same setting. The linear VAE has 200 dimensional latent variable x and use a single fully-connected layer to produce the logits (for Bernoulli likelihoods) or the mean (for Gaussian likelihoods) of the distribution of y. B.2.3 Additional figures for nonlinear VAEs In Fig. 8 we plot the average training ELBOs as a function of training steps from the K = 4 experiment in Section 5.2. B.2.4 Additional figures for linear VAEs We plot the gradient variance and average training ELBOs of training linear VAEs in Figures 9,10,11, and 12.
Michalis K. Titsias, Jiaxin Shi RLOO Double CV DisARM RELAX (3 evals) MNIST Linear 503.01 ± 0.22 504.33 ± 0.98 504.43 ± 0.93 513.38 ± 0.52 Nonlinear 668.07 ± 0.40 676.87 ± 1.18 668.03 ± 0.61 688.58 ± 0.52 Fashion-MNIST Linear 29.75 ± 0.40 31.08 ± 0.24 31.71 ± 0.20 37.54 ± 0.30 Nonlinear 179.52 ± 0.23 186.35 ± 0.64 182.65 ± 0.47 196.38 ± 0.66 Omniglot Linear 245.73 ± 0.33 245.97 ± 1.02 247.70 ± 0.85 255.69 ± 0.70 Nonlinear 443.51 ± 0.93 446.95 ± 0.63 446.22 ± 1.38 462.30 ± 0.91 Table 5: Training binary latent VAEs with Gaussian likelihoods using K = 2 (except RELAX which needs 3 evaluations of f ) on non-binarized MNIST, Fashion-MNIST, and Omniglot. We report the average ELBO on the training set over 5 independent runs. RLOO Double CV DisARM ARMS MNIST : Linear −111.89 ± 0.09 −111.79 ± 0.09 −112.01 ± 0.06 −111.87 ± 0.02 Nonlinear −100.50 ± 0.22 −99.89 ± 0.12 −100.67 ± 0.07 −100.07 ± 0.08 Fashion-MNIST : Linear −254.59 ± 0.16 −254.52 ± 0.23 −255.01 ± 0.10 −254.67 ± 0.20 Nonlinear −239.03 ± 0.15 −238.98 ± 0.18 −239.20 ± 0.15 −238.50 ± 0.13 Omniglot: Linear −118.89 ± 0.02 −118.95 ± 0.02 −118.97 ± 0.01 −118.87 ± 0.02 Nonlinear −114.75 ± 0.07 −114.56 ± 0.06 −115.05 ± 0.07 −114.57 ± 0.06 Table 6: Training binary latent VAEs with K = 4 on dynamically binarized MNIST, Fashion MNIST, and Omniglot. We report the average ELBO on the training set over 5 independent runs. RLOO Double CV DisARM ARMS MNIST : Linear 516.65 ± 0.54 515.79 ± 0.71 512.47 ± 0.72 514.55 ± 0.71 Nonlinear 687.83 ± 0.50 691.51 ± 0.75 683.28 ± 0.89 687.26 ± 1.21 Fashion-MNIST : Linear 36.70 ± 0.41 36.61 ± 0.34 34.90 ± 0.52 37.56 ± 0.43 Nonlinear 195.27 ± 0.24 199.01 ± 0.60 192.96 ± 0.29 197.25 ± 0.48 Omniglot: Linear 257.43 ± 0.16 257.88 ± 0.69 254.99 ± 0.69 258.22 ± 0.18 Nonlinear 460.23 ± 1.42 463.03 ± 0.94 458.38 ± 0.88 463.30 ± 0.86 Table 7: Training binary latent VAEs with Gaussian likelihoods using K = 4 on non-binarized MNIST, Fashion- MNIST, and Omniglot. We report the average ELBO on the training set over 5 independent runs.
Double Control Variates for Gradient Estimation in Discrete Latent Variable Models 100 102 104 106 108 200K 400K 600K 800K 1000K Training Step OBLE gniniarT 238 115 239 116 240 117 241 118 242 119 Double CV RELAX (3 evals) 243 120 RLOO ARMS DisARM 121 244 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step 700 675 650 625 600 575 550 200K 400K 600K 800K 1000K Training Step OBLE gniniarT 200 450 180 400 160 140 350 120 Double CV RELAX (3 evals) 300 RLOO ARMS 100 DisARM 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step Figure 8: Average training ELBOs for nonlinear binary latent VAEs trained by different estimators with K = 4 (except RELAX which needs 3 evaluations of f ) on MNIST, Fashion-MNIST, and Omniglot. Top: Using Bernoulli likelihoods and dynamically binarized datasets. Bottom: Using Gaussian likelihoods and non-binarized datasets. 0.0175 0.0150 0.0125 0.0100 0.0075 0.0050 0.0025 0.0000 200K 400K 600K 800K 1000K Training Step ecnairaV tneidarG MNIST Fashion-MNIST Omniglot 0.006 0.05 Double CV RLOO 0.005 0.04 DisARM 0.004 RELAX (3 evals) 0.03 0.003 0.02 0.002 0.01 0.001 0.00 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step 112 114 116 118 120 200K 400K 600K 800K 1000K Training Step OBLE gniniarT 119 256 120 258 121 260 122 Double CV RLOO 123 DisARM 262 RELAX (3 evals) 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step Figure 9: Training linear binary latent VAEs with Bernoulli likelihoods with K = 2 (except RELAX which needs 3 evaluations of f ) on dynamically binarized MNIST, Fashion-MNIST, and Omniglot. Top: Variance of gradient estimates. Bottom: Average ELBO on training examples.
Michalis K. Titsias, Jiaxin Shi 0.30 0.25 0.20 0.15 0.10 0.05 200K 400K 600K 800K 1000K Training Step ecnairaV tneidarG MNIST Fashion-MNIST Omniglot 0.5 0.25 Double CV DisARM RLOO RELAX (3 evals) 0.4 0.20 0.3 0.15 0.2 0.10 0.1 0.05 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step 500 480 460 440 420 200K 400K 600K 800K 1000K Training Step OBLE gniniarT 40 260 30 240 20 10 220 0 10 200 20 Double CV DisARM 30 180 RLOO RELAX (3 evals) 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step Figure 10: Training linear binary latent VAEs with Gaussian likelihoods with K = 2 (except RELAX which needs 3 evaluations of f ) on non-binarized MNIST, Fashion-MNIST, and Omniglot. Top: Variance of gradient estimates. Bottom: Average ELBO on training examples. 0.005 0.004 0.003 0.002 0.001 200K 400K 600K 800K 1000K Training Step ecnairaV tneidarG MNIST Fashion-MNIST Omniglot 0.0018 0.014 Double CV RELAX (3 evals) 0.0016 RLOO ARMS 0.012 DisARM 0.0014 0.010 0.0012 0.008 0.0010 0.006 0.0008 0.004 0.0006 0.002 0.0004 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step 112 113 114 115 116 117 118 200K 400K 600K 800K 1000K Training Step OBLE gniniarT 254 119.0 255 119.5 120.0 256 120.5 257 121.0 Double CV RELAX (3 evals) 258 121.5 RLOO ARMS DisARM 122.0 259 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step Figure 11: Training linear binary latent VAEs with Bernoulli likelihoods with K = 4 (except RELAX which needs 3 evaluations of f ) on dynamically binarized MNIST, Fashion-MNIST, and Omniglot. Top: Variance of gradient estimates. Bottom: Average ELBO on training examples.
Double Control Variates for Gradient Estimation in Discrete Latent Variable Models 0.10 0.08 0.06 0.04 0.02 200K 400K 600K 800K 1000K Training Step ecnairaV tneidarG MNIST Fashion-MNIST Omniglot 0.08 0.175 Double CV RELAX (3 evals) 0.07 RLOO ARMS 0.150 DisARM 0.06 0.125 0.05 0.100 0.04 0.075 0.03 0.050 0.02 0.025 0.01 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step 520 500 480 460 440 200K 400K 600K 800K 1000K Training Step OBLE gniniarT 40 260 30 250 20 240 230 10 220 0 210 Double CV RELAX (3 evals) 10 200 RLOO ARMS 20 190 DisARM 200K 400K 600K 800K 1000K 200K 400K 600K 800K 1000K Training Step Training Step Figure 12: Training linear binary latent VAEs with Gaussian likelihoods with K = 4 (except RELAX which needs 3 evaluations of f ) on non-binarized MNIST, Fashion-MNIST, and Omniglot. Top: Variance of gradient estimates. Bottom: Average ELBO on training examples.
