Hierarchical Bayesian Bandits Joey Hong Branislav Kveton Manzil Zaheer Mohammad Ghavamzadeh UC Berkeley∗ Amazon∗ Google DeepMind Google Research Abstract ing, an action could be showing an advertisement and its reward could be an indicator of a click. Meta-, multi-task, and federated learning can More statistically-efficient exploration is the primary be all viewed as solving similar tasks, drawn topic of bandit papers. This is attained by leverag- from a distribution that reflects task similar- ing the structure of the problem, such as the form of ities. We provide a unified view of all these the reward distribution (Garivier and Cappe, 2011), problems, as learning to act in a hierarchical prior distribution over model parameters (Thompson, Bayesian bandit. We propose and analyze a 1933; Agrawal and Goyal, 2012; Chapelle and Li, 2012; natural hierarchical Thompson sampling al- Russo et al., 2018), conditioning on known feature vec- gorithm (HierTS) for this class of problems. tors (Dani et al., 2008; Abbasi-Yadkori et al., 2011; Our regret bounds hold for many variants of Agrawal and Goyal, 2013), or modeling the process by the problems, including when the tasks are which the total reward arises (Radlinski et al., 2008; solved sequentially or in parallel; and show Kveton et al., 2015a; Gai et al., 2012; Chen et al., 2016; that the regret decreases with a more infor- Kveton et al., 2015b). In this work, we solve multiple mative prior. Our proofs rely on a novel to- similar bandit tasks, and each task teaches the agent tal variance decomposition that can be ap- how to solve other tasks more efficiently. plied beyond our models. Our theory is com- We formulate the problem of learning to solve similar plemented by experiments, which show that bandit tasks as regret minimization in a hierarchical the hierarchy helps with knowledge sharing Bayesian model (Gelman et al., 2013). Each task is among the tasks. This confirms that hierar- parameterized by a task parameter, which is sampled chical Bayesian bandits are a universal and i.i.d. from a distribution parameterized by a hyper- statistically-efficient tool for learning to act parameter. The parameters are unknown and this re- with similar bandit tasks. lates all tasks, in the sense that each task teaches the agent about any other task. We derive Bayes regret bounds that reflect the structure of the problem and 1 INTRODUCTION show that the price for learning the hyper-parameter is low. Our derivations use a novel total variance de- A stochastic bandit (Lai and Robbins, 1985; Auer composition, which decomposes the parameter uncer- et al., 2002; Lattimore and Szepesvari, 2019) is an on- tainty into per-task uncertainty conditioned on know- line learning problem where a learning agent sequen- ing the hyper-parameter and hyper-parameter uncer- tially interacts with an environment over n rounds. In tainty. After that, we individually bound each uncer- each round, the agent takes an action and receives a tainty source by elliptical lemmas (Dani et al., 2008; stochastic reward. The agent aims to maximize its ex- Abbasi-Yadkori et al., 2011). Our approach can be ex- pected cumulative reward over n rounds. It does not actly implemented and analyzed in hierarchical multi- know the mean rewards of the actions a priori, and armed and linear bandits with Gaussian rewards, but must learn them by taking the actions. This induces can be extended to other graphical model structures. the exploration-exploitation dilema: explore, and learn We build on numerous prior works that study a simi- more about an action; or exploit, and take the action lar structure, under the names of collaborating filter- with the highest estimated reward. In online advertis- ing bandits (Gentile et al., 2014; Kawale et al., 2015; Li et al., 2016), bandit meta-learning and multi-task Proceedings of the 25th International Conference on Artifi- learning (Azar et al., 2013; Deshmukh et al., 2017; Bas- cial Intelligence and Statistics (AISTATS) 2022, Valencia, Spain. PMLR: Volume 151. Copyright 2022 by the au- thor(s). ∗The work started while being at Google Research. 2202 raM 5 ]GL.sc[ 2v92960.1112:viXra
Hierarchical Bayesian Bandits tani et al., 2019; Cella et al., 2020; Kveton et al., 2021; Moradipari et al., 2021), and representation learning Q µ ∗ θ s,∗ Y s,t (Yang et al., 2021). Despite it, we make major novel contributions, both in terms of a more general setting t = 1, 2, . . . and analysis techniques. Our setting relaxes the as- s = 1, 2, . . . , m sumptions that the tasks are solved in a sequence and that exactly one task is solved per round. Moreover, Figure 1: Graphical model of our hierarchical Bayesian while the design of our posterior sampling algorithm bandit. is standard, we make novel contributions in its analy- sis. In the sequential setting (Section 6.1), we derive a Bayes regret bound by decomposing the posterior co- tasks. In a recommender system, each task could be variance, which is an alternative to prior derivations an individual user. The task s ∈ [m] is parameterized based on filtered mutual information (Russo and Van by a task parameter θ s,∗ ∈ Θ, which is sampled i.i.d. Roy, 2016; Lu and Van Roy, 2019). This technique is from a task prior distribution θ s,∗ ∼ P (· | µ ∗); which general, simple, and yields tighter regret bounds be- is parameterized by an unknown hyper-parameter µ ∗. cause it avoids marginal task parameter covariance; The agent acts at discrete decision points, which are and so is of a broad interest. In the concurrent setting integers and we call them rounds. At round t ≥ 1, the (Section 6.3), we bound the additional regret due to agent is asked to act in a set of tasks S ⊆ [m]. It takes t not updating the posterior after each interaction. This actions A = (A ) , where A ∈ A is the action i ds itn ao nn a-t lr yi sv ei sa .l Oan ud r a Bam ya esjo rr egd re ep ta brt ou ur ne dfr fo om r to ht ih s e sr etb ta inn g- in task s; t and recs, et ivs e∈ sS rt ewards Y t s =,t (Y s,t) s∈St ∈ R|St|, where Y ∼ P (· | A ; θ ) is a stochastic reward for s,t s,t s,∗ is the first of its kind. taking action A in task s. The rewards are drawn s,t The paper is organized as follows. In Section 2, we i.i.d. from their respective distributions. The set S t formalize our setting of hierarchical Bayesian bandits. can depend arbitrarily on the history. The assumption In Section 3, we introduce a natural Thompson sam- that the action set A is the same across all tasks and pling algorithm (HierTS) for solving it. In Section 4, rounds is only to simplify exposition. we instantiate it in hierarchical Gaussian models. In In hierarchical Bayesian bandits, the hyper-parameter Section 5, we review key ideas in our regret analyses, µ is initially sampled from a hyper-prior Q known by ∗ including a novel total covariance decomposition that the learning agent. Our full model is given by allows us to analyze posteriors in hierarchical models. In Section 6, we prove Bayes regret bounds for HierTS µ ∗ ∼ Q , in sequential and concurrent settings. Finally, in Sec- θ | µ ∼ P (· | µ ) , ∀s ∈ [m] , s,∗ ∗ ∗ tion 7, we evaluate HierTS empirically to confirm our Y | A , θ ∼ P (· | A ; θ ) , ∀t ≥ 1, s ∈ S , s,t s,t s,∗ s,t s,∗ t theoretical results. and also visualized in Figure 1. Note that P denotes both the task prior and reward distribution; but they 2 SETTING can be distinguished based on their parameters. Our setting is an instance of a hierarchical Bayesian model We use the following notation. Random variables are commonly used in supervised learning (Lindley and capitalized, except for Greek letters like θ and µ. For Smith, 1972; Zhang and Yang, 2017), and has been any positive integer n, we define [n] = {1, . . . , n}. The studied in bandits in special cases where tasks appear indicator function is denoted by 1{·}. The i-th entry sequentially (Kveton et al., 2021; Basu et al., 2021). of vector v is v . If the vector is already indexed, such i Our learning agent interacts with each of m tasks for as v , we write v . A matrix with diagonal entries v j j,i at most n times. So the total number of rounds varies, is diag (v). For any matrix M ∈ Rd×d, the maximum as it depends on the number of tasks that the agent eigenvalue is λ (M ) and the minimum is λ (M ). The 1 d interacts with simultaneously in each round. However, big O notation up to logarithmic factors is O˜. the maximum number of interactions is mn. The goal Now we present our setting for solving similar bandit is to minimize the Bayes regret (Russo and Van Roy, tasks. Each task is a bandit instance with actions a ∈ 2014) defined as A, where A denotes an action set. Rewards of actions   are generated by reward distribution P (· | a; θ), where (cid:88) (cid:88) θ ∈ Θ is an unknown parameter shared by all actions. BR(m, n) = E  r(A s,∗; θ s,∗) − r(A s,t; θ s,∗) , We assume that the rewards are σ2-sub-Gaussian and t≥1 s∈St denote by r(a; θ) = E [Y ] the mean reward of where A = arg max r(a; θ ) is the optimal ac- Y ∼P (·|a;θ) s,∗ a∈A s,∗ action a under θ. The learning agent interacts with m tion in task s. The expectation in BR(m, n) is over µ , ∗
Joey Hong, Branislav Kveton, Manzil Zaheer, Mohammad Ghavamzadeh Algorithm 1 Hierarchical Thompson sampling. includes observations of multiple tasks. 1: Input: Hyper-prior Q To sample θ , we must address how the uncertainty s,t 2: Initialize Q 1 ← Q over the unknown hyper-parameter µ and task pa- ∗ 3: for t = 1, 2, . . . do rameters θ is modeled. The key idea is to maintain s,∗ 4: Observe tasks S t ⊆ [m] a hyper-posterior Q over µ , given by t ∗ 5: Sample µ ∼ Q t t 6: for s ∈ S do Q (µ) = P (µ = µ | H ) , t t ∗ t 7: Compute P (θ | µ ) ∝ L (θ)P (θ | µ ) s,t t s,t t 8: Sample θ ∼ P (· | µ ) and then perform two-stage sampling. In particular, s,t s,t t 9: Take action A s,t ← arg max a∈A r(a; θ s,t) in round t, we first sample hyper-parameter µ t ∼ Q t. 10: Observe reward Y s,t Next, for any task s ∈ S t, we sample the task param- eter θ ∼ P (· | µ ), where 11: Update Q s,t s,t t t+1 P (θ | µ) = P (θ = θ | µ = µ, H ) . s,t s,∗ ∗ s,t θ s,∗ for s ∈ [m], and actions A s,t of the agent. While In P s,t(· | µ), we only condition on the history of task weaker than a traditional frequentist regret, the Bayes s, since θ s,∗ is independent of the other task histories regret is a practical performance metric, as we are of- given µ ∗ = µ (Figure 1). This process clearly samples ten interested in an average performance (Hong et al., from the true posterior, which is given by 2020; Kveton et al., 2021). For example, when recom- (cid:90) mending to a group of users, it is natural to optimize P (θ = θ | H ) = P (θ = θ, µ = µ | H ) dµ (1) s,∗ t s,∗ ∗ t over the whole population rather than an individual. µ (cid:90) Our definition of BR(m, n) is also dictated by the fact = P (θ | µ)Q (µ) dµ , s,t t that m and n are the primary quantities of interest in µ our regret analyses. Our goal is to minimize BR(m, n) without knowing µ ∗ and θ s,∗ a priori. where P s,t(θ | µ) ∝ L s,t(θ)P (θ | µ) and Since the set of tasks S can be chosen arbitrarily, our L (θ) = (cid:81) P (y | a; θ) t s,t (a,y)∈Hs,t setting is general and subsumes many prior settings. For instance, when the agent interacts with the same denotes the likelihood of rewards in task s given task task for n rounds before shifting to the next one, and parameter θ. S = {(cid:100)t/n(cid:101)}, we get a meta-learning bandit (Kveton t The pseudo-code of HierTS is shown in Algorithm 1. et al., 2021). More generally, when the agent interacts Sampling in (1) can be implemented exactly in Gaus- with the tasks sequentially, |S | = 1, our setting can t sian graphical models (Section 4). These models have be viewed as multi-task learning where any task helps interpretable closed-form posteriors, which permit the the agent to solve other tasks. Therefore, we recover regret analysis of HierTS. In practice, HierTS can be a multi-task bandit (Wan et al., 2021). Finally, when implemented for any posterior distributions, but may the agent acts in multiple tasks concurrently, |S | > 1, t require approximate inference (Doucet et al., 2001) to we recover the setting of collaborative filtering bandits tractably sample from the posterior. (Gentile et al., 2014; Li et al., 2016) or more recently federated bandits (Shi and Shen, 2021). Our algorithm 4 HIERARCHICAL GAUSSIAN and its analysis apply to all of these settings. BANDITS 3 ALGORITHM Now we instantiate HierTS in hierarchical Gaussian models. This yields closed-form posteriors, which per- We take a Bayesian view and use hierarchical Thomp- mit regret analysis (Section 5). We discuss generaliza- son sampling (TS), which we call HierTS, to solve our tion to other distributions in Section 5.4. problem class. HierTS samples task parameters from We assume that the environment is generated as their posterior conditioned on history. Specifically, let H = ((A , Y )) denote the history of all s,t s,(cid:96) s,(cid:96) (cid:96)<t, s∈S(cid:96) µ ∼ N (µ , Σ ) , (2) interactions of HierTS with task s until round t, and ∗ q q θ | µ ∼ N (µ , Σ ) , ∀s ∈ [m] , H = (H ) be the concatenation of all histories s,∗ ∗ ∗ 0 t s,t s∈[m] up to round t. For each task s ∈ S t in round t, HierTS Y s,t | A s,t, θ s,∗ ∼ N (A(cid:62) s,tθ s,∗, σ2) , ∀t ≥ 1, s ∈ S t , samples θ ∼ P (θ = · | H ) and then takes action s,t s,∗ t A = arg max r(a; θ ). The key difference from where Σ ∈ Rd×d and Σ ∈ Rd×d are covariance ma- s,t a∈A s,t q 0 classical Thompson sampling is that the history H trices; µ , µ , θ are d-dimensional vectors; the set t q ∗ s,∗
Hierarchical Bayesian Bandits of actions is A ⊆ Rd; and the mean reward of action 4.2 Linear Bandit with Gaussian Rewards a ∈ A is r(a; θ) = a(cid:62)θ. The reward noise is N (0, σ2). This formulation captures both the multi-armed and Now we study a d-dimensional linear bandit, which is linear bandits, since the actions in the former can be instantiated as (2) as follows. The task parameter θ s,∗ viewed as vectors in a standard Euclidean basis. We are coefficients in a linear model. The covariance ma- assume that all of µ q, Σ q, Σ 0, and σ are known by the trices Σ q are Σ 0 are positive semi-definite and known. agent. This assumption is only needed in the analysis The reward distribution of action a is N (a(cid:62)θ s,∗, σ2), of HierTS, where we require an analytically tractable where σ > 0 is a known reward noise. posterior. We relax it in our experiments (Section 7), Similarly to Section 4.1, we obtain closed-form poste- where we learn these quantities from past data. riors using Kveton et al. (2021). The hyper-posterior in round t is Q = N (µ¯ , Σ¯ ), where t t t 4.1 Gaussian Bandit µ¯ = Σ¯ (cid:16) Σ−1µ + (cid:88) B − G (Σ−1 + G )−1B (cid:17) t t q q s,t s,t 0 s,t s,t We start with a K-armed Gaussian bandit, which we s∈[m] instantiate as (2) as follows. The task parameter θ s,∗ = Σ¯ (cid:16) Σ−1µ + (cid:88) (Σ + G−1)−1G−1B (cid:17) , is a vector of mean rewards in task s, where θ s,∗,i is t q q 0 s,t s,t s,t the mean reward of action i. The covariance matrices s∈[m] are diagonal, Σ q = σ q2I K and Σ 0 = σ 02I K. We assume Σ¯ − t 1 = Σ− q 1 + (cid:88) G s,t − G s,t(Σ− 0 1 + G s,t)−1G s,t that both σ > 0 and σ > 0 are known. The reward q 0 s∈[m] distribution of action i is N (θ s,∗,i, σ2), where σ > 0 is = Σ−1 + (cid:88) (Σ + G−1)−1 . (5) a known reward noise. q 0 s,t s∈[m] Because Σ and Σ are diagonal, the hyper-posterior q 0 Here in round t factors across the actions. Specifically, it is Q = N (µ¯ , Σ¯ ), where Σ¯ = diag (cid:0) (σ¯2 ) (cid:1) and G = σ−2 (cid:88) 1{s ∈ S } A A(cid:62) t t t t t,i i∈[K] s,t (cid:96) s,(cid:96) s,(cid:96)   (cid:96)<t µ¯ t,i = σ¯ t2 ,i  µ σq q2,i + s(cid:88) ∈[m] N s,tN ,iσs, 02t, +i σ2 NB s s, ,t t, ,i i  , (3) i ts ast khe so uu pte tr op rr oo ud nu dct t o af nt dhe features of taken actions in (cid:88) σ¯−2 = σ−2 + (cid:88) N s,t,i . B s,t = σ−2 1{s ∈ S (cid:96)} A s,(cid:96)Y s,(cid:96) t,i q s∈[m] N s,t,iσ 02 + σ2 (cid:96)<t is their sum weighted by the observed rewards. Simi- Here N = (cid:80) 1{s ∈ S , A = i} is the number s,t,i (cid:96)<t (cid:96) s,(cid:96) larly to (3), it is helpful to view (5) as a multivariate of times that action i is taken in task s up to round Gaussian posterior where each task is a single observa- t and B = (cid:80) 1{s ∈ S , A = i} Y is its total s,t,i (cid:96)<t (cid:96) s,(cid:96) s,(cid:96) tion. The observation of task s is the least squares es- reward. The hyper-posterior is derived in Appendix D timate of θ from task s, G−1B , and its covariance s,∗ s,t s,t of Kveton et al. (2021). To understand it, it is helpful is Σ + G−1. Again, the tasks with many observations to view it as a Gaussian posterior where each task is 0 s,t affect the value of µ¯ more, because G−1 approaches a a single observation. The observation of task s is the t s,t zero matrix in these tasks. In this setting, the covari- empirical mean reward estimate of action i in task s, ance approaches Σ , because even the unknown task B /N , and its variance is (N σ2 + σ2)/N . 0 s,t,i s,t,i s,t,i 0 s,t,i parameter θ would be a noisy observation of µ with The tasks with more observations affect the value of s,∗ ∗ covariance Σ . µ¯ more, because their mean reward estimates have 0 t,i lower variances. The variance never decreases below After the hyper-parameter is sampled, µ ∼ Q , the t t σ 02, because even the actual mean reward θ s,∗,i would task parameter is sampled, θ s,t ∼ N (µ˜ s,t, Σ˜ s,t), where be a noisy observation of µ with variance σ2. ∗,i 0 µ˜ = Σ˜ (cid:0) Σ−1µ + B (cid:1) , (6) s,t s,t 0 t s,t After the hyper-parameter is sampled, µ ∼ Q , the task parameter is sampled, θ ∼ N (µ˜ , t Σ˜ ), wt here Σ˜ − s,t1 = Σ− 0 1 + G s,t . s,t s,t s,t Σ˜ = diag (cid:0) (σ˜2 ) (cid:1) and The above is the posterior of a linear model with a s,t s,t,i i∈[K] Gaussian prior N (µ , Σ ) and Gaussian observations. (cid:18) (cid:19) t 0 µ B µ˜ = σ˜2 t + s,t,i , (4) s,t,i s,t,i σ2 σ2 0 5 KEY IDEAS IN OUR ANALYSES 1 N σ˜−2 = + s,t,i . s,t,i σ2 σ2 0 This section reviews key ideas in our regret analyses, Note that the above is a Gaussian posterior with prior including a novel variance decomposition for the pos- N (µ , σ2I ) and N observations. terior of a hierarchical Gaussian model. Due to space t 0 K s,t,i
Joey Hong, Branislav Kveton, Manzil Zaheer, Mohammad Ghavamzadeh constraints, we only discuss the linear bandit in Sec- 5.2 Total Variance Decomposition tion 4.2. Due to the hierarchical structure of our problem, it is difficult to reason about the rate at which Σˆ “de- 5.1 Bayes Regret Bound s,t creases”. In this work, we propose a novel variance de- Fix round t and task s ∈ S . Since HierTS is a poste- composition that allows this. The decomposition uses t rior sampling algorithm, both the posterior sample θ the law of total variance (Weiss, 2005), which states s,t and the unknown task parameter θ are i.i.d. condi- that for any X and Y , s,∗ tioned on H . Moreover, (1) is a marginalization and t var [X] = E [var [X | Y ]] + var [E [X | Y ]] . conditioning in a hierarchical Gaussian model given in Figure 1. Therefore, although we never explicitly de- If X = θ was a scalar task parameter and Y = µ was rive θ | H , we know that it is a multivariate Gaus- s,∗ t a scalar hyper-parameter, and we conditioned on H, sian distribution (Koller and Friedman, 2009); and we the law would give denote it by P (θ = θ | H ) = N (θ; µˆ , Σˆ ). s,∗ t s,t s,t Following existing Bayes regret analyses (Russo and var [θ | H] = E [var [θ | µ, H] | H] + var [E [θ | µ, H] | H] . Van Roy, 2014), we have that This law extends to covariances (Weiss, 2005), where E (cid:2) A(cid:62) θ − A(cid:62) θ | H (cid:3) = the conditional variance var [· | H] is substituted with s,∗ s,∗ s,t s,∗ t E (cid:2) A(cid:62) (θ − µˆ ) | H (cid:3) + E (cid:2) A(cid:62) (µˆ − θ ) | H (cid:3) . the covariance cov [· | H]. We show the decomposition s,∗ s,∗ s,t t s,t s,t s,∗ t for a hierarchical Gaussian model below. Conditioned on history H , we observe that µˆ − θ t s,t s,∗ is a zero-mean random vector and that A is inde- 5.3 Hierarchical Gaussian Models s,t p the end Be an yt eo sf reit g. reH te in sc be oE un(cid:2) A de(cid:62) s d,t( aµˆ ss,t − θ s,∗) (cid:12) (cid:12) H t(cid:3) = 0 and Recall that Σˆ s,t = cov [θ s,∗ | H t]. We derive a general formula for decomposing cov [θ | H ] below. To sim-   s,∗ t plify notation, we consider a fixed task s and round t, BR(m, n) ≤ E (cid:88) (cid:88) E (cid:2) A(cid:62) s,∗(θ s,∗ − µˆ s,t) | H t(cid:3)  . and drop subindexing by them. t≥1 s∈St Lemma 2. Let θ | µ ∼ N (µ, Σ ) and H = (x , Y )n 0 t t t=1 The following lemma provides an upper bound on the be n observations generated as Y | θ, x ∼ N (x(cid:62)θ, σ2). t t t Bayes regret for m tasks, with at most n interactions Let P (µ | H) = N (µ; µ¯, Σ¯ ). Then with each, using the sum of posterior variances cov [θ | H] = (Σ−1 + G)−1 +   0 (cid:88) (cid:88) (Σ−1 + G)−1Σ−1Σ¯ Σ−1(Σ−1 + G)−1 , V(m, n) = E  (cid:107)A s,t(cid:107)2 Σˆ s,t . (7) 0 0 0 0 t≥1 s∈St where G = σ−2 (cid:80)n x x(cid:62). Moreover, for any x ∈ Rd, t=1 t t The proof is deferred to Appendix A. x(cid:62)(Σ−1 + G)−1Σ−1Σ¯ Σ−1(Σ−1 + G)−1x Lemma 1. For any δ > 0, the Bayes regret BR(m, n) 0 0 0 0 λ2(Σ )λ (Σ¯ ) in a hierarchical linear bandit (Section 4.2) is bounded ≤ 1 0 1 (cid:107)x(cid:107)2 . by λ2 d(Σ 0) 2 (cid:112) 2dmnV(m, n) log(1/δ) + (cid:112) 2/πσ maxd 3 2 mnδ , Proof. By definition, where σ m2 ax = λ 1(Σ 0) + λ2 1(Σ 0)λ 1(Σ q)/λ2 d(Σ 0). When cov [θ | µ, H] = (Σ−1 + G)−1 , the action space is finite, |A| = K, we also get 0 E [θ | µ, H] = cov [θ | µ, H] (Σ−1µ + B) , (cid:112) (cid:112) 0 2mnV(m, n) log(1/δ) + 2/πσ Kmnδ . max where B = σ−2 (cid:80)n x Y . Because cov [θ | µ, H] does t=1 t t not depend on µ, E [cov [θ | µ, H] | H] = cov [θ | µ, H]. Therefore, to bound the regret, we only need to bound In addition, since B is a constant conditioned on H, the posterior variances induced by the taken actions. The main challenge is that our posterior is over mul- cov [E [θ | µ, H] | H] tiple variables. As can be seen in (1), it comprises the hyper-posterior Q over µ and the conditional P = cov (cid:2) cov [θ | µ, H] Σ−1µ (cid:12) (cid:12) H(cid:3) t ∗ s,t 0 over θ s,∗. For any fixed µ ∗, P s,t(· | µ ∗) should concen- = (Σ−1 + G)−1Σ−1Σ¯ Σ−1(Σ−1 + G)−1 . 0 0 0 0 trate at θ as the agent gets more observations from s,∗ task s. In addition, Q should concentrate at µ as the This proves the first claim. The second claim follows t ∗ agent learns more about µ from all tasks. from standard norm and eigenvalue inequalities. ∗
Hierarchical Bayesian Bandits We use Lemma 2 as follows. For task s and round t, The rest of the analysis, where our main contributions the posterior covariance decomposes as are, would not change. Σˆ = (Σ−1 + G )−1 + (8) s,t 0 s,t 6 REGRET BOUNDS (Σ−1 + G )−1Σ−1Σ¯ Σ−1(Σ−1 + G )−1 . 0 s,t 0 t 0 0 s,t This section bounds the Bayes regret of HierTS in the The first term is cov [θ | µ , H ] and captures uncer- s,∗ ∗ t linear bandit in Section 4.2. Our bounds are special- tainty in θ conditioned on µ . The second term de- s,∗ ∗ ized to multi-armed bandits in Appendix D. The key pends on hyper-posterior covariance Σ¯ and represents t idea is to bound the posterior variances V(m, n) in (7) uncertainty in µ . Since the first term is exactly Σ˜ ∗ s,t and then substitute the bound into the infinite-action in (6), while the second term is weighted by it, both bound in Lemma 1. We bound the variances using the are small when we get enough observations for task s. total covariance decomposition in Section 5.2. With- The above also says that (cid:107)A (cid:107)2 = A(cid:62) Σˆ A in s,t Σˆ s,t s,t s,t s,t out loss of generality, we assume that the action set A (7) decompose into the two respective norms, which is a subset of a unit ball, that is max (cid:107)a(cid:107) ≤ 1 for a∈A 2 yields our regret decomposition. any action a ∈ A. We make the following contributions in theory. First, 5.4 Extensions we prove regret bounds using a novel variance decom- position (Section 5.2), which improves in constant fac- So far, we only focused on hierarchical Gaussian mod- tors over classical information-theory bounds (Russo els with known hyper-prior and task prior covariances. and Van Roy, 2016). Second, we prove the first Bayes This is only because they have closed-form posteriors regret bound for the setting where an agent that in- that are easy to interpret and manipulate, without re- teracts with multiple tasks simultaneously. sorting to approximations (Doucet et al., 2001). This choice simplifies algebra and allows us to focus on the This section has two parts. In Section 6.1, we assume key hierarchical structure of our problem. We believe that only one action is taken in any round t, |S | = 1. t that the tools developed in this section can be applied We call this setting sequential, and note that it is the more broadly. We discuss this next. primary setting studied by prior works (Kveton et al., 2021; Basu et al., 2021). In Section 6.3, we focus on a Lemma 1 decomposes the Bayes regret into posterior concurrent setting, where a single action can be taken variances and upper bounds on the regret due to tail in up to L tasks in any round t, |S | ≤ L ≤ m. The events. The posterior variance can be derived for any t challenge of this setting is that the task parameters exponential-family posterior with a conjugate prior. are only updated after all actions are taken. On the other hand, the tail inequalities require sub- Gaussianity, which is a property of many exponential- 6.1 Sequential Regret family distributions. Lemma 2 decomposes the posterior covariance in a hi- The following theorem provides a regret bound for the erarchical Gaussian model. It relies on the law of total sequential setting. covariance, which holds for any distribution, to ob- Theorem 3 (Sequential regret). Let |S | = 1 for all t tain the task and hyper-parameter uncertainties. We rounds t and A ⊆ Rd. Choose δ = 1/(mn). Then the expect that similar lemmas can be proved for other hi- Bayes regret of HierTS is erarchical models, so long as closed-form expressions (cid:112) for the respective uncertainties exist. Another notable BR(m, n) ≤ d 2mn[c m + c ] log(mn) + c , 1 2 3 property of our decomposition is that it does not re- quire the marginal posterior of θ s,∗. We view it as a where c 3 = O(d 3 2 ), strength. It means that our approach can be applied (cid:18) (cid:19) λ (Σ ) λ (Σ )n to complex graphical models where the marginal un- c = 1 0 log 1 + 1 0 , 1 log(1 + σ−2λ (Σ )) σ2d certainty may be hard to express, but the conditional 1 0 (cid:18) (cid:19) and prior uncertainties are readily available. c = c qc log 1 + λ 1(Σ q)m , 2 log(1 + σ−2c ) λ (Σ ) One limitation of our analyses is that we bound the q d 0 λ2(Σ )λ (Σ ) Bayes regret, instead of a stronger frequentist regret. c = 1 0 1 q , c = 1 + σ−2λ (Σ ) . This simplifies our proofs while they still capture our q λ2 d(Σ 0) 1 0 problem structure. Our analyses can be extended to the frequentist setting. This only requires a new proof The proof of Theorem 3 is based on three steps. First, of Lemma 1, with martingale bounds for tail events we use Lemma 1. Second, we employ Lemma 2 to de- and anti-concentration bounds for posterior sampling. compose the posterior variance in any round into that
Joey Hong, Branislav Kveton, Manzil Zaheer, Mohammad Ghavamzadeh of the task parameters and hyper-parameter. Finally, lower regret as m → ∞. This is a powerful testament we apply elliptical lemmas to bound each term sepa- to the benefit of learning the hyper-parameter. √ rately. Theorem 3 has a nice interpretation: m c n √ 1 Finally, we want to comment on linear dependence in is the regret for learning task parameters and c mn 2 d and m in Theorem 3. The dependence on d is stan- is the regret for learning the hyper-parameter µ . We ∗ dard in Bayes regret analyses for linear bandits with elaborate on both terms below. infinitely many arms (Russo and Van Roy, 2014; Lu √ The term m c n represents the regret for solving m and Van Roy, 2019). As for the number of tasks m, 1 bandit tasks, which are sampled i.i.d. from a known since the tasks are drawn i.i.d. from the same hyper- prior N (µ , Σ ). Under this assumption, no task pro- prior, they do not provide any additional information ∗ 0 vides information about any other task, and thus the about each other. So, even if the hyper-parameter is term is linear in m. The constant c is O(λ (Σ )) and known, the regret for learning to act in m tasks with 1 1 (cid:112) 0 √ reflects the dependence on the prior width λ (Σ ). n rounds would be O(m n). Our improvements are 1 0 Roughly speaking, when the task prior is half as infor- in constants due to better variance attribution. Other √ √ mative, c doubles and so does m c n. This is the bandit meta-learning works (Kveton et al., 2021; Basu 1 1 expected scaling with conditional uncertainty of θ et al., 2021) made similar observations. Also note that s,∗ given µ . the frequentist regret of LinTS applied to m indepen- ∗ √ The term √ c mn is the regret for learning the hyper- dent linear bandit tasks is O˜(md 3 2 n) (A √grawal and 2 √ Goyal, 2013). This is worse by a factor of d than the parameter. Asymptotically, it is O( m) smaller than √ bound in Theorem 3. m c n. Therefore, for a large number of tasks m, its 1 contribution to the total regret is negligible. This is 6.3 Concurrent Regret why hierarchical Bayesian bandits perform so well in practice. The constant c is O(λ (Σ )) and reflects the 2 1 q Now we investigate the concurrent setting, where the (cid:112) dependence on the hyper-prior width λ (Σ ). When √ 1 0 agent acts in up to L tasks per round. This setting is the hyper-prior is half as informative, c doubles and √ 2 challenging because the hyper-posterior Q t is not up- so does c mn. This is the expected scaling with the 2 dated until the end of the round. This is because the marginal uncertainty of µ . ∗ task posteriors are not refined with the observations from concurrent tasks. This delayed feedback should 6.2 Tightness of Regret Bounds increase regret. Before we show it, we make the fol- lowing assumption on the action space. One shortcoming of our current analysis is that we do Assumption 1. There exist actions {a }d ⊆ A and not provide a matching lower bound. To the best of η > 0 such that λ ((cid:80)d a a(cid:62)) ≥ η. i i=1 our knowledge, Bayes regret lower bounds are rare and d i=1 i i do not match existing upper bounds. The only lower This assumption is without loss of generality. Specifi- bound that we are aware of is Ω(log2 n) in Theorem 3 cally, if Rd was not spanned by actions in A, we could of Lai (1987). The bound is for K-armed bandits and project A into a subspace where the assumption would it is unclear how to apply it to structured problems. hold. Our regret bound is below. Seminal works on Bayes regret minimization (Russo Theorem 4 (Concurrent regret). Let |S | ≤ L ≤ m and Van Roy, 2014, 2016) do not match it. Therefore, t and A ⊆ Rd. Let δ = 1/(mn). Then the Bayes regret to show that our problem structure is reflected in our of HierTS is bound, we compare the regret of HierTS to baselines that have more information or use less structure. (cid:112) BR(m, n) ≤ d 2mn[c m + c ] log(mn) + c , 1 2 3 Now we compare the regret of HierTS to two LinTS (Agrawal and Goyal, 2013) baselines that do use our where c 1, c q, and c are defined as in Theorem 3, hierarchical model. This first is an oracle LinTS that (cid:18) (cid:19) c c c λ (Σ )m k Itn so Bw as yµ e∗ s, ra egn rd etso wh oa us ldm bo ere asin if nor Tm ha et oio ren mth 3an wiH ti he cr 2TS =. c 2 = log(1 +q 4 σ−2c q) log 1 + λ1 d(Σq 0) , 0. Not surprisingly, it is lower than that of HierTS. σ−2λ (Σ )(λ (Σ ) + σ2/η) c = 1 + 1 q 1 0 , The second baseline is LinTS that knows that µ ∗ ∼ 4 λ 1(Σ q) + (λ 1(Σ 0) + σ2/η)/L N (µ , Σ ), but does not model the structure that the q q tasks share µ ∗. In this case, each task parameter can and c 3 = O(d 23 m). be viewed as having prior N (µ , Σ + Σ ). The regret q q 0 of this algorithm would be as in Theorem 3 with c = The key step in the proof is to modify HierTS as fol- 2 0, while λ (Σ ) in c would be λ (Σ + Σ ). Since c lows. For the first d interactions with any task s, we 1 0 1 1 q 0 1 is multiplied by m while c is not, HierTS would have take actions {a }d . This guarantees that we explore 2 i i=1
Hierarchical Bayesian Bandits all directions within the task, and allows us to bound Figure 2. In plots (a) and (b), we show how the regret losses from not updating the task posterior with con- scales with the number of rounds for small (σ = 0.5) q current observations. This modification of HierTS is and large (σ = 1) hyper-prior width. As suggested in q trivial and analogous to popular initialization in ban- Section 6.1, HierTS outperforms TS that does not try dits, where each arm is pulled once in the first rounds to learn µ . It is comparable to OracleTS when σ is ∗ q (Auer et al., 2002). small, but degrades as σ increases. This matches the q regret bound in Theorem 4, where c grows with σ . The regret bound in Theorem 4 is similar to that in 2 q In plot (c), we show how the regret of HierTS varies Theorem 3. There are two key differences. First, the with the number of concurrent tasks L. We observe additional scaling factor c in c is the price for taking 4 2 that it increases with L, but the increase is sublinear, concurrent actions. It increases as more actions L are as suggested in Section 6.3. taken concurrently, but is sublinear in L. Second, c 3 arises due to trivially bounding dm rounds of forced 8 RELATED WORK exploration. To the best of our knowledge, Theorem 4 is the first Bayes regret bound where multiple bandit tasks are solved concurrently. Prior works only proved The most related works are recent papers on bandit frequentist regret bounds (Yang et al., 2021). meta-learning (Bastani et al., 2019; Ortega et al., 2019; Cella et al., 2020; Kveton et al., 2021; Basu et al., 2021; Peleg et al., 2021; Simchowitz et al., 2021), where a 7 EXPERIMENTS learning agent interacts with a single task at a time until completion. Both Kveton et al. (2021) and Basu We compare HierTS to two TS baselines (Section 6.2) et al. (2021) represent their problems using graphical that do not learn the hyper-parameter µ . The first models and apply Thompson sampling to solve them. ∗ baseline is an idealized algorithm that knows µ and The setting of these papers is less general than ours. ∗ uses the true prior N (µ , Σ ). We call it OracleTS. Wan et al. (2021) study a setting where the tasks can ∗ 0 As OracleTS has more information than HierTS, we arrive in any order. We differ from this work in several expect it to outperform HierTS. The second baseline, aspects. First, they only consider a K-armed bandit. which we call TS, ignores that µ is shared among the Second, their model is different. In our notation, Wan ∗ tasks and uses the marginal prior of θ , N (µ , Σ + et al. (2021) assume that the mean reward of action s,∗ q q Σ ), in each task. a in task s is x(cid:62) µ plus i.i.d. noise, where x is an 0 s,a ∗ s,a observed feature vector. The i.i.d. noise prevents gen- We experiment with two linear bandit problems with eralization to a large number of actions. In our work, m = 10 tasks: a synthetic problem with Gaussian re- the mean reward of action a in task s is a(cid:62)θ , where wards and an online image classification problem. The s,∗ θ ∼ N (µ , Σ ). Third, Wan et al. (2021) derive former is used to validate our regret bounds. The lat- s,∗ ∗ 0 a frequentist regret bound, which matches Theorem 3 ter has non-Gaussian rewards and demonstrates that asymptotically, but does not explicitly depend on prior HierTS is robust to prior misspecification. Our setup widths. Finally, Wan et al. (2021) do not consider the closely follows Basu et al. (2021). However, our tasks concurrent setting. To the best of our knowledge, we can arrive in an arbitrary order and in parallel. Due are the first to study Bayesian bandits with arbitrarily to space constraints, we only report the synthetic ex- ordered and concurrent tasks. periment here, and defer the rest to Appendix E. The novelty in our analysis is the total covariance de- The synthethic problem is defined as follows: d = 2, composition, which leads to better variance attribu- |A| = 10, and each action is sampled uniformly from tion in structured models than information-theoretic [−0.5, 0.5]d. Initially, the number of concurrent tasks bounds (Russo and Van Roy, 2016; Lu and Van Roy, is L = 5; but we vary it later to measure its impact 2019; Basu et al., 2021). For instance, take Theorem on regret. The number of rounds is n = 200m/L and 5 of Basu et al. (2021), which corresponds to our se- S is defined as follows. First, we take a random per- t quential meta-learning setting. Forced exploration is mutation of the list of tasks where each task appears needed to make their task term O(λ (Σ )). This is be- exactly 200 times. Then we batch every L consecutive 1 0 cause the upper bound on the regret with filtered mu- elements of the list and set S to the t-th batch. The t tual information depends on the maximum marginal hyper-prior is N (0, Σ ) with Σ = σ2I , the task co- q q q d task parameter covariance, which can be λ (Σ + Σ ). variance is Σ = σ2I , and the reward noise is σ = 0.5. 1 q 0 0 0 d In our analysis, the comparable term c (Theorem 3) We choose σ ∈ {0.5, 1} and σ = 0.1, where σ (cid:29) σ 1 q 0 q 0 is O(λ (Σ )) without any forced exploration. We also so that the effect of learning µ on faster learning of 1 0 ∗ improve upon related analysis of Kveton et al. (2021) θ is easier to measure. s,∗ in several aspects. First, Kveton et al. (2021) analyze The regret of all compared algorithms is reported in only a K-armed bandit. Second, they derive that the
Joey Hong, Branislav Kveton, Manzil Zaheer, Mohammad Ghavamzadeh 200 150 100 50 0 0 100 200 300 400 Round t tergeR Linear Bandit (d = 2, ¾q = 0.500) 200 OracleTS TS 150 HierTS 100 50 0 0 100 200 300 400 Round t tergeR Linear Bandit (d = 2, ¾q = 1.000) 100 OracleTS TS 90 HierTS 80 70 60 50 1 2 3 4 5 6 7 8 9 10 Number of Concurrent Tasks L tergeR Linear (d = 8, ¾q = 0.500) HierTS Figure 2: Evaluation of HierTS on synthetic bandit problems. From left to right, we report the Bayes regret (a) for smaller σ , (b) for larger σ , (c) and as a function of the number of concurrent tasks L. q q √ additional regret for meta-learning is O˜( mn2); while setting recover meta-, multi-task, and federated ban- √ our bound shows O˜( mn). Finally, our setting gener- dits in prior works. We propose a natural hierarchical alizes bandit meta-learning. Thompson sampling algorithm, which can be imple- mented exactly and analyzed in Gaussian models. We Meta- and multi-task bandits have also been studied analyze it using a novel total variance decomposition, in the frequentist setting (Azar et al., 2013; Deshmukh which leads to interpretable regret bounds that scale et al., 2017). Cella et al. (2020) propose a LinUCB al- with the hyper-prior and task prior widths. The ben- gorithm (Abbasi-Yadkori et al., 2011) that constructs efit of hierarchical models is shown in both synthetic an ellipsoid around the unknown hyper-parameter in and real-world domains. a linear bandit. The concurrent setting has also been studied, but with a different shared structure of task While we view our work as solving an extremely gen- parameters. Dubey and Pentland (2020) use a kernel eral problem, there are multiple directions for future matrix, Wang et al. (2021) utilize pairwise distances of work. For instance, we only study a specific hierarchi- task parameters, and Yang et al. (2021) use low-rank cal Gaussian structure in Section 4. However, based on factorization. Our structure, where the task parame- the discussion in Section 5.4, we believe that our tools ters are drawn from an unknown prior, is both novel would apply to arbitrary graphical models with gen- and important to study because it differs significantly eral sub-Gaussian distributions. Another direction for from the aforementioned works. Earlier works on ban- future work are frequentist upper bounds and match- dits with similar instances rely on clustering (Gentile ing lower bounds, in both the frequentist and Bayesian et al., 2014, 2017; Li et al., 2016) and low-rank factor- settings. ization (Kawale et al., 2015; Sen et al., 2017; Katariya et al., 2016, 2017). They analyze the frequentist re- References gret, which is a stronger metric than the Bayes regret. Y. Abbasi-Yadkori, D. Pal, and C. Szepesvari. Im- Except for one work, all algorithms are UCB-like and proved algorithms for linear stochastic bandits. In conservative in practice. In comparison, HierTS uses a Advances in Neural Information Processing Systems natural stochastic structure. This makes it practical, 24, pages 2312–2320, 2011. to the point that the analyzed algorithm performs well in practice without any additional tuning. S. Agrawal and N. Goyal. Analysis of Thompson sam- pling for the multi-armed bandit problem. In Pro- Another related line of work are latent bandits (Mail- ceeding of the 25th Annual Conference on Learning lard and Mannor, 2014; Hong et al., 2020, 2022), where Theory, pages 39.1–39.26, 2012. the bandit problem is parameterized by an unknown latent state. If known, the latent state could help the S. Agrawal and N. Goyal. Thompson sampling for agent to identify the bandit instance that it interacts contextual bandits with linear payoffs. In Proceed- with. These works reason about latent variables; but ings of the 30th International Conference on Ma- the purpose is different from our work, where we intro- chine Learning, pages 127–135, 2013. duce the unknown hyper-parameter µ to relate mul- ∗ P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time tiple similar tasks. analysis of the multiarmed bandit problem. Machine Learning, 47:235–256, 2002. 9 CONCLUSIONS M. G. Azar, A. Lazaric, and E. Brunskill. Sequen- tial transfer in multi-armed bandit with finite set of We study hierarchical Bayesian bandits, a general set- models. In Advances in Neural Information Process- ting for solving similar bandit tasks. Instances of our ing Systems 26, pages 2220–2228, 2013.
Hierarchical Bayesian Bandits H. Bastani, D. Simchi-Levi, and R. Zhu. Meta bandits. In Proceedings of the 34th International dynamic pricing: Transfer learning across experi- Conference on Machine Learning, 2017. ments. CoRR, abs/1902.10918, 2019. URL https: J. Hong, B. Kveton, M. Zaheer, Y. Chow, A. Ahmed, //arxiv.org/abs/1902.10918. and C. Boutilier. Latent bandits revisited. In Ad- S. Basu, B. Kveton, M. Zaheer, and C. Szepesvari. No vances in Neural Information Processing Systems regrets for learning the prior in bandits. In Advances 33, 2020. in Neural Information Processing Systems 34, 2021. J. Hong, B. Kveton, M. Zaheer, M. Ghavamzadeh, L. Cella, A. Lazaric, and M. Pontil. Meta-learning and C. Boutilier. Thompson sampling with a mix- with stochastic linear bandits. In Proceedings of the ture prior. In Proceedings of the 25th International 37th International Conference on Machine Learn- Conference on Artificial Intelligence and Statistics, ing, 2020. 2022. O. Chapelle and L. Li. An empirical evaluation of S. Katariya, B. Kveton, C. Szepesvari, and Z. Wen. Thompson sampling. In Advances in Neural In- DCM bandits: Learning to rank with multiple formation Processing Systems 24, pages 2249–2257, clicks. In Proceedings of the 33rd International 2012. Conference on Machine Learning, pages 1215–1224, 2016. W. Chen, Y. Wang, Y. Yuan, and Q. Wang. Com- binatorial multi-armed bandit and its extension to S. Katariya, B. Kveton, C. Szepesvari, C. Vernade, and probabilistically triggered arms. Journal of Machine Z. Wen. Stochastic rank-1 bandits. In Proceedings Learning Research, 17(50):1–33, 2016. of the 20th International Conference on Artificial Intelligence and Statistics, 2017. V. Dani, T. Hayes, and S. Kakade. Stochastic linear optimization under bandit feedback. In Proceedings J. Kawale, H. Bui, B. Kveton, L. Tran-Thanh, and of the 21st Annual Conference on Learning Theory, S. Chawla. Efficient Thompson sampling for online pages 355–366, 2008. matrix-factorization recommendation. In Advances in Neural Information Processing Systems 28, pages A. A. Deshmukh, U. Dogan, and C. Scott. Multi- 1297–1305, 2015. task learning for contextual bandits. In Advances in Neural Information Processing Systems 30, pages D. Koller and N. Friedman. Probabilistic Graphical 4848–4856, 2017. Models: Principles and Techniques. MIT Press, Cambridge, MA, 2009. A. Doucet, N. de Freitas, and N. Gordon. Sequential Monte Carlo Methods in Practice. Springer, New B. Kveton, C. Szepesvari, Z. Wen, and A. Ashkan. York, NY, 2001. Cascading bandits: Learning to rank in the cas- cade model. In Proceedings of the 32nd International A. Dubey and A. Pentland. Kernel methods for coop- Conference on Machine Learning, 2015a. erative multi-agent contextual bandits. In Proceed- ings of the 37th International Conference on Ma- B. Kveton, Z. Wen, A. Ashkan, and C. Szepesvari. chine Learning, 2020. Tight regret bounds for stochastic combinatorial semi-bandits. In Proceedings of the 18th Inter- Y. Gai, B. Krishnamachari, and R. Jain. Combinato- national Conference on Artificial Intelligence and rial network optimization with unknown variables: Statistics, 2015b. Multi-armed bandits with linear rewards and indi- vidual observations. IEEE/ACM Transactions on B. Kveton, M. Konobeev, M. Zaheer, C.-W. Hsu, Networking, 20(5):1466–1478, 2012. M. Mladenov, C. Boutilier, and C. Szepesvari. Meta- Thompson sampling. In Proceedings of the 38th In- A. Garivier and O. Cappe. The KL-UCB algorithm ternational Conference on Machine Learning, 2021. for bounded stochastic bandits and beyond. In Pro- ceeding of the 24th Annual Conference on Learning T. L. Lai. Adaptive treatment allocation and the Theory, pages 359–376, 2011. multi-armed bandit problem. The Annals of Statis- tics, 15(3):1091–1114, 1987. A. Gelman, J. Carlin, H. Stern, D. Dunson, A. Vehtari, and D. Rubin. Bayesian Data Analysis. Chapman T. L. Lai and H. Robbins. Asymptotically efficient & Hall, 2013. adaptive allocation rules. Advances in Applied Mathematics, 6(1):4–22, 1985. C. Gentile, S. Li, and G. Zappella. Online clustering of bandits. In Proceedings of the 31st International T. Lattimore and C. Szepesvari. Bandit Algorithms. Conference on Machine Learning, pages 757–765, Cambridge University Press, 2019. 2014. Y. LeCun, C. Cortes, and C. Burges. MNIST Hand- C. Gentile, S. Li, P. Kar, A. Karatzoglou, G. Zappella, written Digit Database. http://yann.lecun.com/ and E. Etrue. On context-dependent clustering of exdb/mnist, 2010.
Joey Hong, Branislav Kveton, Manzil Zaheer, Mohammad Ghavamzadeh S. Li, A. Karatzoglou, and C. Gentile. Collaborative M. Simchowitz, C. Tosh, A. Krishnamurthy, D. Hsu, filtering bandits. In Proceedings of the 39th Annual T. Lykouris, M. Dudik, and R. Schapire. Bayesian International ACM SIGIR Conference, 2016. decision-making under misspecified priors with ap- plications to meta-learning. In Advances in Neural D. Lindley and A. Smith. Bayes estimates for the lin- Information Processing Systems 34, 2021. ear model. Journal of the Royal Statistical Society: Series B (Methodological), 34(1):1–18, 1972. W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence X. Lu and B. Van Roy. Information-theoretic con- of two samples. Biometrika, 25(3-4):285–294, 1933. fidence bounds for supervised learning. In Ad- vances in Neural Information Processing Systems R. Wan, L. Ge, and R. Song. Metadata-based multi- 32, 2019. task bandits with bayesian hierarchical models. In Advances in Neural Information Processing Systems O.-A. Maillard and S. Mannor. Latent bandits. In 34, 2021. Proceedings of the 31st International Conference on Machine Learning, pages 136–144, 2014. Z. Wang, C. Zhang, M. K. Singh, L. Riek, and K. Chaudhuri. Multitask bandit learning through A. Moradipari, B. Turan, Y. Abbasi-Yadkori, M. Al- heterogeneous feedback aggregation. In Proceedings izadeh, and M. Ghavamzadeh. Parameter and fea- of the 24th International Conference on Artificial ture selection in stochastic linear bandits. CoRR, Intelligence and Statistics, 2021. abs/2106.05378, 2021. URL https://arxiv.org/ abs/2106.05378. N. Weiss. A Course in Probability. Addison-Wesley, 2005. P. Ortega, J. Wang, M. Rowland, T. Genewein, Z. Kurth-Nelson, R. Pascanu, N. Heess, J. Veness, J. Yang, W. Hu, J. Lee, and S. Du. Impact of repre- A. Pritzel, P. Sprechmann, S. Jayakumar, T. Mc- sentation learning in bandits. In Proceedings of the Grath, K. Miller, M. G. Azar, I. Osband, N. Rabi- 9th International Conference on Learning Represen- nowitz, A. Gyorgy, S. Chiappa, S. Osindero, Y. W. tations, 2021. Teh, H. van Hasselt, N. de Freitas, M. Botvinick, Y. Zhang and Q. Yang. A survey on multi-task learn- and S. Legg. Meta-learning of sequential strate- ing. CoRR, abs/1707.08114, 2017. URL http: gies. CoRR, abs/1905.03030, 2019. URL http: //arxiv.org/abs/1707.08114. //arxiv.org/abs/1905.03030. A. Peleg, N. Pearl, and R. Meir. Metalearning linear bandits by prior update. CoRR, abs/2107.05320, 2021. URL https://arxiv.org/abs/2107.05320. F. Radlinski, R. Kleinberg, and T. Joachims. Learn- ing diverse rankings with multi-armed bandits. In Proceedings of the 25th International Conference on Machine Learning, pages 784–791, 2008. D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Re- search, 39(4):1221–1243, 2014. D. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling. Journal of Machine Learning Research, 17(68):1–30, 2016. D. Russo, B. Van Roy, A. Kazerouni, I. Osband, and Z. Wen. A tutorial on Thompson sampling. Founda- tions and Trends in Machine Learning, 11(1):1–96, 2018. R. Sen, K. Shanmugam, M. Kocaoglu, A. Dimakis, and S. Shakkottai. Contextual bandits with latent confounders: An NMF approach. In Proceedings of the 20th International Conference on Artificial In- telligence and Statistics, 2017. C. Shi and C. Shen. Federated multi-armed bandits. In Proceedings of the 35th AAAI Conference on Ar- tificial Intelligence, 2021.
Hierarchical Bayesian Bandits A Proof of Lemma 1 The first claim is proved as follows. Fix round t and task s ∈ S . Since µˆ is a deterministic function of H , t s,t t and A and A are i.i.d. given H , we have s,∗ s,t t E (cid:2) A(cid:62) s,∗θ s,∗ − A(cid:62) s,tθ s,∗(cid:3) = E (cid:2)E (cid:2) A(cid:62) s,∗(θ s,∗ − µˆ s,t) (cid:12) (cid:12) H t(cid:3)(cid:3) + E (cid:2)E (cid:2) A(cid:62) s,t(µˆ s,t − θ s,∗) (cid:12) (cid:12) H t(cid:3)(cid:3) . Moreover, θ s,∗ − µˆ s,t is a zero-mean random vector independent of A s,t, and thus E (cid:2) A(cid:62) s,t(µˆ s,t − θ s,∗) (cid:12) (cid:12) H t(cid:3) = 0. So we only need to bound the first term above. Let (cid:110) (cid:112) (cid:111) E = (cid:107)θ − µˆ (cid:107) ≤ 2d log(1/δ) s,t s,∗ s,t Σˆ−1 s,t be the event that a high-probability confidence interval for the task parameter θ holds. Fix history H . Then s,∗ t by the Cauchy-Schwarz inequality, E (cid:2) A(cid:62) s,∗(θ s,∗ − µˆ s,t) (cid:12) (cid:12) H t(cid:3) ≤ E (cid:104) (cid:107)A s,∗(cid:107) Σˆ s,t(cid:107)θ s,∗ − µˆ s,t(cid:107) Σˆ− s,1 t (cid:12) (cid:12) (cid:12) H t(cid:105) ≤ (cid:112) 2d log(1/δ) E (cid:104) (cid:107)A (cid:107) (cid:12) (cid:12) H (cid:105) + max (cid:107)a(cid:107) E (cid:104) (cid:107)θ − µˆ (cid:107) 1(cid:8) E¯ (cid:9) (cid:12) (cid:12) H (cid:105) s,∗ Σˆ s,t (cid:12) t a∈A Σˆ s,t s,∗ s,t Σˆ− s,1 t s,t (cid:12) t (cid:124) (cid:123)(cid:122) (cid:125) ≤σmax = (cid:112) 2d log(1/δ) E (cid:104) (cid:107)A (cid:107) (cid:12) (cid:12) H (cid:105) + σ E (cid:104) (cid:107)θ − µˆ (cid:107) 1(cid:8) E¯ (cid:9) (cid:12) (cid:12) H (cid:105) . s,t Σˆ s,t (cid:12) t max s,∗ s,t Σˆ− s,1 t s,t (cid:12) t The equality follows from the fact that Σˆ is a deterministic function of H , and that A and A are i.i.d. s,t t s,∗ s,t given H . Now we focus on the second term above. First, note that t √ (cid:107)θ − µˆ (cid:107) = (cid:107)Σˆ − 1 2 (θ − µˆ )(cid:107) ≤ d(cid:107)Σˆ − 1 2 (θ − µˆ )(cid:107) . s,∗ s,t Σˆ−1 s,t s,∗ s,t 2 s,t s,∗ s,t ∞ s,t By definition, θ − µˆ | H ∼ N (0, Σˆ ), and hence Σˆ − 1 2 (θ − µˆ ) | H is a d-dimensional standard normal s,∗ s,t t s,t s,t s,∗ s,t t variable. Moreover, note that E¯ implies (cid:107)Σˆ − 21 (θ − µˆ )(cid:107) ≥ (cid:112) 2 log(1/δ). Finally, we combine these facts s,t s,t s,∗ s,t ∞ with a union bound over all entries of Σˆ − 1 2 (θ − µˆ ) | H , which are standard normal variables, and get s,t s,∗ s,t t E (cid:104) (cid:107)Σˆ − s,t1 2 (θ s,∗ − µˆ s,t)(cid:107) ∞1(cid:8) E¯ s,t(cid:9) (cid:12) (cid:12) (cid:12) H t(cid:105) ≤ 2 (cid:88)d √1 2π (cid:90) ∞ √ u exp (cid:20) − u 22 (cid:21) du ≤ (cid:114) π2 dδ . i=1 u= 2 log(1/δ) Now we combine all inequalities and have (cid:114) E (cid:2) A(cid:62) s,∗(θ s,∗ − µˆ s,t) (cid:12) (cid:12) H t(cid:3) ≤ (cid:112) 2d log(1/δ) E (cid:104) (cid:107)A s,t(cid:107) Σˆ s,t (cid:12) (cid:12) (cid:12) H t(cid:105) + π2 σ maxd 3 2 δ . Since the above bound holds for any history H , we combine everything and get t     (cid:114) E (cid:88) (cid:88) A(cid:62) s,∗θ s,∗ − A(cid:62) s,tθ s,∗ ≤ (cid:112) 2d log(1/δ) E (cid:88) (cid:88) (cid:107)A s,t(cid:107) Σˆ s,t + π2 σ maxd 23 mnδ t≥1 s∈St t≥1 s∈St (cid:118) (cid:117)   (cid:114) ≤ (cid:112) 2dmn log(1/δ)(cid:117) (cid:117) (cid:116)E (cid:88) (cid:88) (cid:107)A s,t(cid:107)2 Σˆ s,t + π2 σ maxd 23 mnδ . t≥1 s∈St The last step uses the Cauchy-Schwarz inequality and the concavity of the square root. To bound σ , we use Weyl’s inequalities together with (8), the second claim in Lemma 2, and (5). Specifically, max under the assumption that (cid:107)a(cid:107) ≤ 1 for all a ∈ A, we have 2 max (cid:107)a(cid:107)2 ≤ λ (Σˆ ) ≤ λ ((Σ−1 + G )−1) + λ ((Σ−1 + G )−1Σ−1Σ¯ Σ−1(Σ−1 + G )−1) a∈A Σˆ s,t 1 s,t 1 0 s,t 1 0 s,t 0 t 0 0 s,t λ2(Σ )λ (Σ ) ≤ λ (Σ ) + 1 0 1 q = σ2 . 1 0 λ2(Σ ) max d 0
Joey Hong, Branislav Kveton, Manzil Zaheer, Mohammad Ghavamzadeh This concludes the proof of the first claim. The second claim is proved by modifying the first proof as follows. Fix round t and task s ∈ S . Let t (cid:110) (cid:112) (cid:111) E = ∀a ∈ A : |a(cid:62)(θ − µˆ )| ≤ 2 log(1/δ)(cid:107)a(cid:107) s,t s,∗ s,t Σˆ s,t be the event that all high-probability confidence intervals hold. Then we have E (cid:2) A(cid:62) s,∗(θ s,∗ − µˆ s,t) (cid:12) (cid:12) H t(cid:3) ≤ (cid:112) 2 log(1/δ) E (cid:104) (cid:107)A s,t(cid:107) Σˆ s,t (cid:12) (cid:12) (cid:12) H t(cid:105) + E (cid:2) A(cid:62) s,∗(θ s,∗ − µˆ s,t)1(cid:8) E¯ s,t(cid:9) (cid:12) (cid:12) H t(cid:3) . Now note that for any action a, a(cid:62)(θ − µˆ )/(cid:107)a(cid:107) is a standard normal variable. It follows that s,∗ s,t Σˆ s,t E (cid:2) A(cid:62) s,∗(θ s,∗ − µˆ s,t)1(cid:8) E¯ s,t(cid:9) (cid:12) (cid:12) H t(cid:3) ≤ 2 (cid:88) (cid:107)a(cid:107) Σˆ s,t √1 2π (cid:90) ∞ √ u exp (cid:20) − u 22 (cid:21) du ≤ (cid:114) π2 σ maxKδ . a∈A u= 2 log(1/δ) The rest of the proof proceeds as in the first claim, yielding (cid:114) E (cid:2) A(cid:62) s,∗(θ s,∗ − µˆ s,t) (cid:12) (cid:12) H t(cid:3) ≤ (cid:112) 2 log(1/δ) E (cid:104) (cid:107)A s,t(cid:107) Σˆ s,t (cid:12) (cid:12) (cid:12) H t(cid:105) + π2 σ maxKδ . This completes the proof. B Proof of Theorem 3 Lemma 1 says that the Bayes regret BR(m, n) can be bounded by bounding the sum of posterior variances V(m, n). Since |S | = 1, we make two simplifications. First, we replace the set of tasks S by a single task t t S ∈ [m]. Second, there are exactly mn rounds. t Fix round t and task s = S . To reduce clutter, let M = Σ−1 + G . By the total covariance decomposition in t 0 s,t Lemma 2, we have that A(cid:62) Σˆ A (cid:16) (cid:17) (cid:107)A (cid:107)2 = σ2 s,t s,t s,t = σ2 σ−2A(cid:62) Σ˜ A + σ−2A(cid:62) M −1Σ−1Σ¯ Σ−1M −1A s,t Σˆ s,t σ2 s,t s,t s,t s,t 0 t 0 s,t ≤ c log(1 + σ−2A(cid:62) Σ˜ A ) + c log(1 + σ−2A(cid:62) M −1Σ−1Σ¯ Σ−1M −1A ) 1 s,t s,t s,t 2 s,t 0 t 0 s,t = c log det(I + σ−2Σ˜ 1 2 A A(cid:62) Σ˜ 21 ) + c log det(I + σ−2Σ¯ 1 2 Σ−1M −1A A(cid:62) M −1Σ−1Σ¯ 1 2 ) . (9) 1 d s,t s,t s,t s,t 2 d t 0 s,t s,t 0 t The logarithmic terms are introduced using (cid:18) (cid:19) x x u x = log(1 + x) ≤ max log(1 + x) = log(1 + x) , log(1 + x) x∈[0,u] log(1 + x) log(1 + u) which holds for any x ∈ [0, u]. The resulting constants are λ (Σ ) c λ2(Σ )λ (Σ ) c = 1 0 , c = q , c = 1 0 1 q . 1 log(1 + σ−2λ (Σ )) 2 log(1 + σ−2c ) q λ2(Σ ) 1 0 q d 0 The derivation of c uses that 1 A(cid:62) Σ˜ A = λ (Σ˜ ) = λ−1(Σ−1 + G ) ≤ λ−1(Σ−1) = λ (Σ ) . s,t s,t s,t 1 s,t d 0 s,t d 0 1 0 The derivation of c follows from 2 λ2(Σ )λ (Σ ) A(cid:62) M −1Σ−1Σ¯ Σ−1M −1A ≤ λ2(M −1)λ2(Σ−1)λ (Σ¯ ) ≤ 1 0 1 q . s,t 0 t 0 s,t 1 1 0 1 t λ2(Σ ) d 0 This is also proved as the second claim in Lemma 2. Now we focus on bounding the logarithmic terms in (9).
Hierarchical Bayesian Bandits B.1 First Term in (9) This is a per-instance term and can be rewritten as log det(I + σ−2Σ˜ 1 2 A A(cid:62) Σ˜ 1 2 ) = log det(Σ˜ −1 + σ−2A A(cid:62) ) − log det(Σ˜ −1) . d s,t s,t s,t s,t s,t s,t s,t s,t When we sum over all rounds with task s, we get telescoping and the contribution of this term is at most mn (cid:88) 1{S = s} log det(I + σ−2Σ˜ 21 A A(cid:62) Σ˜ 21 ) = log det(Σ˜ −1 ) − log det(Σ˜ −1) = log det(Σ 1 2 Σ˜ −1 Σ 1 2 ) t d s,t s,t s,t s,t s,mn+1 s,1 0 s,mn+1 0 t=1 (cid:18) (cid:19) (cid:18) (cid:19) ≤ d log 1 tr(Σ 1 2 Σ˜ −1 Σ 1 2 ) ≤ d log 1 + λ 1(Σ 0)n , d 0 s,mn+1 0 σ2d where we use that task s appears at most n times. Now we sum over all m tasks and get mn (cid:18) (cid:19) (cid:88) log det(I + σ−2Σ˜ 21 A A(cid:62) Σ˜ 1 2 ) ≤ dm log 1 + λ 1(Σ 0)n . d St,t St,t St,t St,t σ2d t=1 B.2 Second Term in (9) This is a hyper-parameter term. Before we analyze it, let v = σ−1M − 21 A s,t and note that Σ¯ −1 − Σ¯ −1 = (Σ + (G + σ−2A A(cid:62) )−1)−1 − (Σ + G−1)−1 t+1 t 0 s,t s,t s,t 0 s,t = Σ−1 − Σ−1(M + σ−2A A(cid:62) )−1Σ−1 − (Σ−1 − Σ−1M −1Σ−1) 0 0 s,t s,t 0 0 0 0 = Σ−1(M −1 − (M + σ−2A A(cid:62) )−1)Σ−1 0 s,t s,t 0 = Σ− 0 1M − 21 (I d − (I d + σ−2M − 21 A s,tA(cid:62) s,tM − 1 2 )−1)M − 21 Σ− 0 1 = Σ− 0 1M − 1 2 (I d − (I d + vv(cid:62))−1)M − 21 Σ− 0 1 vv(cid:62) = Σ− 0 1M − 1 2 1 + v(cid:62)v M − 21 Σ− 0 1 A A(cid:62) = σ−2Σ−1M −1 s,t s,t M −1Σ−1 , (10) 0 1 + v(cid:62)v 0 where we first use the Woodbury matrix identity and then the Sherman-Morrison formula. Since (cid:107)A (cid:107) ≤ 1, s,t 2 1 + v(cid:62)v = 1 + σ−2A(cid:62) M −1A ≤ 1 + σ−2λ (Σ ) = c . s,t s,t 1 0 Based on the above derivations, we bound the second logarithmic term in (9) as log det(I + σ−2Σ¯ 21 Σ−1M −1A A(cid:62) M −1Σ−1Σ¯ 1 2 ) d t 0 s,t s,t 0 t ≤ c log det(I + σ−2Σ¯ 21 Σ−1M −1A A(cid:62) M −1Σ−1Σ¯ 1 2 /c) d t 0 s,t s,t 0 t = c (cid:2) log det(Σ¯ −1 + σ−2Σ−1M −1A A(cid:62) M −1Σ−1/c) − log det(Σ¯ −1)(cid:3) t 0 s,t s,t 0 t ≤ c (cid:2) log det(Σ¯ −1 ) − log det(Σ¯ −1)(cid:3) . t+1 t The first inequality holds because log(1 + x) ≤ c log(1 + x/c) for any x ≥ 0 and c ≥ 1. The second inequality follows from the fact that we have a rank-1 update of Σ¯ −1. Now we sum over all rounds and get telescoping t mn (cid:88) log det(I + σ−2Σ¯ 21 Σ−1(Σ−1 + G )−1A A(cid:62) (Σ−1 + G )−1Σ−1Σ¯ 1 2 ) d t 0 0 St,t St,t St,t 0 St,t 0 t t=1 (cid:18) (cid:19) ≤ c (cid:2) log det(Σ¯ − m1 n+1) − log det(Σ¯ − 1 1)(cid:3) = c log det(Σ q21 Σ¯ − m1 n+1Σ q1 2 ) ≤ cd log d1 tr(Σ q1 2 Σ¯ − m1 n+1Σ q1 2 ) (cid:18) (cid:19) ≤ cd log(λ 1(Σ q1 2 Σ¯ − m1 n+1Σ q1 2 )) ≤ cd log 1 + λ λ1(Σ (Σq)m ) . d 0
Joey Hong, Branislav Kveton, Manzil Zaheer, Mohammad Ghavamzadeh Finally, we combine the upper bounds for both logarithmic terms and get (cid:34) mn (cid:35) (cid:20) (cid:18) (cid:19) (cid:18) (cid:19)(cid:21) V(m, n) = E (cid:88) (cid:107)A (cid:107)2 ≤ d c m log 1 + λ 1(Σ 0)n + c c log 1 + λ 1(Σ q)m , St,t Σˆ St,t 1 σ2d 2 λ d(Σ 0) t=1 which yields the desired result after we substitute this bound into Lemma 1. To simplify presentation in the main paper, c and c in Theorem 3 include the above logarithmic terms that multiply them. 1 2 C Proof of Theorem 4 From Assumption 1, there exists a basis of d actions such that if all actions in the basis are taken in task s by round t, it is guaranteed that λ (G ) ≥ η/σ2. We modify HierTS to takes these actions first in any task s. Let d s,t C = {s ∈ S : λ (G ) ≥ η/σ2} be the set of sufficiently-explored tasks by round t. t t d s,t Using C , we decompose the Bayes regret as t     (cid:88) (cid:88) (cid:88) (cid:88) BR(m, n) ≤ E  1{s ∈ C t} (A(cid:62) s,∗θ s,∗ − A(cid:62) s,tθ s,∗) + E  1{s (cid:54)∈ C t} (A(cid:62) s,∗θ s,∗ − A(cid:62) s,tθ s,∗) . t≥1 s∈St t≥1 s∈St For any task s and round t, we can trivially bound E (cid:2) (A − A )(cid:62)θ (cid:3) ≤ E (cid:104) (cid:107)A − A (cid:107) (cid:107)θ (cid:107) (cid:105) ≤ 2σ (cid:16) (cid:107)µ (cid:107) + E (cid:104) (cid:107)θ − µ (cid:107) (cid:105)(cid:17) , s,∗ s,t s,∗ s,∗ s,t Σˆ s,1 s,∗ Σˆ− s,1 1 max q Σˆ− s,1 1 s,∗ q Σˆ− s,1 1 (cid:112) where σ = λ (Σ + Σ ) as in Appendix B. Here we use that (cid:107)A −A (cid:107) ≤ 2 and that the prior covariance max 1 q 0 s,∗ s,t 2 of θ is Σˆ = Σ + Σ . We know from (2) that θ − µ ∼ N (0, Σ + Σ ). This means that Σˆ − 1 2 (θ − µ ) is s,∗ s,1 q 0 s,∗ q q 0 s,1 s,∗ q a vector of d independent standard normal variables. It follows that E (cid:104) (cid:107)θ − µ (cid:107) (cid:105) = E (cid:104) (cid:107)Σˆ − 1 2 (θ − µ )(cid:107) (cid:105) ≤ (cid:114) E (cid:104) (cid:107)Σˆ − 1 2 (θ − µ )(cid:107)2(cid:105) = √ d . s,∗ q Σˆ−1 s,1 s,∗ q 2 s,1 s,∗ q 2 s,1 Since s (cid:54)∈ C occurs at most d times for any task s, the total regret due to forced exploration is bounded as t   (cid:88) (cid:88) (cid:16) √ (cid:17) E  1{s (cid:54)∈ C t} (A(cid:62) s,∗θ s,∗ − A(cid:62) s,tθ s,∗) ≤ 2σ max (cid:107)µ q(cid:107) Σˆ−1 + d dm = c 3 . s,1 t≥1 s∈St It remains to bound the first term in BR(m, n). On event s ∈ C , HierTS samples from the posterior and behaves t (cid:104) (cid:105) exactly as Algorithm 1. Therefore, we only need to bound V(m, n) = E (cid:80) (cid:80) 1{s ∈ C } (cid:107)A (cid:107)2 and t≥1 s∈St t s,t Σˆ s,t then substitute the bound into Lemma 1. By the total covariance decomposition in Lemma 2, we have (cid:107)A (cid:107)2 = A(cid:62) Σ˜ A + A(cid:62) M −1Σ−1Σ¯ Σ−1M −1A , (11) s,t Σˆ s,t s,t s,t s,t s,t 0 t 0 s,t where M = Σ−1 + G to reduce clutter. As in Appendix B, we bound the contribution of each term separately. 0 s,t C.1 First Term in (11) This term depends only on Σ˜ , which does not depend on interactions with other tasks than task s. Therefore, s,t the bound is the same as in the sequential case in Appendix B.1, (cid:18) (cid:19) (cid:88) (cid:88) 1{s ∈ C } A(cid:62) Σ˜ A ≤ c dm log 1 + λ 1(Σ 0)n , t s,t s,t s,t 1 σ2d t≥1 s∈St where c is defined in Appendix B. 1
Hierarchical Bayesian Bandits C.2 Second Term in (11) The difference from the sequential setting is in how we bound the second term in (11). Before we had |S | = 1, t while now we have |S | ≤ L ≤ m for some L. Since more than one task is acted upon per round, the telescoping t identity in (10) no longer holds. To remedy this, we reduce the concurrent case to the sequential one. Specifically, suppose that task s ∈ S in round t has access to the concurrent observations of prior tasks in round t, for some t order of tasks S = {S }L . As Theorem 3 holds for any order, we choose the order where sufficiently-explored t t,i i=1 tasks s ∈ C appear first. t Let S = {S }i−1 be the first i − 1 tasks in S according to our chosen order. For s = S , let t,i t,j j=1 t t,i Σ¯ −1 = Σ−1 + (cid:88) (Σ + G−1 )−1 + (cid:88) (Σ + G−1)−1 s,t q 0 z,t+1 0 z,t z∈St,i z∈[m]\St,i be the reciprocal of the hyper-posterior covariance updated with concurrent observations in tasks S . Next we t,i show that Σ¯ and Σ¯ are similar. t s,t Lemma 5. Fix round t and i ∈ [L]. Let s = S and λ (G ) ≥ η/σ2. Then t,i d s,t σ−2λ (Σ )(λ (Σ ) + σ2/η) λ (Σ¯ −1Σ¯ ) ≤ 1 + 1 q 1 0 . 1 s,t t λ (Σ ) + (λ (Σ ) + σ2/η)/L 1 q 1 0 Proof. Using standard eigenvalue inequalities, we have λ (Σ¯ −1 − Σ¯ −1) λ (Σ¯ −1Σ¯ ) = λ ((Σ¯ −1 + Σ¯ −1 − Σ¯ −1)Σ¯ ) ≤ 1 + λ ((Σ¯ −1 − Σ¯ −1)Σ¯ ) ≤ 1 + 1 s,t t . (12) 1 s,t t 1 t s,t t t 1 s,t t t λ (Σ¯ −1) d t By Weyl’s inequalities, and from the definition of Σ¯ , we have t λ (Σ¯ −1) ≥ λ (Σ−1) + (cid:88) λ ((Σ + G−1)−1) = λ (Σ−1) + (cid:88) λ−1(Σ + G−1) d t d q d 0 z,t d q 1 0 z,t z∈[m] z∈[m] (cid:88) ≥ λ (Σ−1) + (λ (Σ ) + λ (G−1))−1 ≥ λ (Σ−1) + (i − 1)(λ (Σ ) + σ2/η)−1 . d q 1 0 1 z,t d q 1 0 z∈[m] In the last inequality, we use that the previous i − 1 tasks S are sufficiently explored. Analogously to (10), t,i Σ¯ −1 − Σ¯ −1 = (cid:88) (Σ + (G + σ−2A A(cid:62) )−1)−1 − (Σ + G−1)−1 s,t t 0 z,t z,t z,t 0 z,t z∈St,i = σ−2 (cid:88) Σ−1M −1 A z,tA(cid:62) z,t M −1Σ−1 , 0 z,t 1 + σ−2A(cid:62) M −1A z,t 0 z∈St,i z,t z,t z,t where M = Σ−1 + G to reduce clutter. Moreover, since (cid:107)A (cid:107) ≤ 1 and σ−2A(cid:62) M −1A ≥ 0, we have z,t 0 z,t z,t 2 z,t z,t z,t λ (Σ¯ −1 − Σ¯ −1) ≤ (i − 1)σ−2 . 1 s,t t Finally, we substitute our upper bounds to the right-hand side of (12) and get λ 1(Σ¯ − s,t1 − Σ¯ − t 1) ≤ (i − 1)σ−2 ≤ σ−2λ 1(Σ q)(λ 1(Σ 0) + σ2/η) , λ (Σ¯ −1) λ−1(Σ ) + (i − 1)(λ (Σ ) + σ2/η)−1 λ (Σ ) + (λ (Σ ) + σ2/η)/L d t 1 q 1 0 1 q 1 0 where we use that the ratio is maximized when i − 1 = L. This completes the proof. Now we return to (11). First, we have that A(cid:62) M −1Σ−1Σ¯ Σ−1M −1A = A(cid:62) M −1Σ−1Σ¯ 1 2 (cid:16) Σ¯ − 1 2 Σ¯ 1 2 Σ¯ 1 2 Σ¯ − 21 (cid:17) Σ¯ 1 2 Σ−1M −1A s,t 0 t 0 s,t s,t 0 s,t s,t t t s,t s,t 0 s,t ≤ λ (Σ¯ − 1 2 Σ¯ 1 2 Σ¯ 1 2 Σ¯ − 21 )A(cid:62) M −1Σ−1Σ¯ Σ−1M −1A 1 s,t t t s,t s,t 0 s,t 0 s,t ≤ λ (Σ¯ −1Σ¯ )A(cid:62) M −1Σ−1Σ¯ Σ−1M −1A , 1 s,t t s,t 0 s,t 0 s,t
Joey Hong, Branislav Kveton, Manzil Zaheer, Mohammad Ghavamzadeh where we use that the above expression is a quadratic form. Next we apply Lemma 5 and get σ−2λ (Σ )(λ (Σ ) + σ2/η) λ (Σ¯ −1Σ¯ ) ≤ 1 + 1 q 1 0 = c . 1 s,t t λ (Σ ) + (λ (Σ ) + σ2/η)/L 4 1 q 1 0 After Σ¯ is turned into Σ¯ , we follow Appendix B.2 and get that the hyper-parameter regret is t s,t (cid:18) (cid:19) λ (Σ )m c c cd log 1 + 1 q , 2 4 λ (Σ ) d 0 where the only difference is the extra factor of c . Finally, we combine all upper bounds and get 4   (cid:20) (cid:18) (cid:19) (cid:18) (cid:19)(cid:21) V(m, n) = E (cid:88) (cid:88) 1{s ∈ C t} (cid:107)A s,t(cid:107)2 Σˆ s,t ≤ d c 1m log 1 + λ 1 σ(Σ 2d0)n + c 2c 4c log 1 + λ λ1 d(Σ (Σq) 0m ) , t≥1 s∈St which yields the desired result after we substitute it into Lemma 1. To simplify presentation in the main paper, c and c in Theorem 4 include the above logarithmic terms that multiply them. 1 2 D Gaussian Bandit Regret Bounds Our regret bounds in Section 6 can be specialized to K-armed Gaussian bandits (Section 4.1). Specifically, when the action set A = {e } is the standard Euclidean basis in RK, Theorems 3 and 4 can be restated as follows. i i∈[K] Theorem 6 (Sequential Gaussian bandit regret). Let |S | = 1 for all rounds t. Let δ = 1/(mn). Then the Bayes t regret of HierTS is (cid:112) BR(m, n) ≤ 2Kmn[c m + c ] log(mn) + c , 1 2 3 where c = O(K), 3 σ2 (cid:18) σ2n (cid:19) σ2c (cid:32) σ2m (cid:33) σ2 c = 0 log 1 + 0 , c = q log 1 + q , c = 1 + 0 . 1 log(1 + σ−2σ2) σ2K 2 log(1 + σ−2σ2) σ2 σ2 0 q 0 The main difference from the proof of Theorem 3 is that we start with the finite-action bound in Lemma 1. Other than that, we use the facts that λ (Σ ) = λ (Σ ) = σ2 and λ (Σ ) = σ2. 1 0 d 0 0 1 q q Theorem 7 (Concurrent Gaussian bandit regret). Let |S | ≤ L ≤ m. Let δ = 1/(mn). Then the Bayes regret t of HierTS is (cid:112) BR(m, n) ≤ 2Kmn[c m + c ] log(mn) + c , 1 2 3 where c and c are defined as in Theorem 6, 1 (cid:32) (cid:33) σ2c c σ2m σ−2σ2(σ2 + σ2) c = q 4 log 1 + q , c = 1 + q 0 , 2 log(1 + σ−2σ2) σ2 4 σ2 + (σ2 + σ2)/L q 0 q 0 and c = O(Km). 3 When we specialize Theorem 4, we note that η = 1, since the action set A is the standard Euclidean basis. E Image Classification Experiment We conduct an additional experiment that considers online classification using a real-world image dataset. The problem is cast as a multi-task linear bandit with Bernoulli rewards. Specifically, we construct a set of tasks where one image class is selected randomly to have high reward. In each task, at every round, K images are uniformly sampled at random as actions, and the aim of the learning agent is to select an image from the
Hierarchical Bayesian Bandits 1600 1400 1200 1000 800 600 400 200 0 0 100 200 300 400 500 600 700 800 Round t tergeR MNIST (pos_label = 0) 2000 OracleTS TS 1500 HierTS 1000 500 0 0 100 200 300 400 500 600 700 800 Round t tergeR MNIST (pos_label = 1) OracleTS TS HierTS STelcarOST 0 STreiH 1 2 3 4 5 6 7 8 9 STelcarOST 0 STreiH 1 2 3 4 5 6 7 8 9 Figure 3: Evaluation of HierTS on multi-task digit classification using MNIST with different positive image classes. On the top, we plot the cumulative Bayes regret at each round. On the bottom, we visualize the most-rewarding image according to the learned hyper-parameter at evenly-spaced intervals. unknown positive image class. The reward of an image from the positive class is Ber(0.9) and for all other classes is Ber(0.1). We use the MNIST dataset (LeCun et al., 2010), which consists of 60, 000 images of handwritten digits, which we split equally into a training and test set. We down-sample each image to d = 49 pixels, which become the feature vector for the corresponding action in the bandit problem. For each digit, the training set is used to estimate µ , Σ , where all three algorithms use Σ but only OracleTS can use µ . The algorithms are evaluated ∗ 0 0 ∗ on the test set. Given a positive digit class, we construct a different task s by sub-sampling from the test set, and computing θ using positive images from the sub-sampled data. For each digit as the positive image class, s,∗ we evaluate our three algorithms on a multi-task linear bandit with m = 10 tasks, n = 400 interactions per task, and K = 30 actions, uniformly sampled from the test images. We chose L = 5 tasks per round, leading to 800 rounds total. We assume a hyper-prior of Q = N (0, I ) with reward noise σ = 0.5 because the rewards are d Bernoulli. Figure 3 shows the performance of all algorithms for two digits across 20 independent runs. We see that HierTS performs very well compared to standard TS. In addition to regret, we also visualized the learned hyper-parameter µ¯ every 80 rounds. We see that HierTS very quickly learns the correct hyper-parameter, t showing that it effectively leverages the shared structure across task. Overall, this experiment shows that even if HierTS assumes a misspecified model of the environment, with non-Gaussian rewards and not knowing the true hyper-prior Q and covariance Σ , HierTS still performs very well. 0
