Learning Equilibria in Mean-Field Games: Introducing Mean-Field PSRO Paul Muller∗ Mark Rowland∗ Romuald Elie∗ Georgios Piliouras† Julien Perolat∗ Mathieu Lauriere‡ Raphael Marinier‡ Olivier Pietquin‡ Karl Tuyls∗ Abstract Recent advances in multiagent learning have seen the introduction of a family of algorithms that revolve around the population-based training method PSRO, showing convergence to Nash, correlated and coarse corre- lated equilibria. Notably, when the number of agents increases, learning best-responses becomes exponentially more difficult, and as such ham- pers PSRO training methods. The paradigm of mean-field games pro- vides an asymptotic solution to this problem when the considered games are anonymous-symmetric. Unfortunately, the mean-field approximation introduces non-linearities which prevent a straightforward adaptation of PSRO. Building upon optimization and adversarial regret minimization, this paper sidesteps this issue and introduces mean-field PSRO, an adap- tation of PSRO which learns Nash, coarse correlated and correlated equi- libria in mean-field games. The key is to replace the exact distribution computation step by newly-defined mean-field no-adversarial-regret learn- ers, or by black-box optimization. We compare the asymptotic complexity of the approach to standard PSRO, greatly improve empirical bandit con- vergence speed by compressing temporal mixture weights, and ensure it is theoretically robust to payoff noise. Finally, we illustrate the speed and F1-score of mean-field PSRO on several mean-field games, demonstrating convergence to strong and weak equilibria. 1 Introduction This paper introduces a new mean-field supervised learning algorithm, Mean- Field Policy Space Response Oracles (MF-PSRO), guaranteed to converge to Nash, correlated and coarse-correlated equilibria in a large variety of games, without any hypothesis thereupon. Policy Space Response Oracles (PSRO) [20] is originally a two-player zero-sum game algorithm meant to be a generalization ∗Deepmind, corresponding author: pmuller@deepmind.com †Singapore University of Technology and Design ‡Google Brain 1 2202 guA 92 ]TG.sc[ 2v05380.1112:viXra
of double-oracle [25], fictitious play [5], and independent reinforcement learn- ing [24]. The algorithm’s main loop is composed of two steps: given a policy set, compute an optimal distribution of play. Then, compute a best-response to this distribution, add it to the set and re-iterate. Remarkably, recent years have shown the algorithm’s versatility by demonstrating great advances in learn- ing N -player equilibria using PSRO-derived approaches, managing to converge towards α-Rank [31; 35] optimal strategy cycles [28], or towards (coarse) cor- related equilibria1 [23]. However, both latter approaches’ convergence results rely on potentially fully exploring the space of deterministic strategies, which grows exponentially in the number of players. Computing a best response in the general case of randomized opponent strategies also becomes exponentially more complex as the number of players increases, even with symmetric simpli- fications such as anonymity [38], centralized settings [19], or fully cooperative settings [32]. Although anonymity can allow polynomial-time approximation schemes for computing approximate Nash equilibria [8; 9], in practice such al- gorithms are typically too slow for real life applications. A more promising way to address such complexity issues is by approximation in the case of symmetric games by considering asymptotic versions thereof, where the number of players is infinite and only their distribution matters: mean-field games [17; 22]. The question of learning Nash equilibria in mean-field games has been re- ceiving a growing amount of attention, and many methods have been recently proposed. Among these, we can distinguish those relying on fixed-point con- traction [1; 13; 39], fictitious-play [7; 14; 11; 34] or online mirror descent [33]. Comparatively, learning correlated and coarse correlated equilibria in mean-field games has not yet, to the best of our knowledge, been studied. The literature has only started introducing notions of mean-field correlated equilibria [6; 10]. Our central question is: What are the modifications required for PSRO to successfully converge towards Nash, correlated and coarse correlated equilibria in mean-field games? In order to answer it, after introducing the framework of interest (Section 2), we expand on the obstacles encountered when attempting to adapt PSRO to mean-field games (Section 3), identify and treat the cases where a straightfor- ward adaptation is possible, then generalize the setting to mean-field games with finite states, actions and times. Note that the general treatment is fundamen- tally different for Mean-Field Nash equilibria (Section 4), and for Mean-Field (coarse) correlated equilibria (Section 5). Finally, we test our algorithms on a number of OpenSpiel [21] games in Section 6, demonstrating convergence, and, where possible, comparing with alternative benchmarks. 1A broad relaxation of Nash, correlated equilibria that is closely connected to regret min- imization. It is sometimes referred to as Hannan consistency [30; 16; 27]. 2
2 Background 2.1 Definitions Given a set Y, we name ∆(Y) the set of distributions over Y. A game is a tuple (X , A, r, P, µ ) where X is the finite set of states, A is 0 the finite set of actions, r : X × A × ∆(X ) → R is the reward function where ∆(X ) is the set of probability distributions over X , P : X × A → X is the state transition function, assumed not to depend over ∆(X ), µ ∈ ∆(X ) is the initial 0 state occupancy measure. We take Π the set of deterministic policies, which is finite and whose convex hull spans all game policies, a policy being a function π : X → ∆(A). We name J the expected payoff function (cid:88) J(π, µ) := µπ(x)π(x, a)r(x, a, µ) x∈X ,a∈A where µπ is the expected state occupancy measure of a representative player playing policy π. We note that in Mean-Field games, a single player has no influence on the reward function, since it has no influence on the state distri- bution µ: only behavior changes that are wide enough to modify the player state distribution µ can change the MDP’s reward function. State occupancy measures can be defined in several ways, and our derivations apply to all: • γ-discounted: µπ(x) = µ (x) + γ (cid:80) (cid:80) p(x|x(cid:48), a)π(x(cid:48), a)µπ(x(cid:48)) 0 x(cid:48)∈X a∈A • Finite-horizon: µπ (x) = (cid:80) (cid:80) p(x|x(cid:48), a)π(x(cid:48), a)µπ(x(cid:48)) with µπ = µ s+1 s 0 0 x(cid:48)∈X a∈A (in which case another summation term over s appears in J, or we assume states to contain current time s). Given policies π , ..., π ∈ Π, we call restricted game the stateless game 1 n where players choose one policy among {π |1 ≤ i ≤ n} at the beginning of the i game, then keep playing it until the end. We also define meta-games, which are normal-form games whose payoff matrix for player 1 is, at row i and column j, J(π i, µπj ) - and the transpose thereof for player 2. The complex relationship between these notions, which are equivalent in N -player games, is explored in Section 3. A correlation device, a notion introduced in [29], ρ is a distribution over distributions of policies: ρ ∈ ∆(∆(Π)), where ∆(Π) is the set of distribution over Π. It is used to sample population distributions ν ∈ ∆(Π), from which individual population recommendations π are in turn sampled: the distribution of policies over the whole mean-field population follows ν with probability ρ(ν). Given a sequence of distributions (ν ) and a distribution (ρ ) over them, we t t t t write (ρ , ν ) the correlation device recommending ν with probability ρ . t t t t t The empirical play of a sequence ν , ..., ν is the correlation device which 1 T uniformly selects one of the joint members of the sequence: ∀1 ≤ t ≤ T, ρ(ν ) = t 1 . T 3
We write µ(ν) the state occupancy measure of the population when policies are distributed according to ν. In our case, the dynamics p do not depend on µ, so we have that µ(ν) = (cid:80) ν(π)µπ. We note that we restrict ourselves to the π fully discrete setting. We also write π(ν) the policy resulting from sampling an initial policy according to ν and playing it until the end of the game. 2.2 Mean-field equilibria Definition 1 (mean-field Nash equilibrium). A mean-field Nash equilib- rium (MFNE) is a policy π such that, when the whole population plays π, no agent has an incentive to deviate, ie. J(π(cid:48), µπ) − J(π, µπ) ≤ 0 , ∀π(cid:48) ∈ Π . The following two equilibria have been introduced by Muller et al. [29], and we refer the reader to this paper for more details on, and justifications of, their formulations. Definition 2 (mean-field coarse-correlated equilibrium). A mean-field coarse- correlated equilibrium (MFCCE) is a correlation device ρ from which play- ers do not have an incentive to deviate before being given their recommendations, ie. E [J(π(cid:48), µ(ν)) − J(π, µ(ν))] ≤ 0 , ∀π(cid:48) ∈ Π. ν∼ρ,π∼ν Definition 3 (mean-field correlated equilibrium). A mean-field correlated equilibrium (MFCE) is a correlation device ρ such that players do not have an incentive to deviate even after being given their recommendations, ie. ρ(π)E [J(π(cid:48), µ(ν)) − J(π, µ(ν))] ≤ 0 , ∀π, π(cid:48) ∈ Π ν∼ρ(·|π) where ρ(π) = (cid:80) ν(π)ρ(ν) assuming ρ only has atoms, and ρ(ν|π) = ν(π)ρ(ν) . (cid:80) ν(cid:48)(π)ρ(ν(cid:48)) ν ν(cid:48) As usual, (cid:15)-variants of these equilibria are defined by changing 0 in the r.h.s. of the above inequalities by (cid:15) > 0: these are approximate equilibria where one may only gain up to (cid:15) by deviating unilaterally (For Coarse-Correlated Equilibria) or per-policy (For Correlated Equilibria). When applied to restricted games, we call these equilibria, restricted MFNE, restricted MFCE and restricted MFCCE. 2.3 PSRO in N -player games PSRO [20] is a generalization of Double Oracle [26], and as such is an iterated best-response algorithm for computing Nash equilibria in N -player games. The algorithm, presented in Algorithm 1 initiates with sets containing random poli- cies. At each iteration, an optimal policy distribution is computed over the policy sets, and a best response to this distribution is computed for each player. If all best responses were already in each player’s policy set, the algorithm ter- minates; it continues otherwise. 4
Algorithm 1: PSRO(Meta-Solver) (N -player games) Result: Policy sets (Π∗ = {πk, ..., πk}) for all K players, policy k 1 n k=1..K distributions (ν∗) k k=1..N ∀k, Π1 = {π1} with π1 any policy, ν (π1) = 1.0, n = 1; k k k k k while (Π \ Π ) (cid:54)= ∅ do n+1 n ∀k, Πn+1 = Πn ∪ {BR (ν)} ; k k k n = n + 1; Fill payoff tensors (T ) : k k=1..K ∀x , ..., x , T (x , ..., x ) = Payoff (π , ..., π ); 1 K k 1 K k x1 xK ν = Meta-Solver((T ) ) k k=1..K end The original PSRO paper introduced several different meta-solvers (Uniform, Exact Nash and PRD, an approximate Nash solver), all of which were proven to make PSRO converge to a Nash equilibrium in two-player zero-sum games. Recent work has extended convergence to Alpharank [31]-optimal subsets [28] and to correlated and coarse correlated equilibria [23] in N -player games when using the right meta-solvers and best-responders. Crucially, the game speci- fied by the payoff tensors that the meta-solver computes an equilibrium form is a normal-form matrix game. This yields a ‘linearity of evaluation’ property; specifically, the payoffs when players make use of mixed strategies are straight- forwardly computed from the payoff tensors specifying the payoffs of the pure strategies in the game. In the rest of this paper, unless otherwise directly specified, we consider n to be the current PSRO iteration. 3 Challenges in scaling to mean-field games Our central proposal in this paper is a generalisation of PSRO to the mean-field setting. We introduce two distinct algorithms for the computation of either MFNE or MFCE/MFCCE. Both MF-PSRO algorithms are described as Algo- rithms 2 and 3 below. These two algorithms have a very similar structure to the PSRO as described for N -player games in Section 2.3; within the inner loop, a distribution is com- puted for the restricted game under consideration (either a Nash equilibrium, or a (coarse) correlated equilibrium), and new policies are derived as certain types of best response against the computed equilibrium. Keeping the same insight as [23], we define two different Best Responder functions BR and BR , CE CCE for use with MF-PSRO in computing CEs and CCEs, respectively: • BR (ρ) := arg max (cid:80) ρ(ν)J(π∗, µ(ν)); CCE ν π∗∈Π • BR (π , ρ) := arg max (cid:80) ρ(ν|π )J(π∗, µ(ν)). CE k ν k π∗∈Π 5
Algorithm 2: MF-PSRO(Nash) Result: Policy set Π∗ = {π , ..., π }, Policy Distribution ν∗ ∈ ∆(Π∗) 1 n yielding game Nash π(ν∗) Π = {π } with π any policy, ν (π ) = 1.0, n = 1; 1 1 1 1 1 while (Π \ Π ) (cid:54)= ∅ do n+1 n Π n+1 = Π n ∪ {BR(µπ(νn))} ; n = n + 1 ; ν = arg min max J(π , µ(ν)) − J(π(ν), µ(ν)) ; n i ν∈∆(Πn) i=1,...,n end Algorithm 3: MF-PSRO((C)CE) Result: Policy set Π∗ = {π , ..., π }, (cid:15)-mean-field correlated 1 n equilibrium ρ∗ ∈ ∆(∆(Π∗)) Π = ∅, Π = {π } with π any policy, ρ(δ ) = 1.0, n = 1; 0 1 1 1 π1 while (Π \ Π ) (cid:54)= ∅ do n+1 n (If CE) Π = Π ∪ {BR (π , ρ ) | π , ρ (π ) > 0} ; n+1 n CE i n i n i (If CCE) Π = Π ∪ BR (ρ ); n+1 n CCE n n = n + 1; (If CE) ρ = arg min E [ max J(π , µ(ν)) − J(π, µ(ν))] ; n ν∼ρ,π∼ν i ρ∈∆(∆(Πn)) i=1..n (If CCE) ρ = arg min max E [J(π , µ(ν)) − J(π, µ(ν))]; n ν∼ρ,π∼ν i ρ∈∆(∆(Πn)) i=1,...,n end 6
We note that BR (ρ) is the Best Response corresponding to a unilateral CCE deviation from ρ, ie. deviating before having been given a recommendation, whereas BR (π , ρ) is the best response generated by deviating from recom- CE k mendation π . k Given these proto-algorithms, several important questions are immediately raised. First, are these algorithms guaranteed to return instances of the equi- libria they seek to find? This is a purely mathematical question. Second, how should the restricted game equilibria in the inner loop be computed? As de- scribed in Section 2.3, the restricted game in usual applications of PSRO satisfies a ‘linearity of evaluation’ property. Unfortunately, however, this linearity prop- erty is lost in the case of mean-field games, in which the representative player’s payoff is generally non-linear as a function of the population occupancy mea- sure. This lack of linearity presents a serious barrier in directly applying PSRO to mean-field games, and an important contribution of this paper is how to cir- cumvent this barrier. We do however note that for a limited class of mean-field games, linearity is preserved; we describe the details of this case in Appendix C. The next two sections treat the theoretical and implementation questions raised above for Nash equilibria, and for (coarse) correlated equilibria, in turn. 4 Convergence to Nash equilibria 4.1 Existence and computation of restricted game equi- libria In the inner loop of MF-PSRO(Nash), an important subroutine is the com- putation of a mean-field Nash equilibrium for the restricted game; namely, a distribution ν ∈ ∆(Π ) such that n J(π(cid:48), µ(ν)) − J(π(ν), µ(ν)) ≤ 0 , ∀π(cid:48) ∈ {π , ..., π }. 1 n We note that if at least one such ν exists, then the following optimization problem in the inner loop of MF-PSRO(Nash), which minimizes exploitability, will return a Nash equilibrium ν∗ = arg min max J(π , µ(ν)) − J(π(ν), µ(ν)) . (1) i ν∈∆n i=1...n Fortunately, the conditions of existence for a Nash equilibrium of the re- stricted game - so called restricted Nash equilibrium - only require continuity of r with respect to µ, as shown in the following theorem. Theorem 4 (Existence of restricted Nash equilibria). If the reward function of the game is continuous with respect to µ, then there always exists a restricted game Nash equilibrium. 7
Proof. Let φ : ∆(Π n) → 2∆(Πn) be the best-response map in the restricted game characterized by policies in the set Π : n ∀ν ∈ ∆(Π ), φ(ν) := arg max J(π(ν(cid:48)), µ(ν)). n ν(cid:48)∈∆(Πn) ∆(Π ) is non-empty and convex, together with closed and bounded in a n finite-dimensional space, and therefore compact. For all ν ∈ ∆(Π ), arg max J(π(ν(cid:48)), µ(ν)) ⊆ ∆(Π ) because ∆(Π ) is closed, n n n ν(cid:48)∈∆(Πn) and φ(ν) is therefore non-empty. Let ν , ν ∈ φ(ν), t ∈ [0, 1]. 1 2 J(π(tν + (1 − t)ν ), µ(ν)) = tJ(π(ν ), µ(ν)) + (1 − t)J(π(ν ), µ(ν)) 1 2 1 2 so tν + (1 − t)ν ∈ φ(ν) and φ(ν) is therefore convex. 1 2 The proof of Graph(φ) being closed is provided in Appendix A. It relies on the fact that since r is continuous in µ, so is J, and since the function ν → J(π(ν), µ) is linear for all ν ∈ ∆(Π ), the function (ν , ν ) → J(π(ν ), µ(ν )) n 1 2 1 2 is bicontinuous, which is enough to ensure Graph closedness. We have all the hypotheses required to apply Kakutani’s fixed point theorem [18]: there thus exists ν∗ ∈ ∆(Π ) such that ν∗ ∈ φ(ν∗), ie. ν∗ = arg max J(π(ν(cid:48)), µ(ν∗)), n ν(cid:48) which means that ∀ν(cid:48) ∈ ∆(Π ), J(π(ν(cid:48)), µ(ν∗)) ≤ J(π(ν∗), µ(ν∗)), in other n words: ν∗ is a Nash equilibrium of the restricted game. Having established the existence of Nash equilibria for the restricted mean- field game in the inner loop of MF-PSRO(Nash), we now turn to the problem of how such an equilibrium can be (approximately) computed. As remarked earlier, due to the non-linearity of the restricted game, this problem is a non-linear (and potentially non-convex) optimisation problem over ∆(Π ). Thus, the optimal n solution of Equation (1) can be, in the absence of any additional assumptions on the game, found via Black-Box optimization approaches, such as random search [36], Bayesian optimization [12], evolutionary search (our experiments use CMA-ES [15]), or any other appropriate method for the considered game. 4.2 Convergence to Nash The termination condition of PSRO is the following: if at step N + 1, the new policy π produced by the algorithm is in Π , then the algorithm terminates. n+1 n Given that each π is a deterministic policy, and that the set of deterministic i policies is finite, PSRO will therefore necessarily terminate. We must only prove one thing: Proposition 5 (Termination-optimality). If MF-PSRO(Nash) terminates, it stops at a Nash equilibrium of the true game. Proof. If MF-PSRO(Nash) terminates at step n, then π∗ = arg max J(π, µ(ν)) ∈ Π . n π∈Π 8
Since ν is a Nash equilibrium of the restricted game by assumption, then neces- sarily J(π∗, µ(ν)) ≤ J(π(ν), µ(ν)), and thus ∀π ∈ Π, J(π, µ(ν)) ≤ J(π(ν), µ(ν)), which concludes the proof. Using the former discussion and this property, we deduce Theorem 6 (mean-field PSRO convergence to Nash equilibria). MF-PSRO(Nash) converges to a Nash equilibrium of the true game. 5 Convergence to (coarse) correlated equilibria We now turn our attention to the versions of MF-PSRO that aim to compute mean-field correlated equilibria and mean-field coarse correlated equilibria. 5.1 Overview Computing restricted MF(C)CEs is potentially more involved than comput- ing restricted MFNE; while the optimisation problem defining restricted Nash equilibria is over the finite-dimensional space ∆(Π ), the optimisation problem n defining restricted MF(C)CEs is over the infinite-dimensional space ∆(∆(Π )). n One could resort to computing an approximate MFNE (a special case of both MFCE and MFCCE) using the black-box optimisation approach described in the previous section, but it is possible to exploit the structure of the mean-field game to compute approximate MF(C)CEs more efficiently. The approach we pursue is fundamentally based on no-regret learning; we also find opportuni- ties to increase the quality of the approximate equilibrium by post-processing the output of the regret-minimisation algorithm via linear programming; see Figure 2 for an overview of the techniques at play. 5.2 Approximate (coarse) correlated equilibria via regret minimisation Our goal is to approximate an MF(C)CE for the restricted MFG based on the policy set Π = {π , . . . , π }, as required within the inner loop of Algorithm 3. n 1 n Recall that this amounts to solving the optimisation problem ρ = arg min max E [J(π , µ(ν)) − J(π, µ(ν))] n ν∼ρ,π∼ν i ρ∈∆(∆(Πn)) i=1,...,n in the case of coarse correlated equilibria, and ρ = arg min E [ max J(π , µ(ν)) − J(π, µ(ν))] n ν∼ρ,π∼ν i ρ∈∆(∆(Πn)) i=1..n in the case of correlated equilibria. In principle, similar black-box techniques described for approximating Nash equilibria in the previous section may be applied to solve these problems too. However, such an approach is likely to 9
be inefficient in practice, and instead we build on regret-minimisation theory, a classical approach to computing (C)CEs in N player games. The overall approach relies on the fact that if the population distribution µ is fixed, the payoff function E [J(π, µ)] is linear in the distribution ν ∈ ∆(Π ), π∼ν n and we are in fact considering online linear optimisation problems. Focusing first on the case of coarse correlated equilibria, we will make use of Algorithms √ A achieving O( T ) external regret in online linear optimisation, of the form described in Algorithm 4. Algorithm 4: Generic form of regret-minimisation algorithm for online linear optimisation on the domain ∆(Π ). n Result: A sequence of predictions (ν )T such that t t=1 √ max (cid:80)T R (ν) − (cid:80)T R (ν ) = O( T ). ν∈∆(Πn) t=1 t t=1 t t for t = 1, 2, . . . , T do Algorithm makes a prediction ν ∈ ∆(Π ); t n Algorithm observes a linear reward function R : ∆(Π ) → R; t n Algorithm receives the reward R (ν ); t t end We may apply such an algorithm for MF(C)CE computation as shown in Algorithm 5. Algorithm 5: Protocol for computing an approximate MF(C)CE via regret-minimisation for t = 1, 2, . . . , T do Representative player selects distribution ν ∈ ∆(Π ) using a t n regret-minimisation algorithm A based on past loss function (R )t−1 ; s s=1 Player observes reward function R (ν) = E [J(π, µ(v )]; t π∼ν t Representative player receives reward R (ν ) = E [J(π, µ(v )]; t t π∼νt t end Return empirical average ρ = 1 (cid:80)T δ . T t=1 νt This algorithm returns the empirical average 1 (cid:80)T δ , which is in fact an T t=1 νt approximate MF(C)CE for the restricted game, as the following result shows. Proposition 7. The empirical average ρ = 1 (cid:80)T δ returned by Algorithm 5 T t=1 νt using a regret-minimisation algorithm A of the form described in Algorithm 4, √ is a O(1/ T )-MF(C)CE for the restricted mean-field game. Proof. This is a direct computation. The benefit of the representative player 10
deviating to π under the correlation device ρ is i E [J(π , µ(ν)) − E [J(π, µ(ν))]] ν∼ρ i π∼ν T 1 (cid:88) = (J(π , µ(ν )) − E [J(π, µ(ν ))]) T i t π∼νt t t=1 1 (cid:16)√ (cid:17) (cid:16) √ (cid:17) = O T = O 1/ T , T where the penultimate equality follows from the regret-minimising property of algorithm A. The proof for CEs is similar. This result establishes a rigorous means of approximating an MF(C)CE in the restricted game considered within the inner loop of mean-field PSRO, and therefore provides an implementable version of mean-field PSRO. By strength- ening the regret minimisation algorithm described above to minimise internal regret, we obtain a time-average strategy that is an approximate MFCE. In both cases, we have the following correctness guarantee for MF-PSRO. Theorem 8 (MF-PSRO Convergence to MF(C)CEs). MF-PSRO using a no- internal-regret (Respectively no-external-regret) algorithm to compute its MFCE (Respectively MFCCE) with average regret threshold (cid:15) and Best-Response Com- putation BR (Respectively BR ) converges to an (cid:15)-MFCE (Respectively CE CCE an (cid:15)-MFCCE). Proof. Based on previous discussions, we know that PSRO must necessarily terminate. If PSRO terminates when using a restricted MFCCE, we must have (cid:88) π∗ = arg max ρ(ν)J(π, µ(ν)) ∈ Π . n π ν (cid:16) (cid:17) By definition of ρ, (cid:80) ρ(ν) J(π∗, µ(ν)) − J(π(ν), µ(ν)) ≤ (cid:15), and therefore ν (cid:16) (cid:17) (cid:80) ∀π ∈ Π, ρ(ν) J(π, µ(ν)) − J(π(ν), µ(ν)) ≤ (cid:15), ergo: ρ is a mean-field (cid:15)- ν coarse correlated equilibrium. The proof for mean-field correlated equilibria follows a similar line of argu- ments and is detailed in Appendix B. As we will see in the next section, it is often possible to improve upon the uniform mixture of (ν )T output by the regret-minimisation algorithm to t t=1 obtain a more accurate approximation to an MF(C)CE. 5.3 Improving the Bandit: Speed 5.3.1 The No-Regret Speedup Algorithm: Bandit Compression One could use No-regret learners directly to converge towards MF(C)CE, but their equilibrium contains T different distributions. This potentially means a 11
101 10 1 10 3 10 5 10 7 0 50 100 150 200 250 300 Iteration paG ECC Uniform vs. Compressed Rho - CCE Gap Compressed rho Uniform rho Figure 1: Uniform vs. Compressed ρ - CCE Gap / Time Mean-Field game Πn={π1,...,πn}⊆Π Restricted MFG {ν1,...,νT }⊆∆(Πn) Induced NFG ( T1 ,νt) t=1..T Regret minimisation Linear programming ε-MF(C)CE Nash equilibrium (ρt,νt) t=1..T Figure 2: Reductions involved in approximation equilibrium computation in MF-PSRO. 12
very high amount of different ν recommended by our (C)CE, which can lead t to learning difficulties on the part of best-responders (since every separate ν must be taken into account), implementation difficulties of equilibria in the real world, and inefficiencies: Indeed, changing per-timestep weights 1 to potentially T non-uniform ρ can lead to converging to (cid:15)(cid:48)-MF(C)CE instead of (cid:15) ones, with t (cid:15)(cid:48) (cid:28) (cid:15), which is illustrated in Figure 1, computed at the first iteration of PSRO, in the Crowd Modelling [34] game. We define (ρ ) as the optimal solution of t t the following optimization problem: min max ρtRegret (2) i ρ i (cid:88) s.t. ∀t ρ ≥ 0, ρ = 1 t t t with Regret [t] := J(π , µ(ν )) − J(π(ν ), µ(ν )). i i t t t We note that Problem (2) can be interpreted as finding the row player’s Nash equilibrium distribution in a zero-sum normal-form game whose payoff matrix for player 1 is Regret. We note that this objective can be expressed linearly. A similar problem can be solved to find better restricted mean-field corre- lated equilibria. First, define (cid:16) (cid:17) Regret (t) = ν (i) J(π , µ(ν )) − J(π , µ(ν )) i,j t j t i t The following problem gives optimal temporal weights ρ for restricted mean-field correlated equilibria min max ρtRegret (3) i,j ρ i,j (cid:88) s.t. ∀t ρ ≥ 0, ρ = 1. t t t This problem can similarly be expressed linearly. The following theorem confirms the optimality of ρ, the solution of Problem (2) or Problem (3): Theorem 9 (Optimality of ρ). If ρ = 1 (cid:80)T δ is a restricted (cid:15)-MFCCE T t=1 νt (respectively (cid:15)-MFCE), then (ρ∗, ν ) , with ρ∗ the optimal solution of Problem t t t 2 (respectively 3), yields a restricted (cid:15)(cid:48)-MF(C)CE of the restricted game, with (cid:15)(cid:48) ≤ (cid:15); and no other ρ distribution over (ν ) can yield an (cid:15)(cid:48)(cid:48)-MF(C)CE with t t (cid:15)(cid:48)(cid:48) < (cid:15)(cid:48). Proof. For restricted MFCCEs, the deviation incentive against the correlation device sampling ν with probability ρ in the restricted game is t t E [J(π(cid:48), µ(ν)) − J(π, µ(ν))] = max ρtRegret . ν∼ρ,π∼ν i i Since the uniform distribution is a possible value for ρ, we necessarily have max ρtRegret ≤ max 1 (cid:80) Regret [t] = (cid:15), which concludes that part of the i i i T t i proof. The proof for restricted MFCEs follows the same line of arguments, and is detailed in Appendix G.5. 13
Optimality of the solutions of problems (2) and (3) directly follows from their definitions together with the above derivations. Given the empirical tendency of this approach to compress temporal dis- tribution, we name it bandit compression. Empirically, it allows us to find much more accurate (Figure 1) and sparser (Appendix D) distributions than (cid:0) (cid:1) uniformly averaging over ν , and in a much lower number of steps. Yet, this t t algorithm is only exact in the case where the regret used by the algorithm is noiseless. The next question is therefore, how sensitive is bandit compression to noise in the regret matrix? 5.3.2 On the value-continuity of min-max problems We provide bounds on computed Average Regrets differences when J is per- turbed by an additive random variable (cid:15): J˜(π, µ) = J(π, µ) + (cid:15), giving rise to notation Regret(cid:15), and to the identity, if we write (cid:15)˜ = (cid:15) − (ν )t(cid:15) , Regret(cid:15) = i t t t t i Regret + (cid:15)˜ . i i We write Regret = min max ρtRegret , Regret(cid:15) = min max ρtRegret(cid:15) ∗ i ∗ i ρ i ρ i We name i and ρ terms such that Regret = (ρ )tRegret , and i(cid:15) and ρ(cid:15) the same value∗ s for Re∗ gret(cid:15) . ∗ ∗ i∗ ∗ ∗ ∗ The quantity we wish to bound is how much additional regret we experience in expectation (ie. without noise) when using the noisy mixture weight ρ(cid:15) ∗ instead of ρ , which we name ∆ = max(ρ(cid:15) )tRegret − (ρ )tRegret . ∗ O i ∗ i ∗ i∗ Proposition 10 (Value-continuity of min-max). The optimality gap ∆ is O bounded in the following way: 0 ≤ ∆ ≤ (ρ )t(cid:15)˜ − min(ρ(cid:15) )t(cid:15)˜ ≤ 2||(cid:15)˜|| ≤ 4||(cid:15)|| . O ∗ i(cid:15) ∗ i ∗ i ∞ ∞ Proof. By optimality of ρ , we already have that ∆ ≥ 0. ∗ O ∆ = max(ρ(cid:15) )tRegret − (ρ )tRegret O i ∗ i ∗ i∗ = max(ρ(cid:15) )t(Regret + (cid:15)˜ ) − (ρ(cid:15) )t(cid:15)˜ − (ρ )tRegret i ∗ i i ∗ i ∗ i∗ ≤ (ρ(cid:15) )t(Regret + (cid:15)˜ ) − min(ρ(cid:15) )t(cid:15)˜ − (ρ )t(Regret + (cid:15)˜ ) + (ρ )t(cid:15)˜ ∗ i(cid:15) ∗ i(cid:15) ∗ i ∗ i ∗ i(cid:15) ∗ i(cid:15) ∗ ∗ i(cid:15) ∗ ≤ (ρ(cid:15) − ρ )t(Regret + (cid:15)˜ ) + (ρ )t(cid:15)˜ − min(ρ(cid:15) )t(cid:15)˜ ∗ ∗ i(cid:15) ∗ i(cid:15) ∗ ∗ i(cid:15) ∗ i ∗ i ≤ (ρ )t(cid:15)˜ − min(ρ(cid:15) )t(cid:15)˜ ≤ 2||(cid:15)˜|| ∗ i(cid:15) ∗ i ∗ i ∞ ∀t, (cid:15)˜ = (cid:15) − (ν )t(cid:15) , and (cid:15) ≤ ||(cid:15)|| and −(ν )t(cid:15) ≤ ||(cid:15)|| , therefore ||(cid:15)˜|| ≤ t t t t t ∞ t t ∞ ∞ 2||(cid:15)|| , which concludes the proof. ∞ The tightness of this bound can be verified via noting that if ρ = ρ(cid:15) and ∗ ∗ the minimum of (ρ )t(cid:15) is reached for i = i(cid:15) , then the optimality gap is null. ∗ i ∗ We discuss this bound in more details in Appendix E, where we compute its value on several examples. 14
5.3.3 The improved PSRO algorithm We add bandit compression onto Algorithm 5, accompanied with a few optimiza- tion criteria, yielding Algorithm 6. The improvements and their motivations are discussed in Appendix G.2. Remark 11 (Use of the Algorithm for Nash-Convergence). We note that one can also use Algorithm 6 for convergence towards MFNE if one uses an iterative solver for computing the Nash equilibrium - in that case, A is the Nash solver, and Regret is the exploitability. Since a Nash equilibrium only uses a single ∗ distribution, one can either bypass solving Problem 2, or solve it trivially with ρ(ν ) = 1. ∗ Algorithm 6: Sped-up mean-field PSRO((C)CE) Result: Policy set Π∗ = {π , ..., π }, (cid:15)-MF(C)CE ρ∗ 1 n Π = ∅, Π = {π } with π any policy, ρ(δ ) = 1.0, N = 1; 0 1 1 1 π1 while (Π \ Π ) (cid:54)= ∅ or ρ > ρ do n+1 n tol lim Π = Π ∪ {BR (π , ρ ) | π , ρ(π ) > 0} ; n+1 n (C)CE i T i i if Π == Π then n+1 n ρ = ρtol tol 2 end n = n + 1; Initialize A(Π ); n Step Count = 0; while Regret ¿ ρ do ∗ tol Step Count += 1 ; Do one step of A(Π ) ; n+1 if Step Count ≡ 0[τ ] then Compress Compute ρ optimal solution of Problem 2 (CCE) / 3 (CE) ; ∗ Compute ρ ’s associated regret Regret ; ∗ ∗ end end ρ = ρ n+1 ∗ end 5.4 Complexity discussion The use of traditional solvers, as has been the case in PSRO so far, requires filling a payoff table. At a given iteration n, this means estimating n match results for the newly added Best Response (The other match results being stored). Property 12 (Payoff matrix estimation complexity). When match payoff esti- mation is done via sampling match outcomes, the number of matches T neces- sary to reach within-(cid:15) estimation precision with probability α is T = O( n ). α(cid:15)2 15
Proof. If we have T episodes to gather on 2n + 1 matches, the most natural (though not necessarily most efficient) way to distribute our compute budget is to give each match T episodes. 2n+1 The variance of an estimated match score Jˆ is therefore Var(Jˆ) = Var(J) = T 2n+1 (2n + 1) Var(J) where Var(J) is the variance of the random variable representing T match outcomes for J. Using Chebyshev’s inequality, we have P(|Jˆ − J| ≥ (cid:15)) ≤ Var(Jˆ) = (2n + (cid:15)2 1) Var(J) . If we aim to be within (cid:15)-precision of J with probability α, i.e. P(|Jˆ − T (cid:15)2 J| ≥ (cid:15)) = α, we need T = O( n ). α(cid:15)2 We contrast this with the complexity of using no-external- and internal- regret learners, given that one chooses an efficient algorithm: Property 13 (Bandit (cid:15)-Regret Complexity). The number of game matches T (cid:16) (cid:17) necessary to reach within-(cid:15) average regret is T = O n3 log(n) for no-internal- (cid:15)2 (cid:16) (cid:17) regret learners, and T = O n log(n) for no-external-regret learners. In the (cid:15)2 case of additively noisy evaluation, where samples are evaluated M times and (cid:16) (cid:17) averaged, these complexities become T = O M n3 log(n) for internal-regret, and (cid:15)2 (cid:16) (cid:17) T = O n M log(n) for external regret; both with probability δ ≥ 1 − n 4σ2 , (cid:15)2 T M(cid:15)2 where σ2 is the noise variance. Proof. The Hedge Algorithm [4] adapted for the partial-information setting [3] (cid:18)(cid:113) (cid:19) (cid:16) (cid:17) has average regret bound (cid:15) = O n log(n) , therefore T = O n log(n) when T (cid:15)2 returns are exact. Optimal Swap-regret minimizers can be derived from optimal external-regret minimizers by running N instances of them in parallel, as shown in [4], therefore (cid:18) (cid:113) (cid:19) (cid:16) (cid:17) (cid:15) = O n n log(n) and T = O n3 log(n) . T (cid:15)2 The additive-payoff noise case is discussed in Appendix G.3. The proof relies on decomposing observed regret in two terms - true-regret and noise, then applying Chebyshev’s concentration inequality and bounding noise terms with ||(cid:15)|| . ∞ We provide a commentary of these results in Appendix G.1, notably compar- ing the traditional PSRO approach’s complexity with the bandit-led approach, analysing noise sensitivity, extending them to the N -player case, and examining the fully-observable setting. 6 Experimental results To demonstrate the viability of our approach, we use three different metrics presented in Section 6.1, which we evaluate when running MF-PSRO on four 16
different mean-field games, which are described in Section 6.2. Evaluation meth- ods are detailed in Section 6.3, and evaluation results are discussed in Section 6.4. 6.1 Evaluation metrics For a given correlation device ρ, we define (cid:88) (cid:0) (cid:1) CCEGap(ρ) := max ρ(ν) J(π, µ(ν)) − J(π(ν), µ(ν) π ν By construction, we directly have that CCEGap(ρ) = 0 is equivalent to ρ being an MFCCE. In the same fashion, we define CEGap(ρ) := max max (cid:88) ρ(ν|π)(cid:0) J(π(cid:48), µ(ν)) − J(π(ν), µ(ν)(cid:1) π(cid:48) π|ρ(π)>0 ν for MFCE characterisation. Finally, for a given population distribution ν ∈ ∆(Π), we introduce Exploitability(ν) := max J(π, µ(ν)) − J(π(ν), µ(ν)) π so that CCEGap(ρ) = 0, which reaches 0 if and only if ν is an MFNE. 6.2 Evaluation games The four games we use to evaluate convergence include atwo complex games available in OpenSpiel [21], Predator-Prey [33] and Crowd Modeling [34], and two new small normal-form mean-field games, Coop / Betray / Punish and mean-field biased Rock-Paper-Scissors, which are described in detail and mo- tivated in Appendix F.1. Summarily, Coop / Betray / Punish is a 3-action normal-form game where agents can choose to either Cooperate, and all get a good reward; betray and take advantage of others; or punish the betrayers. But punishing agents also take some reward away from cooperators (they must sup- port the punishers). Payoffs are non-linear (quadratic) in distributions. mean- field biased Rock-Paper-Scissors is a classic biased Rock-Paper-Scissors game, where one gets as reward for playing rock the proportion of players playing scissors minus that playing paper, all distributions multiplied by different coef- ficients. 6.3 Evaluation Methods The regret minimizer used by mean-field PSRO((C)CE) is Regret Matching [37], and the Black-Box Optimization method used by mean-field PSRO(Nash) is CMA-ES [15]. As per Remark 11, we use Algorithm 6 for both mean-field PSRO((C)CE) and mean-field PSRO(Nash), since the Nash solver CMA-ES is iterative. 17
Figure 3: CCE Gap of mean-field PSRO(CCE). Regarding convergence to MF(C)CE, since there exists, to the best of our knowledge, no other algorithm known to converge towards these weaker equilib- ria we investigate the convergence behavior of mean-field PSRO((C)CE) with additional payoff noise. Regarding convergence towards MFNE, we compare mean-field PSRO to OMD with several different learning rates, and Fictitious Play, both algorithms available on OpenSpiel. 6.4 Evaluation Results Figure 3 presents the CCE-Gap of mean-field PSRO(CCE), 4, the CE-Gap of mean-field PSRO(CE), while Figure 5 exposes the Exploitability of mean- field PSRO(Nash) on the four mean-field game environments described above. We note that in both normal-form games, mean-field PSRO converges within numerical precision towards mean-field correlated, coarse correlated and Nash equilibria after only a few iterations. 18
Figure 4: CE Gap of mean-field PSRO(CE). 19
Figure 5: Exploitability of mean-field PSRO(Nash). 20
Nash-wise, OMD seems capable to follow PSRO at a similar speed on Coop / Betray / Punish, but fails utterly to converge on mean-field biased Rock-Paper- Scissors. We note that OMD’s convergence is strongly affected by its learning rate. Fictitious play does not manage to find good equilibria in these games. On more complex games, mean-field PSRO quickly converges towards very good correlated (CCE Gap ≈ 10−1), coarse correlated equilibria (CE Gap ≈ 10−1), and mean-field PSRO(Nash) seems to quickly minimize exploitability - but it does much more slowly (time-wise) than both OMD and FP. This hints at a strong potential direction of improvement for mean-field PSRO. We note that in this zoomed-in plot, FP seems to outperform OMD. We provide a zoomed- out version in Appendix F.3 where we see that OMD, with the correct learning rate, outperforms Fictitious Play as expected. 7 Limitations Despite its modularity, several improvements on our approach are envisioned for further research. First, our approach cannot efficiently select higher-welfare (C)CEs over lower ones. This problem is known to be NP-Hard in general but learning approaches could hold the key to unlocking these possibilities (see the more detailed discussion in Appendix G.4). Second, mean-field PSRO(Nash) re- lies on a black-box algorithm, whose characteristics strongly impacts the speed and equilibrium F1-score of the algorithm. Finding a principled, general and fast Nash solver in complex restricted games, like we have for mean-field (C)CEs, could yield great improvements, both theoretically and performance-wise. Fi- nally, our method is much slower than OMD or Fictitious Play on large games. This is largely due to a combination of slow payoff evaluation (be it sampled payoff or exact payoff) and relatively large amounts of steps needed to find a restricted equilibrium. 8 Conclusion We have introduced a new mean-field Multi-Agent supervised learning al- gorithm, Mean-Field PSRO, and demonstrated its ability to converge to Nash, correlated and coarse correlated equilibria both theoretically and empirically in various benchmark games. Additionally, the approach was succesfully sped up using a new method named bandit compression, which is motivated by noise robustness and empirical speed. The approach has only been tested so far using the computation of exact best-responses. We expect Reinforcement-Learning algorithms to work out of the box, and answering this question would unlock (C)CE convergence in very large and complex games. 21
References [1] Berkay Anahtarci, Can Deha Kariksiz, and Naci Saldi. 2020. Q-learning in regularized mean-field games. In arXiv. [2] Siddharth Barman and Katrina Ligett. 2015. Finding Any Nontrivial Coarse Correlated Equilibrium Is Hard. arXiv:1504.06314 [cs.GT] [3] Avrim Blum and Yishay Mansour. 2005. From External to Internal Regret. [4] A. Blum and Y. Mansour. 2007. Learning, Regret minimization, and Equi- libria. In Algorithmic Game Theory, Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V. Vazirani (Eds.). Cambridge University Press, Chap- ter 4, 79–102. [5] George W Brown. 1951. Iterative solution of games by fictitious play. Ac- tivity analysis of production and allocation 13, 1 (1951), 374–376. [6] Luciano Campi and Markus Fischer. 2021. Correlated equilibria and mean field games: a simple model. arXiv:2004.06185 [math.OC] [7] Pierre Cardaliaguet and Saeed Hadikhanloo. 2015. Learning in Mean Field Games: the Fictitious Play. arXiv:1507.06280 [math.OC] [8] Constantinos Daskalakis and Christos Papadimitriou. 2007. Computing equilibria in anonymous games. In 48th Annual IEEE Symposium on Foun- dations of Computer Science (FOCS’07). IEEE, 83–93. [9] Constantinos Daskalakis and Christos H Papadimitriou. 2008. Discretized multinomial distributions and Nash equilibria in anonymous games. In 2008 49th Annual IEEE Symposium on Foundations of Computer Science. IEEE, 25–34. [10] Laura Degl’Innocenti. 2018. Correlated equilibria in static mean-field games. [11] Romuald Elie, Julien Perolat, Mathieu Lauri`ere, Matthieu Geist, and Olivier Pietquin. 2020. On the convergence of model free learning in mean field games. In Proceedings of the AAAI Conference on Artificial Intelli- gence, Vol. 34. 7143–7150. [12] Peter I. Frazier. 2018. A Tutorial on Bayesian Optimization. arXiv:1807.02811 [stat.ML] [13] Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. 2020. A General Framework for Learning Mean-Field Games. arXiv:2003.06069 [cs.LG] [14] Saeed Hadikhanloo and Francisco Jos´e Silva. 2018. Finite mean field games: fictitious play and convergence to a first order continuous mean field game. arXiv:1805.05940 [math.OC] 22
[15] Nikolaus Hansen. 2016. The CMA Evolution Strategy: A Tutorial. arXiv:1604.00772 [cs.LG] [16] Sergiu Hart and Andreu Mas-Colell. 2013. Simple adaptive strategies: from regret-matching to uncoupled dynamics. Vol. 4. World Scientific. [17] Minyi Huang, Roland P Malham´e, Peter E Caines, et al. 2006. Large popu- lation stochastic dynamic games: closed-loop McKean-Vlasov systems and the Nash certainty equivalence principle. Communications in Information & Systems 6, 3 (2006), 221–252. [18] Shizuo Kakutani. 1941. A generalization of Brouwer’s fixed point theorem. Duke mathematical journal 8, 3 (1941), 457–459. [19] Arbaaz Khan, Clark Zhang, Daniel D. Lee, Vijay Kumar, and Alejan- dro Ribeiro. 2018. Scalable Centralized Deep Multi-Agent supervised learning via Policy Gradients. arXiv:1805.08776 [cs.LG] [20] Marc Lanctot et al. 2017. A Unified Game-Theoretic Approach to Multia- gent supervised learning. arXiv:1711.00832 [cs.AI] [21] Marc Lanctot et al. 2020. OpenSpiel: A Framework for supervised learning in Games. arXiv:1908.09453 [cs.LG] [22] Jean-Michel Lasry and Pierre-Louis Lions. 2007. Mean Field Games. Japanese Journal of Mathematics 2 (03 2007), 229–260. https://doi. org/10.1007/s11537-007-0657-8 [23] Luke Marris, Paul Muller, et al. 2021. Multi-Agent Training beyond Zero- Sum with Correlated Equilibrium Meta-Solvers. arXiv:2106.09435 [cs.MA] [24] Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. 2012. Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems. The Knowledge Engineering Review 27, 1 (2012), 1–31. [25] H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. 2003. Planning in the presence of cost functions controlled by an adversary. In International Conference on Machine Learning (ICML). [26] H. Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. 2003. Plan- ning in the presence of cost functions controlled by an adversary. [27] Barnab´e Monnot and Georgios Piliouras. 2017. Limits and limitations of no-regret learning in games. The Knowledge Engineering Review 32 (2017). [28] Paul Muller et al. 2020. A Generalized Training Approach for Multiagent Learning. arXiv:1909.12823 [cs.MA] 23
[29] Paul Muller, Romuald Elie, Mark Rowland, Mathieu Lauriere, Julien Pero- lat, Sarah Perrin, Matthieu Geist, Georgios Piliouras, Olivier Pietquin, and Karl Tuyls. 2022. Learning Correlated Equilibria in Mean-Field Games. https://doi.org/10.48550/ARXIV.2208.10138 [30] Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V Vazirani. 2007. Algorithmic game theory. Cambridge University Press. [31] Shayegan Omidshafiei, Christos Papadimitriou, Georgios Piliouras, Karl Tuyls, Mark Rowland, Jean-Baptiste Lespiau, Wojciech M. Czarnecki, Marc Lanctot, Julien Perolat, and Remi Munos. 2019. α-Rank: Multi- Agent Evaluation by Evolution. arXiv:1903.01373 [cs.MA] [32] Afshin OroojlooyJadid and Davood Hajinezhad. 2021. A Review of Cooper- ative Multi-Agent Deep supervised learning. arXiv:1908.03963 [cs.LG] [33] Julien Perolat et al. 2021. Scaling up Mean Field Games with Online Mirror Descent. arXiv:2103.00623 [cs.AI] [34] Sarah Perrin et al. 2020. Fictitious Play for Mean Field Games: Continuous Time Analysis and Applications. arXiv:2007.03458 [math.OC] [35] Mark Rowland, Shayegan Omidshafiei, Karl Tuyls, Julien Perolat, Michal Valko, Georgios Piliouras, and Remi Munos. 2019. Multiagent Eval- uation under Incomplete Information. In Advances in Neural Informa- tion Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran As- sociates, Inc. https://proceedings.neurips.cc/paper/2019/file/ 510f2318f324cf07fce24c3a4b89c771-Paper.pdf [36] Francisco J Solis and Roger J-B Wets. 1981. Minimization by random search techniques. Mathematics of operations research 6, 1 (1981), 19–30. [37] Oskari Tammelin. 2014. Solving Large Imperfect Information Games Using CFR+. arXiv:1407.5042 [cs.GT] [38] Tanvi Verma, Pradeep Varakantham, and Hoong Chuin Lau. 2020. En- tropy based Independent Learning in Anonymous Multi-Agent Settings. arXiv:1803.09928 [cs.LG] [39] Qiaomin Xie, Zhuoran Yang, Zhaoran Wang, and Andreea Minca. 2020. Provable fictitious play for general mean-field games. arXiv preprint arXiv:2010.04211 (2020). 24
Appendices for Learning Equilibria in Mean-Field Games: Introducting Mean-Field PSRO A Proof of graph closedness for restricted Nash existence We wish to use the Kakutani Fixed Point theorem to prove the existence of restricted Nash equilibria. To do this, we have proved all the required hypothesis aside from graph-closedness of φ, the best-response function. We prove here that Graph(φ) is closed. Proof. Graph(φ) = {(ν, ν(cid:48)) ∈ ∆(Π ) × ∆(Π ) | ν(cid:48) ∈ φ(ν)}. Let (ν1, ν2) be a n n k k k sequence of elements of Graph(φ) which converges towards (ν1, ν2) ∈ ∆(Π ) × ∗ ∗ n ∆(Π ). n r is continuous in µ, therefore J is also continuous in µ. Since J : (ν , ν ) → 1 2 J(π(ν ), µ(ν )) is linear in ν because J (π(ν ), µ(ν )) = (cid:80) νi J (π , µ(ν )), it 1 2 1 1 2 i 1 i 2 is also bicontinuous. Since J is bicontinuous, let (cid:15) > 0 and α > 0 be such that ∀(ν , ν ) ∈ 1 2 ∆(Π ) × ∆(Π ) such that d (cid:0) (ν , ν ), (ν1, ν2)(cid:1) ≤ α, n n 1 2 ∗ ∗ |J (π(ν ), µ(ν )) − J(π(ν1), µ(ν2))| ≤ (cid:15) 1 2 ∗ ∗ with d a metric over ∆(Π ) × ∆(Π ) under which J is continuous. Let N > 0 n n 0 be such that ∀n ≥ N , d (cid:0) (ν1, ν2), (ν1, ν2)(cid:1) ≤ α, and let n ≥ N . 0 k k ∗ ∗ 0 By bicontinuity and triangle inequality, J(cid:0) π(ν), µ(ν2)(cid:1) ≤ (cid:15) + J(cid:0) π(ν), µ(ν2)(cid:1) ∗ n −J(cid:0) π(ν1), µ(ν2)(cid:1) ≤ (cid:15) − J(cid:0) π(ν1), µ(ν2)(cid:1) ∗ ∗ n n And by optimality of ν1 against µ(ν2), ∀ν ∈ ∆(Π ), n n n J(cid:0) π(ν), µ(ν2)(cid:1) − J(cid:0) π(ν1), µ(ν2)(cid:1) ≤ 0 n n n We then have, ∀ν ∈ ∆(Π ), n J(π(ν), µ(ν2)) − J(π(ν1), µ(ν2)) ≤ 2(cid:15) + J(π(ν), µ(ν2)) − J(π(ν1), µ(ν2)) ∗ ∗ ∗ n n n ≤ 2(cid:15) This is true for all ν, so also for their sup: sup J(π(ν), µ(ν2)) − J(π(ν1), µ(ν2)) ≤ 2(cid:15) ∗ ∗ ∗ ν 25
Finally, this is true for all (cid:15) > 0. Taking (cid:15) to 0, we have that J(π(ν1), µ(ν2)) = ∗ ∗ sup J(π(ν), µ(ν2)), and thus (ν1, ν2) ∈ Graph(φ). Therefore Graph(φ) is ν ∗ ∗ ∗ closed. B Proof of convergence of mean-field PSRO to- wards mean-field correlated equilibria Proof. If PSRO terminates when using a restricted mean-field correlated equilib- rium, then it means that ∀π , ρ(π ) > 0, π∗(π ) = arg max (cid:80) ρ(ν|π )J(π, µ(ν)) ∈ k k k ν k π (cid:16) (cid:17) Π . By definition of ρ, ∀π ∈ Π , ρ(π) (cid:80) ρ(ν|π) J(π∗(π), µ(ν))−J(π, µ(ν)) ≤ n n ν (cid:16) (cid:17) (cid:15), and therefore ∀π(cid:48) ∈ Π, (cid:80) ρ(ν) J(π(cid:48), µ(ν)) − J(π, µ(ν)) ≤ (cid:15), ergo: ρ is a ν mean-field (cid:15)-correlated equilibrium. C The Linear special case C.1 Definitions Definition 14 (Diff-Affinity). We say that a function f : x, z → f (x, z) is diff-affine in z, or z-diff-affine, if ∀x, y, ∆ (f ) : z → f (x, z) − f (y, z) is affine. x,y Property 15. If r is of the form r(x, a, µ) = C(µ) + r (x, a)tµ + r (x, a), with 1 2 C any function of µ, then J is diff-affine in µ. Provided r is C2, this property is also necessary. We note that our following proofs’ logic can also be applied with an approx- imate version of diff-affinity, where (cid:88) J(π(cid:48), µ(ν)) − J(π, µ(ν)) ≤ ν(π) (J(π(cid:48), µπ) − J(π, µπ)) + (cid:15) π this is for example the case when r = f + g, with f a diff-affine function in µ, and ∀(x, a, µ, µ(cid:48)), |g(x, a, µ(cid:48)) − g(x, a, µ)| ≤ (cid:15). In this case, mean-field PSRO converges to (cid:15) variants of our equilibria. Remark 16. Requiring that f : x, z → f (x, z) to be such that ∀x, y, ∆ (f ) : x,y z → f (x, z) − f (y, z) is convex (ie. is diff-convex) is equivalent to requiring that f be diff-affine. Proof. Let f be diff-convex. Then we know that, since f is scalar, ∆ (f ) is as x,y well for all x, y. If f is twice-differentiable in z, so is ∆ (f ) for all values of x, y. x,y The convexity condition on ∆ (f ) can be rewritten, if f is twice-differentiable, x,y as 26
d2∆ (f ) ∀x, y, x,y ≥ 0 dz2 d2f (x, z) d2f (y, z) ≥ dz2 dz2 Inverting x and y, we find that we have necessarily, ∀x, y, d2f(x,z) = d2f(y,z) = dz2 dz2 c(z), therefore we know that ∀x, z, f (x, z) = C(z) + a(x)z + b(x) C.2 Normal-form games equilibria and links to mean-field restricted games This section presents results linking restricted games with normal-form repre- sentations under the µ-Diff-Affinity condition. We name Π = {π , ..., π } the n 1 n set of policies used by the restricted game in the following. C.2.1 Nash equilibrium We wish to compute the restricted mean-field Nash equilibrium of given policies π 1, ..., π n. To do this, we store values (J(π i, µπj )) i,j in a payoff matrix and com- pute the Nash equilibrium of the two-player game defined as follows : Player 1 receives the payoff received when player 1 chooses a deviating policy i and player 2 chooses the population-generating policy j; Player 2 receives the transposed payoff (ie. Player 1 picks the population-generating policy i and Player 2 picks the deviating policy j).  J(π 1, µπ1) ... J(π 1, µπn)   J(π 1, µπ1) ... J(π n, µπ1)   J(π 2, µπ1) ... J(π 2, µπn)   J(π 1, µπ2) ... J(π n, µπ2)       ... ... ...  ,  ... ... ...      J(π n−1, µπ1) ... J(π n−1, µπn) J(π 1, µπn−1) ... J(π n, µπn−1) J(π n, µπ1) ... J(π n, µπn) J(π 1, µπn) ... J(π n, µπn) Theorem 17 (Normal-form and restricted game equivalence). If J is µ-diff- affine, then any symmetric Nash equilibrium of the symmetric two-player game defined above is also a Nash-equilibrium of the restricted mean-field game defined by π , ..., π . 1 n Proof. Let ν be a symmetric Nash equilibrium of the normal-form game. Then 27
we have that (cid:88) (cid:88) ∀π(cid:48) ∈ {π , ..., π }, ν ν (J(π(cid:48), µπj ) − J(π , µπj )) ≤ 0 1 n i j i i j   (cid:88) (cid:88) (cid:88) (cid:88) ν i∆ π(cid:48),πi(J)  ν jµπj  ≤ ν iν j∆ π,πi(J)(µπ j ) ≤ 0 i j i j (cid:124) (cid:123)(cid:122) (cid:125) =µ(ν) (cid:88) J(π, µ(ν)) − ν J(π , µ(ν)) ≤ 0 i i i J(π, µ(ν)) − J(π(ν), µ(ν)) ≤ 0 where the last line comes from the fact that π(ν) is exactly the policy resulting from sampling π from ν at the start of every episode. Therefore π(ν) is a Nash equilibrium of the game if we restrict deviations to be within the set of the (π ) . i i We must note one important corollary: since the Nash equilibrium of a µ- diff-affine restricted game can be expressed as the symmetric Nash equilibrium of a 2-player symmetric normal-form game, then, according to the Nash Theorem, this Nash equilibrium always exists. This, in turn, guarantees the existence of correlated and coarse-correlated equilibria in µ-diff-affine games. Corollary 18 (Restricted game equilibrium existence). In a µ-diff-affine re- stricted game, Nash, correlated and coarse-correlated equilibria always exist. C.2.2 Restricted-game coarse correlated equilibrium We define a restricted game coarse correlated equilibrium as a recommendation device ρ which recommends population distributions ν ∈ ∆(Π ) such that n (cid:88) (cid:88) (cid:88) max ρ(ν) ν ν (J(π , µ ) − J(π , µ )) ≤ 0 i j k j i j πk ν i j i.e. max (cid:88) ρ(ν) (e − ν)t J ν ≤ 0 k ek ν We note that although the set ∆(Π) is not discrete in general, the above equation is written using a sum (Though we note it could also be written us- ing an integral), the reason being algorithmic. Indeed, given K the number of searched different ν with non-zero support in ρ, our optimization process searches for K different ν ∈ RN , and their distribution ρ ∈ RK, instead of k searching over the infinite-dimensional space P(∆(Π)). 28
We propose below a maximum-margin solution with parameter K, which one can solve using Quadratic Programming by introducing intermediary variables Objective: min max (cid:88) ρ (e − ν )t J ν k l k k ρ,ν1,...,νK el k Probability constraint: ρ ≥ 0, 1tρ = 1 ∀k, ν ≥ 0, 1tν = 1 k k Though note that other objectives and formulations are possible, for exam- ple a maximum-entropy one. We note that the CCE constraint is quadratic, therefore QCP-capable solvers are required, though expect usual simplifications to hold. K (cid:88) Objective: min ρ νt log(ν ) (4) k k k ρ,ν1,...,νK k=1 CCE-Constraint: ∀l, (cid:88) ρ (e − ν )t J ν ≤ 0 (5) k l k k k Probability constraint: ρ ≥ 0, 1tρ = 1 (6) ∀k, ν ≥ 0, 1tν = 1 k k Or maximum-entropy with KL-regularization imposing differences between population recommendations, for 0 ≤ λ ≤ 1, K (cid:32) K (cid:33) Objective: min (cid:88) ρ νt log(ν ) − λ (cid:88) νt log( ν k ) (7) ρ,ν1,...,νK k k k k ν k(cid:48) k=1 k(cid:48)=1 CCE-Constraint: ∀l, (cid:88) ρ (e − ν )t J ν ≤ 0 (8) k l k k k Probability constraint: ρ ≥ 0, 1tρ = 1 (9) ∀k, ν ≥ 0, 1tν = 1 k k C.2.3 Restricted-game correlated equilibrium We define a restricted game correlated equilibrium as a recommendation device ρ which recommends population distributions ν ∈ ∆(Π ) such that n 29
(cid:88) (cid:88) (cid:88) max ρ(ν) ν ν (J(π , µ ) − J(π , µ )) ≤ 0 i j k j i j i πk ν j (cid:88) max (cid:88) ρ(ν)ν (e − e )t J ν ≤ 0 i k i k i ν And thus, we have ∀i, k, (cid:88) ρ(ν)ν (e − e )t J ν ≤ 0 i k i ν As before, given a fixed number of different population distributions K, we suggest three different optimization objectives : A maximum-margin, quadratic optimization one Objective: min (cid:88) max (cid:88) ρ(ν)ν (e − e )t J ν i k i ρ,ν1,...,νK i k ν Prob. constraint: ρ ≥ 0, 1tρ = 1 ∀k, ν ≥ 0, 1tν = 1 k k A maximum-entropy one K (cid:88) Objective: max ρ νt log(ν ) (10) k k k ρ,ν1,...,νK k=1 CE-Constraint: ∀i, k, (cid:88) ρ(ν)ν (e − e )t J ν ≤ 0 (11) i k i ν Probability constraint: ρ ≥ 0, 1tρ = 1 (12) ∀k, ν ≥ 0, 1tν = 1 k k Or a maximum-entropy with KL-regularization imposing differences between population recommendations, when 0 ≤ λ ≤ 1 K (cid:32) K (cid:33) Objective: max (cid:88) ρ νt log(ν ) − λ (cid:88) νt log( ν k ) ρ,ν1,...,νK k k k k ν k(cid:48) k=1 k(cid:48)=1 (13) CE-Constraint: ∀i, k, (cid:88) ρ(ν)ν (e − e )t J ν ≤ 0 (14) i k i ν Probability constraint: ρ ≥ 0, 1tρ = 1 (15) ∀k, ν ≥ 0, 1tν = 1 k k 30
C.3 Mean-Field PSRO: Convergence to Nash equilibria in diff-affine games MF-PSRO is defined in a very similar way to standard PSRO in diff-affine games: start with a restricted policy set Π , and, at each step N , compute the 0 Π restricted Nash equilibrium ν , and compute a best response π to this n n n+1 Π mixed according to ν . If π ∈ Π , then Π mixed according to ν is a n n n+1 n n n Nash equilibrium of the true game, otherwise the algorithm continues. Algorithm 7: MF-PSRO(Nash) (Diff-Convex case) Result: Policy set Π∗ = {π , ..., π }, Policy Distribution ν∗ yielding 1 n game Nash π(ν∗) Π = {π } with π any policy, ν(π ) = 1.0, N = 1; 1 1 1 1 while V BR(µπ(ν)),µπ(ν) > V π(ν),µπ(ν) do Π = Π ∪ {BR(µπ(ν))} ; n+1 n n = n + 1; ∀i, j ≤ n, M i,j = E[J(π i, µπj )] ; ν = Matrix Nash Solver([M, M t]) end C.4 Mean-Field PSRO: Convergence to (coarse) corre- lated equilibria in diff-affine games When the game is µ-diff-affine, we have the following property Property 19. In a µ-diff-affine game, any (coarse) correlated equilibrium of the restricted game is a (coarse) correlated equilibrium of the True game when deviations are restricted to the set of known policies (π ) n n Proof. Since the game is µ-diff-affine, for all ν ∈ ∆(Π ), π ∈ Π we have n k n (cid:88) J(π , µ(ν)) − J(π , µ(ν)) = ν (J(π , µ ) − J(π , µ )) k i j k j i j j Let ρ be the correlation device of an MFCE of the restricted game. Then (cid:88) (cid:88) (cid:88) max ρ(ν)ν ν (J(π , µ ) − J(π , µ )) ≤ 0 i j k j i j i πk∈Πn ν j (cid:124) (cid:123)(cid:122) (cid:125) ≥J(πk,µ(ν))−J(πi,µ(ν)) (cid:88) (cid:88) max ρ(ν)ν(π ) (J(π , µ(ν)) − J(π , µ(ν))) ≤ 0 i k i i πk∈Πn ν therefore ρ is a mean-field correlated equilibrium. 31
Let ρ be the correlation device of an MFCCE of the restricted game. Then (cid:88) (cid:88) (cid:88) max ρ(ν)ν ν (J(π , µ ) − J(π , µ )) ≤ 0 i j k j i j πk∈Πn i ν j (cid:124) (cid:123)(cid:122) (cid:125) ≥J(πk,µ(ν))−J(πi,µ(ν)) (cid:88) max ρ(ν) (J(π , µ(ν)) − J(π(ν), µ(ν))) ≤ 0 k πk∈Πn ν Our algorithm is thus Algorithm 8: MF-PSRO((C)CE) (Diff-Convex case) Result: Policy set Π∗ = {π , ..., π }, Policy Distribution ν∗ yielding 1 n game Nash π(ν∗) Π = {π } with π any policy, ν(π ) = 1.0, N = 1; 1 1 1 1 while V BR(C)CE(µπ(ν)),µπ(ν) > V π(ν),µπ(ν) do Π = Π ∪ {BR (µπ(ν))} ; n+1 n (C)CE N = N + 1; ∀i, j ≤ N, M i,j = E[J(π i, µπj )] ; ν = Restricted-game mean-field (C)CE Solver(M ) end D Bandit Compression Sparsity Figure 6 illustrates the sparsity of the bandit compression distribution, com- puted on the same example as figure 1. E Value-Continuity of Minimax : Examples Assuming each (cid:15) is a random gaussian variable with variance σ > 0, the term i (cid:18) (cid:19) max ρt (cid:15) is such that P(max ρt (cid:15) ≤ y) = Φn √ y where Φ is a standard i ∆ i i ∆ i σ ρt ∆ρ∆ gaussian CDF, and the term on the right ρt(cid:15) ∼ \(0, σ2 ). (cid:15) i∆ ρt (cid:15)ρ(cid:15) To get an estimation of the magnitude of this gap’s distribution, we assume N to be high enough that we can ignore the term ρt(cid:15) for our numerical appli- cation. Using the 5σ rule, we find that P (cid:16) ∆ ≤(cid:15) 5σi∆ (cid:112) ρt ρ (cid:17) ≥ 0.9999994N . O ∆ ∆ Assuming σ = 0.1, N = 50 and ρt ρ = 1 (Fully uniform distribution), ∆ ∆ 50 P (∆ ≤ 0.07) ≥ 0.99997. When ρ is fully focused on one point, ρt ρ = 1, O ∆ ∆ ∆ and the former equation becomes P (∆ ≤ 0.5) ≥ 0.99997 O We note that this is a pessimistic estimate for several reasons 32
100 10 1 10 2 0 50 100 150 200 250 300 Iteration ytisrapS Compressed Rho Sparsity Figure 6: Bandit compression: sparsity / time. 33
• ρ should presumably not be focused on a single point, and therefore the (cid:112) term 5σ ρtρ will be quite lower • This does not take into account the complex relationship and dependence between the two terms max ρt (cid:15) and ρt(cid:15) i ∆ i (cid:15) i∆ F Experiments: Details and Additional Results F.1 Game description and motivation Both games have 3 actions, A, B and C, whose rewards depend on the ac- tion distribution of the population. For mean-field Biased Indirect Rock Paper Scissors, we have r(A, µ) = 0.5 ∗ µ(B) − 0.3 ∗ µ(C) r(B, µ) = 0.3 ∗ µ(C) − 0.7 ∗ µ(A) r(C, µ) = 0.7 ∗ µ(A) − 0.5 ∗ µ(B) For Coop / Betray / Punish, we have 20 r(A, µ) = µ(A) − (µ(A) − µ(C)) ∗ µ(C) − 2µ(B) 9 r(B, µ) = 2(µ(A) − µ(B)) − 238µ(C) 200 r(C, µ) = (µ(A) − µ(C)) ∗ µ(C) 9 F.2 Sped-up mean-field PSRO parameter effects PSRO Parameter Effect when Increased T Improved asymptotic convergence, lowers speed M Lower noise, improves convergence at fixed T , lower speed ρ Lower precision, higher speed Tol τ Higher speed if costly compression, otherwise lower Compress F.3 MF-PSRO(Nash) Figure 7 shows exploitability of MF-PSRO(Nash), OMD for several learning rates, and Fictitious Play. G On No-Regret Learners, Bandit Compression and Sped-Up mean-field PSRO G.1 Commentary on Complexity Analysis • Minimizing swap regret directly has higher complexity than payoff matrix estimation by a factor n2log(n) in worst cases. 34
101 100 10 1 10 2 0 200 400 600 800 1000 ytilibatiolpxE Predator-Prey 2D Crowd Modelling 101 100 10 1 10 2 0 200 400 600 800 1000 102 10 1 10 4 10 7 10 10 10 13 0 200 400 600 800 1000 iteration ytilibatiolpxE Algorithm MF-PSRO(Nash) OMD-LR=1e-3 Coop / Betray / Punish Mean-Field Biased Indirect Rock-Paper-Scissors OMD-LR=0.01 OMD-LR = 0.1 10 1 Fictitious Play 10 3 10 5 10 7 10 9 10 11 10 13 0 200 400 600 800 1000 iteration Figure 7: Algorithm Exploitabilities 35
• Minimizing external regret directly has higher complexity than payoff ma- trix estimation by a factor log(n) in worst cases. • Using regret minimizers directly provides the user with a useable distri- bution over policies • Estimating the payoff matrix means the user still has to run an algorithm over said payoff matrix, which could have large complexity (Linear solvers have complexity O(n2+γ) with γ > 0, for example) • The relationship between payoff uncertainty and solver output uncertainty is difficult to analyze in general, due to the strong non-linearities of solvers. Indeed, picture the following 0-sum game: three strategies face off, π 1 has payoff 100 against π and π , and π has payoff 1 against π . Any 2 3 2 3 reasonable (cid:15)-error in estimating the payoff obtained by playing π would 1 not change e.g. its Nash distribution, or the distribution of a correlated equilibrium. In contrary, in a game where all average payoffs are very close to 0 (π barely beats π and π , and π barely beats π ), an (cid:15)-error 1 2 3 2 3 could lead to a reversal of these interactions (In the noisy payoff matrix, it could be that π beats π which beats π ), thus completely changing 2 3 1 the computed distribution. • We do not yet fully understand the complexity reduction granted by Ban- dit Compression, which could greatly lower asymptotic complexity of the Bandit approach. • This complexity insight can be transferred to the N-player case. In this case, one needs to compute (n + 1)N − nN = O(nN−1) matches, and (cid:16) (cid:17) the estimation complexity is therefore T = O nN−1 . The number of α(cid:15)2 different actions is nN , so the complexity of minimizing internal regret is (cid:16) (cid:17) (cid:16) (cid:17) O Nn3N log(n) and it is O NnN log(n) for external regret minimization. (cid:15)2 (cid:15)2 • If we can observe all policies’ payoffs at no additional cost, the regret (cid:16) (cid:17) (cid:16) (cid:17) bounds become O n log(n) for internal regret, and O log(n) for ex- (cid:15)2 (cid:15)2 ternal regret. The complexity for payoff matrix estimation nevertheless remains O( n α+ (cid:15)21 ), as one is interested in (J(π n, µπk )) k and (J(π k, µπn)) k G.2 Improvements on sped-up PSRO algorithm Sped-Up PSRO includes the following new features: • ρ : This term is a regret threshold. If the optimal solution of Problem 2 tol or 3 yields regret lower than this term, we consider the equilibrium search as successful. • ρ and new loop conditions: The PSRO loop does not terminate anymore lim when Π == Π , what it does then is refine its current equilibrium by n+1 n 36
halving ρ at every iteration where Π == Π , until ρ == ρ , tol n+1 n tol lim with ρ set to a very low value, typically 10−12 lim • τ : Typically set to 1, this value allows one not to optimize prob- Compress lems 2 or 3 at every regret minimization step. This can be especially useful when computing MFCEs, for which the problem is much slower to solve than for MFCCEs. G.3 Proof of Complexity in Noisy Payoff case In the case where payoffs are additively noisy, Regret can be decomposed into two terms: Regret = R + R˜ , where R is the true, noiseless regret, and R˜ i i i i (cid:16) (cid:17) is the noise-derived regret. Given α > 0, after O M n3 log(n) steps, we know α2 that Regret ≤ α . We have that True Regret − Regret = max R − (max R + 2 j j i i R˜ ) ≤ max −R˜ , and P(max −R˜ ≥ α) ≤ (cid:80) P(−R˜ ≥ α). Given that R˜ = i i i i i i i i 1 (cid:80) (cid:15) [t] − νt(cid:15)[t], and (cid:15) is averaged over M samples and thus has σ2 variance, T t i t t M then Var(−R˜ ) ≤ σ2 and Chebyshev’s inequality yields P(−R˜ ≥ α ) ≤ 4σ2 , i T M i 2 T Mα2 thus yielding P(max −R˜ ≥ α) ≤ n 4σ2 . We then have P(True Regret ≤ α) ≥ i i T Mα2 P(Regret + max −R˜ ≤ α) ≤ P(max −R˜ ≤ α ) ≤ 1 − n 4σ2 The probability i i i i 2 T Mα2 (cid:16) (cid:17) of the true regret being lower than α after O M n3 log(n) steps is therefore α2 δ ≥ 1 − n σ2 . Since each regret steps is now composed of M times as many T Mα2 rollouts, the total rollout-complexity of the algorithm must be multiplied by M , which concludes the proof. G.4 On the complexity of improving Welfare We have so far introduced a method that learns Nash, correlated and coarse correlated equilibria in mean-field games. A subsequent question for correlated and coarse correlated equilibria is, could we influence the learning process for it to find high-welfare equilibria instead of low-welfare ones ? This problem of high-welfare convergence was shown by [2] to be NP-hard in general, with the notable exception of succinct aggregate games, for which the existence of polynomial algorithms converging to high-welfare equilibria is proven. However, their method relies on a discretization of and grid-search over the aggregate space, the space of statistics summarizing the behavior of other players. At the n-th step of mean-field PSRO, discretizing the n-dimensional prob- ability vector space with step size 1 amounts to considering matrices of size M ≥ n (cid:0) M (cid:1)n , a complexity exponential in the number of iterations, therefore pro- 2 hibitive. We therefore leave open the question of high-welfare convergence for now. 37
G.5 Optimality of ρ for correlated equilibria ∗ For restricted MFCEs: The deviation incentive against policy π recommended by the correlation i device sampling ν with probability ρ in the restricted game is t t max ρ(π)E [J(π(cid:48), µ(ν)) − J(π, µ(ν))] ν∼ρ(·|π) π,π(cid:48) (cid:88) (cid:16) (cid:17) = max ρ ν (i) J(π , µ(ν )) − J(π , µ(ν )) t t j t i t i,j t = max ρtRegret i,j i,j Since the uniform distribution is a possible solution of Problem 3, we thus have that the average max deviation incentive against ρ∗ the solution of Problem 3 is lower than or equal to that of the uniform distribution, which concludes this part of the proof. H On Monotonicity in Restricted Games Property 20. A game is monotonic if and only if all its restricted games are. Proof. Assume all restricted games are monotonic, take π , π two policies of 1 2 the game, and take the monotonic game containing only π , π . By assumption, 1 2 it is monotonic, ie. ∀ν , ν ∈ ∆ ({π , π }), 1 2 1 2 J (ν , ν ) + J (ν , ν ) − J (ν , ν ) − J (ν , ν ) ≤ 0 r 1 1 r 2 2 r 1 2 r 2 1 with J (ν, ν(cid:48)) = J(π(ν), µ(ν(cid:48))). It suffices to take ν = δ and ν = δ to r 1 π1 2 π2 directly have J(π , µπ1) + J(π , µπ2) − J(π , µπ1) − J(π , µπ2) ≤ 0 1 2 2 1 and since this is true for all π , π , the game is monotonic. 1 2 Assume the game is monotonic. Take π , ..., π with N > 0 and consider 1 N their derived restricted game. Let ν , ν ∈ ∆({π , ..., π }). 1 2 1 N J (ν , ν )+J (ν , ν )−J (ν , ν )−J (ν , ν ) = J(π(ν ), µ(ν ))+J(π(ν ), µ(ν ))−J(π(ν ), µ(ν ))−J(π(ν ), µ(ν )) r 1 1 r 2 2 r 2 1 r 1 2 1 1 2 2 2 1 1 2 given that ∀ν, µ(ν) = µπ(ν). Since π(ν ) and π(ν ) are both policies of the true 1 2 game, and the true game is monotonic, J(π(ν ), µ(ν )) + J(π(ν ), µ(ν )) − J(π(ν ), µ(ν )) − J(π(ν ), µ(ν )) ≤ 0 1 1 2 2 2 1 1 2 and thus J (ν , ν ) + J (ν , ν ) − J (ν , ν ) − J (ν , ν ) ≤ 0 r 1 1 r 2 2 r 2 1 r 1 2 which concludes the proof. 38
