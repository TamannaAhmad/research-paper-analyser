More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech Michael Hassid Michelle Tadmor Ramanovich Brendan Shillingford Google Research Google Research DeepMind hassid@google.com tadmor@google.com shillingford@deepmind.com Miaosen Wang Ye Jia Tal Remez DeepMind Google Research Google Research miaosen@deepmind.com jiaye@google.com talremez@google.com Abstract In this paper we present VDTTS, a Visually-Driven Text- Speech to-Speech model. Motivated by dubbing, VDTTS takes ad- vantage of video frames as an additional input alongside VDTTS text, and generates speech that matches the video signal. "it's not an We demonstrate how this allows VDTTS to, unlike plain TTS entitlement models, generate speech that not only has prosodic varia- it's a demand" tions like natural pauses and pitch, but is also synchronized to the input video. Figure 1. Given a text and video frames of a speaker, VDTTS generates speech with prosody that matches the video signal. Experimentally, we show our model produces well- synchronized outputs, approaching the video-speech syn- chronization quality of the ground-truth, on several chal- In contrast, we extend the TTS setting to input not only lenging benchmarks including “in-the-wild” content from text, but also facial video frames, producing speech that VoxCeleb2. Supplementary demo videos demonstrating matches the facial movements of the input video. The re- video-speech synchronization, robustness to speaker ID sult is audio that is not only synchronized to the video but swapping, and prosody, presented at the project page.1 also retains the original prosody, including pauses and pitch changes that can be inferred from the video signal, provid- ing a key piece in producing high-quality dubbed videos. 1. Introduction In this work, we present VDTTS, a visually-driven TTS model. Given text and corresponding video frames of a Post-sync, or dubbing (in the film industry), is the pro- speaker speaking, our model is trained to generate the cor- cess of re-recording dialogue by the original actor in a con- responding speech (see Fig. 1). As opposed to standard vi- trolled environment after the filming process to improve au- sual speech recognition models, which focus on the mouth dio quality. Sometimes, a replacement actor is used instead region [3], we provide the full face to avoid potentially of the original actor when a different voice is desired such excluding information pertinent to the speaker’s delivery. as Darth Vader’s character in Star Wars [1]. This gives the model enough information to generate speech Work in the area of automatic audio-visual dubbing of- which not only matches the video but also recovers aspects ten approaches the problem of generating content with syn- of prosody, such as timing and emotion. Despite not being chronized video and speech by (1) applying a text-to-speech explicitly trained to generate speech that is synchronized to (TTS) system to produce audio from text, then (2) modify- the input video, the learned model still does so. ing the frames so that the face matches the audio [2]. The Our model is comprised of four main components. Text second part of this approach is particularly difficult, as it and video encoders process the inputs, followed by a multi- requires generation of photorealistic video across arbitrary source attention mechanism that connects these to a decoder filming conditions. that produces mel-spectrograms. A vocoder then produces waveforms from the mel-spectrograms. 1Project page: http://google-research.github.io/lingvo-lab/vdtts We evaluate the performance of our method on GRID [4] 2202 raM 32 ]VC.sc[ 2v93101.1112:viXra
as well as on challenging in-the-wild videos from Vox- TTS prosody control Skerry-Ryan et al. [22] define Celeb2 [5]. To validate our design choices and training pro- prosody as “the variation in speech signals excluding pho- cess, we also present an ablation study of key components netics, speaker identity, and channel effects.” Standard TTS of our method, model architecture, and training procedure. approaches tend to be trained to produce neutral speech, due Demo videos are available on the project page,1 the difficulty of modeling prosody. demonstrating video-speech synchronization, robustness to Great efforts have been made towards transferring or speaker ID swapping, and prosody. We encourage readers controlling the prosody of TTS audio. Wang et al. [23] cre- to take a look. ated a style embedding by using a multi-headed attention Our main contributions are that we: module between the encoded input audio sequence and the global style tokens (GSTs). They trained a model jointly • present and evaluate a novel visual TTS model, trained with the Tacotron model using the reconstruction loss of on a wide variety of open-domain YouTube videos; the mel-spectrograms. At inference time, they construct the style embedding from the text to enable style control, or • show it achieves state-of-the-art video-speech synchro- from other audio for style transfer. nization on GRID and VoxCeleb2 when presented with A Variational Auto-Encoder (VAE) latent representation arbitrary unseen speakers; and of speaking style was used by [24]. During inference time, • demonstrate that our method recovers aspects of they alter speaking style by manipulating the latent embed- prosody such as pauses and pitch while producing nat- ding, or by obtained it from a reference audio. Hsu et al. ural, human-like speech. [25] used a VAE to create two levels of hierarchical la- tent variables, the first representing attribute groups, and the second representing more specific attribute configurations. 2. Related work This setup allows fine-grained control of the generated au- dio prosody including accent, speaking rate, etc. Text-to-speech (TTS) engines which generate natural sounding speech from text, have seen dazzling progress in recent years. Methods have shifted from parametric mod- Silent-video-to-speech In this setup, a silent video is els towards increasingly end-to-end neural networks [6, 7]. presented to a model that tries to generate speech con- This shift enabled TTS models to generate speech that sistent with the mouth movements, without providing sounds as natural as professional human speech [8]. Most text. Vid2Speech [26] uses a random forest approaches consist of three main components: an encoder (CNN) that generates an acoustic feature for each frame that converts the input text into a sequence of hidden repre- of a silent video. Lipper [27] use a closeup video of lips sentations, a decoder that produces acoustic representations and produces text and speech, while [28] directly generated like mel-spectrograms from these, and finally a vocoder that the speech without a vocoder. Prajwal et al. [29] propose a constructs waveforms from the acoustic representations. speaker specific lip-reading model. Some methods including Tacotron and Tacotron 2 use an attention-based autoregressive approach [7, 9, 10]; followup work such as FastSpeech [11, 12], Non-Attentive Tacotron Datasets For our task, we require triplets consisting of: (NAT) [8, 13] and Parallel Tacotron [14, 15], often replace a facial video, the corresponding speech audio, and a text recurrent neural networks with transformers. transcript. The video and text are used as model inputs, Extensive research has been conducted on how to invert mel-spectrograms back into waveforms; since the former Utts. Hrs. Vocab. Speakers ID Source is a compressed audio representation, it is not generally GRID [4] 34K 43 51 34 (cid:51) Studio invertible. For example, the seminal work of Griffin and LRS2 [30] 47K 29 18K - (cid:55) BBC Lim [16] proposes a simple least-squares approach, while LRS3 [31] 32K 30 17K 3.8K (cid:55) TED/TEDx modern approaches train models to learn task-specific map- VoxCeleb2 [5] 1M 2442 ˜35K* 6.1K (cid:51) YouTube LSVSR [2, 3] 3M 3130 127K ˜464K (cid:55) YouTube pings that can capture more of the audio signal, including the approaches of WaveNet as applied to Tacotron 2 [9], Table 1. Audio-visual speech dataset size comparison in terms MelGAN [6, 17], or more recent work like WaveGlow [18] of number of utterances, hours, and vocabulary. Numbers are which trains a flow-based conditional generative model, shown before processing; the resulting number of utterances we DiffWave [19] which propose a probabilistic model for con- use for VoxCeleb2 and LSVSR are smaller. In Yang et al. [2], ditional and unconditional waveform generation, or Wave- LSVSR is called MLVD. (*VoxCeleb2 lacks transcripts, so we Grad [20] that make use of data density gradients to gener- use an English-only automated transcription model [32] to pro- ate waveforms. In our work, we use the fully-convolutional duce transcripts for training purposes, also used for vocabulary SoundStream vocoder [21]. size measurement in this table.)
VDTTS Input Frames Video Encoder Multi Source Spectrogram Vocoder Generated Speech Attention Decoder Input Text Text Encoder Speaker ID Speaker Encoder Figure 2. The overall architecture of our model. Colors: inputs: yellow, trainable: blue, frozen: purple, output: orange. whereas the speech audio is used as ground-truth for metrics Let T and T be the length of input video frame and x y and loss computation. phoneme sequences respectively. Let D , D and D be w h c GRID is a standard dataset filmed under consistent con- the width, height and the number of channels of the frames, ditions [4]. LRW [33] and LRS2 [30] are based on high- D e the dimension of the phoneme embeddings, and P the quality BBC television content, and LRS3 [31] is based on set of phonemes. TED talks; however, these datasets are restricted to aca- We begin with an input pair composed of a source video demic use only. VoxCeleb2 [5] and LSVSR [2, 3], being frame sequence x ∈ RTx×Dw×Dh×Dc and a sequence of based on open-domain YouTube data, contain the widest phonemes y ∈ PTy . range of people, types of content, and words. A compar- The video encoder receives a frame sequence as input, ison of dataset size appears in Table 1. produces a hidden representation for each frame, and then In this work, we adopt GRID as a standard benchmark, concatenates these representations, i.e., and VoxCeleb2 and LSVSR due to their greater difficulty. h = VideoEncoder(x) ∈ RTx×Dm, (1) x Automated dubbing A common approach to automated where D is the hidden dimension of the model. m dubbing is to generate or modify the video frames to match Similarly, the text encoder receives the source phonemes a given clip of audio speech [2, 34, 35, 36, 37, 38, 39, 40, and produces a hidden representation, 41]. This wide and active area of research uses approaches that vary from conditional video generation, to retrieval, to h = TextEncoder(y) ∈ RTy×Dm. (2) y 3D models. Unlike this line of work, we start from a fixed video and generate audio instead. The speaker encoder maps a speaker to a 256- Recent visual TTS work uses both text and video frames dimensional speaker embedding, to train a TTS model, much like our approach. Concur- rent work to ours [42, 43] take this approach, the former us- d = SpeakerEncoder(speaker ) ∈ R256. (3) i i ing GRID and the latter using just LRS2. Unlike our work, these approaches explicitly constrain output signal length The autoregressive decoder receives as input the two hid- and attention weights to encourage synchronization. den representations h and h , and the speaker embedding x y d , and predicts the mel-spectrogram of the synthesized i 3. Method speech using the attention context, In this section, we describe the architecture of the pro- zˆt = Decoder(zˆt−1, h , h , d ). (4) x y i posed model and depict its components. Full architec- tural and training details are given in Appendix A and Ap- Finally, the predicted mel-spectrogram [zˆ1, zˆ2, . . . , zˆTz ] pendix B respectively. is transformed to a waveform using a frozen pretrained neu- ral vocoder [21]. Overview Fig. 2 illustrates the overall architecture of the VTTS model. As shown, and similarly to [44], the archi- Video encoder Our video encoder is inspired by VGG3D tecture consists of (1) a video encoder, (2) a text encoder, as in [3]. However, unlike their work and similar lipread- (3) a speaker encoder, (4) an autoregressive decoder with a ing work, we use a full face crop instead of a mouth-only multi-source attention mechanism, and (5) a vocoder. The crop to avoid potentially excluding information that could method follows [44] using the combined L + L loss. be pertinent to prosody, such as facial expressions. 1 2
Text encoder Our text encoder is derived from Tacotron 2’s [9] text encoder. Each phoneme is first embedded in a ct Multi Source D -dimensional embedding space. Then the sequence of e Attention phoneme embeddings is passed through convolution layers Project and a Bi-LSTM layer. Concat Speaker encoder In order to enable our model to handle Spectrogram a multi-speaker environment, we use a frozen, pretrained ct x ct y Decoder speaker embedding model [45] following [10]. When the speaker ID is provided in the dataset, as for GRID and Vox- ct-1 x Attention Attention ct-1 y Celeb2, we generate embeddings per utterance and average over all utterances associated with the speaker, normalizing h h x y the result to unit norm. For LSVSR the speaker identity qt is unavailable, so we compute the embedding per-utterance. At test time, while we could use an arbitrary speaker embed- Figure 3. The Multi Source Attention Mechanism. ding to make the voice match the speaker for comparison purposes, we use the average speaker embedding over the audio clips from this speaker. We encourage the reader to The input of the decoder at timestep t is the projection of refer to the project page 1, in which example videos demon- the concatenation of the two contexts described above via a strate how VDTTS preforms when speaker voice embed- linear layer, dings are swapped between different speakers. ct = Linear([ct , ct ]). (7) x y While [44] aggregated the context vectors using summation, Decoder Our RNN-based autoregressive decoder is simi- we found that a concatenation and projection work better in lar to the one proposed by [9], and consists of four parts: a our setting as shown in Sec. 4.6. pre-net, a fully connected network reprojecting the previous We use a Gaussian mixture attention mechanism [46] for decoder output onto a lower dimension before it is used as both modalities (video and text), since it is a soft monotonic input for future time steps; an attention module, in our case attention which is known to achieve better results for speech multi-source attention, discussed later; an LSTM core; and synthesis [47, 48, 49]. a post-net which predicts the final mel-spectrogram output. Full architectural details appear in Appendix A. The decoder receives as input the output sequences of the: video encoder h x, the text phoneme encoder h y as well 4. Experiments as the speaker embedding produced by the speaker encoder d , and generates a mel-spectrogram of the speech signal zˆt. To evaluate the performance of the proposed video en- i In contrast to [9], which do not support speaker voice em- hanced TTS model we conducted experiments on two very beddings, we concatenate them to the output of the pre-net different public datasets: GRID [4] and VoxCeleb2 [5]. to enable our model to be used in a multi-speaker environ- GRID presents a controlled environment allowing us to test ment, i.e: the input for the multi-source attention at timestep our method on high quality, studio captured videos with a t is small vocabulary, in which the same speakers appear in both qt = concat(PreNet(zˆt−1), d ). (5) the train and test sets. VoxCeleb2, however, is much more i in-the-wild, therefore it is more diverse in terms of appear- ance (illumination, image quality, audio noise, face angles, Multi-source attention A multi-source attention mech- etc.), and the set of speakers in the test set do not appear anism, similar to that of Textual Echo Cancellation [44], in the training set. This allows us to test the ability of the allows selecting which of the outputs of the encoder are model to generalize to unseen speakers. passed to the decoder in each timestep. The multi-source attention, as presented in Fig. 3, has 4.1. Evaluation Metrics an individual attention mechanism for each of the encoders, We objectively evaluate prosodic F1-score, video-speech without weights sharing between them. At each timestep t, synchronization, and word error rate (WER). We further each attention module outputs an attention context, evaluate synchronization subjectively with human ratings as ct = Att (qt, ct−1, h ); ct = Att (qt, ct−1, h ), (6) described below. x x x x y y y y Pitch (fundamental frequency, F 0) and voicing contours where qt is the output of the pre-net layer of the decoder are computed using the output of the YIN pitch tracking al- at timestep t. gorithm [50] with a 12.5ms frame shift. For cases in which
the predicted signal is too short we pad using a domain- 4.1.3 Lip Sync Error appropriate padding up to the length of the reference. If it We use Lip Sync Error - Confidence (LSE-C) and Lip Sync is too long we clip it shorter. Error - Distance (LSE-D) [56] to measure video-speech In the remainder of this section we define and provide synchronization between the predicted audio and the video intuition for the metrics in the experimental section. signal. The measurements are taken using a pretrained SyncNet model [57]. 4.1.1 Mel Cepstral Distortion (MCD ) [51] K is a mel-spectrogram distance measure defined as: 4.1.4 Word Error Rate (WER) T −1 (cid:118) (cid:117) K A TTS model is expected to produce an intelligible speech MCD = 1 (cid:88) (cid:117) (cid:116)(cid:88) (f − fˆ )2, (8) signal consistent with the input text. To measure this ob- T t,k t,k jectively, we measure WER as determined by an automatic t=0 k=1 speech recognition (ASR) model. To this end we use a state- where fˆ and f are the k-th Mel-Frequency Cepstral of-the-art ASR model as proposed in [32], trained on the t,k t,k Coefficient (MFCC) [52] of the t-th frame from the ref- LibriSpeech [58] training set. The recognizer was not al- erence and the predicted audio respectively. We sum the tered or fine-tuned. squared differences over the first K = 13 MFCCs, skip- Since LSVSR is open-ended content, and out-of-domain ping f (overall energy). MFCCs are computed using a compared to the audiobooks in LibriSpeech, the ASR per- t,0 25ms window and 10ms step size. formance may result in a high WER even on ground-truth audio. Thus, we only use the WER metric for relative com- parison. In Appendix C, we compute the WER on predic- 4.1.2 Pitch Metrics tions from a text-only TTS model trained on several datasets We compute the following commonly used prosody met- to establish a range of reasonable WERs; we confirm that a rics over the pitch and voicing sequences produced from rather high WER is to be expected. the synthesized and the ground-truth waveforms [48, 53]. 4.1.5 Video-speech sync Mean Opinion Score (MOS) F0 Frame Error (FFE) [54] measures the percentage of frames that either contain a 20% pitch error or a voicing We measured video-speech synchronization quality with a decision error. 3-point Likert scale with a granularity of 0.5. Each rater is (cid:80) 1[|p − p(cid:48) | > 0.2p ]1[v = v(cid:48)] + 1[v (cid:54)= v(cid:48)] required to watch a video at least twice before rating it and a FFE = t t t t t t t t rater cannot rate more than 18 videos; each video is rated by T (9) 3 raters. Each evaluation was conducted independently; dif- where p, p(cid:48) are the pitch, and v, v(cid:48) are the voicing contours ferent models were not compared pairwise. The (averaged) computed over the predicted and ground-truth audio. MOS ratings are shown as a 90% confidence interval. In Sec. 4.4 we rate a total of 200 videos each containing Gross Pitch Error (GPE) [55] measures the percentage of a unique speaker, while in Sec. 4.3 we chose 5 clips per frames where pitch differed by more than 20% on frames speaker resulting in a total of 165 videos. and voice was present on both the predicted and reference audio. 4.2. Data preprocessing (cid:80) 1[|p − p(cid:48) | > 0.2p ]1[v = v(cid:48)] Several preprocessing steps were conducted before train- GPE = t t t t t t (10) (cid:80) 1[v = v(cid:48)] ing and evaluating our models, including audio filtering, t t t face cropping and limiting example length. where p, p(cid:48) are the pitch, and v, v(cid:48) are the voicing contours We follow a similar methodology first proposed by [3] computed over the predicted and ground-truth audio. while creating the LSVSR dataset. We limit the duration of all examples to be in the range of 1 to 6 seconds, and Voice Decision Error (VDE) [55] measures the proportion transcripts are filtered through a language classifier [60] to of frames where the predicted audio is voiced differently include only English. We also remove utterances which than the ground-truth. have less than one word per second on average, since they (cid:80) 1[v (cid:54)= v(cid:48)] do not contain enough spoken content. We filter blurry VDE = t t t (11) clips and use a neural network [61] to verify that the au- T dio and video channels are aligned. Then, we apply a land- where v, v(cid:48) are the voicing contours computed over the pre- marker as in [62] and keep segments where the face yaw dicted and ground-truth audio. and pitch remain within ±15◦ and remove clips where an
MOS ↑ LSE-C ↑ LSE-D ↓ WER ↓ MCD ↓ FFE ↓ GPE ↓ VDE ↓ GROUND-TRUTH [42] - 7.68 6.87 - - - - - VISUALTTS [42] - 5.81 8.50 - - - - - GROUND-TRUTH 2.68 ± 0.04 7.24 6.73 26% - - - - TTS-TEXTONLY [59] 1.51 ± 0.05 3.39 10.44 19% 15.76 0.48 0.30 0.42 VDTTS-LSVSR 2.10 ± 0.06 5.85 7.93 55% 12.81 0.37 0.21 0.32 VDTTS-GRID 2.55 ± 0.05 6.97 6.85 26% 7.89 0.14 0.07 0.11 Table 2. GRID evaluation. This table shows our experiments on the GRID dataset. The top two rows present the numbers as they appear in VISUALTTS [42]. GROUND-TRUTH shows the metrics as evaluated on the original speech/video. TTS-TEXTONLY shows the performance of a vanilla text-only TTS model, while VDTTS-LSVSR and VDTTS-GRID are our model when trained on LSVSR and GRID respectively. While VDTTS-GRID archives the best overall performance, it is evident VDTTS-LSVSR generalizes well enough to the GRID dataset to outperform VISUALTTS [42]. See Sec. 4.1 for an explanation of metrics; arrows indicate if higher or lower is better. MOS ↑ LSE-C ↑ LSE-D ↓ WER ↓ MCD ↓ FFE ↓ GPE ↓ VDE ↓ GROUND-TRUTH 2.79 ± 0.03 7.00 7.51 - - - - - TTS-TEXTONLY [59] 1.77 ± 0.05 1.82 12.44 4% 14.67 0.59 0.38 0.42 VDTTS-VOXCELEB2 2.50 ± 0.04 5.99 8.22 48% 12.17 0.46 0.31 0.30 VDTTS-LSVSR 2.45 ± 0.04 5.92 8.25 25% 12.23 0.46 0.29 0.31 Table 3. VoxCeleb2 evaluation. GROUND-TRUTH shows the synchronization quality of the original VoxCeleb2 speech and video. TTS- TEXTONLY represents a vanilla text-only TTS model, while VDTTS-VOXCELEB2 and VDTTS-LSVSR are our model when trained on VoxCeleb2 and LSVSR respectively. By looking at the WER, it is evident VDTTS-VOXCELEB2 generates unintelligible results, while VDTTS-LSVSR generalizes well to VoxCeleb2 data and produces better quality overall. See Sec. 4.1 for an explanation of metrics; arrows indicate if higher or lower is better. eye-to-eye width of less than 80 pixels. Using the extracted method using the same methodology reported by the au- and smoothed landmarks, we discard minor lip movements thors. To that end, we take 100 random videos from each and nonspeaking faces using a threshold filter. The land- speaker as a test set. We use the remainder 900 exam- marks are used to compute and apply an affine transforma- ples per speaker as training data, and also for generating tion (without skew) to obtain canonicalized faces. Audio is a lookup-table containing the speaker embedding, averaged filtered [63] to reduce non-speech noise. and normalized per speaker, as explained in Sec. 3. At test We use this methodology to collect a similar dataset to time we present our models with video frames alongside the LSVSR [3], which we use as our in-the-wild training set transcript and the average speaker embedding. with 527, 746 examples, and also to preprocess our version We evaluate our method using the metrics mentioned in of VoxCeleb2, only changing the maximal face angle to 30◦ Sec. 4.1, and compare it to several baselines: (1) VISU- to increase dataset size. Running the same processing as ALTTS [42]; (2) PnG NAT TTS zero-shot voice transfer- described above on VoxCeleb2 results in 71, 772 train, and ring model from [59], a state-of-the-art TTS model trained 2, 824 test examples. As for GRID which we use as our on the LibriTTS [64] dataset, denoted as TTS-TEXTONLY; controlled environment, we do not filter the data, and only (3) our model when trained over LSVSR (see Sec. 4.2); and use the face cropping part of the aforementioned pipeline to (4) our model trained on the GRID training set. generate model inputs. Unfortunately, VisualTTS [42] did not provide their ran- dom train/test splits. Therefore, we report the original met- 4.3. Controlled environment evaluation rics as they appear in [42] alongside the numbers we found In order to evaluate our method in a controlled environ- over our test set. Luckily, the two are comparable, as can be ment we use the GRID dataset [4]. GRID is composed of seen by the two rows in Table 2 named GROUND-TRUTH. studio video recordings of 33 speakers (originally 34, one is The results appear in Table 2. Observe that, when trained corrupt). There are 1000 videos of each speaker, and in each on GRID, our method outperforms all other methods over video a sentence is spoken with a predetermined “GRID” in all metrics except WER. Moreover, our model trained on format. The vocabulary of the dataset is relatively small and LSVSR, as we will see in a later section, gets better video- all videos were captured in a controlled studio environment speech synchronization results than VisualTTS, which was over a green screen with little head pose variation. trained on GRID, showing that our “in-the-wild” model We compare VDTTS to the recent VisualTTS [42] generalizes to new domains and unseen speakers.
GT VDTTS TTS- TEXTONLY [59] F 0(VDTTS) F 0(GT) F 0(TTS- TEXTONLY) F 0(GT) Time Time (a) (b) Figure 4. Qualitative examples. We present two examples (a) and (b) from the test set of VoxCeleb2 [5]. From top to bottom: input face images, ground-truth (GT) mel-spectrogram, mel-spectrogram output of VDTTS, mel-spectrogram output of a vanilla TTS model TTS- TEXTONLY, and two plots showing the normalized pitch F 0 (normalized by mean nonzero pitch, i.e. mean is only over voiced periods) of VDTTS and TTS-TEXTONLY compared to the ground-truth signal. For actual videos we refer the reader to the project webpage. 4.4. In-the-wild evaluation LSVSR. As hypothesized, this leads to a great improve- ment in WER and reduced the error to only 24% while leav- In this section we evaluate VDTTS on the in-the-wild ing most other metrics comparable. For more details refer data from the test set of VoxCeleb2 [5]. This is an open- to Appendix C. source dataset made of in-the-wild examples of people For qualitative examples of VDTTS-LSVSR we refer speaking and is taken from YouTube. We preprocess the the reader to Sec. 4.5. data as described in Sec. 4.2. Since this data is not tran- scribed, we augment the data with transcripts automatically 4.5. Prosody using video generated using [32], yielding 2,824 high quality, automat- We selected two inference examples from the test set ically transcribed test videos. We create a speaker embed- of VoxCeleb2 to showcase the unique strength of VDTTS, ding lookup table by averaging and normalizing the speaker which we present in Fig. 4. In both examples, the video voice embeddings from all examples of the same speaker. frames provide clues about the prosody and word timing. As a baseline we again use TTS-TEXTONLY, a text-only Such visual information is not available to the text-only TTS TTS model from [59]. The results are shown in Table 3. model, TTS-TEXTONLY [59], to which we compare. Initially we trained our model on the train set of Vox- In the first example (see Fig. 4(a)), the speaker talks Celeb2, called VDTTS-VOXCELEB2. Unfortunately, as at a particular pace that results in periodic gaps in the can be seen by the high WER of 48%, the model produced ground-truth mel-spectrogram. The VDTTS model pre- difficult-to-comprehend audio. We hypothesized that noisy serves this characteristic and generates mel-spectrograms automated transcripts were the culprit, so trained the model that are much closer to the ground-truth than the ones gen- on an alternative in-the-wild dataset with human generated erated by TTS-TEXTONLY without access to the video. transcripts, LSVSR, we denote this model by VDTTS- Similarly, in the second example (see Fig. 4(b)), the
LSE-C ↑ LSE-D ↓ WER ↓ MCD ↓ FFE ↓ GPE ↓ VDE ↓ Looking at VDTTS-small, makes it evident that increas- Full VDTTS 5.92 8.25 25% 12.23 0.46 0.29 0.31 ing D beyond what was originally suggested by Ding et al. VDTTS-no-sp-emb 1.49 12.14 27% 14.5 0.67 0.43 0.37 m [44] is required. VDTTS-small 1.48 12.45 38% 14 0.6 0.4 0.43 VDTTS-sum-att 5.74 8.47 28% 12.22 0.46 0.29 0.31 Another interesting model is VDTTS-no-text, which has VDTTS-no-text 5.90 8.28 98% 12.99 0.53 0.35 0.35 access only to the video frame input without any text. In VDTTS-no-video 1.44 12.62 27% 14.36 0.58 0.34 0.47 terms of video-speech synchronization it is on par with the VDTTS-video-len 1.58 12.37 28% 13.98 0.59 0.37 0.42 VDTTS-mouth 5.51 8.59 29% 12.24 0.52 0.41 0.31 full model for LSE-C and LSE-D, but fails to produce words as can be seen by its high WER. Intriguingly, outputs from Table 5. Ablation study, showing different variations of the this model look clearly synchronized, but sounds like En- VDTTS model and hence the contribution of these components to glish babbling, as can be seen in the examples on the project the performance of VDTTS. See Sec. 4.6 for a detailed explana- page1. On one hand, this shows that the text input is nec- tion of the different models, and Sec. 4.1 for definitions of metrics. essary in order to produce intelligible content, and on the Arrows indicate if higher or lower is better. other hand it shows the video is sufficient for inferring syn- chronization and prosody without having access to the un- derlying text. speaker takes long pauses between some of the words. This Although it seems that VDTTS-video-len shows simi- can be observed by looking at the gaps in the ground-truth lar results to the VDTTS-no-video model, the former pro- mel-spectrogram. These pauses are captured by VDTTS duces speech signal which corresponds to the original scene and are reflected in the predicted result below, whereas the length (as desired), which the latter does not. mel-spectrogram of TTS-TEXTONLY does not capture this Lastly, the VDTTS-mouth performs a bit worse than the aspect of the speaker’s rhythm. full model, which suggests that the use of the full face crop We also plot F 0 charts to compare the pitch generated is indeed beneficial to the model. by each model to the ground-truth pitch. In both examples, the F 0 curve of VDTTS fits the ground-truth much better 5. Discussion and Future Work than the TTS-TEXTONLY curve, both in the alignment of speech and silence, and also in how the pitch changes over In this paper we presented VDTTS, a novel visually- time. driven TTS model that takes advantage of video frames as To view the videos and other examples, we refer the an input and generates speech with prosody that matches reader to the project page1. the video signal. Such a model can be used for post-sync or dubbing, producing speech synchronized to a sequence 4.6. Ablation of video frames. Our method also naturally extends to In this section we conduct an ablation study to better un- other applications such as low-quality speech enhancement derstand the contribution of our key design choices. in videos and audio restoration in captioned videos. Results are presented in Table 5 using the following ab- VDTTS produces near ground-truth quality on the GRID breviations for the models: (1) VDTTS-no-sp-emb: VDTTS dataset. On open-domain “in-the-wild” evaluations, it pro- without the use of a speaker embedding. Although unlikely, duces well-synchronized outputs approaching the video- this version could possibly learn to compensate for the miss- speech synchronization quality of the ground-truth and per- ing embedding using the person in the video. (2) VDTTS- forms favorably compared to alternate approaches. small: VDTTS with smaller encoders and decoder, with Intriguingly, VDTTS is able to produce video- D = 512 as in [9]. (3) VDTTS-sum-att: VDTTS using a synchronized speech without any explicit losses or con- m summation (as in [44]) instead of concatenation in the Multi straints to encourage this, suggesting complexities such Source Attention mechanism. (4) VDTTS-no-text: VDTTS as synchronization losses or explicit modeling are unnec- without text input, can be thought of as a silent-video-to- essary. Furthermore, we demonstrated that the text and speech model. (5) VDTTS-no-video: VDTTS without video speaker embedding supply the speech content and voice, input, can be thought of as a TTS model. (6) VDTTS-video- while the prosody is produced by the video signal. Our re- len: VDTTS trained with empty frames, used as a baseline sults also suggest that the “easiest” solution for the model of a TTS model which is aware of the video length. (7) to learn is to infer prosody visually, rather than modeling it VDTTS-mouth: VDTTS which is trained and evaluated on from the text. the mouth region only (as in most speech recognition mod- In the context of synthesis it is important to address the els). potential for misuse by generating convincingly false audio. VDTTS-no-sp-emb performs poorly on the video-speech Since VDTTS is trained using video and text pairs in which synchronization metrics LSE-C and LSE-D, likely due to the speech pictured in the video corresponds to the text spo- underfitting since the model is unable to infer the voice of ken, synthesis from arbitrary text is out-of-domain, making the speaker using only the video. it unlikely to be misused.
References [14] I. Elias, H. Zen, J. Shen, Y. Zhang, Y. Jia, R. Weiss, and Y. Wu, “Parallel Tacotron: Non-autoregressive and control- [1] V. Canby, “Lucas returns with “The Jedi”,” The New York lable TTS,” in ICASSP, 2021. 2 Times, p. 24, May 1983. 1 [15] I. Elias, H. Zen, J. Shen, Y. Zhang, Y. Jia, R. Skerry-Ryan, [2] Y. Yang, B. Shillingford, Y. Assael, M. Wang, W. Liu, and Y. Wu, “Parallel Tacotron 2: A non-autoregressive neu- Y. Chen, Y. Zhang, E. Sezener, L. C. Cobo, M. Denil, Y. Ay- ral TTS model with differentiable duration modeling,” in IN- tar, and N. de Freitas, “Large-scale multilingual audio visual TERSPEECH, 2021. 2 dubbing,” arXiv:2011.03530 [cs, eess], Nov. 2020. 1, 2, 3 [3] B. Shillingford, Y. Assael, M. W. Hoffman, T. Paine, [16] D. Griffin and J. Lim, “Signal estimation from modified C. Hughes, U. Prabhu, H. Liao, H. Sak, K. Rao, L. Ben- short-time fourier transform,” IEEE Transactions on acous- nett et al., “Large-scale visual speech recognition,” arXiv tics, speech, and signal processing, vol. 32, no. 2, pp. 236– preprint arXiv:1807.05162, 2018. 1, 2, 3, 5, 6 243, 1984. 2 [4] M. Cooke, J. Barker, S. Cunningham, and X. Shao, “An [17] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh, audio-visual corpus for speech perception and automatic J. Sotelo, A. de Bre´bisson, Y. Bengio, and A. Courville, speech recognition,” The Journal of the Acoustical Society “Melgan: Generative adversarial networks for conditional of America, vol. 120, no. 5, pp. 2421–2424, 2006. 1, 2, 3, 4, waveform synthesis,” arXiv preprint arXiv:1910.06711, 6 2019. 2 [5] J. S. Chung, A. Nagrani, and A. Zisserman, “Vox- [18] R. Prenger, R. Valle, and B. Catanzaro, “Waveglow: A flow- celeb2: Deep speaker recognition,” arXiv preprint based generative network for speech synthesis,” in ICASSP arXiv:1806.05622, 2018. 2, 3, 4, 7 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. [6] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, 3617–3621. 2 A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “Wavenet: A generative model for raw audio,” arXiv preprint [19] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, arXiv:1609.03499, 2016. 2 “Diffwave: A versatile diffusion model for audio synthesis,” arXiv preprint arXiv:2009.09761, 2020. 2 [7] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio et al., [20] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and “Tacotron: Towards end-to-end speech synthesis,” arXiv W. Chan, “Wavegrad: Estimating gradients for waveform preprint arXiv:1703.10135, 2017. 2 generation,” arXiv preprint arXiv:2009.00713, 2020. 2 [8] Y. Jia, H. Zen, J. Shen, Y. Zhang, and Y. Wu, “PnG BERT: Augmented BERT on phonemes and graphemes for neural [21] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and TTS,” in INTERSPEECH, 2021. 2 M. Tagliasacchi, “Soundstream: An end-to-end neural audio codec,” arXiv preprint arXiv:2107.03312, 2021. 2, 3 [9] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, R. A. Saurous, [22] R. J. Skerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D. Stan- Y. Agiomyrgiannakis, and Y. Wu, “Natural TTS synthesis by ton, J. Shor, R. J. Weiss, R. Clark, and R. A. Saurous, “To- conditioning WaveNet on Mel spectrogram predictions,” in wards End-to-End Prosody Transfer for Expressive Speech ICASSP, 2018. 2, 4, 8 Synthesis with Tacotron,” arXiv:1803.09047 [cs, eess], Mar. 2018. 2 [10] Y. Jia, Y. Zhang, R. J. Weiss, Q. Wang, J. Shen, F. Ren, Z. Chen, P. Nguyen, R. Pang, I. L. Moreno, and Y. Wu, [23] Y. Wang, D. Stanton, Y. Zhang, R. J. Skerry-Ryan, E. Bat- “Transfer learning from speaker verification to multispeaker tenberg, J. Shor, Y. Xiao, F. Ren, Y. Jia, and R. A. text-to-speech synthesis,” in NeurIPS, 2018. 2, 4 Saurous, “Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis,” [11] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.- arXiv:1803.09017 [cs, eess], Mar. 2018. 2 Y. Liu, “FastSpeech: Fast, robust and controllable text to speech,” in NeurIPS, 2019. 2 [24] Y.-J. Zhang, S. Pan, L. He, and Z.-H. Ling, “Learning latent [12] Y. Ren, C. Hu, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, “Fast- representations for style control and transfer in end-to-end Speech 2: Fast and high-quality end-to-end text-to-speech,” speech synthesis,” arXiv:1812.04342 [cs, eess], Feb. 2019. in ICLR, 2021. 2 2 [13] J. Shen, Y. Jia, M. Chrzanowski, Y. Zhang, I. Elias, H. Zen, [25] W.-N. Hsu, Y. Zhang, R. J. Weiss, H. Zen, Y. Wu, Y. Wang, and Y. Wu, “Non-Attentive Tacotron: Robust and control- Y. Cao, Y. Jia, Z. Chen, J. Shen, P. Nguyen, and R. Pang, lable neural TTS synthesis including unsupervised duration “Hierarchical Generative Modeling for Controllable Speech modeling,” arXiv preprint arXiv:2010.04301, 2020. 2 Synthesis,” arXiv e-prints, Oct. 2018. 2
[26] A. Ephrat and S. Peleg, “Vid2speech: speech reconstruc- [39] O. Fried, A. Tewari, M. Zollho¨fer, A. Finkelstein, E. Shecht- tion from silent video,” in 2017 IEEE International Confer- man, D. B. Goldman, K. Genova, Z. Jin, C. Theobalt, and ence on Acoustics, Speech and Signal Processing (ICASSP). M. Agrawala, “Text-based editing of talking-head video,” IEEE, 2017, pp. 5095–5099. 2 arXiv preprint arXiv:1906.01524, 2019. 3 [27] Y. Kumar, R. Jain, K. M. Salik, R. R. Shah, Y. Yin, and [40] H. Kim, M. Elgharib, M. Zollho¨fer, H.-P. Seidel, T. Beeler, R. Zimmermann, “Lipper: Synthesizing thy speech using C. Richardt, and C. Theobalt, “Neural style-preserving multi-view lipreading,” in Proceedings of the AAAI Confer- visual dubbing,” ACM Transactions on Graphics (TOG), ence on Artificial Intelligence, vol. 33, no. 01, 2019, pp. vol. 38, no. 6, pp. 1–13, 2019. 3 2588–2595. 2 [41] A. Jha, V. Voleti, V. Namboodiri, and C. Jawahar, “Cross- [28] R. Mira, K. Vougioukas, P. Ma, S. Petridis, B. W. language speech dependent lip-synchronization,” in ICASSP Schuller, and M. Pantic, “End-to-end video-to-speech syn- 2019-2019 IEEE International Conference on Acoustics, thesis using generative adversarial networks,” arXiv preprint Speech and Signal Processing (ICASSP). IEEE, 2019, pp. arXiv:2104.13332, 2021. 2 7140–7144. 3 [29] K. Prajwal, R. Mukhopadhyay, V. P. Namboodiri, and C. Jawahar, “Learning individual speaking styles for ac- [42] J. Lu, B. Sisman, R. Liu, M. Zhang, and H. Li, “Visualtts: curate lip to speech synthesis,” in Proceedings of the Tts with accurate lip-speech synchronization for automatic IEEE/CVF Conference on Computer Vision and Pattern voice over,” arXiv preprint arXiv:2110.03342, 2021. 3, 6 Recognition, 2020, pp. 13 796–13 805. 2 [43] C. Hu, Q. Tian, T. Li, Y. Wang, Y. Wang, and H. Zhao, “Neu- [30] J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, “Lip ral dubber: Dubbing for silent videos according to scripts,” reading sentences in the wild,” in 2017 IEEE Conference on arXiv preprint arXiv:2110.08243, 2021. 3 Computer Vision and Pattern Recognition (CVPR). IEEE, 2017, pp. 3444–3453. 2, 3 [44] S. Ding, Y. Jia, K. Hu, and Q. Wang, “Textual echo cancel- lation,” arXiv preprint arXiv:2008.06006, 2020. 3, 4, 8 [31] T. Afouras, J. S. Chung, and A. Zisserman, “Lrs3-ted: a large-scale dataset for visual speech recognition,” arXiv [45] L. Wan, Q. Wang, A. Papir, and I. L. Moreno, “Generalized preprint arXiv:1809.00496, 2018. 2, 3 end-to-end loss for speaker verification,” in 2018 IEEE Inter- national Conference on Acoustics, Speech and Signal Pro- [32] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, cessing (ICASSP). IEEE, 2018, pp. 4879–4883. 4 B. Li, Y. Wu, and Q. V. Le, “Improved noisy student training for automatic speech recognition,” arXiv preprint [46] A. Graves, “Generating sequences with recurrent neural net- arXiv:2005.09629, 2020. 2, 5, 7 works,” arXiv preprint arXiv:1308.0850, 2013. 4 [33] J. S. Chung and A. Zisserman, “Lip reading in the wild,” in [47] M. He, Y. Deng, and L. He, “Robust sequence-to-sequence Asian Conference on Computer Vision. Springer, 2016, pp. acoustic modeling with stepwise monotonic attention for 87–103. 3 neural tts,” arXiv preprint arXiv:1906.00672, 2019. 4 [34] A. Lahiri, V. Kwatra, C. Frueh, J. Lewis, and C. Bregler, “Lipsync3d: Data-efficient learning of personalized 3d talk- [48] R. Skerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D. Stan- ing faces from video using pose and lighting normalization,” ton, J. Shor, R. Weiss, R. Clark, and R. A. Saurous, “Towards in Proceedings of the IEEE/CVF Conference on Computer end-to-end prosody transfer for expressive speech synthe- Vision and Pattern Recognition, 2021, pp. 2755–2764. 3 sis with tacotron,” in international conference on machine learning. PMLR, 2018, pp. 4693–4702. 4, 5 [35] S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher- Shlizerman, “Synthesizing obama: learning lip sync from [49] A. Polyak and L. Wolf, “Attention-based wavenet autoen- audio,” ACM Transactions on Graphics (TOG), vol. 36, coder for universal voice conversion,” in ICASSP 2019-2019 no. 4, p. 95, 2017. 3 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2019, pp. 6800–6804. 4 [36] R. Kumar, J. Sotelo, K. Kumar, A. de Bre´bisson, and Y. Ben- gio, “Obamanet: Photo-realistic lip-sync from text,” arXiv [50] A. De Cheveigne and H. Kawahara, “Yin, a fundamental preprint arXiv:1801.01442, 2017. 3 frequency estimator for speech and music,” Journal of the [37] P. KR, R. Mukhopadhyay, J. Philip, A. Jha, V. Nambood- Acoustical Society of America, 111(4): 1917–1930, 2002. 4 iri, and C. Jawahar, “Towards automatic face-to-face transla- tion,” in Proceedings of the 27th ACM International Confer- [51] R. Kubichek, “Mel-cepstral distance measure for objective ence on Multimedia, 2019, pp. 1428–1436. 3 speech quality assessment,” pp. 125–128, 1993. 5 [38] L. Song, W. Wu, C. Qian, R. He, and C. C. Loy, “Every- [52] V. Tiwari, “Mfcc and its applications in speaker recognition,” body’s talkin’: Let me talk as you want,” arXiv preprint International journal on emerging technologies, vol. 1, no. 1, arXiv:2001.05201, 2020. 3 pp. 19–22, 2010. 5
[53] B. Sisman, J. Yamagishi, S. King, and H. Li, “An overview of voice conversion and its challenges: From statistical mod- eling to deep learning,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. 5 [54] W. Chu and A. Alwan, “Reducing f0 frame error of f0 tracking algorithms under noisy conditions with an un- voiced/voiced classification frontend,” in 2009 IEEE Inter- national Conference on Acoustics, Speech and Signal Pro- cessing. IEEE, 2009, pp. 3969–3972. 5 [55] A. S. I. T. I. K. Nakatani, Tomohiro and T. Kondo, “A method for fundamental frequency estimation and voicing decision: Application to infant utterances recorded in real acoustical environments,” Speech Communication, 50(3), pp. 203–214, 2008. 5 [56] V. P. N. KR Prajwal, Rudrabha Mukhopadhyay and C. Jawa- har, “A lip sync expert is all you need for speech to lip gen- eration in the wild,” in 28th ACM International Conference on Multimedia, 2020, pp. 484–492. 5 [57] J. S. Chung and A. Zisserman, “Out of time: automated lip sync in the wild,” in PWorkshop on Multi-view Lip-reading (ACCV), 2016. 5 [58] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib- rispeech: an asr corpus based on public domain audio books,” in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206–5210. 5 [59] Y. Jia, M. T. Ramanovich, T. Remez, and R. Pomerantz, “Translatotron 2: Robust direct speech-to-speech transla- tion,” arXiv preprint arXiv:2107.08661, 2021. 6, 7 [60] A. Salcianu, A. Golding, A. Bakalov, C. Alberti, D. Andor, D. Weiss, E. Pitler, G. Coppola, J. Riesa, K. Ganchev et al., “Compact language detector v3,” 2018. 5 [61] J. S. Chung and A. Zisserman, “Lip reading in profile,” 2017. 5 [62] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embedding for face recognition and clustering,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 815–823. 5 [63] I. Kavalerov, S. Wisdom, H. Erdogan, B. Patton, K. Wilson, J. Le Roux, and J. R. Hershey, “Universal sound separation,” in 2019 IEEE Workshop on Applications of Signal Process- ing to Audio and Acoustics (WASPAA), 2019, pp. 175–179. 6 [64] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, “Libritts: A corpus de- rived from librispeech for text-to-speech,” arXiv preprint arXiv:1904.02882, 2019. 6, 13
A. Detailed model architecture Video Encoder Input size D = D = 128, D = 1 w h c Max Pooling per layer (F, T, T, T, T) Number of output channels per layer (64, 128, 256, 512, 512) Stride per layer (2, 1, 1, 1, 1) Kernel size (for all layers) (3, 3, 3) Activation (for all layers) ReLU Normalization (for all layers) Group Norm Text Encoder Input size D = 512 e Conv layers × 3 2048 5×1 kernel with 1×1 stride Activation (for all Conv layers) ReLU Bi-LSTM 1024-dim per direction Normalization (for all Conv layers) Batch Norm Multi Source Attention Attention input size D = 2048 m GMM attention (per source) 128-dim context Linear Projection Fully connected layer Decoder PreNet 2 fully connected layers with 256 neurons and ReLU act. LSTM × 2 1024-dim Bi-LSTM 1024-dim per direction PostNet 5 conv layers with 512 5×1 kernel with 1×1 stride and TanH act. Normalization (for all Decoder layers) Batch Norm Teacher forcing prob 1.0 B. Training hyperparameters Training learning rate 0.0003 learning rate scheduler type Linear Rampup with Exponential Decay scheduler decay start 40k steps scheduler decay end 300k steps scheduler warm-up 400 steps batch size 512 Optimizer optimizer details Adam with β = 0.9, β = 0.999 1 2 Regularization L2 regularization factor 1e-06
C. Word error rate discussion As explained in Sec. 4.4, our VoxCeleb2 transcripts are automatically generated and thus contain transcription er- rors. As a result one can expect the WER for models trained on this data to be non-zero. In order to validate this hypoth- esis, that the result of such noisy data leads to a non-zero WER, we trained a version of the our model that accepts only text as input (without silent video), denoted as TTS- OUR. TTS-OUR was trained twice, once on the LibriTTS [64] dataset, and a second time when using our in-the-wild LSVSR dataset. When looking at Table 6 it is clear that when trained on LibriTTS this model achieves a low WER of 7%, while the same model when trained on in-the-wild dataset get a WER of 27%. This suggests that a WER in the region of [20%, 30%] should be expected when using LSVSR. That being said, we believe reporting WER is valuable as a sanity check for noisy datasets, specially when trying to capture more than just the words. Training data WER LIBRITTS 7% LSVSR 27% Table 6. Comparison of WER on the VoxCeleb2 test set for our text only TTS model (TTS-OUR) when trained on different datasets.
