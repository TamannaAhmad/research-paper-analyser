2202 luJ 62 ]GL.sc[ 3v05631.1112:viXra Latent Space Smoothing for Individually Fair Representations Momchil Peychev1 , Anian Ruoss2⋆ , Mislav Balunović1 , Maximilian Baader1 , and Martin Vechev1 1Department of Computer Science, ETH Zurich 2DeepMind, London {momchil.peychev,mislav.balunovic,mbaader,martin.vechev}@inf.ethz.ch anianr@deepmind.com Abstract. Fair representation learning transforms user data into a rep- resentation that ensures fairness and utility regardless of the downstream application. However, learning individually fair representations, i.e., guar- anteeing that similar individuals are treated similarly, remains challeng- ing in high-dimensional settings such as computer vision. In this work, we introduce LASSI, the first representation learning method for certifying individual fairness of high-dimensional data. Our key insight is to lever- age recent advances in generative modeling to capture the set of similar individuals in the generative latent space. This enables us to learn indi- vidually fair representations that map similar individuals close together by using adversarial training to minimize the distance between their rep- resentations. Finally, we employ randomized smoothing to provably map similar individuals close together, in turn ensuring that local robustness verification of the downstream application results in end-to-end fairness certification. Our experimental evaluation on challenging real-world im- age data demonstrates that our method increases certified individual fairness by up to 90% without significantly affecting task utility. Keywords: fair representation learning, individual fairness, smoothing 1 Introduction Deep learning models are increasingly deployed in critical domains, such as face detection [75], credit scoring [38], or crime risk assessment [6], where decisions of the model can have wide-ranging impacts on society. Unfortunately, the models and datasets employed in these settings are biased [7,43], which raises concerns against their usage for such tasks and causes regulators to hold organizations accountable for the discriminatory effects of their models [18,19,22,23,78]. In this regard, fair representation learning [89] is a promising bias mitigation approach that transforms data to prevent discrimination regardless of the con- crete downstream application while simultaneously maintaining high task utility. The approach is highly modular [60]: the data regulator defines the fairness no- tion, the data producer learns a fair representation that encodes the data, and the ⋆ Work partially done while the author was at ETH Zurich.
2 M. Peychev et al. Data Producer Data Consumer Samples a Classifier C x pale r cs r cs z G r dcs dcs Encoder E LASSI R R C(r cs + δ) = C(r cs) E(x) = z G R(z G) = r R and R(z G) = r cs w.bh.p. for all kδbk ≤ dcs b Fig. 1: Overview of our framework LASSI. The left part shows the data pro- ducer who captures the set of individuals similar to x by interpolating along the attribute vector a . The data producer then uses adversarial training and cen- pale ter smoothing to compute a representation that provably maps all similar points into the ℓ -ball of radius d around r . The right part shows the data consumer 2 cs cs who can certify individual fairness, i.e., prove that all similar individuals receive the same classification outcome, of the end-to-end model by checking whether the certified radius obtained via randomized smoothing exceeds d . cs data consumers employ the transformed data in downstream tasks. Recent work successfully augmented fair representation learning with guarantees [24,68], but its application to high-dimensional data, such as images, remains challenging. Key challenge: scaling to high-dimensional data and real-world mod- els The two central challenges of individually fair representation learning, which requires similar individuals to be treated similarly, are: (i) designing a suitable input similarity metric [87,89] and (ii) enforcing that similar individuals are prov- ably treated similarly according to that metric. For low-dimensional tabular data, prior work has typically measured input similarity in terms of the input features (age, income, etc.), using, e.g., logical constraints [68] or weighted ℓ -metrics [86]. p However, characterizing the similarity of high-dimensional data, such as images, at the input-level, e.g., by comparing pixels, is infeasible. Moreover, proving that all points in the infinite set of similar individuals obtain the same clas- sification requires propagating this set through the model. Unfortunately, for high-dimensional applications this is unattainable for prior work using (mixed- integer) linear programming solvers [16,77], which only scale to small networks. This work In this work, we introduce latent space smoothing for individually fair representations (LASSI), a method that addresses both of the above chal- lenges. Our approach leverages two recent advances: the emergence of powerful generative models [41], which enable the definition of image similarity for indi- vidual fairness, and the scalable certification of deep models [10], which allows proving individual fairness. A high-level overview of our approach is shown in Fig. 1. Concretely, we use generative modeling [41] to enable data regulators to define input similarity by varying a continuous attribute of the image, such as pale skin in Fig. 1. To enforce that similar individuals are provably treated similarly, we further base our approach on smoothing: (i) the data producer uses center smoothing [44] to learn a representation that provably maps similar in-
Latent Space Smoothing for Individually Fair Representations 3 dividuals close together, and (ii) the data consumer certifies local ℓ -robustness 2 using randomized smoothing [10], thereby proving individual fairness of the end- to-end model. Therefore, our approach enables data regulators to impose fairness notions of the form: “For a given person, all people differing only in skin tone should receive the same classification” and allows data producers and consumers to independently learn a representation and classification models that provably enforce this notion. To measure input similarity, the data producer leverages the ability of a bi- jective generative model to interpolate along the direction of an attribute vector in the latent space, which is impractical in the pixel space. As a result, the set of similar individuals can be defined by a line segment in the latent space (center part of the data producer in Fig. 1), corresponding to an elaborate curve in the input space (left part of the data producer in Fig. 1), which cannot be concisely captured by, e.g., an ℓ -ball. Thus, the data producer learns a representation R p that maps all points of the latent line segment close together in the representa- tion space by using adversarial training to minimize the distance between similar individuals. However, as adversarial training cannot provide guarantees on this maximum distance, the data producer uses center smoothing [44] to adjust the representation such that its smoothed version R provably maps all similar points into an ℓ 2-ball of radius d cs around a centerbr cs with high probability (right part of the data producer in Fig. 1). Finally, the data consumer only needs to prove that the certified radius (violet in the data consumer part of Fig. 1) of its smoothed classifier C around r is larger than d to obtain an individual cs cs fairness certificate for thbe end-to-end model M := C ◦ R ◦ E. Our experimental evaluation on several image clbassibfication tasks shows that training with LASSI significantly increases the number of individuals for which we can certify individual fairness, with respect to multiple different sensitive attributes, as well as their combinations. Overall, we certify up to 90% more than the baselines. Furthermore, we demonstrate that the representations obtained by LASSI can be used to solve classification tasks that were unseen during training. Main contributions We make the following contributions: – A novel input similarity metric for high-dimensional data defined via inter- polation in the latent space of generative models. – A scalable representation learning method with individual fairness certifica- tion for models using high-dimensional data via randomized smoothing. – A large-scale evaluation of our method on various image classification tasks. 2 Related Work In this work, we consider individual fairness, which requires that similar individ- uals be treated similarly [14]. In contrast, group fairness enforces specific clas- sification statistics to be equal across different groups of the population [14,28]. While both fairness notions are desirable, they also both suffer from certain shortcomings. For instance, models satisfying group fairness may still discrimi- nate against individuals [14] or subgroups [36]. In contrast, the central challenge
4 M. Peychev et al. limiting practical adoption of individual fairness is the lack of a widely accepted similarity metric [87]. While recent work has made progress in developing simi- larity metrics for tabular data [31,57,62,80,88], defining similarity concisely for high-dimensional data remains challenging and is a key contribution of our work. Fair representation learning A wide range of methods has been proposed to learn fair representations of user data. Most of these works consider group fair- ness and employ techniques such as adversarial learning [15,37,50,55], disentan- glement [11,53,69], duality [73], low-rank matrix factorization [63], and distribu- tion alignment [3,54,90]. Fair representation learning for individual fairness has recently gained attention, with similarity metrics based on logical formulas [68], Wasserstein distance [20,45], fairness graphs [46], and weighted ℓ -norms [89]. p Unfortunately, none of these approaches can capture the similarity between in- dividuals for the high-dimensional data we consider in our work. Bias in high-dimensional data A long line of work has investigated the biases of models operating on high-dimensional data, such as images [82,84] and text [5,49,64,76], showing, e.g., that black women obtain lower F1-score in commercial face classification [7,43,66]. Importantly, these models not only learn but also amplify the biases of the training data [29,91], even for balanced datasets [81]. A key challenge for bias mitigation in high-dimensional settings is that, unlike tabular data, sensitive attributes such as age or skin tone are not directly encoded as features. Thus, prior work has often relied on generative models [2,12,13,33,39,40,47,48,67,70] or computer simulations [59] to manipulate these sensitive attributes and check whether the perturbed instances are classi- fied the same. However, unlike our work, these methods only tested for bias empirically and do not provide fairness guarantees. Recent work also explored using generative models to define [27,85] or certify [61] robustness, but without focusing on fairness. Fairness certification Regulatory agencies are increasingly holding organiza- tions accountable for the discriminatory effects of their machine learning mod- els [18,19,22,23,78]. Accordingly, designing algorithms with fairness guarantees has become an active area of research [1,3,4,9,24,71]. However, unlike our work, most approaches for individual fairness certification consider pretrained models and thus cannot be employed in fair representation learning [32,79,86]. In con- trast, [68] learn individually fair representations with provable guarantees for low-dimensional tabular data, providing a basis for our approach. However, nei- ther the similarity notions nor the certification methods employed by [68] scale to high-dimensional data, which is the primary focus of our work. 3 Background This section provides the necessary background on individual fairness, fair rep- resentation learning, generative modeling, and randomized smoothing. Individual fairness The seminal work of [14] defined individual fairness as “treating similar individuals similarly”. In this work, we consider the concrete
Latent Space Smoothing for Individually Fair Representations 5 instantiation of this notion from [68]: an individual x is similar to x with respect ′ to a binary input similarity metric φ : Rn×Rn → {0, 1} if and only if φ(x, x ) = 1. ′ A model M : Rn → Y is individually fair at x ∈ Rn if it classifies all individuals similar to x (as measured by φ) the same, i.e., ∀x ′ ∈ Rn : φ (x, x ′) =⇒ M (x) = M (x ′) . (1) For example, a credit rating algorithm is individually fair for a given person if all similar applicants (e.g., similar income and repayment history) receive the same credit rating. Our goal is to learn a model M that maximizes the number of points x from the distribution for which we can guarantee that Eq. (1) is satisfied. Defining a suitable input similarity metric φ is one of the key challenges limiting practical applications of individual fairness, and in Sec. 4.1 we will show how to employ generative modeling to overcome this obstacle for high-dimensional data. Fair representation learning Fair representation learning [89] partitions the model M : Rn → Y into a data producer P : Rn → Rk, which maps input points x ∈ Rn into a representation space Rk that satisfies a given fairness notion while maintaining downstream utility, and a data consumer C : Rk → Y that solves a downstream task taking only the transformed data points r := P (x) ∈ Rk as in- puts. Importantly, the consumers (potentially indifferent to fairness) can employ standard training methods to obtain fair classifiers that are useful across a variety of different tasks. We base our approach on the LCIFR framework [68], which learns representations with individual fairness guarantees for low-dimensional tabular data. LCIFR defines a family of similarity notions and leverages (mixed- integer) linear programming methods for fairness certification. However, high- dimensional applications are out of reach for LCIFR because both the similarity notions and linear programming methods are tailored to low-dimensional tabular data. In particular, similarity is defined via logical formulas operating on the fea- tures of x, which is infeasible for, e.g., images, which cannot be compared solely at the pixel level. Moreover, while linear programming methods work well for small networks, they do not scale to real-world computer vision models. In this work, we show how to resolve these two key concerns to generalize the high-level idea of LCIFR to real-world, high-dimensional applications. Generative modeling Normalizing flows, such as Glow [41], recently emerged as a promising generative modeling approach due to their exact log-likelihood evaluation, efficient inference and synthesis, and useful latent space for down- stream tasks. Unlike GANs [25] or VAEs [42], normalizing flows are bijective models consisting of an encoder E : Rn → Rq and a decoder D : Rq → Rn for which x = D (E (x)). Glow’s input space Rn and latent space Rq have the same dimensionalities n = q. Its latent space captures important data attributes, thus enabling latent space interpolation such as changing the age of a person in an image. While attribute manipulation via latent space interpolation has also been investigated in the fairness context for GANs and VAEs [2,13,33,39,48,67], Glow’s key advantages are the existence of an encoder (unlike GANs, which can- not represent an input point in the latent space efficiently) and the bijectivity of
6 M. Peychev et al. the end-to-end model (VAEs cannot reconstruct the input point exactly). Our key idea is to leverage Glow to define image similarity by interpolating along the directions defined by certain sensitive attributes in the latent space. Smoothing Unlike (mixed-integer) linear programming [16,77], smoothing ap- proaches [10] can compute local robustness guarantees for any type of classifier C : Rk → Y, regardless of its complexity and scale. To that end, [10] construct a smoothed classifier C : Rk → Y, which returns the most probable classification of C for an input r ∈b Rk when perturbed by random noise from N (0, σ2 I). Us- rs ing a sampling-based approach, [10] establish a local robustness guarantee of the form: ∀δ ∈ Rk such that kδk < d we have C (r + δ) = C (r) with probability 2 rs 1 − α , where α can be made arbitrarily smball. Thus, C bwill classify all points rs rs in the ℓ 2-ball of radius d rs around r the same with higbh probability. Recently, [44] introduced center smoothing, which extends this approach from classifica- tion to multidimensional regression. Concretely, for a function R : Rq → Rk, cen- ter smoothing uses sampling and approximation to compute a smooth version R : Rq → Rk, which maps z ∈ Rq to the center point r := R (z) of a minimum cs ebnclosing ball containing at least half of the points r i ∼ R(bz + N (0, σ c2 sI)) for i ∈ {1, . . . , m}. Then, for ǫ > 0 and ∀z ∈ Rq such that kz − z k ≤ ǫ, we have ′ ′ 2 kR (z) − R (z ) k ≤ d with probability at least 1 − α . That is, center smooth- ′ 2 cs cs inbg compubtes a sound upper bound d cs on the ℓ 2-ball of the function outputs of R for all points in the ℓ -ball of radius ǫ around z. 2 b 4 High-Dimensional Individually Fair Representations In this section, we describe how our method defines a set of similar individuals (Sec. 4.1), learns individually fair representations for these points (Sec. 4.2), and finally, certifies individual fairness for them (Sec. 4.4). Our approach is general, but we focus on images for presentational purposes. 4.1 Similarity via a Generative Model We consider two individuals x and x to be similar if they differ only in their ′ continuous sensitive attributes. However, semantic attributes, such as skin color, cannot be captured conveniently via the input features of x. Thus, our key idea is to define similarity in the latent space of a generative model G. We compute a vector a ∈ Rq associated with the sensitive attribute, such that interpolating along the direction of a in the latent space and reconstructing back to the input space results in a meaningful semantic transformation of that attribute. There is active research investigating different ways of computing a [13,30,41,48,67], and we will empirically show that our method is compatible with any such method. Computing a We define individual similarity in the latent space of Glow [41]. Our method is independent of the actual computation of a, which we demon- strate by instantiating four different attribute vector types. Let z = E(x) be G
Latent Space Smoothing for Individually Fair Representations 7 the latent code of x in the generative latent space. First, following [41], we com- pute a by calculating the average latent vectors z for samples with the G,pos attribute and z for samples without it and set a to their difference, a = G,neg z − z . Second, following [13], we train a linear classifier sign(a z + b) G,pos G,neg ⊤ G to predict the presence of the attribute from z and take a to be the vec- G tor orthonormal to the decision boundary of the linear classifier. Finally, we employ [48] and [67] who build on these methods, accounting for the possible correlations between the sensitive and target attributes. In all cases, moving in one direction of a in the latent space increases the presence of the attribute and interpolating in the opposite direction decreases it. LASSI is independent of the sensitive attribute vector computation and will immediately benefit from all advancements in this area. We evaluate with vectors computed by [41] and [13] in the main paper (Sec. 5) and present further results with vectors from [48,67] in App. E. Individual similarity in latent space Using the genera- tive model G and the attribute vector a, we define the set of individuals similar to x in the latent space of G as S (x) := {z + t · a | |t| ≤ ǫ} ⊆ Rq (bottom of Fig. 2). Here, ǫ denotes G the maximum perturbation level applied to the attribute. We x consider G, a, and ǫ to be a part of the similarity specification set by the data regulator. Crucially, S (x) contains an infinite Encoder E number of points but is compactly represented in the latent space of G as a line segment. In contrast, the same set repre- zG + ǫapale sented directly in the input space, Sin (x) := D (S (x)) ⊆ Rn, Samples obtained by decoding the latent representations in S (x) with zG apale D, cannot be abstracted conveniently (top of Fig. 2). More- over, this approach for constructing S (x) can be extended zG − ǫapale to multiple sensitive attributes by interpolating along their Fig. 2: Similarity attribute vectors simultaneously. Referring back to the nota- in latent space. tion in Sec. 3, we formally define the input similarity metric φ to satisfy φ (x, x ) ⇐⇒ x ∈ Sin (x). ′ ′ 4.2 Learning Individually Fair Representations Assuming that the generative model G = (E, D) is pretrained and given (e.g., by the data regulator), in this section we describe the learning of the representation R : Rq → Rk, which maps from the generative latent space Rq directly to the representation space Rk. The representation R is trained separately from the data consumer, the classifier C, whose training is explained in the next section. Adversarial loss We encourage similar treatment for all points in Sin (x) by training R to map them close to each other in Rk, minimizing the loss L (x) = max kR (z ) − R (z ) k . (2) adv G ′ 2 z′ S(x) ∈ Minimizing L (x) is a min-max optimization problem, and adversarial train- adv ing [56] is known to work well in such settings. Because the underlying domain of
8 M. Peychev et al. the inner maximization problem is simply the line segment S (x), we perform a random adversarial attack in which we sample s points z ∼ U (S (x)) uniformly i at random from S (x) and approximate L (x) ≈ maxs kR (z ) − R (z ) k . adv i=1 G i 2 This efficient attack is typically more effective [17] than the first-order methods such as FGSM [26] and PGD [56] when the search space is low-dimensional. Classification loss To ensure that the learned representations remain useful for downstream tasks, we introduce an auxiliary classifier C to predict a ground aux truth target label y by adding an additional classification loss term: L (x, y) = cross_entropy C ◦ R (z ) , y . (3) cls aux G (cid:0) (cid:1) Reconstruction loss The downstream task may not always be known to the data producer a priori, and thus our representations should ideally transfer to a variety of such tasks. To that end, we optionally utilize a reconstruction loss, which is designed to preserve the signal from the original data [55,68]: L (x) = kz − Q (R (z )) k , (4) recon G G 2 where Q : Rk → Rq denotes a reconstruction network. The representation R, the auxiliary classifier C , and the reconstruction aux network Q are trained jointly using stochastic gradient descent to minimize the combined objective λ L (x, y) + λ L (x) + λ L (x) . (5) 1 cls 2 adv 3 recon Trading off fairness, F1-score, and transferability is a multi-objective optimiza- tion problem, an active area of research. Here, we follow [55,68] and use a linear scalarization scheme, with the hyperparameters λ , λ and λ balancing the 1 2 3 three losses, but our method is also compatible with other schemes [51,58,83]. 4.3 Training Classifier C Once we have learned the representation R, we can use it to train any classifier C (often different from the auxiliary one C ). As we will apply smoothing to C, aux we train it by adding isotropic Gaussian noise to its inputs during the training process, as in [10]. We use the outputs of R ◦ E (and not the smoothed version R ◦ E) as inputs to train C, since repeatedly smoothing the pipeline at this step ibs computationally expensive and because the distance between the smoothed and the unsmoothed outputs is generally small [44]. 4.4 Certifying Individual Fairness via Latent Space Smoothing With R and C trained as described above, we now construct the end-to-end model M : Rn → Y for which, given an input x, we can certify individual fairness of the form ∀x ′ ∈ Sin (x) : M (x) = M (x ′) , (6) with arbitrarily high probability.
Latent Space Smoothing for Individually Fair Representations 9 Algorithm 1 Certifying the individual fairness of C ◦ R ◦ E for the input x. function Certify(E, R, C, x) b b Let z G = E (x). Then, r cs = Rb (z G) and d cs from center smoothing [44]. if center smoothing abstained then return Abstain Smooth C [10]: obtain the certified radius d rs around r cs (i.e., same classification) if d < d then return Certified cs rs else return Not Certified Given a point z in the latent space of G, we define the zG +ǫapale function gz (t) := R (z + t · a) for t ∈ R. We apply the cen- Samples ter smoothing procedure presented by [44] to obtain gz, the apale smoothed version of gz, and define R (z) := gz (0) sucbh that zG for all z ′ ∈ S (x), kR (z)−R (z ′) k 2 ≤bd cs (see Fbig. 3). Next, we zG −ǫapale smooth the classifiebr C to obbtain its ℓ 2-robustness radius d rs. If LASSI R d < d , then the end-to-end model M = C ◦R◦E certifiably cs rs satisfies individual fairness at x (as definedb inb Eq. (6)) with high probability. Concretely, if we instantiate center smooth- rcs ing with confidence α and randomized smoothing with con- rR dcs cs fidence α , then the individual fairness certificate holds with rs probability at least 1 − α − α (union bound). The com- Fig. 3: Center cs rs positional certification procedure is summarized in Alg. 1. Its smoothing the correctness is formalized in Thm. 1 with a detailed proof in similarity set. App. A. Theorem 1. Assume that we have a bijective generative model G = (E, D) used to define the similarity set Sin (x) for a given input x. Let Alg. 1 perform center smoothing [44] with confidence 1 − α and randomized smoothing [10] cs with confidence 1 − α . If Alg. 1 returns Certified for the input x, then the rs end-to-end model M = C ◦ R ◦ E is individually fair for x with respect to Sin (x) with probability at least b1 −bα cs − α rs. 5 Experiments We now evaluate LASSI and present the key findings: (i) LASSI enforces in- dividual fairness and keeps F1-score high, (ii) LASSI handles various sensitive attributes and attribute vectors, and (iii) LASSI representations transfer to un- seen tasks. Datasets We evaluate LASSI on two datasets. CelebA [52] contains 202,599 aligned and cropped face images of real-world celebrities. The images are anno- tated with the presence or absence of 40 face attributes with various correlations between them [13]. As CelebA is highly imbalanced, we also experiment with Fair- Face [34]. It is balanced on race and contains 97,698 released images (padding
10 M. Peychev et al. 0.25) of individuals from 7 race and 9 age groups. We split the training set ran- domly (80:20 ratio) and evaluate on the validation set because the test set is not publicly shared. Further information about the datasets (including experimental “unfairness” of different attributes computed on CelebA) is in App. B. Experimental setup The following setup is used for all experiments, unless stated otherwise. We use images of size 64×64, and for each dataset pretrain a Glow model G with 4 blocks of 32 flows, using an open-source PyTorch [65] implementation [72]. We use a = z − z and set ǫ = 1 such that Sin (x) G,pos G,neg contains realistic high-quality reconstructions (confirmed by manual inspection). Thus, the similarity specification (Sec. 4.1) for enforcing individual fairness is determined by G and the radius ǫ. We implement the representation R as a fully- connected network that propagates Glow’s latent code of an input x through two hidden layers of sizes 2048 and 1024, mapping to a 512-dimensional space. The final layer applies zero mean and unit variance normalization ensuring that all components of R’s output are in the same range when Gaussian noise is added during smoothing. A linear classifier C is used for predicting the target label. Our fairness-unaware baseline (denoted as Naive) is standard representation learning of R without adversarial and reconstruction losses (λ = λ = 0). When 2 3 training LASSI, we set the classification loss weight λ = 1, except for the trans- 1 fer learning experiments. A recent work [67] proposed generating synthetic im- ages with a ProGAN [35] to balance the dataset. Their method is not concerned with individual fairness and their transformation of latent representations may change other, non-sensitive attributes. Nevertheless, we employ [67]’s high-level idea of augmenting the training set with synthetic samples from a generative model (Glow in our case). For each training sample x, we synthesize and ran- domly sample s additional images from Sin (x) in every epoch. Then, we pro- ceed with representation learning of R on the augmented dataset. We denote this baseline, addapted to the individual fairness setting, as DataAug. We do not compare with LCIFR [68] as our individual similarity specifications cannot be directly encoded as logical formulas over the input features of x and because its certification is based on expensive solvers that do not scale to Glow and large models. We list all selected hyperparameters for all experiments, based on an an ex- tensive hyperparameter search on the validation sets, in App. C (details provided for the CelebA dataset). The hyperparameter study shows that LASSI works for a wide range of hyperparameter values and demonstrates that λ controls the 2 trade-off between F1-score and fairness. We report the F1-score and the certified individual fairness of the models measured on 312 samples from CelebA’s test set (every 64-th) and 343 samples from FairFace’s test set (every 32-nd). The certified fairness refers to the percentage of test samples for which Alg. 1 returns Certified, i.e., for which we can prove that Eq. (6) holds, guaranteeing that all similar individuals (according to our similarity definition) are classified the same. This metric is denoted as “Fair” in the tables. The evaluation of a single data point takes up to 6 seconds due to the sampling required by the smoothing procedures, which is why we do not report results on the whole test sets. We ran
Latent Space Smoothing for Individually Fair Representations 11 (a) Pale_Skin (b) Young (c) Blond_Hair (d) Heavy_Makeup (e) Pale_Skin + Young (f) Pale_Sking + Young + Blond Fig. 4: Similar points from Sin (x), as reconstructed by Glow, for multiple sensi- tive attribute combinations. Central images correspond to the original input. We vary t uniformly (left to right) in the [− ǫ , ǫ ] range, n = number of sensitive √n √n attributes, ǫ = 1. For n > 1, all attribute vectors are multiplied by the same t. the experiments on GeForce RTX 2080 Ti GPUs and release all the code and models to reproduce our results at https://github.com/eth-sri/lassi. Single sensitive attribute We experiment with 4 different continuous sensi- tive attributes from CelebA: Pale_Skin, Young, Blond_Hair and Heavy_Makeup on two tasks: predicting Smiling and Earrings. We chose attributes with dif- ferent balance ratios that have been used in prior work [13], while avoiding attributes that perpetuate harmful stereotypes [13] (e.g., avoiding Male). Glow can also be used to generate discrete attributes, but then fairness certification can be done via enumeration because partial eyeglasses or hats, for example, are not plausible. Fig. 4 provides example images from Sin (x) for a single x. The Earrings task is considerably more imbalanced than Smiling, with 78.21% majority class F1-score on our test subset. Because of the high correlation be- tween Earrings and Makeup, we run LASSI with increased λ for this pair of 2 attributes. We show the results in Tab. 1 averaged over 5 runs with different random seeds. The results indicate that data augmentation helps, but is not enough. LASSI significantly improves the certified fairness, compared to the baselines, with a minor loss of F1-score on Smiling and even acts as a helpful regularizer on the imbalanced Earrings task. In App. D we report the standard deviations demonstrating that LASSI consistently enforces individual fairness with low vari- ance and further evaluate empirical (i.e., non-certifiable) fairness metrics. Multiple sensitive attributes In the next experiment, we combine the sen- sitive attributes Pale_Skin, Young and Blond_Hair and predict Smiling. The similarity sets w.r.t. which we certify individual fairness are defined as S (x) = {E (x) + t · a | ktk ≤ ǫ}. The results in Tab. 1 (rows 5 – 6) show that the Pi i i 2
12 M. Peychev et al. Table 1: Evaluation of LASSI on the CelebA dataset, showing that LASSI signif- icantly increases certified individual fairness compared to the baselines without affecting the classification F1-score, even increasing it for imbalanced tasks. Re- ported means averaged over 5 runs, see App. D for standard deviations. Naive DataAug LASSI (ours) Task Sensitive attribute(s) Acc Fair Acc Fair Acc Fair Pale_Skin 86.3 0.6 85.7 12.2 85.9 98.0 Young 86.3 38.2 85.9 43.0 86.3 98.8 Blond_Hair 86.3 3.4 86.6 9.4 86.4 94.7 Smiling Heavy_Makeup 86.3 0.4 85.3 13.7 85.6 91.3 Pale+Young 86.0 0.4 85.8 9.9 85.8 97.3 Pale+Young+Blond 86.2 0.0 86.4 3.6 85.5 86.5 Pale_Skin 81.3 24.3 81.0 40.4 85.0 98.5 Young 81.4 59.2 79.9 72.0 84.5 98.0 Earrings Blond_Hair 81.4 9.2 82.2 30.5 84.8 96.2 Heavy_Makeup 81.6 20.5 80.3 49.2 82.3 98.7 certified fairness drops as the similarity sets become more complex, as expected, but LASSI still successfully enforces individual fairness in these cases. Larger images and different attribute vectors Next, we explore if LASSI can also work with larger images. We increase the dimensionality of the CelebA images to 128×128, pretrain Glow with 5 blocks and keep the rest of the hyper- parameters the same. The results are consistent with those already presented in Tab. 1: LASSI increases the certified individual fairness by up to 77% on the Smiling task (see App. D for detailed results). We also instantiate LASSI with the alternative attribute vector type [13] introduced in Sec. 4.1 (with ǫ = 10). Although interpolating along the vector which is perpendicular to the linear deci- sion boundary of the sensitive attribute possibly reduces the correlations leaked into the similarity sets, Tab. 2 shows that LASSI still improves the certified fairness by up to 16% compared to the baselines. This improvement is 9.7% and 6.1% for the attribute vectors proposed by [67] and [48] respectively, further demonstrating that LASSI can be useful for various attribute vector types. More details about these experiments are provided in App. E. Transfer learning To demonstrate the modularity of our approach, we show that LASSI can learn fair and transferable representations which are useful for unseen downstream tasks. To that end, we turn off the classification loss, con- sistent with prior work [55] (λ = 0, i.e., the representation R is trained un- 1 supervised), and enable the reconstruction loss (λ = 0.1). The reconstruction 3 network Q has an architecture symmetric to that of R. In Tab. 3 we report the accuracies and the certified fairness on 7 different, relatively well-balanced, down- stream tasks. The models perform slightly worse compared to the case where the
Latent Space Smoothing for Individually Fair Representations 13 Table 2: Evaluation with a perpendicular to the linear decision boundary of the sensitive attribute [13] (Sec. 4.1) on the Smiling task, showing that LASSI is not limited to a specific attribute vector type. Naive DataAug LASSI (ours) Sensitive attribute(s) Acc Fair Acc Fair Acc Fair Pale_Skin 86.4 34.0 85.9 90.3 86.5 98.8 Young 86.3 73.1 86.2 90.3 86.8 97.9 Blond_Hair 86.2 71.4 86.1 88.8 86.7 98.8 Heavy_Makeup 86.2 11.5 86.3 87.4 86.8 98.8 Pale+Young 86.2 28.6 85.8 84.7 86.5 98.6 Pale+Young+Blond 86.2 23.7 85.9 82.2 86.4 98.7 Table 3: Transfer learning results, demonstrating that LASSI can still achieve high certified individual fairness even when the downstream tasks are not known. Sens. attrib.: Pale (P) Young (Y) Blond (B) P + Y P + Y + B Transfer task Acc Fair Acc Fair Acc Fair Acc Fair Acc Fair Smiling 86.2 93.1 86.0 95.4 85.1 93.8 85.9 92.2 85.1 87.0 High_Cheeks 81.7 92.6 82.3 96.0 81.3 92.2 80.8 93.0 80.6 84.5 Mouth_Open 81.5 91.2 82.4 94.3 82.4 87.5 81.6 90.1 82.5 80.8 Lipstick 88.3 94.0 85.8 95.8 86.8 91.2 85.1 90.6 86.2 81.0 Heavy_Makeup 86.5 93.0 83.5 95.3 85.6 89.3 83.7 90.0 83.3 80.4 Wavy_Hair 79.2 93.3 77.5 95.8 78.0 91.3 77.6 91.5 78.8 85.3 Eyebrows 78.3 92.1 78.3 94.7 78.9 89.6 77.8 92.2 78.7 85.6 downstream task is known in advance, but the obtained certified individual fair- ness is still consistently high – more than 80% for the most complex similarity specification (P+Y+B) and above 90% for the simpler ones. Standard deviations and baseline accuracies on these tasks are reported in App. D. Training on FairFace dataset To verify that LASSI works well in different settings, we also evaluate on the FairFace [34] dataset. We select Race=Black as a sensitive attribute and predict Age. This is a very challenging multi-class task with around 60% state of the art F1-score. Therefore, we create two easier tasks: Age-2, predicting if an individual is younger or older than 30, and Age-3 with three target ranges: [0 − 19], [20 − 39], and 40+. Tab. 4 reports the results for ǫ = 0.5. We verify that transfer learning also works in this setup by training on Age-2 and then transferring the representations to all three tasks. As the tasks are related, increasing the classification loss weight λ on the base task from 0 to 1
14 M. Peychev et al. Table 4: Results on FairFace, showing that LASSI can significantly improve the certified individual fairness even on balanced datasets. The adversarial loss weight is λ = 0.1 for all models except Naive, the transfer models are trained 2 on Age-2 with reconstruction loss weight λ = 0.1. LASSI is trained on the 3 corresponding tasks with adversarial but without reconstruction loss (λ = 1, 1 λ = 0). 3 Naive DataAug Transfer λ1=0 Transfer λ1=0.01 LASSI Task Acc Fair Acc Fair Acc Fair Acc Fair Acc Fair Age-2 69.0 5.7 68.9 4.8 66.4 91.7 74.9 91.7 72.0 95.0 Age-3 67.0 0.0 67.1 0.6 63.0 85.6 67.7 88.0 65.1 90.8 Age (all) 42.2 0.0 39.9 0.0 34.3 72.0 37.1 77.5 41.5 65.9 0.01, increases both the transfer downstream F1-score and the certified fairness. The highest certified fairness is generally obtained when the downstream task is known and the model is trained on it (LASSI, λ = 1). 1 6 Limitations and Future Work We now discuss some of the limitations of LASSI. First, our method trains indi- vidually fair models, but it does not guarantee that models satisfy other fairness notions, e.g., group fairness. While individual fairness is a well-studied research area, recent work argues that it does not qualify as a valid fairness notion as it can be insufficient to guarantee fairness in certain instances and risks encod- ing implicit human biases [21]. Moreover, the validity of our fairness certificates depends heavily on the generative model used by LASSI. In particular, the sim- ilarity sets S (x) considered in our work may not be exhaustive enough as there can be latent points outside S (x) that correspond to input points that would be perceived as similar to x by a human observer. This can also happen if the generative model is not powerful enough to generate all possible instances and combinations of similar individuals. For the above reasons, it is hard to obtain formal guarantees about G and the computed certificates may not al- ways transfer from G to the real world. We explore this issue further in App. F where we experiment with 3D Shapes [8], a procedurally generated dataset with known ground truth similarity sets. Future work can consider addressing these challanges by performing extensive manual human inspection of reconstructions produced by G (similar to App. G). Moreover, all future advancements in the active research area of normalizing flows will immediately improve the quality of our certificates.
Latent Space Smoothing for Individually Fair Representations 15 7 Conclusion We proposed LASSI, which defines image similarity with respect to a generative model via attribute manipulation, allowing us to capture complex image trans- formations such as changing the age or skin color, which are otherwise difficult to characterize. Further, we were able to scale certified representation learning for individual fairness to real-world high-dimensional datasets by using randomized smoothing-based techniques. Our extensive evaluation yields promising results on several datasets and illustrates the practicality of our approach. Acknowledgments We thank Seyedmorteza Sadat for his help with prelimi- nary investigations and the anonymous reviewers for their insightful feedback. References 1. Albarghouthi, A., D’Antoni, L., Drews, S., Nori, A.V.: Fairsquare: probabilistic verification of program fairness. Proc. ACM Program. Lang. (2017) 2. Balakrishnan, G., Xiong, Y., Xia, W., Perona, P.: Towards causal benchmarking of bias in face analysis algorithms. In: Computer Vision - ECCV 2020 - 16th European Conference (2020) 3. Balunovic, M., Ruoss, A., Vechev, M.T.: Fair normalizing flows. CoRR (2021) 4. Bastani, O., Zhang, X., Solar-Lezama, A.: Probabilistic verification of fairness prop- erties via concentration. Proc. ACM Program. Lang. (2019) 5. Bolukbasi, T., Chang, K., Zou, J.Y., Saligrama, V., Kalai, A.T.: Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In: Advances in Neural Information Processing Systems 29 (2016) 6. Brennan, T., Dieterich, W., Ehret, B.: Evaluating the predictive validity of the compas risk and needs assessment system. Criminal Justice and Behavior (2009) 7. Buolamwini, J., Gebru, T.: Gender shades: Intersectional F1-score disparities in commercial gender classification. In: Conference on Fairness, Accountability and Transparency (2018) 8. Burgess, C., Kim, H.: 3d shapes dataset. https://github.com/deepmind/3dshapes- dataset/ (2018) 9. Choi, Y., Dang, M., den Broeck, G.V.: Group fairness by probabilistic modeling with latent fair decisions. In: Thirty-Fifth AAAI Conference on Artificial Intelli- gence (2021) 10. Cohen, J.M., Rosenfeld, E., Kolter, J.Z.: Certified adversarial robustness via ran- domized smoothing. In: Proceedings of the 36th International Conference on Ma- chine Learning (2019) 11. Creager, E., Madras, D., Jacobsen, J., Weis, M.A., Swersky, K., Pitassi, T., Zemel, R.S.: Flexibly fair representation learning by disentanglement. In: Proceedings of the 36th International Conference on Machine Learning (2019) 12. Dash, S., Sharma, A.: Counterfactual generation and fairness evaluation using adversarially learned inference. CoRR (2020) 13. Denton, E., Hutchinson, B., Mitchell, M., Gebru, T.: Detecting bias with generative counterfactual face attribute augmentation. CoRR (2019) 14. Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R.S.: Fairness through awareness. In: Innovations in Theoretical Computer Science (2012)
16 M. Peychev et al. 15. Edwards, H., Storkey, A.J.: Censoring representations with an adversary. In: 4th International Conference on Learning Representations (2016) 16. Ehlers, R.: Formal verification of piece-wise linear feed-forward neural networks. In: Automated Technology for Verification and Analysis - 15th International Sym- posium (2017) 17. Engstrom, L., Tran, B., Tsipras, D., Schmidt, L., Madry, A.: Exploring the land- scape of spatial robustness. In: Proceedings of the 36th International Conference on Machine Learning (2019) 18. EU: Ethics guidelines for trustworthy ai (2019) 19. EU: Proposal for a regulation of the european parliament and of the council lay- ing down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts (2021) 20. Feng, R., Yang, Y., Lyu, Y., Tan, C., Sun, Y., Wang, C.: Learning fair representa- tions via an adversarial framework. CoRR (2019) 21. Fleisher, W.: What’s fair about individual fairness? In: AAAI/ACM Conference on AI, Ethics, and Society, Virtual Event (2021) 22. FTC: Using artificial intelligence and algorithms (2020) 23. FTC: Aiming for truth, fairness, and equity in your company’s use of ai (2021) 24. Gitiaux, X., Rangwala, H.: Learning smooth and fair representations. In: The 24th International Conference on Artificial Intelligence and Statistics (2021) 25. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C., Bengio, Y.: Generative adversarial nets. In: Advances in Neural Information Processing Systems 27 (2014) 26. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples. In: 3rd International Conference on Learning Representations (2015) 27. Gowal, S., Qin, C., Huang, P., Cemgil, A.T., Dvijotham, K., Mann, T.A., Kohli, P.: Achieving robustness in the wild via adversarial mixing with disentangled represen- tations. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020) 28. Hardt, M., Price, E., Srebro, N.: Equality of opportunity in supervised learning. In: Advances in Neural Information Processing Systems 29 (2016) 29. Hendricks, L.A., Burns, K., Saenko, K., Darrell, T., Rohrbach, A.: Women also snowboard: Overcoming bias in captioning models. In: Computer Vision - ECCV 2018 - 15th European Conference (2018) 30. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., Lerchner, A.: beta-vae: Learning basic visual concepts with a constrained vari- ational framework. In: 5th International Conference on Learning Representations (2017) 31. Ilvento, C.: Metric learning for individual fairness. In: 1st Symposium on Founda- tions of Responsible Computing (2020) 32. John, P.G., Vijaykeerthy, D., Saha, D.: Verifying individual fairness in machine learning models. In: Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence (2020) 33. Joo, J., Kärkkäinen, K.: Gender slopes: Counterfactual fairness for computer vision models by attribute manipulation. CoRR (2020) 34. Kärkkäinen, K., Joo, J.: Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In: IEEE Winter Conference on Applications of Computer Vision (2021) 35. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for improved quality, stability, and variation. In: 6th International Confer- ence on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
Latent Space Smoothing for Individually Fair Representations 17 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net (2018), https://openreview.net/forum?id=Hk99zCeAb 36. Kearns, M.J., Neel, S., Roth, A., Wu, Z.S.: Preventing fairness gerrymandering: Au- diting and learning for subgroup fairness. In: Proceedings of the 35th International Conference on Machine Learning (2018) 37. Kehrenberg, T., Bartlett, M., Thomas, O., Quadrianto, N.: Null-sampling for in- terpretable and fair representations. In: Computer Vision - ECCV 2020 - 16th European Conference (2020) 38. Khandani, A.E., Kim, A.J., Lo, A.W.: Consumer credit-risk models via machine- learning algorithms. Journal of Banking & Finance (2010) 39. Kim, B., Wattenberg, M., Gilmer, J., Cai, C.J., Wexler, J., Viégas, F.B., Sayres, R.: Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In: Proceedings of the 35th International Conference on Machine Learning (2018) 40. Kim, H., Shin, S., Jang, J., Song, K., Joo, W., Kang, W., Moon, I.: Counterfactual fairness with disentangled causal effect variational autoencoder. In: Thirty-Fifth AAAI Conference on Artificial Intelligence (2021) 41. Kingma, D.P., Dhariwal, P.: Glow: Generative flow with invertible 1x1 convolutions. In: Advances in Neural Information Processing Systems 31 (2018) 42. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. In: 2nd International Conference on Learning Representations (2014) 43. Klare, B., Burge, M.J., Klontz, J.C., Bruegge, R.W.V., Jain, A.K.: Face recognition performance: Role of demographic information. IEEE Trans. Inf. Forensics Secur. (2012) 44. Kumar, A., Goldstein, T.: Center smoothing: Certified robustness for networks with structured outputs. Advances in Neural Information Processing Systems 34 (2021) 45. Lahoti, P., Gummadi, K.P., Weikum, G.: ifair: Learning individually fair data repre- sentations for algorithmic decision making. In: 35th IEEE International Conference on Data Engineering (2019) 46. Lahoti, P., Gummadi, K.P., Weikum, G.: Operationalizing individual fairness with pairwise fair representations. Proc. VLDB Endow. (2019) 47. Lang, O., Gandelsman, Y., Yarom, M., Wald, Y., Elidan, G., Hassidim, A., Freeman, W.T., Isola, P., Globerson, A., Irani, M., Mosseri, I.: Explaining in style: Training a gan to explain a classifier in stylespace. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 693–702 (October 2021) 48. Li, Z., Xu, C.: Discover the unknown biased attribute of an image classifier. In: Pro- ceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 14970–14979 (October 2021) 49. Liang, P.P., Wu, C., Morency, L., Salakhutdinov, R.: Towards understanding and mitigating social biases in language models. In: Proceedings of the 38th Interna- tional Conference on Machine Learning (2021) 50. Liao, J., Huang, C., Kairouz, P., Sankar, L.: Learning generative adversarial repre- sentations (GAP) under fairness and censoring constraints. CoRR (2019) 51. Lin, X., Zhen, H., Li, Z., Zhang, Q., Kwong, S.: Pareto multi-task learning. In: Advances in Neural Information Processing Systems 32 (2019) 52. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In: IEEE International Conference on Computer Vision (2015)
18 M. Peychev et al. 53. Locatello, F., Abbati, G., Rainforth, T., Bauer, S., Schölkopf, B., Bachem, O.: On the fairness of disentangled representations. In: Advances in Neural Information Processing Systems 32 (2019) 54. Louizos, C., Swersky, K., Li, Y., Welling, M., Zemel, R.S.: The variational fair autoencoder. In: 4th International Conference on Learning Representations (2016) 55. Madras, D., Creager, E., Pitassi, T., Zemel, R.S.: Learning adversarially fair and transferable representations. In: Proceedings of the 35th International Conference on Machine Learning (2018) 56. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learn- ing models resistant to adversarial attacks. In: 6th International Conference on Learning Representations (2018) 57. Maity, S., Xue, S., Yurochkin, M., Sun, Y.: Statistical inference for individual fairness. In: 9th International Conference on Learning Representations (2021) 58. Martínez, N., Bertrán, M., Sapiro, G.: Minimax pareto fairness: A multi objec- tive perspective. In: Proceedings of the 37th International Conference on Machine Learning (2020) 59. McDuff, D.J., Cheng, R., Kapoor, A.: Identifying bias in AI using simulation. CoRR (2018) 60. McNamara, D., Ong, C.S., Williamson, R.C.: Costs and benefits of fair representa- tion learning. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (2019) 61. Mirman, M., Hägele, A., Bielik, P., Gehr, T., Vechev, M.T.: Robustness certifica- tion with generative models. In: 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation (2021) 62. Mukherjee, D., Yurochkin, M., Banerjee, M., Sun, Y.: Two simple ways to learn individual fairness metrics from data. In: Proceedings of the 37th International Conference on Machine Learning (2020) 63. Oneto, L., Donini, M., Pontil, M., Maurer, A.: Learning fair and transferable repre- sentations with theoretical guarantees. In: 7th IEEE International Conference on Data Science and Advanced Analytics (2020) 64. Park, J.H., Shin, J., Fung, P.: Reducing gender bias in abusive language detection. In: Proceedings of the 2018 Conference on Empirical Methods in computer vision (2018) 65. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E.Z., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning library. In: Wallach, H.M., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E.B., Garnett, R. (eds.) Advances in Neural Information Processing Systems 32 (2019) 66. Raji, I.D., Buolamwini, J.: Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial AI products. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (2019) 67. Ramaswamy, V.V., Kim, S.S.Y., Russakovsky, O.: Fair attribute classification through latent space de-biasing. In: IEEE Conference on Computer Vision and Pattern Recognition (2021) 68. Ruoss, A., Balunovic, M., Fischer, M., Vechev, M.T.: Learning certified individually fair representations. In: Advances in Neural Information Processing Systems 33 (2020) 69. Sarhan, M.H., Navab, N., Eslami, A., Albarqouni, S.: Fairness by learning orthog- onal disentangled representations. In: Computer Vision - ECCV 2020 - 16th Euro- pean Conference (2020)
Latent Space Smoothing for Individually Fair Representations 19 70. Sattigeri, P., Hoffman, S.C., Chenthamarakshan, V., Varshney, K.R.: Fairness GAN: generating datasets with fairness properties using a generative adversarial network. IBM J. Res. Dev. (2019) 71. Segal, S., Adi, Y., Pinkas, B., Baum, C., Ganesh, C., Keshet, J.: Fairness in the eyes of the data: Certifying machine-learning models. In: AAAI/ACM Conference on AI, Ethics, and Society (2021) 72. Seonghyeon, K.: Glow pytorch (commit: 97081ff1). https://github.com/rosinality/glow-pytorch (2020) 73. Song, J., Kalluri, P., Grover, A., Zhao, S., Ermon, S.: Learning controllable fair representations. In: The 22nd International Conference on Artificial Intelligence and Statistics (2019) 74. Stark, L.: Facial recognition is the plutonium of ai. XRDS 25(3), 50–55 (apr 2019). https://doi.org/10.1145/3313129, https://doi.org/10.1145/3313129 75. Sun, X., Wu, P., Hoi, S.C.H.: Face detection using deep learning: An improved faster RCNN approach. Neurocomputing (2018) 76. Tatman, R.: Gender and dialect bias in youtube’s automatic captions. In: Pro- ceedings of the First ACL Workshop on Ethics in computer vision (2017) 77. Tjeng, V., Xiao, K.Y., Tedrake, R.: Evaluating robustness of neural networks with mixed integer programming. In: 7th International Conference on Learning Repre- sentations (2019) 78. UN: The right to privacy in the digital age (2021) 79. Urban, C., Christakis, M., Wüstholz, V., Zhang, F.: Perfectly parallel fairness certification of neural networks. Proc. ACM Program. Lang. (2020) 80. Wang, H., Grgic-Hlaca, N., Lahoti, P., Gummadi, K.P., Weller, A.: An empirical study on learning fairness metrics for COMPAS data with human supervision. CoRR (2019) 81. Wang, T., Zhao, J., Yatskar, M., Chang, K., Ordonez, V.: Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations. In: IEEE/CVF International Conference on Computer Vision (2019) 82. Wang, Z., Qinami, K., Karakozis, I.C., Genova, K., Nair, P., Hata, K., Russakovsky, O.: Towards fairness in visual recognition: Effective strategies for bias mitigation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020) 83. Wei, S., Niethammer, M.: The fairness-F1-score pareto front. CoRR (2020) 84. Wilson, B., Hoffman, J., Morgenstern, J.: Predictive inequity in object detection. CoRR (2019) 85. Wong, E., Kolter, J.Z.: Learning perturbation sets for robust machine learning. In: 9th International Conference on Learning Representations (2021) 86. Yeom, S., Fredrikson, M.: Individual fairness revisited: Transferring techniques from adversarial robustness. In: Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (2020) 87. Yurochkin, M., Bower, A., Sun, Y.: Training individually fair ML models with sensitive subspace robustness. In: 8th International Conference on Learning Rep- resentations (2020) 88. Yurochkin, M., Sun, Y.: Sensei: Sensitive set invariance for enforcing individual fairness. In: 9th International Conference on Learning Representations (2021) 89. Zemel, R.S., Wu, Y., Swersky, K., Pitassi, T., Dwork, C.: Learning fair representa- tions. In: Proceedings of the 30th International Conference on Machine Learning (2013) 90. Zhao, H., Coston, A., Adel, T., Gordon, G.J.: Conditional learning of fair repre- sentations. In: 8th International Conference on Learning Representations (2020)
20 M. Peychev et al. 91. Zhao, J., Wang, T., Yatskar, M., Ordonez, V., Chang, K.: Men also like shopping: Reducing gender bias amplification using corpus-level constraints. In: Proceedings of the 2017 Conference on Empirical Methods in computer vision (2017)
Latent Space Smoothing for Individually Fair Representations 21 Ethics Statement This work proposed a novel method for certifying the individual fairness of mod- els operating on high-dimensional data. Progress on this challenging problem could enable fairness auditing for high-risk computer vision applications, such as facial recognition. Recent work [74] argues that facial recognition algorithms can have undesirable, socially toxic, and divisive consequences. For instance, it was demonstrated that they may perpetuate and reinforce racial and gender bias [7,13]. Therefore, they must be applied carefully, considering the social dy- namics and context in which they occur. Accordingly, following prior work [13], we refrained from using unstable social constructs, such as gender, or normatively judgemental attributes, such as “attractive” or “chubby”, in our research. One way to limit the potential harms of facial analysis technologies is to con- trol and regulate their usage. Our work aims to help fill this gap by presenting a methodology for enforcing individual fairness via certification. As highlighted in our paper, we acknowledge that the quality of the generative models is a signifi- cant bottleneck of our certificates. For example, they may encode various biases present in the data. Another possible source of bias is the human perception and social constructs which can potentially impact the validity of our similar- ity specifications. Nevertheless, we believe that we can still leverage generative models and their latent space to construct more meaningful individual fairness specifications on high-dimensional data than those allowed by prior work. More broadly, developing rigorous, standardized processes for auditing and certifying facial recognition models (including human inspection, e.g., by considering the reconstructed images in App. G) should complement the contributions presented in our work. Finally, future quality advancements in generative modelling and normalizing flows can directly translate into stronger guarantees of our method, enabling certified fair application of models using rich, high-dimensional data. A Proof of Thm. 1 This section provides a formal proof of the following: Theorem 1. Assume that we have a bijective generative model G = (E, D) used to define the similarity set Sin (x) for a given input x. Let Alg. 1 perform center smoothing [44] with confidence 1 − α and randomized smoothing [10] cs with confidence 1 − α . If Alg. 1 returns Certified for the input x, then the rs end-to-end model M = C ◦ R ◦ E is individually fair for x with respect to Sin (x) with probability at least b1 −bα cs − α rs. To prove Thm. 1, we will make use of the following randomized and center smoothing theorems proved in the literature: Theorem 2 (Adapted from [10]). Let C : Rk → Y be a classifier and let ε ∼ N (0, σ r2 sI). Let C be defined such that C (r) = arg max c P ε(C(r + ε) = c). b b ∈Y
22 M. Peychev et al. Suppose c ∈ Y and p , p ∈ [0, 1] satisfy: A A B P ε(C(r + ε) = c A) ≥ p A ≥ p B ≥ max P ε(C(r + ε) = c B). (7) cB=cA 6 Then C(r + δ) = c for all δ satisfying kδk < d , where d := σrs (Φ 1(p ) − A 2 rs rs 2 − A Φ 1(pb )). − B Here, Y denotes the set of class labels, Φ is the cumulative distribution function (CDF) of the standard normal distribution N (0, 1), and Φ 1 is its inverse. − Theorem 3 (Adapted from [44]). Let g : Ra → Rk and gˆ: Ra → Rk is an approximation of the smoothed version of g, which maps t ∈ Ra to the center point gˆ (t) of a minimum enclosing ball containing at least half of the points r ∼ g(t + N (0, σ2 I)), i ∈ {1, . . . , m}. Then, for ǫ > 0, with probability at least i cs 1 − α we have, cs ∀t ′ s.t. kt − t ′k 2 ≤ ǫ, kgˆ(t) − gˆ(t ′)k 2 ≤ d cs. (8) We now proceed to proving Thm. 1: Proof. Assume that Alg. 1 returns Certified for the input x. We need to show that with probability at least 1 − α − α cs rs ∀x ∈ Sin (x) : M (x) = M (x ) , (Eq. 6) ′ ′ where M = C ◦ R ◦ E. By the definition of Sin (x) and E being the inverse of D, we have for abll xb ∈ Sin (x), z = E(x ) ∈ S (x), hence it suffices to prove ′ ′ ′ ∀z ′ ∈ S (x) : C ◦ R (z G) = C ◦ R (z ′) , (9) b b b b where z = E (x). G Next, recall the definition of gz (t) := R (z + t · a) and note that for z ′ = z + t · a, the center smoothing of ′ gz′ (t): samples from gz′ (cid:0)t + N (0, σ c2 s) (cid:1) = R (cid:0)z ′ + (cid:0)t + N (0, σ c2 s) (cid:1) · a (cid:1) ; gz (tc+ t ′): samples from gz (cid:0)t + t ′ + N (0, σ c2 s) (cid:1) = R (cid:0)z + (cid:0)t + t ′ + N (0, σ c2 s) (cid:1) · a (cid:1) . b Since z ′ = z + t ′ · a, the sampling distributions are the same, hence gz′ (t) = gz (t + t ′), and in particular R (z ′) = gz′ (0) = gz (t ′). c b Now, let us get back to Ebq. (9). B cy definit bion of S (x), for all z ′ ∈ S (x), z ′ = z G + t ′ · a for some t ′ ∈ [−ǫ, ǫ]. Moreover, r cs = R (z G) = gzG (0) and R (z ′) = gzG (t ′). Thm. 3 tells us that with probability at bleast 1 − αdcs b d ∀t ′ ∈ [−ǫ, ǫ] . kgzG (0) − gzG (t ′) k 2 ≤ d cs (10) ⇐⇒ ∀z ∈ S (x) . krd− R (zd) k ≤ d , ′ cs ′ 2 cs b
Latent Space Smoothing for Individually Fair Representations 23 provided that the center smoothing computation of r does not abstain. cs Finally, we consider the last component of the pipeline – the smoothed clas- sifier C. Provided that C does not abstain at the input r , Thm. 2 provides us cs with abradius d rs aroundb r cs such that with probability at least 1 − α rs ∀δ s.t. kδk < d , C (r ) = C (r + δ) 2 rs cs cs (11) ⇐⇒ ∀r s.t. kr − r k <b d , C (rb ) = C (r ) . ′ cs ′ 2 rs cs ′ b b If Alg. 1 returns Certified, that is d < d , combining Eq. (10) and (11) and cs rs applying the union bound, we obtain that with probability at least 1 − α − α cs rs we have C (r ) = C R (z ) for all z ∈ S (x). That is, cs (cid:16) ′ (cid:17) ′ b b b ∀z ′ ∈ S (x) : C ◦ R (z G) = C ◦ R (z ′) , (12) b b b b as required by Eq. (9). The same proof technique can also be extended to the multiple attribute vectors case. ⊓⊔ B Datasets and Dataset Statistics In this section we provide further information and statistics about the datasets used in this work. CelebA1 [52] is restricted to non-commercial research and education purposes and its authors [52] do not own the copyrights. FairFace [34] is licensed under CC BY 4.0. Tab. 5 contains statistics about the sensitive at- tributes and their corresponding attribute vectors. The lengths of the CelebA attribute vectors are computed for 64×64 images. In Tab. 6 we report the base accuracies of two standard classifiers trained on the Smiling and Earrings CelebA tasks. The first classifier is a ResNet- 18 network trained directly on the original images. The other one is a fully connected network operating on their Glow latent representations, z = E (x). G We remark that none of these classifiers involves representation learning. We report the means and standard deviations, averaged over 5 runs with different random seeds, on the validation and test sets, where the test set is the same subset on which we report the results in the main paper. The base accuracies on the downstream tasks used for the transfer learning experiments are reported in App. D. In order to estimate the relative “unfairness” associated with each sensitive attribute, in Tab. 7 we compute the empirical individual fairness of the two classifiers. For each data point x, we sample 9 points from Sin (x) evenly (15 points for Pale+Young+Blond). If all samples are classified the same, we add the original data point x to the empirical fairness counter. Note that this procedure cannot certify that all points from Sin (x) are classified the same. Therefore, these results come with no provable guarantees and serve as upper bounds of the certified individual fairness of the classifiers. 1 https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html
24 M. Peychev et al. Table 5: Sensitive attribute statistics. The positive and negative sample ratio is reported for the training set, as the attribute vectors are computed on it. Dataset Sensitive attribute Pos (%) Neg (%) kz G,pos − z G,negk2 Pale_Skin 4.3 95.7 11.5 Young 77.9 22.1 7.8 CelebA Blond_Hair 14.9 85.1 15.8 Heavy_Makeup 38.4 61.6 11.9 FairFace Race=Black 14.1 85.9 10.9 Table 6: Baseline accuracies for the Smiling and Earrings CelebA tasks. The ResNet-18 classifier takes the original images as an input, while the z classifier G is a fully connected network classifying their Glow latent representations. Neither of these classifiers involves representation learning. Majority class Acc (ResNet-18) Acc (z G) Task Valid Test Valid Test Valid Test Smiling 51.7 52.6 92.1 ± 0.2 90.9 ± 0.7 89.4 ± 0.1 87.2 ± 1.1 Earrings 80.9 78.2 86.2 ± 0.8 88.2 ± 1.1 84.7 ± 0.1 85.2 ± 0.9 Table 7: Empirical individual fairness of the base classifiers evaluated via sam- pling. These results come with no provable guarantees and serve as upper bounds of the certified individual fairness. Emp. Fair (ResNet-18) Emp. Fair (z ) G Task Sensitive attribute(s) Valid Test Valid Test Pale_Skin 74.1 ± 1.0 75.2 ± 1.1 75.8 ± 0.6 79.9 ± 1.2 Young 87.7 ± 0.5 90.1 ± 0.7 95.2 ± 0.6 96.8 ± 0.9 Blond_Hair 89.1 ± 1.1 89.4 ± 1.5 81.9 ± 1.6 84.6 ± 2.9 Smiling Heavy_Makeup 82.3 ± 1.0 82.5 ± 1.2 74.9 ± 1.6 78.2 ± 2.3 Pale+Young 71.5 ± 0.9 72.6 ± 1.1 75.8 ± 0.6 79.9 ± 1.2 Pale+Young+Blond 70.3 ± 0.5 70.6 ± 0.9 72.5 ± 0.6 76.9 ± 1.1 Pale_Skin 92.8 ± 0.9 90.4 ± 1.2 91.5 ± 1.0 91.5 ± 1.6 Young 90.6 ± 1.8 87.7 ± 1.5 93.0 ± 1.2 94.7 ± 1.0 Earrings Blond_Hair 89.7 ± 2.2 86.9 ± 2.5 88.3 ± 1.9 89.7 ± 2.2 Heavy_Makeup 85.8 ± 3.4 82.2 ± 3.3 74.4 ± 4.4 73.3 ± 3.8
Latent Space Smoothing for Individually Fair Representations 25 C data augmentation In this section, we perform an extensive hyperparameter search in order to select suitable values for the hyperparameters. We evaluate on 311 samples from the validation set of CelebA (again, every 64-th), on the Smiling task with sensitive attributes Pale_Skin and Young. Afterwards, we reuse the same hyperparameter values for all tasks with very minor changes (which we verify by running the experiments on the validation set first). The tunable hyperparameters, as well as the range of values that we consider about them, are as follows: – Adversarial loss weight: λ ∈ {0, 0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25} 2 – Gaussian noise added during center smoothing of R: σ ∈ {0.5, 0.55, 0.6, 0.65, 0.7, 0.75} cs – Gaussian noise added during randomized smoothing of C: σ ∈ {0.1, 0.25, 0.5, 1, 2.5, 5, 10, 25} rs Tuning σ and the baselines We begin with selecting the value for σ . cs cs It is not used during the training of R and C, but is an integral part of the center smoothing computation which is performed during inference and is the most time-consuming component of the model pipeline. More concretely, both r = R (z ) and d depend on σ , in turn affecting both the F1-score and cs G cs cs the certbified individual fairness. We evaluate the Naive model with all candidate values for σ and show the results in Tab. 8. We observe very little variation cs in F1-score, while the best certified individual fairness and the smallest average center smoothing radii are obtained at σ = 0.6 and 0.65. While there is no cs significant difference in performace between these two configurations, we expect that the slightly larger value for σ would generally produce smaller center cs smoothing radii, leading to increased certified fairness. Therefore, we set σ = cs 0.65 for all experiments (except for FairFace, where we use ǫ = 0.5 and scale σ correspondingly, i.e., σ = 0.325). Using the same σ values for both the cs cs cs baselines and LASSI allows us to attribute the improvements of the results to the additional training mechanisms that we apply and not merely to different hyperparameter values. We perform a similar evaluation on the validation set of the other baseline, DataAug, and from the results in Tabs. 8 and 9 we set σ = 10 for both Naive rs and DataAug. Although σ = 5 seems to work slightly better for Young, we rs remark that Young is also the fairest of all considered sensitive attributes, so we choose a more conservative value that would be suitable for all of them. Tuning λ Next, we incorporate the adversarial loss weight λ to the training 2 2 and explore its impact on the model in Tab. 10. The certified individual fair- ness increases with increasing λ , until λ = 0.05, and stays at the same level 2 2 afterwards. Interestingly, the F1-score is mostly unaffected. We set λ = 0.05 2 and σ = 2.5 for LASSI, as they give most of the fairness boost obtained from rs adversarial training, while keeping the F1-score high. Notably, the hyperparame- ter tuning demonstrates that LASSI successfully enforces and certifies individual
26 M. Peychev et al. fairness for a wide range of hyperparameter values and is not highly sensitive to them. Table 8: Results of Naive on the validation subset of CelebA for different values of σ and σ . The third column contains the mean center smoothing radii cs rs corresponding to the different σ values. Smaller is generally better for certified cs individual fairness (see the condition in Alg. 1). σ rs Sens. attribute σ cs Mean d cs Metric 0.1 0.25 0.5 1 2.5 5 10 25 Acc 87.8 87.8 87.5 88.4 89.1 88.7 88.7 84.6 0.5 42.25 Fair 0 0 0 0 0 0 0 0 Acc 87.8 87.8 87.8 88.4 88.7 88.7 88.4 84.6 0.55 34.19 Fair 0 0 0 0 0 0 0 0 Acc 87.8 87.5 87.8 88.4 88.7 88.7 88.7 84.6 0.6 33.34 Fair 0 0 0 0 0 0 1.0 0 Pale_Skin Acc 87.5 87.5 87.8 88.4 88.8 88.7 88.4 84.6 0.65 33.37 Fair 0 0 0 0 0 0 1.0 0 Acc 87.5 87.5 87.5 88.4 88.4 88.7 88.1 84.6 0.7 33.72 Fair 0 0 0 0 0 0 1.0 0 Acc 87.8 88.1 88.1 88.4 88.7 89.1 88.1 84.6 0.75 34.18 Fair 0 0 0 0 0 0 1.0 0 Acc 88.1 88.1 87.8 87.8 88.7 88.7 88.1 85.2 0.6 8.16 Fair 0 0 0 5.1 36.3 58.8 58.5 39.9 Young Acc 88.1 88.1 87.8 87.8 88.7 88.7 88.1 84.9 0.65 8.16 Fair 0 0 0 4.8 36.3 58.8 58.2 39.5 Table 9: Results of the DataAug baseline on the validation set of CelebA for σ = 0.65 and different values of σ . cs rs σ rs Sens. attribute σ cs Mean d cs Metric 0.1 0.25 0.5 1 2.5 5 10 25 Acc 87.5 87.5 87.8 87.8 88.7 89.4 88.7 84.9 Pale_Skin 0.65 14.52 Fair 0 0 0 0 0 28.3 31.5 10.0 Acc 87.5 87.8 87.8 89.1 88.7 88.7 88.7 84.9 Young 0.65 7.09 Fair 0 0 0 1.6 46.6 65.6 65.0 48.9
Latent Space Smoothing for Individually Fair Representations 27 Table 10: Results of LASSI on the validation subset of CelebA for different values of λ and σ , while keeping σ = 0.65. The certified individual fairness increases 2 rs cs with increasing λ , until the λ = 0.05 level. 2 2 σ rs Sens. attribute λ2 Metric 0.1 0.25 0.5 1 2.5 5 10 25 Acc 86.5 86.8 87.1 87.5 89.1 89.4 88.1 84.9 0.001 Fair 0 0 0 0 0 13.2 12.9 1.9 Acc 87.8 88.1 88.4 88.7 90.4 89.1 86.5 83.3 0.0025 Fair 0 0 0 0 15.4 27.3 24.8 5.5 Acc 87.8 87.8 88.1 87.8 89.7 89.1 87.5 82.0 0.005 Fair 0 0 0 0 35.0 40.5 37.0 15.4 Acc 88.1 88.1 87.8 88.1 89.4 90.0 87.5 82.0 0.01 Fair 0 0 0 9.3 46.0 49.5 47.6 27.7 Pale_Skin Acc 88.4 88.1 88.1 88.4 89.1 89.7 87.5 82.3 0.025 Fair 0 1.9 9.6 49.2 64.3 66.2 64.0 47.9 Acc 87.8 87.8 88.1 88.1 89.7 89.4 86.8 83.0 0.05 Fair 45.0 97.1 97.7 98.1 96.1 96.1 95.5 93.6 Acc 86.5 86.5 86.5 86.8 86.5 85.9 83.3 76.8 0.1 Fair 57.9 93.6 93.9 94.5 96.8 96.1 94.9 88.1 Acc 87.1 87.1 87.1 87.5 87.5 85.9 79.4 67.8 0.25 Fair 96.8 96.1 96.1 96.1 98.1 97.4 93.2 79.7 Acc 89.1 89.1 88.1 89.4 89.4 89.1 88.7 84.6 0.05 Fair 97.1 97.7 98.4 99.0 99.0 98.7 96.5 96.1 Acc 88.1 88.7 89.4 89.4 88.7 88.7 87.8 82.6 Young 0.1 Fair 58.5 94.9 94.9 96.8 97.1 95.8 96.1 92.3 Acc 88.4 88.4 88.4 88.7 88.4 88.1 86.8 77.8 0.25 Fair 98.4 98.1 98.4 99.4 99.4 98.7 95.2 89.4 Selected experiment hyperparameters Here, we summarize the hyperpa- rameter values selected for the final experiments. We use ǫ = 1 for all similarity set definitions except the experiments with: (i) the alternative attribute vectors from [13,48], where ǫ = 10, and (ii) FairFace, where ǫ = 0.5. We maintain the ǫ/σ ratio, which impacts center smoothing, setting σ = 0.65 by default (as cs cs stated in the sections above) and using σ = 6.5 and 0.325 when ǫ = 10 and 0.5 cs respectively. Our smoothing arguments are consistent with prior work [10,44]:
28 M. Peychev et al. – Randomized smoothing [10]: α = 0.001, N = 100,000, N = 2000. rs rs 0,rs – Center smoothing [44]: α = 0.01, N = 10,000, N = 10,000. cs cs 0,cs The rest of the model hyperparameters are listed in Tab. 11. In the CelebA 64×64 and 128×128 setups, we run LASSI with λ = 0.25 for the (target=Earrings, 2 sensitive=Makeup) pair of attributes because of the high correlation between them. We train the representation R for 20 epochs in the transfer experiments (CelebA, FairFace) and 5 epochs otherwise. The linear classifier C is trained for 1 epoch. We generally set a lower value to σ when the task is more difficult and rs the downstream classifier is therefore less confident. Overall, we remark that the hyperparameter values are similar and within the same range for all models and experiments, meaning that our approach does not require substantial fine-tuning. Table 11: Hyperparameters used for the different model and experiment setups. Model / Dataset Experiment(s) Hyperparameters 64×64 and λ1 = 1; λ2 = 0 (Naive, DataAug) and 0.05 (LASSI); λ3 = 0; CelebA 128×128 σrs = 10 (Naive, DataAug) and 2.5 (LASSI); s = 10 (DataAug, LASSI). Transfer λ1 = 0; λ2 = 0.05; λ3 = 0.1; σrs = 0.5; s = 10. Naive λ1 = 1; λ2 = λ3 = 0; σrs = 5 (Age-2) and 0.1 (Age-3, Age (all)). FairFace LASSI λ1 = 1; λ2 = 0.1; λ3 = 0; σrs = 0.25; s = 10. Transfer λ1 ∈ {0, 0.001, 0.01}; λ2 = λ3 = 0.1; σrs = 0.1; s = 10. 3D Shapes Naive λ1 = 1; λ2 = λ3 = 0; σrs = 5. (App. F) LASSI λ1 = 1; λ2 = 0.1; λ3 = 0; σrs = 1; s = 100. D More Experimental Results on CelebA This section provides further details about the experiments on the CelebA dataset with the standard attribute vector from [41], a = z − z (Sec. 4.1). G,pos G,neg 64×64 images Tab. 12 contains the means and the standard deviations of the accuracies and the certified individual fairness of the CelebA 64×64 experiments summarized in Tab. 1, averaged over 5 runs. The standard deviation of Naive and DataAug’s fairness is high, while LASSI consistently enforces certified individual fairness with low variance. Moreover, in Tab. 13 we check for what fraction of the test subset the models classify the similarity set endpoints the same as the original data point. Note that this is again another empirical estimate, serving as an upper bound of the certified individual fairness of the models. Nevertheless, LASSI outperforms the baselines on that metric as well. More importantly, out of all 150 combinations
Latent Space Smoothing for Individually Fair Representations 29 Table 12: Means and standard deviations of the accuracies and the certified individual fairness reported in Tab. 1, averaged over 5 runs with different random seeds on the Smiling (rows 1-6) and Earrings (rows 7-10) tasks. Naive DataAug LASSI (ours) Sens. attribs.: Acc Fair Acc Fair Acc Fair Pale_Skin 86.3 ± 1.5 0.6 ± 0.5 85.7 ± 1.2 12.2 ± 14.7 85.9 ± 1.3 98.0 ± 0.5 Young 86.3 ± 1.8 38.2 ± 23.4 85.9 ± 1.6 43.0 ± 30.7 86.3 ± 1.3 98.8 ± 0.6 Blond_Hair 86.3 ± 1.6 3.4 ± 3.1 86.6 ± 1.0 9.4 ± 10.0 86.4 ± 1.0 94.7 ± 1.5 Heavy_Makeup 86.3 ± 1.1 0.4 ± 0.4 85.3 ± 1.7 13.7 ± 8.8 85.6 ± 1.6 91.3 ± 8.1 P+Y 86.0 ± 1.5 0.4 ± 0.4 85.8 ± 1.4 9.9 ± 12.7 85.8 ± 0.9 97.3 ± 0.9 P+Y+B 86.2 ± 1.7 0.0 ± 0.0 86.4 ± 1.0 3.6 ± 3.8 85.5 ± 0.4 86.5 ± 2.7 Pale_Skin 81.3 ± 2.2 24.3 ± 35.6 81.0 ± 2.3 40.4 ± 32.6 85.0 ± 0.5 98.5 ± 0.9 Young 81.4 ± 2.2 59.2 ± 18.0 79.9 ± 1.4 72.0 ± 24.1 84.5 ± 1.0 98.0 ± 1.1 Blond_Hair 81.4 ± 2.2 9.2 ± 17.5 82.2 ± 2.8 30.5 ± 40.9 84.8 ± 0.5 96.2 ± 2.6 Heavy_Makeup 81.6 ± 1.9 20.5 ± 13.0 80.3 ± 1.9 49.2 ± 37.0 82.3 ± 0.6 98.7 ± 0.7 Table 13: Empirical evaluation of the individual fairness of the models computed by comparing their predictions on the original test samples to the model predic- tions on the endpoints of the corresponding similarity sets. Task Sensitive attribute(s) Naive DataAug LASSI (ours) Pale_Skin 78.4 ± 2.1 90.1 ± 1.9 99.6 ± 0.2 Young 95.3 ± 0.4 96.7 ± 0.5 99.6 ± 0.2 Blond_Hair 83.3 ± 0.7 93.9 ± 1.5 99.2 ± 0.4 Smiling Heavy_Makeup 75.8 ± 2.4 88.3 ± 0.8 97.9 ± 1.6 Pale+Young 78.0 ± 2.0 89.0 ± 2.2 99.4 ± 0.5 Pale+Young+Blond 77.9 ± 2.1 87.4 ± 0.9 96.9 ± 0.7 Pale_Skin 97.1 ± 1.6 99.1 ± 0.7 99.5 ± 0.4 Young 98.5 ± 1.4 99.5 ± 0.5 99.2 ± 0.4 Earrings Blond_Hair 96.7 ± 3.4 98.5 ± 0.4 99.1 ± 0.7 Heavy_Makeup 92.2 ± 6.6 98.1 ± 1.1 99.7 ± 0.3 of models, tasks and sensitive attributes (3 model types, 10 task-attribute pairs, 5 random seeds), in 8 combinations there is only 1 test sample which we certify as individually fair but the endpoints classifications mismatch. In all other com- binations, no such situation occurs, serving as another test for the correctness of our certificates. One test sample out of 312 is 0.32%, which is within our confidence of 1 − α − α = 98.9%. cs rs
30 M. Peychev et al. 128×128 images Keeping all hyperparameters the same, we evaluate LASSI on images of size 128×128. The results in Tab. 14 indicate that LASSI increases the certified individual fairness in this setting as well, while also slightly improving the classification F1-score. We attribute this to the richer and larger latent space of Glow, which is potentially more easily separable in this case. Table 14: Evaluation of LASSI on 128×128-dimensional images, demonstrating that it significantly increases the certified individual fairness for larger images as well. Evaluated tasks: Smiling (rows 1-6) and Earrings (rows 7-10). Naive DataAug LASSI (ours) Sens. attribs.: Acc Fair Acc Fair Acc Fair Pale_Skin 88.8 ± 1.0 0.0 ± 0.0 89.6 ± 0.5 0.0 ± 0.0 90.0 ± 1.1 70.6 ± 14.2 Young 88.7 ± 0.7 46.0 ± 16.2 88.8 ± 1.0 47.6 ± 20.2 89.7 ± 0.7 97.2 ± 1.6 Blond_Hair 88.8 ± 0.9 0.1 ± 0.1 89.4 ± 1.1 0.0 ± 0.0 90.1 ± 0.8 77.8 ± 10.2 Heavy_Makeup 89.0 ± 0.9 2.5 ± 3.5 89.6 ± 1.1 30.4 ± 20.7 90.2 ± 0.3 87.6 ± 3.9 P+Y 88.8 ± 1.0 0.0 ± 0.0 89.4 ± 1.3 8.7 ± 16.5 90.2 ± 0.5 69.4 ± 9.7 P+Y+B 88.7 ± 0.8 0.0 ± 0.0 89.9 ± 1.5 4.4 ± 9.6 90.2 ± 0.7 72.7 ± 5.0 Pale_Skin 80.1 ± 1.4 0.0 ± 0.0 80.1 ± 2.5 0.1 ± 0.1 84.4 ± 0.9 90.4 ± 2.5 Young 80.2 ± 1.4 73.5 ± 20.4 80.3 ± 1.5 78.2 ± 18.1 85.5 ± 1.4 96.4 ± 1.7 Blond_Hair 80.2 ± 1.4 0.0 ± 0.0 80.6 ± 2.0 0.0 ± 0.0 83.9 ± 0.9 89.7 ± 4.0 Heavy_Makeup 80.3 ± 1.4 42.1 ± 15.9 80.1 ± 1.9 65.1 ± 31.1 81.7 ± 1.3 98.3 ± 1.3 Transfer learning Tab. 15 contains the base standard accuracies on the trans- fer tasks. Tab. 16 reports the means and the standard deviations of LASSI on the Smiling task when solved in a transfer learning setting.
Latent Space Smoothing for Individually Fair Representations 31 Table 15: Baseline accuracies on the transfer CelebA tasks. As before, the ResNet- 18 classifier takes as an input the original images, while the z classifier is a fully G connected network classifying their Glow latent representations. Neither of these classifiers involves representation learning. Majority class Acc (ResNet-18) Acc (z G) Task Valid Test Valid Test Valid Test Smiling 51.7 52.6 92.1 ± 0.2 90.9 ± 0.7 89.4 ± 0.1 87.2 ± 1.1 High_Cheeks 55.1 51.9 87.2 ± 0.2 86.8 ± 0.4 84.3 ± 0.1 83.8 ± 0.7 Mouth_Open 51.8 53.8 92.7 ± 0.3 92.9 ± 0.7 88.1 ± 0.2 89.6 ± 1.1 Lipstick 55.4 54.8 91.5 ± 0.2 90.5 ± 0.8 89.2 ± 0.1 90.6 ± 1.1 Heavy_Makeup 61.0 58.7 90.2 ± 0.4 89.9 ± 0.4 87.8 ± 0.1 88.6 ± 1.1 Wavy_Hair 72.3 65.1 82.7 ± 1.8 76.3 ± 3.3 80.9 ± 0.5 81.7 ± 0.4 Eyebrows 74.2 71.8 83.5 ± 0.5 81.1 ± 0.6 80.1 ± 0.1 79.4 ± 1.6 Table 16: Mean and standard deviation of the accuracies and the certified indi- vidual fairness of LASSI on Smiling in a transfer learning setting (Tab. 3). Task Sensitive attribute(s) Acc Fair Pale_Skin 86.2 ± 1.1 93.1 ± 2.4 Young 86.0 ± 1.2 95.4 ± 1.0 Smiling Blond_Hair 85.1 ± 1.6 93.8 ± 1.8 Pale+Young 85.9 ± 0.3 92.2 ± 0.7 Pale+Young+Blond 85.1 ± 0.7 87.0 ± 2.3 E Different Attribute Vector Types In this section, we demonstrate that LASSI is independent of the actual compu- tation of the attribute vector a and that it can improve the individual fairness for various attribute vector types. Denton et al. [13] First, in Tab. 17 we report the means and the standard deviations of the accuracies and the certified individual fairness from Tab. 2. The attribute vector a used here is orthogonal to the decision boundary of the linear classifier sign(a z + b) [13] (Sec. 4.1), with its length set to ǫ = 10. ⊤ G Ramaswamy et al. [67] Next, we adapt the attribute vector computation proposed by [67] by computing sample-specific vectors a = z − z for every i G,i G′ ,i x from the training set, where z = E(x ) and z is as defined in [67, Eq. i G,i i G′ ,i (3)]. All sample-specific a ’s share the same direction, so we can average them i to obtain the global attribute vector a = 1 N a and set ǫ = 1. N Pi=1 i
32 M. Peychev et al. Table 17: Means and standard deviations of the accuracies and the certified individual fairness reported in Tab. 2, averaged over 5 runs with different random seeds on the Smiling task. Naive DataAug LASSI (ours) Sens. attribs.: Acc Fair Acc Fair Acc Fair Pale_Skin 86.4 ± 1.7 34.0 ± 5.4 85.9 ± 1.5 90.3 ± 3.9 86.5 ± 1.3 98.8 ± 1.2 Young 86.3 ± 1.8 73.1 ± 3.5 86.2 ± 1.5 90.3 ± 3.3 86.8 ± 1.0 97.9 ± 1.2 Blond_Hair 86.2 ± 1.8 71.4 ± 4.0 86.1 ± 1.8 88.8 ± 2.7 86.7 ± 1.4 98.8 ± 0.7 Heavy_Makeup 86.2 ± 1.6 11.5 ± 2.5 86.3 ± 1.1 87.4 ± 1.6 86.8 ± 1.0 98.8 ± 0.9 P+Y 86.2 ± 1.8 28.6 ± 3.4 85.8 ± 1.5 84.7 ± 4.1 86.5 ± 1.2 98.6 ± 1.8 P+Y+B 86.2 ± 1.7 23.7 ± 2.1 85.9 ± 1.8 82.2 ± 5.2 86.4 ± 1.1 98.7 ± 0.5 Li and Xu [48] Finally, [48] discover biased attributes of pre-trained classifiers. To that end, we train a ResNet-18 on the Smiling task. Then, we run [48]’s optimization procedure to iteratively find 3 biased attribute vectors (each or- thogonal to the target and to the other attribute vectors) for that model using Glow as the generative model. We use ǫ = 10 for these vectors. Tab. 18 shows that LASSI significantly improves the certified individual fairness while maintaining the same high F1-score level for [67] and [48], as with the attribute vectors from [13,41], when evaluated on the Smiling task. Table 18: Evaluation of LASSI on CelebA using sensitive attribute vectors from [48,67]. We denote [48]’s vectors as a , a , and a since they are not 0 1 2 necessarily associated with a sensitive attribute (unlike [13,41,67]). As for the vectors from [13,41] (Tabs. 1 and 2), LASSI significantly increases certified fair- ness without affecting the F1-score. Naive DataAug LASSI (ours) a Sens. attribs.: Acc Fair Acc Fair Acc Fair Pale_Skin 86.3 ± 1.8 89.0 ± 3.9 86.0 ± 1.5 92.4 ± 2.6 86.8 ± 1.2 98.6 ± 1.0 Young 86.3 ± 1.8 95.1 ± 1.5 86.2 ± 1.6 95.6 ± 1.8 86.9 ± 1.2 99.5 ± 0.5 Blond_Hair 86.2 ± 1.8 90.8 ± 3.5 86.2 ± 1.6 89.7 ± 3.0 86.8 ± 1.1 98.8 ± 0.3 [67] Heavy_Makeup 86.3 ± 1.8 92.8 ± 1.4 86.0 ± 1.6 94.4 ± 1.4 86.7 ± 1.2 99.4 ± 0.3 P+Y 86.3 ± 1.8 88.0 ± 3.9 86.2 ± 1.9 91.5 ± 4.1 86.7 ± 1.1 98.8 ± 0.9 P+Y+B 86.3 ± 1.8 85.6 ± 4.3 86.5 ± 1.5 88.7 ± 5.4 86.7 ± 1.3 98.4 ± 0.9 a 0 86.2 ± 1.8 92.3 ± 2.1 86.3 ± 1.6 94.8 ± 3.7 86.9 ± 1.4 99.3 ± 0.9 [48] a 0+a 1 86.3 ± 1.8 90.7 ± 2.7 86.4 ± 1.5 93.4 ± 1.2 86.9 ± 1.1 98.3 ± 1.3 a 0+a 1+a 2 86.3 ± 1.8 90.1 ± 2.8 86.3 ± 1.7 92.4 ± 1.6 86.8 ± 1.0 98.5 ± 0.6
Latent Space Smoothing for Individually Fair Representations 33 F Certification with Ground Truth Data An essential part of the evaluation is demonstrating that the fairness certificates obtained using the generative model can transfer to ground truth data. However, CelebA does not contain images of the same individual with different attributes, e.g., the same individual with different skin colors. Thus, we experiment with the 3D Shapes dataset (Apache-2.0 license) [8], which provides images of 3D shapes that are procedurally generated from 6 independent latent factors: floor hue, wall hue, object hue, scale, shape, and orientation. Therefore, we can obtain ground truth images of the same object with varying latent factors. The 3D Shapes dataset is typically used to investigate disentanglement properties of unsupervised learning methods, e.g., in the context of fairness [53]. The goal is to show that the similarity set computed by Glow captures a given latent factor (as in Fig. 12) and that certification with respect to this set will result in certification of the ground truth. To that end, we experiment with orientation as the continuous sensitive attribute. It has v = 15 possible values, the most among all latent factors, providing for the most rigorous evaluation. The target attribute is set to object hue, which has 10 different classes. We filter the original training set to create a biased one, correlating orienta- tion and object hue. We only keep those samples in the training set for which: (i) hue ≤ 5 and orient ≤ 7, or (ii) hue ≥ 6 and orient ≥ 9. We extend the attribute vector computation from Sec. 4.1 [41] (performed on the original, unfiltered training set) to non-binary attributes, defining a = z −z , where ij G,i G,j 1 ≤ i, j ≤ v are sensitive attribute values. Based on the construction of the biased training set, we let the similarity set S (x) to be defined by all attribute vectors {a } for which i < 8 < j (7 · 7 = 49 vectors) and set ǫ = 1. We train Naive ij (λ = 1; λ = λ = 0; σ = 5) and LASSI (λ = 1; λ = 0.1; λ = 0; σ = 1; 1 2 3 rs 1 2 3 rs s = 100) models and report results on 300 samples from the test set. When running LASSI on 3D Shapes, we sample more points (s = 100) compared to the other datasets in order to accommodate for the more complex similarity set, defined by many more attribute vectors. In the evaluation, apart from reporting the F1-score and the certified fairness (CertFair) on the (unbiased) test subset, for each sample we also obtain the v similar ground truth data points, i.e., the same shape at v different orientations, while fixing all other factors. The empirical unfairness (EmpUnfair) in this case is the percentage of test samples for which the downstream classifier does not classify all v ground truth individually similar images the same. Moreover, if any of the v similar data points is certified, we check whether all v similar ground truth data points obtain the same classification, indicating ground truth fairness. Tab. 19 shows that LASSI substantially increases the F1-score and the cer- tified individual fairness (w.r.t. the similarity set computed using Glow), while being nearly 100% empirically fair on the ground truth images. That is, in 0.3% of the test samples there were different classification outcomes among their v similar (ground-truth) samples. Crucially, in all of these cases, our method did not certify individual fairness for any of the v similar data points, showing that the certificates transfer to the ground truth.
34 M. Peychev et al. Table 19: Evaluation on 3D Shapes for the task object hue. The certification rate (CertFair) and the percentage of ground truth empirically unfair data points (EmpFair) sum up below 100%. Method: Naive LASSI (ours) Sens. attrib. Acc CertFair EmpUnfair (↓) Acc CertFair EmpUnfair (↓) orientation 32.0 0 69.3 100 81.3 0.3 G More Examples of Similar Individuals Here, we provide further samples from the similarity sets Sin (x) (defined with a = z − z ), as reconstructed by Glow, for various inputs x randomly G,pos G,neg drawn from our evaluation subsets. A summary of all configurations is listed in Tab. 20. The images in the middle of the CelebA and FairFace reconstructions correspond to the original inputs. The perturbations range uniformly between [− ǫ , ǫ ], where n is the number of sensitive attributes. For n > 1, all attribute √n √n vectors are multiplied by the same t before adding them to the latent represen- tation of the original inputs. ǫ = 1 for CelebA and 3D Shapes and ǫ = 0.5 for FairFace. Table 20: Example image reconstructions from the similarity sets in this work. Dataset Sensitive attribute(s) Figure Pale_Skin Fig. 5 Young Fig. 6 CelebA Blond_Hair Fig. 7 Heavy_Makeup Fig. 8 Pale + Young Fig. 9 Pale + Young + Blond Fig. 10 FairFace Race=Black Fig. 11 3D Shapes orientation Fig. 12
Latent Space Smoothing for Individually Fair Representations 35 Fig. 5: Similar individuals from Sin (x), for x in the CelebA dataset, obtained by varying the sensitive attribute Pale_Skin.
36 M. Peychev et al. Fig. 6: Similar individuals from Sin (x), for x in the CelebA dataset, obtained by varying the sensitive attribute Young.
Latent Space Smoothing for Individually Fair Representations 37 Fig. 7: Similar individuals from Sin (x), for x in the CelebA dataset, obtained by varying the sensitive attribute Blond_Hair.
38 M. Peychev et al. Fig. 8: Similar individuals from Sin (x), for x in the CelebA dataset, obtained by varying the sensitive attribute Heavy_Makeup.
Latent Space Smoothing for Individually Fair Representations 39 Fig. 9: Similar individuals from Sin (x) obtained by simultaneously varying the sensitive attributes Pale_Skin + Young.
40 M. Peychev et al. Fig. 10: Similar individuals from Sin (x) obtained by simultaneously varying the sensitive attributes Pale_Skin + Young + Blond.
Latent Space Smoothing for Individually Fair Representations 41 Fig. 11: Similar individuals from Sin (x), for x in FairFace and ǫ = 0.5, obtained by varying the sensitive attribute Race=Black.
42 M. Peychev et al. Fig. 12: Sampled shapes at 15 different ground truth orientations. The original (above) and the corresponding reconstructions (below) obtained from interpolat- ing along one of the attribute vectors, a (see App. F for details), grouped 1,15 together.
