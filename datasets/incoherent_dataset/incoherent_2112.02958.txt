Automap: Towards Ergonomic Automated Parallelism for ML Models Michael Schaarschmidt ∗ Dominik Grewe Dimitrios Vytiniotis Tamara Norman James Molloy Jonathan Godwin Norman A. Rink Vinod Nair Dan Belov DeepMind Adam Paszke Georg Stefan Schmid Google Research EPFL Abstract The rapid rise in demand for training large neural network architectures has brought into focus the need for partitioning strategies, for example by using data, model, or pipeline parallelism. Implementing these methods is increasingly supported through program primitives, but identifying efficient partitioning strategies requires expensive experimentation and expertise. We present the prototype of an automated partitioner that seamlessly integrates into existing compilers and existing user workflows. Our partitioner enables SPMD-style parallelism that encompasses data parallelism and parameter/activation sharding. Through a combination of inductive tactics and search in a platform-independent partitioning IR, automap can recover expert partitioning strategies such as Megatron sharding for transformer layers. 1 Introduction Driven by recent progress in the design and evaluation of large language models [7, 19, 17, 9], training techniques for large deep neural networks have become critically important for progress in machine learning systems. These networks are trained by combining multiple parallelism strategies and executing them across many accelerator devices. Identifying an effective combination of ap- proaches such as data, model [29] or pipeline parallelism [13, 22, 23, 34] depends on the specific model architecture, accelerator characteristics, and distributed device topology. Selecting from a growing array of techniques such as micro-batching, rematerialisation, or parameter offloading [27] is further complicated by expensive experimentation, with large models requiring up to thousands of accelerators. Beyond training larger models, organisations managing diverse accelerator fleets can improve hardware utilisation by partitioning models to fit onto older accelerators with less memory. Tensor programming frameworks like TensorFlow [2], PyTorch [26] and JAX [6] increasingly provide end-user primitives or add-in libraries to help define the parallelisation strategy. JAX exposes several function transformations to control parallelism such as pmap (typically, but not exclusively, used for batch parallelism), pjit or xmap for fine-grained control of model parallelism and interfacing tools such as the XLA SPMD partitioner [19, 33]. While these are powerful tools which enable experts to compose advanced parallelism strategies, they still require specialised skill sets to achieve the desired hardware utilisation. For instance, users must come up with expert sharding specifications for parameters and possibly intermediate values of a model to productively use the pjit API, which in turn drives the XLA SPMD partitioning infrastructure. ∗Correspondence: mschaarschmidt@deepmind.com. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. 1202 ceD 6 ]GL.sc[ 1v85920.2112:viXra
JAX user code with Automap Automap rewrite engine TPUs/GPUs # Training step … def update_fn(params, …, inputs) High Level Partitioning Partitioned Lowered to # … Data parallel over 2 hosts, model JAX Op (e Hr Lat Oio )ns IR pr (Hog Lr Oa )m XLA hat ra drg we at r e … tracing to # m p ea shra =lle [l ( bo av te cr h 4 , 2d )e , v (mice os d. el, 4)] XLA pL oe lia cr yn ae nd d Compiler … automap(update_fn, search mesh, ...) … Figure 1: End-to-end workflow overview. Motivation and challenges. Though automated partitioning and distribution of ML workloads has been explored before in the research community [20, 36, 21, 14, 15, 32, 21], we present here the unique challenges motivating our work: • We need integration into existing backend compilers/runtimes widely used in production and already targeting different accelerator platforms, most notably XLA [1]. We explicitly want to avoid having to re-implement kernels for a specific architecture to be partition-aware [14]. • We need integration into existing user workflows, in our case arbitrary computations specified in JAX without any user rewriting. Our partitioner may have to deal with XLA programs consisting of hundreds of thousands of operators, with hundreds or thousands of parameters. We cannot use a predefined library of partitioning-aware layers (e.g. Keras layers, as in [14]) because we wish to allow researchers (our prevalent group of users) to freely innovate with arbitrary tensor computations and experimental layers. • We aim for a fast solution that allows an effective research development cycle, i.e. a solution comparable to the overhead to schedule an experiment, perhaps minutes but not hours. Key ideas we offer for discussion. In this paper, we present our design prototype and preliminary results of a data-driven automatic partitioner, automap. The challenging setting we face has implica- tions (a) on the design of our partitioning infrastructure to make it compatible with existing compilers and user workflows, and (b) on the design of our automation. To address problem (a) we offer a novel IR (“dialect” in MLIR terms [18]) that is layered on top of another tensor dialect (XLA HLO in our prototype, but this could be adapted to any dialect) and allows us to express distribution decisions of computations from the base dialect. The distribution decisions are expressed as rewrite rules controlled by an agent, providing maximal flexibility to incorporate new rewrites (Section 2.1). Problem (b) is particularly challenging: The most important implication of our setup is the need to manage large, unstructured programs. Contrary to coarse-grained layer-based approaches, it would be impractical to explore decisions2 for all operations in a program via search. On the other hand, ML models can be trained to make parallel decisions for all operations [36] at this scale, but such models typically rely on fine-tuning for new types of programs, and this makes the approach harder to integrate with a researcher’s workflow. Moreover, evaluating the “goodness” of a partitioning solution, e.g., the reduction in peak working memory, requires at least a static analysis (e.g. a liveness analysis) over the result of lowering and optimising a large (50-100k ops) program to an accelerator-local program. This reinforces the need for solutions that get to good performance within few trials and rewriting steps. Hence, we started our investigations by exploring (i) hybrid search-learning techniques, and (ii) incremental partitioning decisions and the use of compiler rewriting passes that propagate these decisions across the program where possible. A key idea we use is to imitate the behaviour of expert users partitioning their models (e.g. users of GSPMD [33]) to design inductive biases that reduce the number of decisions taken by an agent. Early results show how this approach can recover expert partitioning such as the Megatron transformer layer sharding [29]. We identify important challenges for scaling search up to many layers and improving its robustness to the details of the model architecture. Further related work. There is further related work, not discussed in detail in this paper – for instance DistIR [28] is also an MLIR-based representation for distributed tensor programs focusing on MPMD (for arguments for/against MPMD see [33]). ROC [15] is an extension of [14] specifically for GraphNets using a learned cost model. Tofu [32] is a hierarchical graph partitioner based on 2A decision may be to determine whether a value (e.g. an argument or an intermediate) should be replicated or sharded, and if the latter, upon which dimension and along which devices (mesh axis). 2
dynamic programming. Other plausible techniques for partitioning such as constraint-based encoding of cost models, often combined with equality saturation [35], or BRKGA [25]-based options are all insufficient – at least out of the box – for our purposes due to the difficulties in obtaining cost models. Another promising avenue is to design progressively accurate sequences of cost models that do not all require a fully materialised program, as in TVM [10]. 2 System design Our rewrite engine is implemented in MLIR [18] with an XLA backend, and a Python API in JAX. Users interact with our system by designing normal JAX models and then pass their main update function to our partitioner. The JAX functions can already include user-managed parallelism (e.g. batch parallelism), and our system will further partition them along additional mesh axes. JAX functions are converted to XLA computations, and then lowered to our rewriting dialect PartIR [31] in MLIR (overview in Figure 1). Rewrites and inductive tactics are exposed to the partitioner via a compiler environment (§2.2). Rewrite sequences executed by the partitioner are lowered to an SPMD variant of our partitioning IR, and evaluated through compiler-internal cost models (estimating peak memory, runtime, and communication). PartIR is platform-independent but we have implemented an XLA backend to seamlessly support CPU, TPU [16], and GPU worfklows. func @main(%arg0: tensor <8 x16xf32 >, %arg1: tensor <16 x64xf32 >, %arg2: tensor <64 xf32 >) -> tensor <8 x64xf32 > { %0 = mhlo.dot %arg0, %arg1 : tensor <8 x64xf32 > %1 = mhlo.broadcast_in_dim %arg2 { broadcast_dims = 1} : tensor <8 x64xf32 > %2 = mhlo.add %0, %1 : tensor <8 x64xf32 > return %2 : tensor <8 x64xf32 > } func @main(%arg0: tensor <8 x16xf32 >, %arg1: tensor <16 x64xf32 >, %arg2: tensor <64 xf32 >) -> tensor <8 x64xf32 > attributes { mesh_shape = #partir.mesh <"shard"=2>} { %0 = partir.tile 1 "shard" ( %rshard : !partir.range <2>) { %4 = partir.slice 1 %arg1[ %rshard ] : tensor <16 x32xf32 > partir.yield %4 : tensor <16 x32xf32 > } %1 = mhlo.dot %arg0, %0 : tensor <8 x64xf32 > %2 = mhlo.broadcast_in_dim %arg2 { broadcast_dims = 1} : tensor <8 x64xf32 > %3 = mhlo.add %1, %2 : tensor <8 x64xf32 > return %3 : tensor <8 x64xf32 > } func @main(%arg0: tensor <8 x16xf32 >, %arg1: tensor <16 x64xf32 >, %arg2: tensor <64 xf32 >) -> tensor <8 x64xf32 > attributes { mesh_shape = #partir.mesh <"shard"=2>} { %0 = partir.atomic "shard" { partir.yield %arg0 : tensor <8 x16xf32 > } %1 = partir.tile 1 "shard" ( %rshard : !partir.range <2>) { %2 = partir.slice 1 %arg1[ %rshard ] : tensor <16 x32xf32 > %3 = mhlo.dot %0, %2 : tensor <8 x32xf32 > %4 = partir.slice 0 %arg2[ %rshard ] : tensor <32 xf32 > %5 = mhlo.broadcast_in_dim %4 { broadcast_dims = 1} : tensor <8 x32xf32 > %6 = mhlo.add %3, %5 : tensor <8 x32xf32 > partir.yield %6 : tensor <8 x32xf32 > } return %1 : tensor <8 x64xf32 > } Figure 2: Top: A small MLIR MHLO program representing a single linear layer in slightly simplified notation. Middle: the same program where %arg1 has been expressed as a tiling loop on dimension 1. Bottom: the final PartIR program after propagation. Note that looping on dimension 1 of %arg1 (of size 64) means that we can also partition the dot product along dimension 1, and essentially pull the whole computation inside the tiling loop by operating on slices of %arg2. In the final program %arg0 automatically got wrapped inside an “atomic” region to signify that it will remain replicated. 3
2.1 Partitioning IR To expose partitioning decisions, we represent tensor programs in PartIR which is an MLIR “dialect” layered on top of MHLO, an MLIR encoding of the XLA Higher Level Operations (HLO). At the core of our IR, which operates on statically shaped multi-dimensional arrays, are tiling loop-like operators which express parallel computations that compute a distributed tensor value. Instead of allowing unrestricted parallel loops, we force users to declare logical mesh axes with fixed sizes and make sure that every such loop in a program is associated with an axis and same-axes loops never occur nested. This guarantees that our programs can compile as a single SPMD kernel. An example of a 2×4 mesh is given in the left of Figure 1, requiring a total of 8 devices for execution. Rewriting actions include actions to express the distribution of an intermediate variable as well as several flavors of propagation of partitioning information (i) from operands to results; (ii) from results to operands, and (iii) from a subset of operands to the rest. These propagation rules are enabled by a registry containing a declarative specification of this behaviour for each operator in the underlying tensor dialect. Rewrites always preserve semantics, decoupling search policies from correctness. Figure 2 illustrates a small MHLO program representing a dense layer, how tiling decisions are expressed in our IR, and the result of propagation. Finally, the tiling loops in our IR lower to a dialect suitable for expressing SPMD computations – Figure 3 shows the result of lowering the final program in Figure 2. Optimising data transfers and reasoning about cost happens at this level of the stack, before we eventually compile back to accelerator-specific HLO code and feed back into the XLA compiler/runtime. A detailed technical exposition of our IR stack is the subject of a different paper. func @main(%arg0: f32 [8 ,16], %arg1: f32 [16 ,64{"shard"}]>, %arg2: f32 [64{"shard"}]>) -> f32 [8 ,64{"shard"}]> attributes { mesh_shape = #partir.mesh <"shard"=2>} { %0 = partir.spmd ( %arg1, %arg0, %arg2) ["shard"] ( %rshard : !partir.range <2>, %arg3: tensor <16 x32xf32 >, %arg4: tensor <8 x16xf32 >, %arg5: tensor <32 xf32 >) { %2 = mhlo.dot %arg4, %arg3 : tensor <8 x32xf32 > %3 = mhlo.broadcast_in_dim %arg5 { broadcast_dims = 1 } : tensor <8 x32xf32 > %4 = mhlo.add %2, %3 : tensor <8 x32xf32 > partir.yield %4 : tensor <8 x32xf32 > } return %0 } Figure 3: An SPMD program after partitioning using axis “shard”. A distributed tensor type like f32[16, 64{"shard"}] means that the value has a global shape of [16, 64], but is nevertheless sharded in chunks of shape [16, 32] (since here axis "shard" is of size 2). 2.2 Automated partitioner The partitioner interacts with a rewriting environment exposing rewriting tactics to distribute/partition values, and tactics involving patterns that apply globally throughout the module. Prior work [36, 3] has highlighted the cost of auto-regressive rewriting which scales unfavourably with model size. Our approach is guided by minimising the number of rewriting decisions. To improve robustness across a wide range of programs, we propose to combine search and learning based on several observations: • Users often do not need to solve heterogeneous partitioning problems but typically map programs to rectangular device meshes such as TPU slices. Faster automation is enabled by restricting partitioning to using pre-defined mesh axes as the structure of tiling loops and their allowable nesting is fixed ahead of time. • When developing a partitioning strategy, users can often assign some decisions themselves based on knowledge of model and devices, such as selecting a data parallel axis. This allows the partitioner to focus on difficult decisions such as only the model-parallel strategy. • Experts do not approach partitioning by investigating individual operations but consider key structural elements such as parameters, certain activations or inputs, optimiser/network state etc. to formulate a high level strategy. They often then hand-craft annotations for a handful of internal nodes that they deem important to guide tools like the XLA GSPMD partitioner. 4
def update(params, opt_state, batch): def update(params, opt_state, batch): # Same as in Fig. 2. loss, grads = jax.value_and_grad( loss_fn)(params, batch) # Specify mapping of axes on devices. grads = jax.lax.psum(grads, "batch") device_layout = np.reshape( updates, opt_state = opt.update(grads, np.array(jax.devices()), (2, 4)) opt_state) mesh = Mesh(device_layout , params = optax.apply_updates(params, ("batch", "model")) updates) return params, opt_state, loss # Manual data-, automated model parallel. # Axis "batch" is specified # Update calls now execute in SPMD # for batch, the third input argument. # fashion. update, spec = automap( update = jax.pmap(update, update, mesh, ['model'], (None, axis_name="batch") None, 0) (None, None, 0))(*args) Figure 4: Using JAX’s pmap (parallel map) Figure 5: Automap allows combining manual function transform enables the use of multi- and automated parallelism. Users specify a host data parallelism over an axis "batch". mesh layout and where manual axes apply. We leverage all three observations to design our automated partitioner. For a physical set of devices, (e.g. 8), users explicitly specify the logical axes for different forms of parallelism, concretely by providing a set of axes names and sizes (e.g. {(“batch”, 2), (“model”, 4)}. The partitioner only searches over tiling decisions involving axes which it is explicitly instructed to use – while users remain in control of the others. 2.3 Search and learning The partitioner must select a sequence of rewrites and propagation tactics to optimise some desirable cost function such as the execution cost. Large models frequently reach 50-100k HLO operations, and even the set of interesting nodes, in our experience ≈ 1% of operations, can be impractically large for search. Instead of opting for a fully learned solution which may require fine-tuning on unseen programs, we first experiment with a hybrid approach. A learner narrows down the most relevant subset of program nodes for partitioning, and search selects the final decisions. Search. We implemented Monte Carlo Tree Search (MCTS) [8] with upper confidence bound for trees (UCT). Instead of exposing all program operations for assigning mesh axes, we initialise a worklist of all ‘interesting operation nodes’ when traversing the program i.e., the function arguments to the MLIR representation consisting of weights and biases, optimiser state, and model inputs. The action space exposes actions to insert tiling loops that partition each tensor and dimension by each predefined axis. After applying an action, its consequences are propagated conservatively backward and forward through the program – for instance if a pointwise operator receives equi- partitioned arguments it may also be executed to produce an equi-partitioned result. Propagation can get stuck in internal nodes for which insufficient information exists (e.g. not enough arguments are partitioned); and these internal nodes with a need for non-trivial decisions resurface back to our worklist. This is a key difference compared to the heuristics-based sharding propagation underlying XLA GSPMD [19]. Another global rewriting decision we expose is a pass that infers the tiling of the rest of the arguments from only some of them. This pass allows us with only a few tiling decisions for some parameters/inputs of a model to induce sharding for other parameters/inputs. If applied at every step, it quickly reduces the number of remaining decisions at the cost of increased wall clock time, and we are experimenting with different mechanics to expose this to an agent. Learning. Using the worklist still exposes too many nodes for search, but careful analysis of the state-of-the-art sharding of transformers [29] shows that in certain situations only a handful need to be selected to fully partition a model, so we apply learning to rank them. Our compiler featurises operation nodes as a concatenation of operation type, operand shapes, and existing partitioned axes. Edges encode program dataflow and MLIR program structure. We then compute a per-node relevance score using a node-embedding where a learned model predicts for each input to the MLIR program a ranking corresponding to the importance of this node to be partitioned, and the top-k (k = 25) most relevant nodes are then passed to MCTS to select the final rewriting sequence. In summary, we combine inductive propagation tactics, search and learning to deliver automated model parallelism with a relatively small number of explicit decisions. 5
End-to-end user example. We illustrate how end-users interact with automap in Figure 5 in com- parison to using an existing JAX parallel primitive in Figure 4. Automap is instrumented using existing JAX tooling for describing positional axes and meshes (based on xmap/pjit). In addition to a partitioned callable, automap returns a specification of partitioning decisions for inputs and outputs. These specifications can then be used to partition function inputs such as parameters, optimiser state, or network state. 3 Towards an evaluation Results. We investigate the performance of our prototype on a transformer model and compare to a well known reference strategy. We implemented a GPT-3 [7] style 24-layer transformer model which requires ≈26 GB of memory at batch size 1 (not fit for a single TPU v3 device at 16 GB RAM), and which has just over 50k operations, and 1150 arguments. We then evaluated our prototype’s ability to discover Megatron-style [29] sharding through search, and when combined with a learner. The search mechanism is guided by multiple cost statistics. First, a peak liveness analysis exposes an approximate memory estimate. This is a conservative estimate, and XLA compilation can further improve required memory through optimisations such as fusion. Second, we minimise the number of bytes communicated through reduction operations. The learned model was trained on a dataset of 20k transformer variants. To generate training data, we selected random model arguments (1000 per model), and exhaustively partitioned all argument dimensions. Our model was trained to imitate the highest scoring strategy. The model was implemented based on an Interaction Network [5] using JAX with Haiku, Jraph for GraphNets, and Optax for training [6, 4, 11, 12]. Megatron is a highly scalable [24] training strategy for transformers [30] which exploits intra- layer model parallelism to minimise the number of required all-reduces. We view Megatron as a representative for a widely used expert strategy. Achieving Megatron is measured through gathering statistics on collectives in the partitioned model. 1.0 0.8 0.6 0.4 0.2 0.0 100 500 1k 2.5k 5k Search budget (episodes) noitcarf dnuof nortageM MCTS 0.5 MCTS + learned filter 0.4 0.3 0.2 0.1 0.0 100 500 1k 2.5k 5k Search budget (episodes) Figure 6: Comparing search progress using MCTS only and combined with a learned fil- ter to discover Megatron-style sharding. )s( emitnur UPT MCTS MCTS + learned filter Megatron Figure 7: TPU v3 runtimes of the solutions found. Near Megatron solutions only incur a small performance penalty. In Figure 6 we illustrate the success rate in discovering Megatron (over 50 search attempts) for a number of search budgets. Results show that several thousands of episodes are required to reliably discover expert-level sharding. We then evaluated the search result for each run on TPU v3 (Figure 7). A key insight is that our search at shorter budgets frequently discovers solutions near Megatron (i.e. few redundant collectives) which are in practice almost as fast (as highlighted by the runtimes of solutions using the learned filter, which are near Megatron from 500 episodes onwards, i.e. requiring few minutes of search). Solutions typically required 2-20 decisions. While these results are encouraging, more work is needed to support a learned system in interactive compiler workflows to be able to handle a variety of (generally unpredictable) user programs. Scaling with compiler hints. Our results show that discovering semantically meaningful strategies is possible in principle. However, we found that discovering these strategies for larger models critically relies on propagating sharding information through subtly shared constants and other computations across layers. Such sharing is brittle and cannot be relied on for a general solution towards rewriting deeper networks. As machine learning models commonly consist of repeated blocks (such as attention blocks in Transformers, residual blocks in ConvNets, or message passing layers in Graph nets), search 6
techniques scale unfavourably when having to explicitly rewrite each layer. We therefore also implemented the ability to exploit model structure in automap by allowing users to group repeated layers together and exposing only a single set of decisions per group. The mechanism used was that of named scopes which are commonly used in libraries such as Haiku [4]. Figures 8 and 9 present the effect of grouping. 1 0.8 0.6 0.4 0.2 0 0 100 200 300 400 500 Search episode nruter edosipE 1 Megatron (expert) 0.8 0.6 0.4 0.2 1 Layer 24 Layers 0 0 100 200 300 400 500 Search episode Figure 8: Searching Transformer sharding strategies with grouped attention blocks via compiler hints drastically improves results. nruter edosipE Megatron (expert) No grouping Grouping Figure 9: Impact of grouping when not rely- ing on propagation of sharding information via shared constants across layers. Notably, when allowing for compiler hints on layer groups, Megatron can be found reliably in a small number of episodes without requiring to propagate through shared dependencies across layers. Without grouping or shared dependency propagation, Megatron is not found for a 24 layer Transformer. As grouping only requires users to provide the name scope for any relevant group (e.g. "attention-block"), this provides an attractive path for initial real world use cases. Other models. We were also able to partition other models such as GraphNets where no one-size- fits-all expert strategy exists. Here, the automap prototype in first experiments was able to discover simple manual strategies such as input edge sharding that allow practitioners to begin experimentation with larger graphs and models. Discussion. Our results illustrate how effective partitioning strategies can be reached using a rewriting system through the combination of data-driven strategies and inductive tactics, and optionally exploiting high-level model structure. More work is required to understand the right combination of user-provided structure information, search, and learning in order to balance minimal user efforts and time to good solutions. The results presented here were also initially restricted to sharding within the devices of a single host while assuming data parallelism across hosts which simplifies the communication cost model. More advanced cost models will be required to model multi-host communication as well as down-stream changes to models during device-specific lowering (e.g. through fusion). Finally, further work is also needed to support additional automated partitioning strategies such as pipeline parallelism or ZeRO offloading. References [1] XLA: Optimizing compiler for machine learning, 2017. URL https://www.tensorflow. org/xla. [2] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large- scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow. org/. Software available from tensorflow.org. 7
[3] Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan, Shreyan Gupta, Hongzi Mao, and Mohammad Alizadeh. Learning generalizable device placement algorithms for distributed machine learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d‘Alché Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/71560ce98c8250ce57a6a970c9991a5f-Paper.pdf. [4] Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Claudio Fantacci, Jonathan Godwin, Chris Jones, Tom Hennigan, Matteo Hessel, Steven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Lena Martens, Vladimir Mikulik, Tamara Norman, John Quan, George Papa- makarios, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, Wojciech Stokowiec, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL http://github.com/deepmind. [5] Peter W. Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray Kavukcuoglu. Interaction networks for learning about objects, relations and physics. CoRR, abs/1612.00222, 2016. URL http://arxiv.org/abs/1612.00222. [6] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. [7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165. [8] Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter I. Cowling, Stephen Tavener, Diego Perez, Spyridon Samothrakis, Simon Colton, and et al. A survey of monte carlo tree search methods. IEEE TRANSACTIONS ON COMPUTATIONAL INTELLIGENCE AND AI, 2012. [9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. [10] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Haichen Shen, Eddie Q. Yan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. TVM: end-to-end optimization stack for deep learning. CoRR, abs/1802.04799, 2018. URL http://arxiv.org/abs/1802. 04799. [11] Jonathan Godwin*, Thomas Keck*, Peter Battaglia, Victor Bapst, Thomas Kipf, Yujia Li, Kimberly Stachenfeld, Petar Velicˇkovic´, and Alvaro Sanchez-Gonzalez. Jraph: A library for graph neural networks in jax., 2020. URL http://github.com/deepmind/jraph. [12] Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020. URL http://github.com/deepmind/dm-haiku. 8
[13] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 103–112, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html. [14] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond data and model parallelism for deep neural networks. In Ameet Talwalkar, Virginia Smith, and Matei Zaharia, editors, Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019. mlsys.org, 2019. URL https://proceedings.mlsys.org/book/265.pdf. [15] Zhihao Jia, Sina Lin, Mingyu Gao, Matei Zaharia, and Alex Aiken. Improving the ac- curacy, scalability, and performance of graph neural networks with roc. In I. Dhillon, D. Papailiopoulos, and V. Sze, editors, Proceedings of Machine Learning and Systems, vol- ume 2, pages 187–198, 2020. URL https://proceedings.mlsys.org/paper/2020/ file/fe9fc289c3ff0af142b6d3bead98a923-Paper.pdf. [16] Norman P. Jouppi, Cliff Young, Nishant Patil, David A. Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, Richard C. Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. CoRR, abs/1704.04760, 2017. URL http://arxiv.org/abs/1704.04760. [17] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. [18] Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Oleksandr Zinenko. Mlir: Scaling compiler infrastructure for domain specific computation. In 2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO), pages 2–14, 2021. doi: 10.1109/ CGO51591.2021.9370308. [19] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. CoRR, abs/2006.16668, 2020. URL https://arxiv.org/abs/2006.16668. [20] Azalia Mirhoseini, Hieu Pham, Quoc Le, Mohammad Norouzi, Samy Bengio, Benoit Steiner, Yuefeng Zhou, Naveen Kumar, Rasmus Larsen, and Jeff Dean. Device placement optimization with supervised learning, 2017. URL https://arxiv.org/abs/1706.04972. [21] Azalia Mirhoseini, Anna Goldie, Hieu Pham, Benoit Steiner, Quoc V. Le, and Jeff Dean. A hier- archical model for device placement. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hkc-TeZ0W. [22] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B. Gibbons, and Matei Zaharia. Pipedream: generalized pipeline parallelism for DNN training. In Tim Brecht and Carey Williamson, editors, Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP 2019, Huntsville, ON, 9
Canada, October 27-30, 2019, pages 1–15. ACM, 2019. doi: 10.1145/3341301.3359646. URL https://doi.org/10.1145/3341301.3359646. [23] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei A. Zaharia. Memory- efficient pipeline-parallel dnn training. In ICML, 2021. [24] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. Efficient large-scale language model training on GPU clusters. CoRR, abs/2104.04473, 2021. URL https://arxiv.org/abs/2104.04473. [25] Aditya Paliwal, Felix Gimeno, Vinod Nair, Yujia Li, Miles Lubin, Pushmeet Kohli, and Oriol Vinyals. Reinforced genetic algorithm learning for optimizing computation graphs. In ICLR, 2020. [26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high- performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d‘Alché Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf. [27] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-offload: Democratizing billion-scale model training. CoRR, abs/2101.06840, 2021. URL https://arxiv.org/abs/2101.06840. [28] Keshav Santhanam, Siddharth Krishna, Ryota Tomioka, Andrew Fitzgibbon, and Tim Harris. Distir: An intermediate representation for optimizing distributed neural networks. In Proceed- ings of the 1st Workshop on Machine Learning and Systems, EuroMLSys ’21, page 15–23, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450382984. doi: 10.1145/3437984.3458829. URL https://doi.org/10.1145/3437984.3458829. [29] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. CoRR, abs/1909.08053, 2019. URL http://arxiv.org/abs/1909.08053. [30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762. [31] Dimitrios Vytiniotis. Declarative abstractions for tensor program partitioning. In Proceed- ings of the 22nd International Symposium on Principles and Practice of Declarative Pro- gramming, PPDP ’20, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450388214. doi: 10.1145/3414080.3414105. URL https://doi.org/10.1145/ 3414080.3414105. [32] Minjie Wang, Chien-chin Huang, and Jinyang Li. Supporting very large models using auto- matic dataflow graph partitioning. In Proceedings of the Fourteenth EuroSys Conference 2019, EuroSys ’19, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450362818. doi: 10.1145/3302424.3303953. URL https://doi.org/10.1145/ 3302424.3303953. [33] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake A. Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen. GSPMD: general and scalable parallelization for ML computation graphs. CoRR, abs/2105.04663, 2021. URL https://arxiv.org/abs/2105.04663. [34] Bowen Yang, Jian Zhang, Jonathan Li, Christopher Ré, Christopher R. Aberger, and Christo- pher De Sa. Pipemare: Asynchronous pipeline parallel DNN training. CoRR, abs/1910.05124, 2019. URL http://arxiv.org/abs/1910.05124. 10
[35] Yichen Yang, Phitchaya Phothilimthana, Yisu Wang, Max Willsey, Sudip Roy, and Jacques Pienaar. Equality saturation for tensor graph superoptimization. In A. Smola, A. Di- makis, and I. Stoica, editors, Proceedings of Machine Learning and Systems, volume 3, pages 255–268, 2021. URL https://proceedings.mlsys.org/paper/2021/file/ 65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf. [36] Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter Ma, Qiumin Xu, Hanxiao Liu, Phitchaya Phothilimtha, Shen Wang, Anna Goldie, Azalia Mirhoseini, and James Laudon. Transferable graph optimizers for ml compilers. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 13844–13855. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/9f29450d2eb58feb555078bdefe28aa5-Paper.pdf. 11
