example_id,generated_summary,reference_summary,rouge1_precision,rouge1_recall,rouge1_fmeasure,rouge2_precision,rouge2_recall,rouge2_fmeasure,rougeL_precision,rougeL_recall,rougeL_fmeasure,bleu_score,embedding_similarity
0,semantic parsing with semi-supervised Sequential Autoencoders. a key re- quirement for effectively training such models is an abundance of supervised data. we propose a novel architecture that ac- commodates semi-supervised training on sequence transduction tasks.,"We present a novel semi-supervised approach
for sequence transduction and apply it to se-
mantic parsing. The unsupervised component
is based on a generative model in which latent
sentences generate the unpaired logical forms.
We apply this method to a number of semantic
parsing tasks focusing on domains with lim-
ited access to labelled training data and ex-
tend those datasets with synthetically gener-
ated logical forms.
1",0.5135135135135135,0.2753623188405797,0.3584905660377359,0.1111111111111111,0.058823529411764705,0.07692307692307691,0.21621621621621623,0.11594202898550725,0.1509433962264151,0.009365521136571802,0.7714605
1,"the VPN achieves 87.6 nats/frame, a score that is near the lower bound on the loss. current approaches range from mean squared error models based on deep neural networks to models that predict quantized image patches. the model operates on pixels without preprocessing and predicts discrete multinomial distributions over raw pixel intensities.","We propose a probabilistic video model, the Video Pixel Network (VPN),
that estimates the discrete joint distribution of the raw pixel values in
a video. The model and the neural architecture reﬂect the time, space
and color structure of video tensors and encode it as a four-dimensional
dependency chain. The VPN approaches the best possible performance on
the Moving MNIST benchmark, a leap over the previous state of the art,
and the generated videos show only minor deviations from the ground truth.
The VPN also produces detailed samples on the action-conditional Robotic
Pushing benchmark and generalizes to the motion of novel objects.
1",0.42592592592592593,0.2169811320754717,0.28750000000000003,0.07547169811320754,0.0380952380952381,0.05063291139240506,0.18518518518518517,0.09433962264150944,0.125,0.005832010983007652,0.54803807
2,"a recent deep reinforcement learning algorithm can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real robots. in this paper, we demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. the main contribution of this paper is a demonstration of autonomous door opening.","Deep Reinforcement Learning for Robotic Manipulation with
Asynchronous Off-Policy Updates
Shixiang Gu∗,1,2,3 and Ethan Holly∗,1 and Timothy Lillicrap4 and Sergey Levine1,5
Abstract— Reinforcement lear",0.13636363636363635,0.3,0.18749999999999997,0.046153846153846156,0.10344827586206896,0.06382978723404256,0.09090909090909091,0.2,0.12500000000000003,0.0027574366655649983,0.72430664
3,"a number of problems in machine learning lack a single unified cost, and instead consist of a hybrid of several models. each of which passes information to other models but tries to minimize its own private loss function. in both cases the information flow is a simple feedforward pass from one model which either takes an action (AC) or generates a sample (GANs) to a second model which evaluates the output of the first model.","Both generative adversarial networks (GAN) in unsupervised learning and actor-
critic methods in reinforcement learning (RL) have gained a reputation for being
difﬁcult to optimize. Practitioners in both ﬁelds have amassed a large number of
strategies to mitigate these instabilities and improve training. Here we show that
GANs can be viewed as actor-critic methods in an environment where the actor
cannot affect the reward. We review the strategies for stabilizing training for each
class of models, both those that generalize between the two and those that are par-
ticular to that model. We also review a number of extensions to GANs and RL
algorithms with even more complicated information ﬂow. We hope that by high-
lighting this formal connection we will encourage both GAN and RL communi-
ties to develop general, scalable, and stable algorithms for multilevel optimization
with deep networks, and to draw inspiration across communities.
1",0.3333333333333333,0.16666666666666666,0.2222222222222222,0.04054054054054054,0.020134228187919462,0.026905829596412554,0.18666666666666668,0.09333333333333334,0.12444444444444445,0.008240159216954734,0.49620923
4,"implicit models use a latent variable z and trans- form it using a deterministic function G that maps from Rm Rd using parameters . implicit models are more natural for many problems, e.g., many of the basic methods for generating non-uniform random variates are based on simple implicit models.","Generative adversarial networks (GANs) provide
an algorithmic framework for constructing gen-
erative models with several appealing proper-
ties: they do not require a likelihood function
to be speciﬁed, only a generating procedure; they
provide samples that are sharp and compelling;
and they allow us to harness our knowledge of
building highly accurate neural network classi-
ﬁers. Here, we develop our understanding of
GANs with the aim of forming a rich view of
this growing area of machine learning—to build
connections to the diverse set of statistical think-
ing on this topic, of which much can be gained
by a mutual exchange of ideas. We frame GANs
within the wider landscape of algorithms for learn-
ing in implicit generative models—models that
only specify a stochastic procedure with which
to generate data—and relate these ideas to mod-
elling problems in related ﬁelds, such as econo-
metrics and approximate Bayesian computation.
We develop likelihood-free inference methods
and highlight hypothesis testing as a principle
for learning in implicit generative models, using
which we are able to derive the objective func-
tion used by GANs, and many other related ob-
jectives. The testing viewpoint directs our fo-
cus to the general problem of density-ratio and
density-difference estimation. There are four ap-
proaches for density comparison, one of which
is a solution using classiﬁers to distinguish real
from generated data. Other approaches such as
divergence minimisation and moment matching
have also been explored, and we synthesise these
views to form an understanding in terms of the re-
lationships between them and the wider literature,
highlighting avenues for future exploration and
cross-pollination.
*Equal contribution 1DeepMind, London, UK. Correspondence
to: Shakir Mohamed <shakir@google.com>, Balaji Lakshmi-
narayanan <balajiln@google.com>.",0.52,0.08904109589041095,0.15204678362573099,0.04081632653061224,0.006872852233676976,0.011764705882352941,0.28,0.04794520547945205,0.08187134502923976,0.00013197974937374562,0.32253924
5,deep RL algorithms are too slow to achieve performance on a real robot. the progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks. we propose using progressive networks to bridge the reality gap that separates simulation from real world domains.,"Sim-to-Real Robot Learning from Pixels with
Progressive Nets
Andrei A. Rusu
DeepMind
London, UK
andreirusu@google.com
Mel Veˇcerík
DeepMind
London, UK
matejvecerik@google.com
Thomas Rothörl
DeepMind
L",0.14035087719298245,0.23529411764705882,0.1758241758241758,0.03571428571428571,0.06060606060606061,0.0449438202247191,0.10526315789473684,0.17647058823529413,0.13186813186813184,0.003324612889947367,0.7377403
6,"a low-level controller generates stable and coherent exploratory behavior. a higher level controller can then recruit these motor behaviors to simplify the solution of complex tasks. the controller has direct access only to task-independent, proprioceptive information.","We study a novel architecture and training procedure for locomotion tasks. A
high-frequency, low-level “spinal” network with access to proprioceptive sensors
learns sensorimotor primitives by training on simple tasks. This pre-trained module
is ﬁxed and connected to a low-frequency, high-level “cortical” network, with
access to all sensors, which drives behavior by modulating the inputs to the spinal
network. Where a monolithic end-to-end architecture fails completely, learning
with a pre-trained spinal module succeeds at multiple high-level tasks, and enables
the effective exploration required to learn from sparse rewards. We test our pro-
posed architecture on three simulated bodies: a 16-dimensional swimming snake, a
20-dimensional quadruped, and a 54-dimensional humanoid (see attached video).
1",0.39473684210526316,0.12096774193548387,0.18518518518518517,0.05405405405405406,0.016260162601626018,0.025,0.3157894736842105,0.0967741935483871,0.14814814814814814,0.0011374004992362266,0.4216212
7,"a sparse version of the recently published Differentiable Neural Computer (SDNC) can be used to scale memory-augmented neural networks with Sparse Reads and Writes. the LSTM architecture has proven to be powerful sequence learning models. but the number of parameters grows proportionally to the square of the memory, making them unsuitable for problems requiring large amounts of long-term memory.","Neural networks augmented with external memory have the ability to learn algorith-
mic solutions to complex tasks. These models appear promising for applications
such as language modeling and machine translation. However, they scale poorly in
both space and time as the amount of memory grows — limiting their applicability
to real-world domains. Here, we present an end-to-end differentiable memory
access scheme, which we call Sparse Access Memory (SAM), that retains the
representational power of the original approaches whilst training efﬁciently with
very large memories. We show that SAM achieves asymptotic lower bounds in
space and time complexity, and ﬁnd that an implementation runs 1,000× faster
and with 3,000× less physical memory than non-sparse models. SAM learns with
comparable data efﬁciency to existing models on a range of synthetic tasks and
one-shot Omniglot character recognition, and can scale to tasks requiring 100,000s
of time steps and memories. As well, we show how our approach can be adapted
for models that maintain temporal associations between memories, as with the
recently introduced Differentiable Neural Computer.
1",0.6229508196721312,0.2087912087912088,0.31275720164609055,0.11666666666666667,0.03867403314917127,0.058091286307053944,0.29508196721311475,0.0989010989010989,0.14814814814814814,0.002591633073249409,0.6825485
8,"a number of model-based methods for machine transliteration have been developed in the past. such models assume character sequences correspond to predictable char- acter sequences in the target orthography, possibly depending on context. some models additionally as- sume that such correspondences are influenced by phonetic information. in many areas, including machine translation, end-to-end deep learning models have become a good alternative to traditional statistical ap- proaches.","Transliteration is a key component of machine
translation systems and software internation-
alization. This paper demonstrates that neural
sequence-to-sequence models obtain state of
the art or close to state of the art results on ex-
isting datasets. In an effort to make machine
transliteration accessible, we open source a
new Arabic to English transliteration dataset
and our trained models.
1",0.2753623188405797,0.3064516129032258,0.2900763358778626,0.029411764705882353,0.03278688524590164,0.031007751937984496,0.17391304347826086,0.1935483870967742,0.183206106870229,0.009310894037007574,0.77553475
9,"AD libraries are enjoying broad use, spurred on by the success of neural networks on some of the most challenging problems of machine learning. the dominant mode of development in these libraries is to define a forward parametric computation, in the form of a directed acyclic graph, that computes the desired objective. a backwards computation for the gradient of the objective can be derived automatically with the chain rule.","The reparameterization trick enables optimizing large scale stochastic computa-
tion graphs via gradient descent. The essence of the trick is to refactor each
stochastic node into a differentiable function of its parameters and a random vari-
able with ﬁxed distribution. After refactoring, the gradients of the loss propa-
gated by the chain rule through the graph are low variance unbiased estimators
of the gradients of the expected loss. While many continuous random variables
have such reparameterizations, discrete random variables lack useful reparame-
terizations due to the discontinuous nature of discrete states. In this work we
introduce CONCRETE random variables—CONtinuous relaxations of disCRETE
random variables.
The Concrete distribution is a new family of distributions
with closed form densities and a simple reparameterization. Whenever a discrete
stochastic node of a computation graph can be refactored into a one-hot bit rep-
resentation that is treated continuously, Concrete stochastic nodes can be used
with automatic differentiation to produce low-variance biased gradients of objec-
tives (including objectives that depend on the log-probability of latent stochastic
nodes) on the corresponding discrete graph. We demonstrate the effectiveness of
Concrete relaxations on density estimation and structured prediction tasks using
neural networks.
1",0.5652173913043478,0.19696969696969696,0.29213483146067415,0.16176470588235295,0.05583756345177665,0.0830188679245283,0.2898550724637681,0.10101010101010101,0.149812734082397,0.00296177766257772,0.37547126
10,a conference paper at ICLR 2017 is the first to address this challenge successfully at scale. we introduce an actor critic with experience replay (ACER) that nearly matches the state-of-the-art performance of deep Q-networks with prioritized replay on Atari. ACER is a valuable tool for improving sample efficiency.,"This paper presents an actor-critic deep reinforcement learning agent with ex-
perience replay that is stable, sample efﬁcient, and performs remarkably well on
challenging environments, including the discrete 57-game Atari domain and several
continuous control problems. To achieve this, the paper introduces several inno-
vations, including truncated importance sampling with bias correction, stochastic
dueling network architectures, and a new trust region policy optimization method.
1",0.4230769230769231,0.3235294117647059,0.3666666666666667,0.0392156862745098,0.029850746268656716,0.03389830508474576,0.21153846153846154,0.16176470588235295,0.18333333333333335,0.005097366364414828,0.7044451
11,"reinforcement learning has seen success in several areas including robotics, computer games and online advertising. in this paper we consider model-free reinforcement learning, where the state-transition function is not known or learned. two alternatives are SARSA (Rummery & Niranjan, 1994) and Q-learning (Watkins, 1989)","Policy gradient is an efﬁcient technique for improving a policy in a reinforcement
learning setting. However, vanilla online variants are on-policy only and not able
to take advantage of off-policy data. In this paper we describe a new technique that
combines policy gradient with off-policy Q-learning, drawing experience from a
replay buffer. This is motivated by making a connection between the ﬁxed points
of the regularized policy gradient algorithm and the Q-values. This connection
allows us to estimate the Q-values from the action preferences of the policy, to
which we apply Q-learning updates. We refer to the new technique as ‘PGQL’,
for policy gradient and Q-learning. We also establish an equivalency between
action-value ﬁtting techniques and actor-critic algorithms, showing that regular-
ized policy gradient techniques can be interpreted as advantage function learning
algorithms. We conclude with some numerical examples that demonstrate im-
proved data efﬁciency and stability of PGQL. In particular, we tested PGQL on
the full suite of Atari games and achieved performance exceeding that of both
asynchronous advantage actor-critic (A3C) and Q-learning.
1",0.43478260869565216,0.10582010582010581,0.17021276595744683,0.15555555555555556,0.03723404255319149,0.060085836909871244,0.30434782608695654,0.07407407407407407,0.11914893617021276,0.001906748033357576,0.634812
12,"reference-aware language models are based on the principle of pointer networks in which copies are made from another source. in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of natural language reference. we propose a language modeling framework that ex-plicitly incorporates reference decisions.","We propose a general class of language
models that treat reference as discrete
stochastic latent variables. This decision
allows for the creation of entity mentions
by accessing external databases of refer-
ents (required by, e.g., dialogue genera-
tion) or past internal state (required to ex-
plicitly model coreferentiality).
Beyond
simple copying, our coreference model
can additionally refer to a referent using
varied mention forms (e.g., a reference to
“Jane” can be realized as “she”), a charac-
teristic feature of reference in natural lan-
guages. Experiments on three representa-
tive applications show our model variants
outperform models based on deterministic
attention and standard language modeling
baselines.
1",0.559322033898305,0.3055555555555556,0.39520958083832336,0.15517241379310345,0.08411214953271028,0.1090909090909091,0.288135593220339,0.1574074074074074,0.20359281437125748,0.007388817921763004,0.73037344
13,"ICLR 2017 LEARNING TO PERFORM PHYSICS EXPERIMENTS VIA DEEP REINFORCEMENT LARNING Misha Denil1 Pulkit Agrawal2 Tejas D Kulkarni1 Tom Erez1 Peter Battaglia1 Nando de Freitas1,3 1DeepMind 2 University of California Berkeley 3Canadian Institute for Advanced Research. the object core system covering aspects such as cohesion, continuity, and contact enables babies and","When encountering novel objects, humans are able to infer a wide range of phys-
ical properties such as mass, friction and deformability by interacting with them
in a goal driven way. This process of active interaction is in the same spirit as
a scientist performing experiments to discover hidden facts. Recent advances in
artiﬁcial intelligence have yielded machines that can achieve superhuman perfor-
mance in Go, Atari, natural language processing, and complex control problems;
however, it is not clear that these systems can rival the scientiﬁc intuition of even
a young child. In this work we introduce a basic set of tasks that require agents
to estimate properties such as mass and cohesion of objects in an interactive sim-
ulated environment where they can manipulate the objects and observe the conse-
quences. We found that deep reinforcement learning methods can learn to perform
the experiments necessary to discover such hidden properties. By systematically
manipulating the problem difﬁculty and the cost incurred by the agent for per-
forming experiments, we found that agents learn different strategies that balance
the cost of gathering information against the cost of making mistakes in different
situations. We also compare our learned experimentation policies to randomized
baselines and show that the learned policies lead to better predictions.
1",0.3269230769230769,0.07906976744186046,0.12734082397003746,0.09803921568627451,0.02336448598130841,0.03773584905660377,0.19230769230769232,0.046511627906976744,0.07490636704119849,0.0004423561455718255,0.67059964
14,"noisy channel models are excellent models of p(output sequence y | input sequence x), provided sufficient input–output pairs are available for estimating their parameters. the noisy channel model operates by selecting outputs that both are a priori likely and that explain the input well. in practice, decoding is a significant computational challenge.","We formulate sequence to sequence transduction as a noisy channel decoding
problem and use recurrent neural networks to parameterise the source and channel
models. Unlike direct models which can suffer from explaining-away effects dur-
ing training, noisy channel models must produce outputs that explain their inputs,
and their component models can be trained with not only paired training samples
but also unpaired samples from the marginal output distribution. Using a latent
variable to control how much of the conditioning sequence the channel model
needs to read in order to generate a subsequent symbol, we obtain a tractable
and effective beam search decoder. Experimental results on abstractive sentence
summarisation, morphological inﬂection, and machine translation show that noisy
channel models outperform direct models, and that they signiﬁcantly beneﬁt from
increased amounts of unpaired output data that direct models cannot easily use.
1",0.49056603773584906,0.18055555555555555,0.2639593908629442,0.1346153846153846,0.04895104895104895,0.07179487179487179,0.24528301886792453,0.09027777777777778,0.1319796954314721,0.007223295580232849,0.61864406
15,"ICLR 2017 LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu,. the agent is trained to detect loop closures as an additional auxiliary task that encourages implicit velocity integration.","Learning to navigate in complex environments with dynamic elements is an impor-
tant milestone in developing AI agents. In this work we formulate the navigation
question as a reinforcement learning problem and show that data efﬁciency and task
performance can be dramatically improved by relying on additional auxiliary tasks
leveraging multimodal sensory inputs. In particular we consider jointly learning
the goal-driven reinforcement learning problem with auxiliary depth prediction
and loop closure classiﬁcation tasks. This approach can learn to navigate from raw
sensory input in complicated 3D mazes, approaching human-level performance
even under conditions where the goal location changes frequently. We provide
detailed analysis of the agent behaviour1, its ability to localise, and its network
activity dynamics, showing that the agent implicitly learns key navigation abilities.
1",0.3829787234042553,0.13846153846153847,0.20338983050847456,0.1956521739130435,0.06976744186046512,0.10285714285714287,0.2553191489361702,0.09230769230769231,0.13559322033898305,0.002803761760832616,0.564782
16,the average human-normalised score of every job in our hyperparameter sweep is 87%. top 3 agents 0% 20% 40% 60% 60% 80% 100% Percentage of Agents in Population 0% 200% 400% 600% 800% 1000% 1200% Human Normalised Performance. a top-down visualization showing the layout of each level can be found in Figure 7 of the Appendix.,"Deep reinforcement learning agents have achieved state-of-the-art results by di-
rectly maximising cumulative reward. However, environments contain a much
wider variety of possible training signals. In this paper, we introduce an agent
that also maximises many other pseudo-reward functions simultaneously by rein-
forcement learning. All of these tasks share a common representation that, like
unsupervised learning, continues to develop in the absence of extrinsic rewards.
We also introduce a novel mechanism for focusing this representation upon ex-
trinsic rewards, so that learning can rapidly adapt to the most relevant aspects
of the actual task. Our agent signiﬁcantly outperforms the previous state-of-the-
art on Atari, averaging 880% expert human performance, and a challenging suite
of ﬁrst-person, three-dimensional Labyrinth tasks leading to a mean speedup in
learning of 10× and averaging 87% expert human performance on Labyrinth.
Natural and artiﬁcial agents live in a stream of sensorimotor data. At each time step t, the agent
receives observations ot and executes actions at. These actions inﬂuence the future course of the
sensorimotor stream. In this paper we develop agents that learn to predict and control this stream,
by solving a host of reinforcement learning problems, each focusing on a distinct feature of the
sensorimotor stream. Our hypothesis is that an agent that can ﬂexibly control its future experiences
will also be able to achieve any goal with which it is presented, such as maximising its future
rewards.
The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. How-
ever, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions
of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor
stream contains an abundance of other possible learning targets. Traditionally, unsupervised learn-
ing attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It
is typically used to accelerate the acquisition of a useful representation. In contrast, our learning
objective is to predict and control features of the sensorimotor stream, by treating them as pseudo-
rewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the
agent’s long-term goals, potentially leading to more useful representations.
Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly
predict the optimal value, the baby must understand how to increase “redness” by various means,
including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a
red object); and communication (crying until the parents bring a red object). These behaviours are
likely to recur for many other goals that the baby may subsequently encounter. No understanding of
these behaviours is required to simply reconstruct the redness of current or subsequent images.
Our architecture uses reinforcement learning to approximate both the optimal policy and optimal
value function for many different pseudo-rewards. It also makes other auxiliary predictions that
serve to focus the agent on important aspects of the task. These include the long-term goal of
predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To
learn more efﬁciently, our agents use an experience replay mechanism to provide additional updates
∗Joint ﬁrst authors. Ordered alphabetically by ﬁrst name.
1
arXiv:1611.05397v1  [cs.LG]  16 Nov 2016
⇡
V
⇡
V
⇡
V
⇡
V
0
0
0
+1
(c) Reward Prediction
rt
}
Replay Buffer
V
V
V
R
R
R
(d) Value Function Replay
t⌧
t⌧+1
t⌧+2
t⌧+3
t⌧−3
t⌧−2
t⌧−1
r⌧
Skewed
sampling
(b) Pixel Control
Qaux
V
…
Environment
…
…
ot
Agent LSTM
Agent ConvNet
Aux DeConvNet
Aux FC net
(a) Base A3C Agent
Figure 1: Overview of the UNREAL agent. (a) The base agent is a CNN-LSTM agent trained on-policy with
the A3C loss (Mnih et al., 2016). Observations, rewards, and actions are stored in a small replay buffer which
encapsulates a short history of agent experience. This experience is used by auxiliary learning tasks. (b) Pixel
Control – auxiliary policies Qaux are trained to maximise change in pixel intensity of different regions of the
input. The agent CNN and LSTM are used for this task along with an auxiliary deconvolution network. This
auxiliary control task requires the agent to learn how to control the environment. (c) Reward Prediction – given
three recent frames, the network must predict the reward that will be obtained in the next unobserved timestep.
This task network uses instances of the agent CNN, and is trained on reward biased sequences to remove the
perceptual sparsity of rewards. (d) Value Function Replay – further training of the value function using the
agent network is performed to promote faster value iteration. Further visualisation of the agent can be found in
https://youtu.be/Uz-zGYrYEjA
to the critics. Just as animals dream about positively or negatively rewarding events more frequently
(Schacter et al., 2012), our agents preferentially replay sequences containing rewarding events.
Importantly, both the auxiliary control and auxiliary prediction tasks share the convolutional neural
network and LSTM that the base agent uses to act. By using this jointly learned representation,
the base agent learns to optimise extrinsic reward much faster and, in many cases, achieves better
policies at the end of training.
This paper brings together the state-of-the-art Asynchronous Advantage Actor-Critic (A3C) frame-
work (Mnih et al., 2016), outlined in Section 2, with auxiliary control tasks and auxiliary reward
tasks, deﬁned in sections Section 3.1 and Section 3.2 respectively. These auxiliary tasks do not re-
quire any extra supervision or signals from the environment than the vanilla A3C agent. The result
is our UNsupervised REinforcement and Auxiliary Learning (UNREAL) agent (Section 3.4)
In Section 4 we apply our UNREAL agent to a challenging set of 3D-vision based domains known
as the Labyrinth (Mnih et al., 2016), learning solely from the raw RGB pixels of a ﬁrst-person view.
Our agent signiﬁcantly outperforms the baseline agent using vanilla A3C, even when the baseline
was augmented with an unsupervised reconstruction loss, in terms of speed of learning, robustness
to hyperparameters, and ﬁnal performance. The result is an agent which on average achieves 87% of
expert human-normalised score, compared to 54% with A3C, and on average 10× faster than A3C.
Our UNREAL agent also signiﬁcantly outperforms the previous state-of-the-art in the Atari domain.
1
RELATED WORK
A variety of reinforcement learning architectures have focused on learning temporal abstractions,
such as options (Sutton et al., 1999b), with policies that may maximise pseudo-rewards (Konidaris
& Barreto, 2009; Silver & Ciosek, 2012). The emphasis here has typically been on the development
of temporal abstractions that facilitate high-level learning and planning. In contrast, our agents do
not make any direct use of the pseudo-reward maximising policies that they learn (although this is
2
an interesting direction for future research). Instead, they are used solely as auxiliary objectives for
developing a more effective representation.
The Horde architecture (Sutton et al., 2011) also applied reinforcement learning to identify value
functions for a multitude of distinct pseudo-rewards. However, this architecture was not used for
representation learning; instead each value function was trained separately using distinct weights.
The UVFA architecture (Schaul et al., 2015a) is a factored representation of a continuous set of
optimal value functions, combining features of the state with an embedding of the pseudo-reward
function. Initial work on UVFAs focused primarily on architectural choices and learning rules for
these continuous embeddings. A pre-trained UVFA representation was successfully transferred to
novel pseudo-rewards in a simple task.
Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016)
factors a continuous set of expected value functions for a ﬁxed policy, by combining an expectation
over features of the state with an embedding of the pseudo-reward function. Successor representa-
tions have been used to transfer representations from one pseudo-reward to another (Barreto et al.,
2016) or to different scales of reward (Kulkarni et al., 2016).
Another, related line of work involves learning models of the environment (Schmidhuber, 2010;
Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could
improve RL agents (e.g. Lin & Mitchell (1992); Li et al. (2015)), this has not yet been shown to
work in rich visual environments.
More recently, auxiliary predictions tasks have been studied in 3D reinforcement learning environ-
ments. Lample & Chaplot (2016) showed that predicting internal features of the emulator, such
as the presence of an enemy on the screen, is beneﬁcial. Mirowski et al. (2016) study auxiliary
prediction of depth in the context of navigation.
2
BACKGROUND
We assume the standard reinforcement learning setting where an agent interacts with an environment
over a number of discrete time steps. At time t the agent receives an observation ot along with a
reward rt and produces an action at. The agent’s state st is a function of its experience up until
time t, st = f(o1, r1, a1, ..., ot, rt). The n-step return Rt:t+n at time t is deﬁned as the discounted
sum of rewards, Rt:t+n = Pn
i=1 γirt+i. The value function is the expected return from state s,
V π(s) = E [Rt:∞|st = s, π], when actions are selected accorded to a policy π(a|s). The action-
value function Qπ(s, a) = E [Rt:∞|st = s, at = a, π] is the expected return following action a
from state s.
Value-based reinforcement learning algorithms, such as Q-learning (Watkins, 1989), or its deep
learning instantiations DQN (Mnih et al., 2015) and asynchronous Q-learning (Mnih et al., 2016),
approximate the action-value function Q(s, a; θ) using parameters θ, and then update parameters
to minimise the mean-squared error, for example by optimising an n-step lookahead loss (Peng
& Williams, 1996), LQ = E
h
(Rt:t+n + γn maxa′ Q(s′, a′; θ−) −Q(s, a; θ))2i
; where θ−are
previous parameters and the optimisation is with respect to θ.
Policy gradient algorithms adjust the policy to maximise the expected reward, Lπ = −Es∼π [R1:∞],
using the gradient ∂Es∼π[R1:∞]
∂θ
= E
 ∂
∂θ log π(a|s)(Qπ(s, a) −V π(s))

(Watkins, 1989; Sutton
et al., 1999a); in practice the true value functions Qπ and V π are substituted with approxima-
tions. The Asynchronous Advantage Actor-Critic (A3C) algorithm (Mnih et al., 2016) constructs
an approximation to both the policy π(a|s, θ) and the value function V (s, θ) using parameters θ.
Both policy and value are adjusted towards an n-step lookahead value, Rt:t+n + γnV (st+n+1, θ),
using an entropy regularisation penalty, LA3C ≈LVR + Lπ −Es∼π [αH(π(s, ·, θ)], where
LVR = Es∼π
h
(Rt:t+n + γnV (st+n+1, θ−) −V (st, θ))2i
.
In A3C many instances of the agent interact in parallel with many instances of the environment,
which both accelerates and stabilises learning. The A3C agent architecture we build on uses an
LSTM to jointly approximate both policy π and value function V , given the entire history of expe-
rience as inputs (see Figure 1 (a)).
3
3
AUXILIARY TASKS FOR REINFORCEMENT LEARNING
In this section we incorporate auxiliary tasks into the reinforcement learning framework in order
to promote faster training, more robust learning, and ultimately higher performance for our agents.
Section 3.1 introduces the use of auxiliary control tasks, Section 3.2 describes the addition of reward
focussed auxiliary tasks, and Section 3.4 describes the complete UNREAL agent combining these
auxiliary tasks.
3.1
AUXILIARY CONTROL TASKS
The auxiliary control tasks we consider are deﬁned as additional pseudo-reward functions in the
environment the agent is interacting with. We formally deﬁne an auxiliary control task c by a reward
function r(c) : S × A →R, where S is the space of possible states and A is the space of available
actions. The underlying state space S includes both the history of observations and rewards as well
as the state of the agent itself, i.e. the activations of the hidden units of the network.
Given a set of auxiliary control tasks C, let π(c) be the agent’s policy for each auxiliary task c ∈C and
let π be the agent’s policy on the base task. The overall objective is to maximise total performance
across all these auxiliary tasks,
arg max
θ
Eπ[R1:∞] + λc
X
c∈C
Eπc[R(c)
1:∞],
(1)
where, R(c)
t:t+n = Pn
k=1 γkr(c)
t
is the discounted return for auxiliary reward r(c), and θ is the set of
parameters of π and all π(c)’s. By sharing some of the parameters of π and all π(c) the agent must
balance improving its performance with respect to the global reward rt with improving performance
on the auxiliary tasks.
In principle, any reinforcement learning method could be applied to maximise these objectives.
However, to efﬁciently learn to maximise many different pseudo-rewards simultaneously in par-
allel from a single stream of experience, it is necessary to use off-policy reinforcement learn-
ing.
We focus on value-based RL methods that approximate the optimal action-values by Q-
learning.
Speciﬁcally, for each control task c we optimise an n-step Q-learning loss L(c)
Q
=
E
h",0.5862068965517241,0.015322217214961695,0.02986385595081247,0.12280701754385964,0.0031559963931469793,0.006153846153846154,0.41379310344827586,0.010815682739972961,0.021080368906455864,3.715788989617831e-18,0.2137632
17,"deep meta-reinforcement learning (RL) is a recurrent neural network that can adapt to changing task conditions. the resulting techniques are able to achieve human- and often superhuman-level performance in an expanding list of domains. deep RL systems typically specialize on one restricted task domain, whereas human learners can flexibly adapt. a deeper RL technique is intended to link it with and distinguish it from previous work using the term meta-learning.","In recent years deep reinforcement learning (RL) systems have attained superhuman
performance in a number of challenging task domains. However, a major limitation
of such applications is their demand for massive amounts of training data. A critical
present objective is thus to develop deep RL methods that can adapt rapidly to new
tasks. In the present work we introduce a novel approach to this challenge, which
we refer to as deep meta-reinforcement learning. Previous work has shown that
recurrent networks can support meta-learning in a fully supervised context. We
extend this approach to the RL setting. What emerges is a system that is trained
using one RL algorithm, but whose recurrent dynamics implement a second, quite
separate RL procedure. This second, learned RL algorithm can differ from the
original one in arbitrary ways. Importantly, because it is learned, it is conﬁgured
to exploit structure in the training domain. We unpack these points in a series of
seven proof-of-concept experiments, each of which examines a key aspect of deep
meta-RL. We consider prospects for extending and scaling up the approach, and
also point out some potentially important implications for neuroscience.
1",0.589041095890411,0.2193877551020408,0.31970260223048325,0.18055555555555555,0.06666666666666667,0.09737827715355805,0.3150684931506849,0.11734693877551021,0.17100371747211895,0.005849974640520859,0.8156677
18,"deep learning is a fast growing subfield of machine learning, with many impressive results. it relies on non-convex functions which are optimized using local gradient descent methods. despite the optimism, the “out of the box” gradient descent is often not working well enough.","Training of a neural network is often formulated as a task of ﬁnding a “good”
minimum of an error surface - the graph of the loss expressed as a function of its
weights. Due to the growing popularity of deep learning, the classical problem
of studying the error surfaces of neural networks is now in the focus of many
researchers. This stems from a long standing question. Given that deep networks
are highly nonlinear systems optimized by local gradient methods, why do they
not seem to be affected by bad local minima? As much as it is often observed
in practice that training of deep models using gradient methods works well, little
is understood about why it happens. A lot of research efforts has been dedicated
recently for proving the good behavior of training neural networks. In this paper
we adapt the complementary approach of studying the possible obstacles. We
present several concrete examples of datasets which cause the error surface to
have a strongly suboptimal local minimum.
1",0.5909090909090909,0.15476190476190477,0.24528301886792456,0.11627906976744186,0.029940119760479042,0.047619047619047616,0.3409090909090909,0.08928571428571429,0.14150943396226415,0.001159876514173941,0.6442654
19,"in this paper we aim to provide an answer to the question what intrinsic options are available to an agent in a given state. we define options as policies with a termination condition, and we are primarily concerned with their consequences – what states in the environment they reach upon termination. this is the set of all things that are possible for an agent to achieve.","In this paper we introduce a new unsupervised reinforcement learning method for
discovering the set of intrinsic options available to an agent. This set is learned by
maximizing the number of different states an agent can reliably reach, as measured
by the mutual information between the set of options and option termination states.
To this end, we instantiate two policy gradient based algorithms, one that creates
an explicit embedding space of options and one that represents options implicitly.
The algorithms also provide an explicit measure of empowerment in a given state
that can be used by an empowerment maximizing agent. The algorithm scales well
with function approximation and we demonstrate the applicability of the algorithm
on a range of tasks.
1",0.6,0.32231404958677684,0.41935483870967744,0.234375,0.125,0.16304347826086957,0.3230769230769231,0.17355371900826447,0.22580645161290322,0.023907395831280983,0.57118195
20,"a computer vision system able to hold conversations about what it sees would be an important step towards in- Questioner Is it a vase? a referring expression is not always possible, as it de- pends on the listener’s state of mind and the context of the scene. the last few years has seen an increasing interest from the computer vision community in tasks towards this goal.","We introduce GuessWhat?!, a two-player guessing game
as a testbed for research on the interplay of computer vision
and dialogue systems. The goal of the game is to locate an
unknown object in a rich image scene by asking a sequence
of questions. Higher-level image understanding, like spa-
tial reasoning and language grounding, is required to solve
the proposed task. Our key contribution is the collection
of a large-scale dataset consisting of 150K human-played
games with a total of 800K visual question-answer pairs on
66K images. We explain our design decisions in collecting
the dataset and introduce the oracle and questioner tasks
that are associated with the two players of the game. We
prototyped deep learning models to establish initial base-
lines of the introduced tasks.",0.3880597014925373,0.1984732824427481,0.2626262626262626,0.045454545454545456,0.023076923076923078,0.030612244897959183,0.2835820895522388,0.1450381679389313,0.19191919191919193,0.005498322295544596,0.57391024
21,"there are three predominant approaches for constructing vector representations of sentences from a sequence of words. the first composes words sequentially using a recurrent neural network, treat-ing the RNN’s final hidden state as the representation of the sentence. in such models, there is no explicit hierarchical organization im-posed on the words, and the. dynamics must learn to simulate it.","We use reinforcement learning to learn tree-structured neural networks for com-
puting representations of natural language sentences. In contrast with prior work
on tree-structured models in which the trees are either provided as input or pre-
dicted using supervision from explicit treebank annotations, the tree structures
in this work are optimized to improve performance on a downstream task. Ex-
periments demonstrate the beneﬁt of learning task-speciﬁc composition orders,
outperforming both sequential encoders and recursive encoders based on treebank
annotations. We analyze the induced trees and show that while they discover
some linguistically intuitive structures (e.g., noun phrases, simple verb phrases),
they are different than conventional English syntactic structures.
1",0.3870967741935484,0.20869565217391303,0.27118644067796605,0.06557377049180328,0.03508771929824561,0.04571428571428571,0.20967741935483872,0.11304347826086956,0.14689265536723164,0.004984545972349643,0.5792977
22,"the lack of algorithms to support continual learning thus remains a key barrier to the development of artificial general intelligence. the mammalian brain may avoid catastrophic forgetting by protecting previously-acquired knowledge in neocortical circuits [Cichon and Gan, 2015], despite the subsequent learning of other tasks.","The ability to learn tasks in a sequential fashion is crucial to the development of
artiﬁcial intelligence. Neural networks are not, in general, capable of this and it
has been widely thought that catastrophic forgetting is an inevitable feature of
connectionist models. We show that it is possible to overcome this limitation and
train networks that can maintain expertise on tasks which they have not experienced
for a long time. Our approach remembers old tasks by selectively slowing down
learning on the weights important for those tasks. We demonstrate our approach is
scalable and effective by solving a set of classiﬁcation tasks based on the MNIST
hand written digit dataset and by learning several Atari 2600 games sequentially.
1",0.45652173913043476,0.17355371900826447,0.25149700598802394,0.08888888888888889,0.03333333333333333,0.048484848484848485,0.32608695652173914,0.12396694214876033,0.17964071856287425,0.015593442921247504,0.66193813
23,"deep neural networks (NNs) have achieved state-of-the-art performance on a wide variety of machine learning tasks. they are becoming increasingly popular in domains such as computer vision, speech recognition, natural language processing, and bioinformatics. NNs are poor at quantifying predictive uncertainty, and tend to produce overconfident predictions. proper uncertainty quantification is crucial for practical applications.","Deep neural networks (NNs) are powerful black box predictors that have recently
achieved impressive performance on a wide spectrum of tasks. Quantifying pre-
dictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian
NNs, which learn a distribution over weights, are currently the state-of-the-art
for estimating predictive uncertainty; however these require signiﬁcant modiﬁca-
tions to the training procedure and are computationally expensive compared to
standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that
is simple to implement, readily parallelizable, requires very little hyperparameter
tuning, and yields high quality predictive uncertainty estimates. Through a series
of experiments on classiﬁcation and regression benchmarks, we demonstrate that
our method produces well-calibrated uncertainty estimates which are as good or
better than approximate Bayesian NNs. To assess robustness to dataset shift, we
evaluate the predictive uncertainty on test examples from known and unknown
distributions, and show that our method is able to express higher uncertainty on
out-of-distribution examples. We demonstrate the scalability of our method by
evaluating predictive uncertainty estimates on ImageNet.
1",0.5862068965517241,0.18681318681318682,0.2833333333333333,0.19298245614035087,0.06077348066298342,0.09243697478991596,0.41379310344827586,0.13186813186813187,0.19999999999999998,0.009704069596829335,0.7095367
24,DeepMind Lab is a first-person 3D game platform built on top of id software’s Quake III Arena. the world is ren- dered with rich science fiction-style visuals. action space allows for fine-grained pointing in a fully 3D world.,"DeepMind Lab is a ﬁrst-person 3D game platform designed for research
and development of general artiﬁcial intelligence and machine learning systems.
DeepMind Lab can be used to study how autonomous artiﬁcial agents may
learn complex tasks in large, partially observed, and visually diverse worlds.
DeepMind Lab has a simple and ﬂexible API enabling creative task-designs
and novel AI-designs to be explored and quickly iterated upon. It is powered
by a fast and widely recognised game engine, and tailored for eﬀective use by
the research community.",0.38095238095238093,0.17582417582417584,0.2406015037593985,0.14634146341463414,0.06666666666666667,0.0916030534351145,0.2857142857142857,0.13186813186813187,0.18045112781954886,0.029734865958559467,0.84113425
25,a conference paper at ICLR 2017 aims to develop agents that can cooperate with others to achieve goals. the most obvious channel of communication is natural language. this is a key step toward the development of AI that can thrive in a world populated by other agents.,"The current mainstream approach to train natural language systems is to expose
them to large amounts of text. This passive learning is problematic if we are in-
terested in developing interactive machines, such as conversational agents. We
propose a framework for language learning that relies on multi-agent communi-
cation. We study this learning in the context of referential games. In these games,
a sender and a receiver see a pair of images. The sender is told one of them is
the target and is allowed to send a message from a ﬁxed, arbitary vocabulary to
the receiver. The receiver must rely on this message to identify the target. Thus,
the agents develop their own language interactively out of the need to communi-
cate. We show that two networks with simple conﬁgurations are able to learn to
coordinate in the referential game. We further explore how to make changes to the
game environment to cause the “word meanings” induced in the game to better re-
ﬂect intuitive semantic properties of the images. In addition, we present a simple
strategy for grounding the agents’ code into natural language. Both of these are
necessary steps towards developing machines that are able to communicate with
humans productively.
1",0.5319148936170213,0.12135922330097088,0.19762845849802374,0.043478260869565216,0.00975609756097561,0.01593625498007968,0.3404255319148936,0.07766990291262135,0.12648221343873514,0.0005271645866997286,0.62470114
26,"PathNet: Evolution Channels Gradient Descent in Super Neural Networks Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, daan Wierstra Google DeepMind, London, UK. we propose that each user of the giant net be given a population of agents whose job is to learn the user-defined task as efficiently as possible.","For artiﬁcial general intelligence (AGI) it would be eﬃcient
if multiple users trained the same giant neural network, per-
mitting parameter reuse, without catastrophic forgetting.
PathNet is a ﬁrst step in this direction. It is a neural net-
work algorithm that uses agents embedded in the neural net-
work whose task is to discover which parts of the network to
re-use for new tasks. Agents are pathways (views) through
the network which determine the subset of parameters that
are used and updated by the forwards and backwards passes
of the backpropogation algorithm. During learning, a tour-
nament selection genetic algorithm is used to select path-
ways through the neural network for replication and muta-
tion. Pathway ﬁtness is the performance of that pathway
measured according to a cost function.
We demonstrate
successful transfer learning; ﬁxing the parameters along a
path learned on task A and re-evolving a new population
of paths for task B, allows task B to be learned faster than
it could be learned from scratch or after ﬁne-tuning. Paths
evolved on task B re-use parts of the optimal path evolved
on task A. Positive transfer was demonstrated for binary
MNIST, CIFAR, and SVHN supervised learning classiﬁca-
tion tasks, and a set of Atari and Labyrinth reinforcement
learning tasks, suggesting PathNets have general applicabil-
ity for neural network training. Finally, PathNet also signif-
icantly improves the robustness to hyperparameter choices
of a parallel asynchronous reinforcement learning algorithm
(A3C).",0.39655172413793105,0.0931174089068826,0.15081967213114755,0.07017543859649122,0.016260162601626018,0.026402640264026406,0.2413793103448276,0.05668016194331984,0.09180327868852459,0.0006050504849895649,0.60845333
27,"Generative Temporal Models (GTMs) are a core requirement for these applications. these tasks require models of high-dimensional observation sequences. they permit counterfactual reasoning, physical predictions, robot localisation, and simulation-based planning among other capacities.","We consider the general problem of modeling temporal data with long-range de-
pendencies, wherein new observations are fully or partially predictable based on
temporally-distant, past observations. A sufﬁciently powerful temporal model
should separate predictable elements of the sequence from unpredictable ele-
ments, express uncertainty about those unpredictable elements, and rapidly iden-
tify novel elements that may help to predict the future. To create such models,
we introduce Generative Temporal Models augmented with external memory sys-
tems. They are developed within the variational inference framework, which pro-
vides both a practical training methodology and methods to gain insight into the
models’ operation. We show, on a range of problems with sparse, long-term tem-
poral dependencies, that these models store information from early in a sequence,
and reuse this stored information efﬁciently. This allows them to perform substan-
tially better than existing models based on well-known recurrent neural networks,
like LSTMs.
1",0.4,0.08974358974358974,0.14659685863874347,0.058823529411764705,0.012903225806451613,0.021164021164021163,0.2857142857142857,0.0641025641025641,0.10471204188481674,0.0010617201619937246,0.5798081
28,"k-NN based methods have become viable for many different problems. one challenge that all nearest neighbor methods share is finding good representations and distance measures between samples. this can make all the difference to the success 1DeepMind, London, UK.","Nearest neighbor (k-NN) methods have been gain-
ing popularity in recent years in light of advances
in hardware and efﬁciency of algorithms. There
is a plethora of methods to choose from today,
each with their own advantages and disadvantages.
One requirement shared between all k-NN based
methods is the need for a good representation and
distance measure between samples.
We introduce a new method called differentiable
boundary tree which allows for learning deep k-
NN representations. We build on the recently
proposed boundary tree algorithm which allows
for efﬁcient nearest neighbor classiﬁcation, re-
gression and retrieval. By modelling traversals
in the tree as stochastic events, we are able to
form a differentiable cost function which is asso-
ciated with the tree’s predictions. Using a deep
neural network to transform the data and back-
propagating through the tree allows us to learn
good representations for k-NN methods.
We demonstrate that our method is able to learn
suitable representations allowing for very efﬁcient
trees with a clearly interpretable structure.",0.6,0.13714285714285715,0.22325581395348837,0.28205128205128205,0.06321839080459771,0.10328638497652584,0.45,0.10285714285714286,0.16744186046511628,0.0019378365397470327,0.6213068
29,understanding synthetic Gradients and decoupled Neural Interfaces (SGs) can be represented as a graph of compu- tational modules. training these networks amounts to optimising the weights associated with the modules of this graph to minimise a loss.,"When training neural networks, the use of Syn-
thetic Gradients (SG) allows layers or modules
to be trained without update locking – without
waiting for a true error gradient to be backprop-
agated – resulting in Decoupled Neural Inter-
faces (DNIs).
This unlocked ability of being
able to update parts of a neural network asyn-
chronously and with only local information was
demonstrated to work empirically in Jaderberg
et al. (2016). However, there has been very lit-
tle demonstration of what changes DNIs and SGs
impose from a functional, representational, and
learning dynamics point of view. In this paper,
we study DNIs through the use of synthetic gra-
dients on feed-forward networks to better under-
stand their behaviour and elucidate their effect
on optimisation.
We show that the incorpora-
tion of SGs does not affect the representational
strength of the learning system for a neural net-
work, and prove the convergence of the learning
system for linear and deep linear models.
On
practical problems we investigate the mechanism
by which synthetic gradient estimators approx-
imate the true loss, and, surprisingly, how that
leads to drastically different layer-wise represen-
tations. Finally, we also expose the relationship
of using synthetic gradients to other error ap-
proximation techniques and ﬁnd a unifying lan-
guage for discussion and comparison.",0.6216216216216216,0.10697674418604651,0.18253968253968256,0.05555555555555555,0.009345794392523364,0.016,0.3783783783783784,0.06511627906976744,0.1111111111111111,8.404207006546877e-05,0.6069066
30,"the framework we propose takes inspiration from feudal reinforcement learning (FRL) introduced by dayan & Hin- ton (1993), where levels of hierarchy within an agent com- municate via explicit goals. a level in the hierarchy communicates to the level below it what must be achieved, but does not specify how to do so.","We introduce FeUdal Networks (FuNs): a novel
architecture for hierarchical reinforcement learn-
ing. Our approach is inspired by the feudal rein-
forcement learning proposal of Dayan and Hin-
ton, and gains power and efﬁcacy by decou-
pling end-to-end learning across multiple levels
– allowing it to utilise different resolutions of
time. Our framework employs a Manager mod-
ule and a Worker module. The Manager operates
at a lower temporal resolution and sets abstract
goals which are conveyed to and enacted by the
Worker. The Worker generates primitive actions
at every tick of the environment. The decoupled
structure of FuN conveys several beneﬁts – in ad-
dition to facilitating very long timescale credit
assignment it also encourages the emergence of
sub-policies associated with different goals set
by the Manager. These properties allow FuN to
dramatically outperform a strong baseline agent
on tasks that involve long-term credit assignment
or memorisation.
We demonstrate the perfor-
mance of our proposed system on a range of tasks
from the ATARI suite and also from a 3D Deep-
Mind Lab environment.",0.4807692307692308,0.13966480446927373,0.21645021645021645,0.0392156862745098,0.011235955056179775,0.017467248908296942,0.3076923076923077,0.0893854748603352,0.13852813852813853,0.0007118959888524808,0.65091807
31,"Count-Based Exploration with Neural Density Models Georg Ostrovski 1 Marc G. Bellemare 1 A aron van den Oord 1 R emi Munos 1 1 Introduction Exploration involves reducing the agent’s uncertainty about the environment’s transition dynamics and attainable rewards. from a theoretical perspective, exploration is now well- understood.","Bellemare et al. (2016) introduced the notion of
a pseudo-count, derived from a density model,
to generalize count-based exploration to non-
tabular reinforcement learning.
This pseudo-
count was used to generate an exploration bonus
for a DQN agent and combined with a mixed
Monte Carlo update was sufﬁcient to achieve
state of the art on the Atari 2600 game Mon-
tezuma’s Revenge. We consider two questions
left open by their work: First, how important is
the quality of the density model for exploration?
Second, what role does the Monte Carlo update
play in exploration? We answer the ﬁrst question
by demonstrating the use of PixelCNN, an ad-
vanced neural density model for images, to sup-
ply a pseudo-count. In particular, we examine the
intrinsic difﬁculties in adapting Bellemare et al.’s
approach when assumptions about the model are
violated. The result is a more practical and gen-
eral algorithm requiring no special apparatus. We
combine PixelCNN pseudo-counts with different
agent architectures to dramatically improve the
state of the art on several hard Atari games. One
surprising ﬁnding is that the mixed Monte Carlo
update is a powerful facilitator of exploration in
the sparsest of settings, including Montezuma’s
Revenge.",0.4117647058823529,0.10194174757281553,0.16342412451361868,0.12,0.02926829268292683,0.04705882352941177,0.29411764705882354,0.07281553398058252,0.11673151750972761,0.0006453683818875129,0.67256945
32,"discriminative and generative models are a more natural fit for generative and discriminative linear models. we use a generative model that shares many parameters across classes and evaluate its performance in this setting. the model uses an LSTM (Hochreiter & Schmidhuber, 1997) to process documents as sequences of words.","We empirically characterize the performance of discriminative and generative
LSTM models for text classiﬁcation. We ﬁnd that although RNN-based gener-
ative models are more powerful than their bag-of-words ancestors (e.g., they ac-
count for conditional dependencies across words in a document), they have higher
asymptotic error rates than discriminatively trained RNN models. However we
also ﬁnd that generative models approach their asymptotic error rate more rapidly
than their discriminative counterparts—the same pattern that Ng & Jordan (2001)
proved holds for linear classiﬁcation models that make more na¨ıve conditional
independence assumptions. Building on this ﬁnding, we hypothesize that RNN-
based generative classiﬁcation models will be more robust to shifts in the data
distribution. This hypothesis is conﬁrmed in a series of experiments in zero-shot
and continual learning settings that show that generative models substantially out-
perform discriminative models.
1",0.625,0.20270270270270271,0.30612244897959184,0.14893617021276595,0.047619047619047616,0.07216494845360824,0.375,0.12162162162162163,0.18367346938775514,0.006149849580735398,0.7238525
33,"deep reinforcement learning agents have achieved state-of-the-art results in a variety of complex environments (Mnih et al., 2015; 2016), often surpassing human perfor- mance. deep Q-networks require more than 200 hours of gameplay in order to achieve scores similar to those achieved by a human player after two hours. neural episodic control (NEC) is a method which tackles the limitations of deep reinforce-ment learning listed above.","Deep reinforcement learning methods attain
super-human performance in a wide range of en-
vironments. Such methods are grossly inefﬁcient,
often taking orders of magnitudes more data than
humans to achieve reasonable performance. We
propose Neural Episodic Control: a deep rein-
forcement learning agent that is able to rapidly
assimilate new experiences and act upon them.
Our agent uses a semi-tabular representation of
the value function: a buffer of past experience con-
taining slowly changing state representations and
rapidly updated estimates of the value function.
We show across a wide range of environments
that our agent learns signiﬁcantly faster than other
state-of-the-art, general purpose deep reinforce-
ment learning agents.",0.5070422535211268,0.3130434782608696,0.3870967741935485,0.18571428571428572,0.11403508771929824,0.14130434782608695,0.30985915492957744,0.19130434782608696,0.23655913978494622,0.007310910506522652,0.7204725
34,"parallel multiscale image generation provides one such way to break weak dependencies. if we merge factors for, e.g. xi and xj, then that dependency is “cut”, so the model becomes slightly less expressive. in this work we show how a very substantial portion of the spatial dependencies in PixelCNN can be cut, with only modest degradation in performance.","PixelCNN achieves state-of-the-art results in
density estimation for natural images. Although
training is fast, inference is costly, requiring one
network evaluation per pixel; O(N) for N pix-
els. This can be sped up by caching activations,
but still involves generating each pixel sequen-
tially. In this work, we propose a parallelized
PixelCNN that allows more eﬃcient inference
by modeling certain pixel groups as condition-
ally independent.
Our new PixelCNN model
achieves competitive density estimation and or-
ders of magnitude speedup - O(log N) sampling
instead of O(N) - enabling the practical genera-
tion of 512 × 512 images. We evaluate the model
on class-conditional image generation, text-to-
image synthesis, and action-conditional video
generation, showing that our model achieves the
best results among non-pixel-autoregressive den-
sity models that allow eﬃcient sampling.",0.423728813559322,0.17857142857142858,0.25125628140703515,0.1206896551724138,0.050359712230215826,0.07106598984771574,0.1694915254237288,0.07142857142857142,0.10050251256281406,0.0038476886187773104,0.6272307
35,"end-to-end optimization of goal-driven and visually grounded dialogue systems has been a long-standing goal of Artificial Intelligence (AI). supervised learning does not account for the intrinsic planning problem that underlies dialogue, i.e. the sequential decision making pro- cess, which makes dialogue consistent over time.","End-to-end design of dialogue systems has recently
become a popular research topic thanks to power-
ful tools such as encoder-decoder architectures for
sequence-to-sequence learning. Yet, most current
approaches cast human-machine dialogue manage-
ment as a supervised learning problem, aiming at
predicting the next utterance of a participant given
the full history of the dialogue. This vision is too
simplistic to render the intrinsic planning problem
inherent to dialogue as well as its grounded na-
ture, making the context of a dialogue larger than
the sole history.
This is why only chit-chat and
question answering tasks have been addressed so
far using end-to-end architectures. In this paper, we
introduce a Deep Reinforcement Learning method
to optimize visually grounded task-oriented dia-
logues, based on the policy gradient algorithm.
This approach is tested on a dataset of 120k dia-
logues collected through Mechanical Turk and pro-
vides encouraging results at solving both the prob-
lem of generating natural dialogues and the task of
discovering a speciﬁc object in a complex picture.
1",0.5306122448979592,0.14444444444444443,0.22707423580786024,0.1875,0.05027932960893855,0.07929515418502203,0.3673469387755102,0.1,0.1572052401746725,0.006369873127486219,0.74134356
36,"ICLR 2017 RECURRENT ENVIRONMENT SIMULATORS Silvia Chiappa, Sébastien Racaniere, Daan Wierstra & Shakir Mohamed DeepMind, London, UK. the need for environment simulation is widespread: in psy- chology, model-based predictive abilities form sensorimotor contingencies that are seen as essential for perception. in reinforcement learning, the ability to imagine the future evolution of an environment is needed to form predictive state representations.","Models that can simulate how environments change in response to actions can be
used by agents to plan and act efﬁciently. We improve on previous environment
simulators from high-dimensional pixel observations by introducing recurrent
neural networks that are able to make temporally and spatially coherent predictions
for hundreds of time-steps into the future. We present an in-depth analysis of the
factors affecting performance, providing the most extensive attempt to advance
the understanding of the properties of these models. We address the issue of
computationally inefﬁciency with a model that does not need to generate a high-
dimensional image at each time-step. We show that our approach can be used to
improve exploration and is adaptable to many diverse environments, namely 10
Atari games, a 3D car racing environment, and complex 3D mazes.
1",0.3770491803278688,0.16546762589928057,0.23,0.06666666666666667,0.028985507246376812,0.0404040404040404,0.21311475409836064,0.09352517985611511,0.12999999999999998,0.003097874797171048,0.66144073
37,"generative models can be classified into explicit models where we have access to the model likelihood function, and implicit models which pro- vide a sampling mechanism for generating data. examples of explicit models are variational auto-encoders (VAEs) (Kingma & Welling, 2013) and Pix- elCNN (Oord et al., 2016)","We train a generator by maximum likelihood and
we also train the same generator architecture by
Wasserstein GAN. We then compare the gener-
ated samples, exact log-probability densities and
approximate Wasserstein distances.
We show
that an independent critic trained to approximate
Wasserstein distance between the validation set
and the generator distribution helps detect over-
ﬁtting. Finally, we use ideas from the one-shot
learning literature to develop a novel fast learn-
ing critic.",0.20833333333333334,0.13513513513513514,0.1639344262295082,0.0,0.0,0.0,0.125,0.08108108108108109,0.09836065573770492,0.003773982733365776,0.4261576
38,the HMDB-51 and UCF-101 datasets have emerged as the standard benchmarks for human action classification. they are not large enough or have suf- ficient variation to train and test the current generation of human actions classification models based on deep learning. this is because each clip is taken from a different video.,"We describe the DeepMind Kinetics human action video
dataset. The dataset contains 400 human action classes,
with at least 400 video clips for each action. Each clip lasts
around 10s and is taken from a different YouTube video. The
actions are human focussed and cover a broad range of
classes including human-object interactions such as play-
ing instruments, as well as human-human interactions such
as shaking hands. We describe the statistics of the dataset,
how it was collected, and give some baseline performance
ﬁgures for neural network architectures trained and tested
for human action classiﬁcation on this dataset. We also
carry out a preliminary analysis of whether imbalance in
the dataset leads to bias in the classiﬁers.",0.5,0.2231404958677686,0.30857142857142855,0.18867924528301888,0.08333333333333333,0.11560693641618497,0.25925925925925924,0.11570247933884298,0.16,0.03306055320395881,0.7479754
39,"the new Kinetics Human Action Video Dataset is two orders of magnitude larger than previous datasets, HMDB-51 and UCF-101. the results suggest that there is always a boost in performance by pre-training, but the ex- tent of the boost varies significantly with the type of architecture.","The paucity of videos in current action classiﬁcation
datasets (UCF-101 and HMDB-51) has made it difﬁcult
to identify good video architectures, as most methods ob-
tain similar performance on existing small-scale bench-
marks.
This paper re-evaluates state-of-the-art architec-
tures in light of the new Kinetics Human Action Video
dataset. Kinetics has two orders of magnitude more data,
with 400 human action classes and over 400 clips per
class, and is collected from realistic, challenging YouTube
videos. We provide an analysis on how current architectures
fare on the task of action classiﬁcation on this dataset and
how much performance improves on the smaller benchmark
datasets after pre-training on Kinetics.
We also introduce a new Two-Stream Inﬂated 3D Con-
vNet (I3D) that is based on 2D ConvNet inﬂation: ﬁl-
ters and pooling kernels of very deep image classiﬁca-
tion ConvNets are expanded into 3D, making it possible
to learn seamless spatio-temporal feature extractors from
video while leveraging successful ImageNet architecture
designs and even their parameters.
We show that, after
pre-training on Kinetics, I3D models considerably improve
upon the state-of-the-art in action classiﬁcation, reaching
80.9% on HMDB-51 and 98.0% on UCF-101.",0.673469387755102,0.15492957746478872,0.25190839694656486,0.2916666666666667,0.0660377358490566,0.10769230769230771,0.4489795918367347,0.10328638497652583,0.16793893129770993,0.009029563571673454,0.7463006
40,"audio-visual correspondence detector network Yes / No Figure 1. a network should learn to determine whether a pair of (video frame, short audio clip) correspond to each other or not. visual and audio events are concurrent in these cases because there is a common cause.","We consider the question: what can be learnt by look-
ing at and listening to a large number of unlabelled videos?
There is a valuable, but so far untapped, source of infor-
mation contained in the video itself – the correspondence
between the visual and the audio streams, and we intro-
duce a novel “Audio-Visual Correspondence” learning task
that makes use of this. Training visual and audio networks
from scratch, without any additional supervision other than
the raw unconstrained videos themselves, is shown to suc-
cessfully solve this task, and, more interestingly, result in
good visual and audio representations. These features set
the new state-of-the-art on two sound classiﬁcation bench-
marks, and perform on par with the state-of-the-art self-
supervised approaches on ImageNet classiﬁcation. We also
demonstrate that the network is able to localize objects in
both modalities, as well as perform ﬁne-grained recognition
tasks.",0.5111111111111111,0.14935064935064934,0.23115577889447234,0.13636363636363635,0.0392156862745098,0.06091370558375635,0.26666666666666666,0.07792207792207792,0.1206030150753769,0.003629598156780656,0.58881354
41,"Reinforcement Learning with a Corrupted Reward Channel Tom Everitt1, Victoria Krakovna2, Laurent Orseau2, Marcus Hutter1, and Shane Legg2 1Australian National University 2DeepMind August 22, 2017 Introduction 2 2 Formalisation 3 3 The Corrupt Reward Problem 4 3.1 No Free Lunch Theorem..","No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents
observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may
prefer states where a sensory error gives it the maximum reward, but where the true reward is actually
small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP.
Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when
trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated.
First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised
reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be
completely managed. Second, by using randomisation to blunt the agent’s optimisation, reward corruption
can be partially managed under some assumptions.
Contents
1",0.30952380952380953,0.0896551724137931,0.13903743315508021,0.07317073170731707,0.020833333333333332,0.032432432432432434,0.2619047619047619,0.07586206896551724,0.11764705882352941,0.0010672530814980884,0.6727752
42,"filtering variational objectives (FIVOs) is a tractable family of objectives for maximum likelihood estimation (MLE) in latent variable models with sequential structure. the goal of MLE is to recover p P that maximizes the marginal log-likelihood, log p(x) = log.","When used as a surrogate objective for maximum likelihood estimation in latent
variable models, the evidence lower bound (ELBO) produces state-of-the-art results.
Inspired by this, we consider the extension of the ELBO to a family of lower bounds
deﬁned by a particle ﬁlter’s estimator of the marginal likelihood, the ﬁltering
variational objectives (FIVOs). FIVOs take the same arguments as the ELBO,
but can exploit a model’s sequential structure to form tighter bounds. We present
results that relate the tightness of FIVO’s bound to the variance of the particle ﬁlter’s
estimator by considering the generic case of bounds deﬁned as log-transformed
likelihood estimators. Experimentally, we show that training with FIVO results
in substantial improvements over training the same model architecture with the
ELBO on sequential data.
1",0.6341463414634146,0.19117647058823528,0.2937853107344633,0.3,0.08888888888888889,0.13714285714285715,0.43902439024390244,0.1323529411764706,0.20338983050847456,0.014167711136176707,0.66621745
43,position-velocity encoders (PVEs) learn to encode images to positions and velocities of task-relevant objects. they compute the velocity state from finite differences in position. iology Laboratory at the Technische Universit at Berlin Abstract.,"PVEs: Position-Velocity Encoders for Unsupervised
Learning of Structured State Representations
Rico Jonschkowski1,2, Roland Hafner1, Jonathan Scholz1, and Martin Riedmiller1
1DeepMind, 2Robotics and B",0.22857142857142856,0.32,0.26666666666666666,0.058823529411764705,0.08333333333333333,0.06896551724137931,0.17142857142857143,0.24,0.19999999999999998,0.0067177027389942,0.6742616
44,"the KL divergence suffers from a significant limitation: it does not take into account how close two outcomes might be, but only their relative probability. estimating the Wasserstein metric from samples yields biased gradients, and may actually lead to the wrong minimum.","The Wasserstein probability metric has received much attention from the machine
learning community. Unlike the Kullback-Leibler divergence, which strictly mea-
sures change in probability, the Wasserstein metric reﬂects the underlying geom-
etry between outcomes. The value of being sensitive to this geometry has been
demonstrated, among others, in ordinal regression and generative modelling. In
this paper we describe three natural properties of probability divergences that re-
ﬂect requirements from machine learning: sum invariance, scale sensitivity, and
unbiased sample gradients. The Wasserstein metric possesses the ﬁrst two prop-
erties but, unlike the Kullback-Leibler divergence, does not possess the third.
We provide empirical evidence suggesting that this is a serious issue in practice.
Leveraging insights from probabilistic forecasting we propose an alternative to
the Wasserstein metric, the Cramér distance. We show that the Cramér distance
possesses all three desired properties, combining the best of the Wasserstein and
Kullback-Leibler divergences. To illustrate the relevance of the Cramér distance in
practice we design a new algorithm, the Cramér Generative Adversarial Network
(GAN), and show that it performs signiﬁcantly better than the related Wasserstein
GAN.
1",0.5,0.11052631578947368,0.1810344827586207,0.0975609756097561,0.021164021164021163,0.03478260869565217,0.2857142857142857,0.06315789473684211,0.10344827586206898,0.0013881923353003627,0.60800886
45,"RNs are simple, plug-and-play, and are exclusively focused on flexible relational reasoning. the ability to reason about the relations between entities and their properties is central to generally intelligent behavior. a number of these approaches, such as deep learning, struggle in data-poor problems where the underlying structure is characterized by sparse but complex relations.","Relational reasoning is a central component of generally intelligent behavior, but has proven
diﬃcult for neural networks to learn. In this paper we describe how to use Relation Networks
(RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational
reasoning.
We tested RN-augmented networks on three tasks: visual question answering
using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human
performance; text-based question answering using the bAbI suite of tasks; and complex reasoning
about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show
that powerful convolutional networks do not have a general capacity to solve relational questions,
but can gain this capacity when augmented with RNs. Our work shows how a deep learning
architecture equipped with an RN module can implicitly discover and learn to reason about
entities and their relations.
1",0.5964912280701754,0.2236842105263158,0.3253588516746412,0.19642857142857142,0.0728476821192053,0.10628019323671496,0.2807017543859649,0.10526315789473684,0.15311004784688995,0.008812328322386214,0.66314304
46,Visual Interaction Network (VIN) is a general-purpose model for predicting future physical states from video data. the model is learnable and can be trained from supervised data sequences which consist of input image frames and target object state values. a third class of methods have been used to predict future state descriptions from pixels.,"From just a glance, humans can make rich predictions about the future state of a wide range of
physical systems. On the other hand, modern approaches from engineering, robotics, and graphics
are often restricted to narrow domains and require direct measurements of the underlying states.
We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics
of a physical system from raw visual observations. Our model consists of a perceptual front-end
based on convolutional neural networks and a dynamics predictor based on interaction networks.
Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of
factored latent object representations. The dynamics predictor learns to roll these states forward in
time by computing their interactions and dynamics, producing a predicted physical trajectory of
arbitrary length. We found that from just six input video frames the Visual Interaction Network
can generate accurate future trajectories of hundreds of time steps on a wide range of physical
systems. Our model can also be applied to scenes with invisible objects, inferring their future states
from their eﬀects on the visible objects, and can implicitly infer the unknown mass of objects. Our
results demonstrate that the perceptual module and the object-based dynamics predictor module can
induce factored latent representations that support accurate dynamical predictions. This work opens
new opportunities for model-based decision-making and planning from raw sensory observations in
complex physical environments.
1",0.6363636363636364,0.1446280991735537,0.23569023569023567,0.18518518518518517,0.04149377593360996,0.06779661016949153,0.4,0.09090909090909091,0.14814814814814814,0.003426894694931516,0.65722877
47,"deep reinforcement learning from human preferences. many tasks involve goals that are complex, poorly-defined, or hard to specify. overcoming this limitation would greatly expand the possible impact of deep RL. if we could successfully communicate our actual objectives, it would be a significant step towards addressing these concerns.","For sophisticated reinforcement learning (RL) systems to interact usefully with
real-world environments, we need to communicate complex goals to these systems.
In this work, we explore goals deﬁned in terms of (non-expert) human preferences
between pairs of trajectory segments. We show that this approach can effectively
solve complex RL tasks without access to the reward function, including Atari
games and simulated robot locomotion, while providing feedback on less than
1% of our agent’s interactions with the environment. This reduces the cost of
human oversight far enough that it can be practically applied to state-of-the-art
RL systems. To demonstrate the ﬂexibility of our approach, we show that we can
successfully train complex novel behaviors with about an hour of human time.
These behaviors and environments are considerably more complex than any which
have been previously learned from human feedback.
1",0.4489795918367347,0.1506849315068493,0.22564102564102564,0.08333333333333333,0.027586206896551724,0.04145077720207254,0.2857142857142857,0.0958904109589041,0.14358974358974358,0.0025366969322194095,0.609113
48,"a large class of GAN variants that aim to address this problem are auto-encoder-based GANs (AE-GANs) that use an auto-endcoder to encourage the model to better represent all the data it is trained with, thus discouraging mode-collapse. the approach will be advantageous since it allows us to overcome the limitations of each of these methods.","Auto-encoding generative adversarial networks (GANs) combine the standard
GAN algorithm, which discriminates between real and model-generated data, with
a reconstruction loss given by an auto-encoder. Such models aim to prevent mode
collapse in the learned generative model by ensuring that it is grounded in all the
available training data. In this paper, we develop a principle upon which auto-
encoders can be combined with generative adversarial networks by exploiting the
hierarchical structure of the generative model. The underlying principle shows
that variational inference can be used a basic tool for learning, but with the in-
tractable likelihood replaced by a synthetic likelihood, and the unknown posterior
distribution replaced by an implicit distribution; both synthetic likelihoods and
implicit posterior distributions can be learned using discriminators. This allows us
to develop a natural fusion of variational auto-encoders and generative adversarial
networks, combining the best of both these methods. We describe a uniﬁed objec-
tive for optimization, discuss the constraints needed to guide learning, connect to
the wide range of existing work, and use a battery of tests to systematically and
quantitatively assess the performance of our method.
1",0.6166666666666667,0.193717277486911,0.2948207171314741,0.15254237288135594,0.04736842105263158,0.07228915662650602,0.36666666666666664,0.11518324607329843,0.17529880478087648,0.0034966962802144806,0.6391078
49,value-decomposition networks for cooperative multi-agent learning (MARL) problems emerge in applications such as coordinating self-driving vehicles and/or traffic signals in a transportation system. each agent has access to its own (“local”) observations and is responsible for choosing actions from its own action set.,"We study the problem of cooperative multi-agent reinforcement learning with a
single joint reward signal. This class of learning problems is difﬁcult because of
the often large combined action and observation spaces. In the fully centralized
and decentralized approaches, we ﬁnd the problem of spurious rewards and a
phenomenon we call the “lazy agent” problem, which arises due to partial observ-
ability. We address these problems by training individual agents with a novel value
decomposition network architecture, which learns to decompose the team value
function into agent-wise value functions. We perform an experimental evaluation
across a range of partially-observable multi-agent domains and show that learning
such value-decompositions leads to superior results, in particular when combined
with weight sharing, role information and information channels.
1",0.425531914893617,0.15384615384615385,0.22598870056497175,0.08695652173913043,0.031007751937984496,0.04571428571428571,0.2553191489361702,0.09230769230769231,0.13559322033898305,0.0021104164691456225,0.67534983
50,programable agents learn to ground the terms of the language in their environment. they learn to distinguish distinct properties that are referenced together during training. when trained on tasks that always reference objects through a conjunction of shape and color they can generalize to tasks that reference objects in isolation.,"We build deep RL agents that execute declarative programs expressed in formal lan-
guage. The agents learn to ground the terms in this language in their environment,
and can generalize their behavior at test time to execute new programs that refer to
objects that were not referenced during training. The agents develop disentangled
interpretable representations that allow them to generalize to a wide variety of
zero-shot semantic tasks.
1",0.58,0.4142857142857143,0.4833333333333334,0.24489795918367346,0.17391304347826086,0.20338983050847456,0.4,0.2857142857142857,0.3333333333333333,0.10924898912298188,0.619554
51,"the development of computational approaches to under- standing grounded language has become a long-standing challenge for human-AI interaction. the approach differs from conventional computational language learning in that the learning and understanding take place with respect to a continuous, situated environment.","We are increasingly surrounded by artiﬁcially intelligent technology that takes decisions
and executes actions on our behalf.
This creates a pressing need for general means to
communicate with, instruct and guide artiﬁcial agents, with human language the most
compelling means for such communication. To achieve this in a scalable fashion, agents
must be able to relate language to the world and to actions; that is, their understanding
of language must be grounded and embodied. However, learning grounded language is a
notoriously challenging problem in artiﬁcial intelligence research. Here we present an agent
that learns to interpret language in a simulated 3D environment where it is rewarded for
the successful execution of written instructions. Trained via a combination of reinforcement
and unsupervised learning, and beginning with minimal prior knowledge, the agent learns to
relate linguistic symbols to emergent perceptual representations of its physical surroundings
and to pertinent sequences of actions.
The agent’s comprehension of language extends
beyond its prior experience, enabling it to apply familiar language to unfamiliar situations
and to interpret entirely novel instructions. Moreover, the speed with which this agent
learns new words increases as its semantic knowledge grows. This facility for generalising
and bootstrapping semantic knowledge indicates the potential of the present approach for
reconciling ambiguous natural language with the complexity of the physical world.",0.5813953488372093,0.11312217194570136,0.1893939393939394,0.047619047619047616,0.00909090909090909,0.015267175572519082,0.32558139534883723,0.06334841628959276,0.10606060606060605,0.00022854771290776996,0.6663628
52,"research on human and animal learning makes clear that this simple definition is not enough to explain the observed rela- tionship between experience and performance. the full pic- ture must also include ‘learning-to-learn,’ a process whereby growing experience causes learning itself to become more ef- ficient.","Motor adaptation displays a structure-learning effect: adapta-
tion to a new perturbation occurs more quickly when the sub-
ject has prior exposure to perturbations with related structure.
Although this ‘learning-to-learn’ effect is well documented, its
underlying computational mechanisms are poorly understood.
We present a new model of motor structure learning, approach-
ing it from the point of view of deep reinforcement learning.
Previous work outside of motor control has shown how recur-
rent neural networks can account for learning-to-learn effects.
We leverage this insight to address motor learning, by import-
ing it into the setting of model-based reinforcement learning.
We apply the resulting processing architecture to empirical
ﬁndings from a landmark study of structure learning in target-
directed reaching (Braun et al., 2009), and discuss its implica-
tions for a wider range of learning-to-learn phenomena.",0.3125,0.1048951048951049,0.15706806282722513,0.0425531914893617,0.014084507042253521,0.021164021164021166,0.22916666666666666,0.07692307692307693,0.11518324607329843,0.0010268611424279653,0.40025455
53,"most exploration heuristics rely on random perturbations of the agent’s policy, such as -greedy (Sutton & Barto, 1998) or entropy regularisation (Williams, 1992), to induce novel behaviours. however such local ‘dithering’ perturbations are unlikely to lead to the large-scale behavioural patterns needed for efficient exploration.","We introduce NoisyNet, a deep reinforcement learning agent with parametric noise
added to its weights, and show that the induced stochasticity of the agent’s policy
can be used to aid efﬁcient exploration. The parameters of the noise are learned
with gradient descent along with the remaining network weights. NoisyNet is
straightforward to implement and adds little computational overhead. We ﬁnd that
replacing the conventional exploration heuristics for A3C, DQN and Dueling agents
(entropy reward and ϵ-greedy respectively) with NoisyNet yields substantially
higher scores for a wide range of Atari games, in some cases advancing the agent
from sub to super-human performance.
1",0.3695652173913043,0.1619047619047619,0.2251655629139073,0.1111111111111111,0.04807692307692308,0.06711409395973154,0.21739130434782608,0.09523809523809523,0.13245033112582782,0.008925314893825182,0.6020813
54,"we focus on a set of novel locomotion tasks that go significantly beyond the previous state-of-the-art for agents trained directly from reinforcement learning. they include a variety of obstacle courses for agents with different bodies (Quadruped, Planar Walker, and Humanoid)","The reinforcement learning paradigm allows, in principle, for complex behaviours
to be learned directly from simple reward signals. In practice, however, it is
common to carefully hand-design the reward function to encourage a particular
solution, or to derive it from demonstration data. In this paper explore how a rich
environment can help to promote the learning of complex behavior. Speciﬁcally,
we train agents in diverse environmental contexts, and ﬁnd that this encourages
the emergence of robust behaviours that perform well across a suite of tasks.
We demonstrate this principle for locomotion – behaviours that are known for
their sensitivity to the choice of reward. We train several simulated bodies on a
diverse set of challenging terrains and obstacles, using a simple reward function
based on forward progress. Using a novel scalable variant of policy gradient
reinforcement learning, our agents learn to run, jump, crouch and turn as required
by the environment without explicit reward-based guidance. A visual depiction of
highlights of the learned behavior can be viewed in this video.
1",0.6046511627906976,0.15028901734104047,0.24074074074074073,0.11904761904761904,0.029069767441860465,0.04672897196261682,0.3023255813953488,0.07514450867052024,0.12037037037037036,0.0008648601857264643,0.7561368
55,"we combine several deep generative approaches to imitation learning in a way that accentuates their individual strengths and addresses their limitations. the end product of this is a robust neural network policy that can imitate a large and diverse set of behaviors using few training demonstrations. in section 3, we show how to engineer an objective function that takes advantage of both generative models to obtain robust policies capturing diverse behaviors.","Deep generative models have recently shown great promise in imitation learning
for motor control. Given enough data, even supervised approaches can do one-shot
imitation learning; however, they are vulnerable to cascading failures when the
agent trajectory diverges from the demonstrations. Compared to purely supervised
methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust
controllers from fewer demonstrations, but is inherently mode-seeking and more
difﬁcult to train. In this paper, we show how to combine the favourable aspects
of these two approaches. The base of our model is a new type of variational
autoencoder on demonstration trajectories that learns semantic policy embeddings.
We show that these embeddings can be learned on a 9 DoF Jaco robot arm in
reaching tasks, and then smoothly interpolated with a resulting smooth interpolation
of reaching behavior. Leveraging these policy representations, we develop a new
version of GAIL that (1) is much more robust than the purely-supervised controller,
especially with few demonstrations, and (2) avoids mode collapse, capturing many
diverse behaviors when GAIL on its own does not. We demonstrate our approach
on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D
humanoid in the MuJoCo physics environment.
1",0.6338028169014085,0.22167487684729065,0.3284671532846715,0.11428571428571428,0.039603960396039604,0.058823529411764705,0.3380281690140845,0.11822660098522167,0.17518248175182483,0.011032639641172683,0.7852653
56,a deep actor-critic architecture consists of two neural networks. the actor neural network has multiple-heads representing different policies with shared lower-level representations. a simple embodied agent is used to generate semantic goals for the agent.,"The Intentional Unintentional Agent:
Learning to Solve Many Continuous Control Tasks
Simultaneously
Serkan Cabi
Sergio Gómez Colmenarejo
Matthew W. Hoffman
Misha Denil
Ziyu Wang
Nando de Freitas
DeepM",0.07894736842105263,0.10344827586206896,0.08955223880597013,0.0,0.0,0.0,0.07894736842105263,0.10344827586206896,0.08955223880597013,0.005311256555131655,0.49646232
57,we propose that concepts are abstractions over a set of primitives. each parent concept in this hierarchy is an 1 arXiv:1707.03389v3 [stat.ML] 6 Jun 2018 Published as a conference paper at ICLR 2018 superordinate basic subordinate.,"The seemingly inﬁnite diversity of the natural world arises from a relatively small
set of coherent rules, such as the laws of physics or chemistry. We conjecture that
these rules give rise to regularities that can be discovered through primarily unsuper-
vised experiences and represented as abstract concepts. If such representations are
compositional and hierarchical, they can be recombined into an exponentially large
set of new concepts. This paper describes SCAN (Symbol-Concept Association
Network), a new framework for learning such abstractions in the visual domain.
SCAN learns concepts through fast symbol association, grounding them in disen-
tangled visual primitives that are discovered in an unsupervised manner. Unlike
state of the art multimodal generative model baselines, our approach requires very
few pairings between symbols and images and makes no assumptions about the
form of symbol representations. Once trained, SCAN is capable of multimodal
bi-directional inference, generating a diverse set of image samples from symbolic
descriptions and vice versa. It also allows for traversal and manipulation of the
implicit hierarchy of visual concepts through symbolic instructions and learnt
logical recombination operations. Such manipulations enable SCAN to break
away from its training data distribution and imagine novel visual concepts through
symbolically instructed recombination of previously learnt concepts.
1",0.48717948717948717,0.09134615384615384,0.15384615384615385,0.02631578947368421,0.004830917874396135,0.008163265306122448,0.2564102564102564,0.04807692307692308,0.08097165991902834,0.0001650671475212648,0.3492416
58,"distral is an emerging subfield of Reinforcement Learning (RL) that relies on deep neural networks as function approximators that can scale RL algorithms to complex and rich environments. a different approach was introduced by, whereby data efficiency is improved by training additional auxiliary tasks jointly with the RL task. however, making progress in this direction requires robust algorithms which do not rely on task-specific design or extensive hyperparameter tuning.","Most deep reinforcement learning algorithms are data inefﬁcient in complex and
rich environments, limiting their applicability to many scenarios. One direction
for improving data efﬁciency is multitask learning with shared neural network
parameters, where efﬁciency may be improved through transfer across related tasks.
In practice, however, this is not usually observed, because gradients from different
tasks can interfere negatively, making learning unstable and sometimes even less
data efﬁcient. Another issue is the different reward schemes between tasks, which
can easily lead to one task dominating the learning of a shared model. We propose
a new approach for joint training of multiple tasks, which we refer to as Distral
(Distill & transfer learning). Instead of sharing parameters between the different
workers, we propose to share a “distilled” policy that captures common behaviour
across tasks. Each worker is trained to solve its own task while constrained to
stay close to the shared policy, while the shared policy is trained by distillation
to be the centroid of all task policies. Both aspects of the learning process are
derived by optimizing a joint objective function. We show that our approach
supports efﬁcient transfer on complex 3D environments, outperforming several
related methods. Moreover, the proposed learning process is more robust and more
stable—attributes that are critical in deep reinforcement learning.
1",0.6,0.19004524886877827,0.28865979381443296,0.07246376811594203,0.022727272727272728,0.03460207612456748,0.21428571428571427,0.06787330316742081,0.10309278350515463,0.002821017162491465,0.6369436
59,"model-based planning involves proposing sequences of actions, evaluating them under a model of the world, and refining these proposals to optimize expected rewards. models support generalization to states not previously experienced, help express the relationship between present actions and future rewards, and can resolve states which are aliased in value-based approximations.","Conventional wisdom holds that model-based planning is a powerful approach
to sequential decision-making. It is often very challenging in practice, however,
because while a model can be used to evaluate a plan, it does not prescribe how
to construct a plan. Here we introduce the “Imagination-based Planner”, the
ﬁrst model-based, sequential decision-making agent that can learn to construct,
evaluate, and execute plans. Before any action, it can perform a variable number
of imagination steps, which involve proposing an imagined action and evaluating
it with its model-based imagination. All imagined actions and outcomes are
aggregated, iteratively, into a “plan context” which conditions future real and
imagined actions. The agent can even decide how to imagine: testing out alternative
imagined actions, chaining sequences of actions together, or building a more
complex “imagination tree” by navigating ﬂexibly among the previously imagined
states using a learned policy. And our agent can learn to plan economically, jointly
optimizing for external rewards and computational costs associated with using
its imagination. We show that our architecture can learn to solve a challenging
continuous control problem, and also learn elaborate planning strategies in a
discrete maze-solving task. Our work opens a new direction toward learning the
components of a model-based planning system and how to use them.
1",0.6226415094339622,0.15137614678899083,0.24354243542435425,0.15384615384615385,0.03686635944700461,0.05947955390334572,0.33962264150943394,0.08256880733944955,0.13284132841328414,0.000903206483013009,0.62173533
60,Imagination-Augmented Agents for Deep Reinforcement Learning Théophane WeberSébastien RacanièreDavid P. ReichertLars Buesing Arthur Guez Danilo Rezende Adria Puigdomènech Badia Oriol Vinyals Nicolas Heess Yujia Li Razvan Pascanu Peter Battaglia Demis Hassabis David Silver Daan Wierstra DeepMind Introduction.,"We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep
reinforcement learning combining model-free and model-based aspects. In con-
trast to most existing model-based reinforcement learning and planning methods,
which prescribe how a model should be used to arrive at a policy, I2As learn to
interpret predictions from a learned environment model to construct implicit plans
in arbitrary ways, by using the predictions as additional context in deep policy
networks. I2As show improved data efﬁciency, performance, and robustness to
model misspeciﬁcation compared to several baselines.
1",0.16666666666666666,0.07608695652173914,0.10447761194029852,0.12195121951219512,0.054945054945054944,0.07575757575757576,0.16666666666666666,0.07608695652173914,0.10447761194029852,0.003119757021381248,0.7901296
61,"a multi-agent reinforcement learning model of common-pool resource appropriation has shaped human history. it is difficult or impossible for agents to exclude one another from accessing them. but when an agent obtains an individual benefit from such a resource, the remaining amount available is ever-so-slightly diminished. over-appropriation negatively impacts the stock, and thus has a negative impact on future flow.","Humanity faces numerous problems of common-pool resource appropriation. This
class of multi-agent social dilemma includes the problems of ensuring sustainable
use of fresh water, common ﬁsheries, grazing pastures, and irrigation systems.
Abstract models of common-pool resource appropriation based on non-cooperative
game theory predict that self-interested agents will generally fail to ﬁnd socially
positive equilibria—a phenomenon called the tragedy of the commons. However,
in reality, human societies are sometimes able to discover and implement stable
cooperative solutions. Decades of behavioral game theory research have sought
to uncover aspects of human behavior that make this possible. Most of that work
was based on laboratory experiments where participants only make a single choice:
how much to appropriate. Recognizing the importance of spatial and temporal
resource dynamics, a recent trend has been toward experiments in more complex
real-time video game-like environments. However, standard methods of non-
cooperative game theory can no longer be used to generate predictions for this case.
Here we show that deep reinforcement learning can be used instead. To that end,
we study the emergent behavior of groups of independently learning agents in a
partially observed Markov game modeling common-pool resource appropriation.
Our experiments highlight the importance of trial-and-error learning in common-
pool resource appropriation and shed light on the relationship between exclusion,
sustainability, and inequality.
1",0.38461538461538464,0.11013215859030837,0.17123287671232876,0.109375,0.030973451327433628,0.048275862068965524,0.27692307692307694,0.07929515418502203,0.12328767123287672,0.004458807430744323,0.57443625
62,"a distributional perspective on reinforcement learning is based on a recursive equation. the distributional Bellman equation states that the distribu- tion of Z is characterized by the interaction of three random variables: the reward R, the next state-action (X′, A′), and its random return Z(X′. a′). by analogy with the well- known case, we call this quantity the value distribution.","In this paper we argue for the fundamental impor-
tance of the value distribution: the distribution
of the random return received by a reinforcement
learning agent. This is in contrast to the com-
mon approach to reinforcement learning which
models the expectation of this return, or value.
Although there is an established body of liter-
ature studying the value distribution, thus far it
has always been used for a speciﬁc purpose such
as implementing risk-aware behaviour. We begin
with theoretical results in both the policy eval-
uation and control settings, exposing a signiﬁ-
cant distributional instability in the latter.
We
then use the distributional perspective to design
a new algorithm which applies Bellman’s equa-
tion to the learning of approximate value distri-
butions.
We evaluate our algorithm using the
suite of games from the Arcade Learning En-
vironment.
We obtain both state-of-the-art re-
sults and anecdotal evidence demonstrating the
importance of the value distribution in approxi-
mate reinforcement learning. Finally, we com-
bine theoretical and empirical evidence to high-
light the ways in which the value distribution im-
pacts learning in the approximate setting.",0.5161290322580645,0.16842105263157894,0.25396825396825395,0.09836065573770492,0.031746031746031744,0.047999999999999994,0.3225806451612903,0.10526315789473684,0.15873015873015872,0.0023332699349931625,0.6608028
63,"Irina Higgins, Arka Pal, london, is the author of this paper. RL agents can learn how to maximise future expected rewards by choosing how to act based on in- coming sensory observations via reinforcement learning. early RL approaches did not scale well to envi- ronments with large state spaces and high-dimensional raw observations.","Domain adaptation is an important open prob-
lem in deep reinforcement learning (RL). In
many scenarios of interest data is hard to ob-
tain, so agents may learn a source policy in a
setting where data is readily available, with the
hope that it generalises well to the target do-
main. We propose a new multi-stage RL agent,
DARLA (DisentAngled Representation Learning
Agent), which learns to see before learning to act.
DARLA’s vision is based on learning a disen-
tangled representation of the observed environ-
ment. Once DARLA can see, it is able to acquire
source policies that are robust to many domain
shifts - even with no access to the target domain.
DARLA signiﬁcantly outperforms conventional
baselines in zero-shot domain adaptation scenar-
ios, an effect that holds across a variety of RL en-
vironments (Jaco arm, DeepMind Lab) and base
RL algorithms (DQN, A3C and EC).",0.3888888888888889,0.14,0.20588235294117646,0.09433962264150944,0.03355704697986577,0.04950495049504951,0.25925925925925924,0.09333333333333334,0.13725490196078433,0.002517136360291127,0.5194385
64,"we propose a general and model-free approach for Reinforcement Learning (RL) on real robotics with sparse rewards. both demonstrations and actual interactions are used to fill a replay buffer and the sam- pling ratio between demonstrations is automatically tuned via a prioritized replay mechanism. the demonstrations replace the need for carefully engineered rewards, and reduce the exploration problem encountered by classical RL approaches.","Leveraging Demonstrations for Deep Reinforcement
Learning on Robotics Problems with Sparse Rewards
Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang
Olivier Pietquin, Bilal Piot, Nicolas Heess
Tho",0.15625,0.37037037037037035,0.21978021978021978,0.047619047619047616,0.11538461538461539,0.06741573033707865,0.125,0.2962962962962963,0.1758241758241758,0.0076902271860888344,0.7932681
65,"StarCraft II is a real-time strategy (RTS) game that combines fast paced micro-actions with the need for high-level planning and execution. it is also multi-agent at a lower-level: each player controls hundreds of units, which need to collaborate to achieve a common goal. the player selects actions among a combinatorial space of approximately 108 possibilities.","This paper introduces SC2LE (StarCraft II Learning Environment), a reinforce-
ment learning environment based on the game StarCraft II. This domain poses
a new grand challenge for reinforcement learning, representing a more difﬁcult
class of problems than considered in most prior work. It is a multi-agent problem
with multiple players interacting; there is imperfect information due to a partially
observed map; it has a large action space involving the selection and control of
hundreds of units; it has a large state space that must be observed solely from
raw input feature planes; and it has delayed credit assignment requiring long-term
strategies over thousands of steps. We describe the observation, action, and reward
speciﬁcation for the StarCraft II domain and provide an open source Python-based
interface for communicating with the game engine. In addition to the main game
maps, we provide a suite of mini-games focusing on different elements of Star-
Craft II gameplay. For the main game maps, we also provide an accompanying
dataset of game replay data from human expert players. We give initial baseline
results for neural networks trained from this data to predict game outcomes and
player actions. Finally, we present initial baseline results for canonical deep rein-
forcement learning agents applied to the StarCraft II domain. On the mini-games,
these agents learn to achieve a level of play that is comparable to a novice player.
However, when trained on the main game, these agents are unable to make signiﬁ-
cant progress. Thus, SC2LE offers a new and challenging environment for explor-
ing deep reinforcement learning algorithms and architectures.
1",0.5833333333333334,0.12962962962962962,0.2121212121212121,0.15254237288135594,0.03345724907063197,0.0548780487804878,0.35,0.07777777777777778,0.12727272727272726,0.0008464464073726,0.60740614
66,"self-supervision is one of the most promising domains for unsu- pervised learning. the most promi- nent present image models are data starved, easily memorizing random labels for large im- age collections. but unsupervised algorithms are still not very effective for training neural networks.","We investigate methods for combining multiple self-
supervised tasks—i.e., supervised tasks where data can be
collected without manual labeling—in order to train a sin-
gle visual representation. First, we provide an apples-to-
apples comparison of four different self-supervised tasks
using the very deep ResNet-101 architecture. We then com-
bine tasks to jointly train a network. We also explore lasso
regularization to encourage the network to factorize the
information in its representation, and methods for “har-
monizing” network inputs in order to learn a more uni-
ﬁed representation. We evaluate all methods on ImageNet
classiﬁcation, PASCAL VOC detection, and NYU depth
prediction. Our results show that deeper networks work
better, and that combining tasks—even via a na¨ıve multi-
head architecture—always improves performance. Our best
joint network nearly matches the PASCAL performance of a
model pre-trained on ImageNet classiﬁcation, and matches
the ImageNet network on NYU depth prediction.",0.3409090909090909,0.0949367088607595,0.14851485148514854,0.023255813953488372,0.006369426751592357,0.01,0.22727272727272727,0.06329113924050633,0.099009900990099,0.0006354733311316217,0.57115257
67,generative model combines discrete latent variable a with a Gaussian p(z|m) the retrieved content ma is dependent on the continuous latent variables to access the memory. this does not provide clean separation between inferring the address to access in memory and the latent factors of variation that account for the variability of the observation.,"Aiming to augment generative models with external memory, we interpret the
output of a memory module with stochastic addressing as a conditional mixture
distribution, where a read operation corresponds to sampling a discrete memory
address and retrieving the corresponding content from memory. This perspective
allows us to apply variational inference to memory addressing, which enables
effective training of the memory module by using the target information to guide
memory lookups. Stochastic addressing is particularly well-suited for generative
models as it naturally encourages multimodality which is a prominent aspect of
most high-dimensional datasets. Treating the chosen address as a latent variable
also allows us to quantify the amount of information gained with a memory lookup
and measure the contribution of the memory module to the generative process.
To illustrate the advantages of this approach we incorporate it into a variational
autoencoder and apply the resulting model to the task of generative few-shot
learning. The intuition behind this architecture is that the memory module can
pick a relevant template from memory and the continuous part of the model can
concentrate on modeling remaining variations. We demonstrate empirically that
our model is able to identify and access the relevant memory contents even with
hundreds of unseen Omniglot characters in memory.
1",0.625,0.16587677725118483,0.26217228464419473,0.2,0.05238095238095238,0.08301886792452831,0.44642857142857145,0.11848341232227488,0.18726591760299627,0.0023930826318692127,0.62176484
68,"the deep Q-Networks algorithm (DQN; van Hasselt, Guez, and Silver 2016) addresses an overestimation bias of Q-learning by decoupling selection and evaluation of the bootstrap action. prioritized experience replay (Schaul et al. 2015) improves data efficiency, by replaying more of ten transitions from which there is more to learn.","The deep reinforcement learning community has made sev-
eral independent improvements to the DQN algorithm. How-
ever, it is unclear which of these extensions are complemen-
tary and can be fruitfully combined. This paper examines
six extensions to the DQN algorithm and empirically studies
their combination. Our experiments show that the combina-
tion provides state-of-the-art performance on the Atari 2600
benchmark, both in terms of data efﬁciency and ﬁnal perfor-
mance. We also provide results from a detailed ablation study
that shows the contribution of each component to overall per-
formance.",0.35294117647058826,0.18947368421052632,0.24657534246575344,0.04,0.02127659574468085,0.02777777777777778,0.19607843137254902,0.10526315789473684,0.13698630136986303,0.0030740175605110513,0.6556039
69,"understanding early word learning in situated artificial agents has long fascinated philoso- phers and cognitive scientists. an infant must induce structure in a stream of continuous visual input, reconcile this structure with consistenen-cies in the linguistic observations, and apply it to inform decisions about how best to respond.","Neural network-based systems can now learn
to locate the referents of words and phrases in
images, answer questions about visual scenes,
and execute symbolic instructions as ﬁrst-
person actors in partially-observable worlds.
To achieve this so-called grounded language
learning, models must overcome challenges
that infants face when learning their ﬁrst
words. While it is notable that models with no
meaningful prior knowledge overcome these
obstacles, researchers currently lack a clear un-
derstanding of how they do so, a problem that
we attempt to address in this paper. For max-
imum control and generality, we focus on a
simple neural network-based language learn-
ing agent, trained via policy-gradient methods,
which can interpret single-word instructions in
a simulated 3D world. Whilst the goal is not
to explicitly model infant word learning, we
take inspiration from experimental paradigms
in developmental psychology and apply some
of these to the artiﬁcial agent, exploring the
conditions under which established human bi-
ases and learning effects emerge. We further
propose a novel method for visualising seman-
tic representations in the agent.
1",0.46938775510204084,0.12637362637362637,0.1991341991341991,0.08333333333333333,0.022099447513812154,0.034934497816593885,0.24489795918367346,0.06593406593406594,0.1038961038961039,0.0013596807860122174,0.6873612
70,"cs.cmu.edu has published a conference paper at ICLR 2018. a surge of interest in using algorithms to automate the manual process of architecture design. these algorithms can be categorized as random with weights prediction (Brock et al., 2017), evolution (Stanley & Miikkulainen, 2017); reinforcement learning (Zhu & Le, 2017).","We explore efﬁcient neural architecture search methods and show that a simple
yet powerful evolutionary algorithm can discover new architectures with excellent
performance. Our approach combines a novel hierarchical genetic representation
scheme that imitates the modularized design pattern commonly adopted by human
experts, and an expressive search space that supports complex topologies. Our
algorithm efﬁciently discovers architectures that outperform a large number of
manually designed models for image classiﬁcation, obtaining top-1 error of 3.6%
on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with
the best existing neural architecture search approaches. We also present results
using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1%
less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.
1",0.30612244897959184,0.11029411764705882,0.16216216216216217,0.020833333333333332,0.007407407407407408,0.01092896174863388,0.14285714285714285,0.051470588235294115,0.07567567567567568,0.0014460233801753984,0.55948514
71,"deep reinforcement learning combines deep learning with reinforcement learning to compute a policy used to drive decision-making. the simplest form of MARL is independent RL (InRL), where each learner is oblivious to the other agents and simply treats all the interaction as part of its (“localized”) environment. there has been relatively little work done in RL community on overfitting to the environment, but they have focused on the fully-observable case.","To achieve general intelligence, agents must learn how to interact with others in
a shared environment: this is the challenge of multiagent reinforcement learning
(MARL). The simplest form is independent reinforcement learning (InRL), where
each agent treats its experience as part of its (non-stationary) environment. In
this paper, we ﬁrst observe that policies learned using InRL can overﬁt to the
other agents’ policies during training, failing to sufﬁciently generalize during
execution. We introduce a new metric, joint-policy correlation, to quantify this
effect. We describe an algorithm for general MARL, based on approximate best
responses to mixtures of policies generated using deep reinforcement learning, and
empirical game-theoretic analysis to compute meta-strategies for policy selection.
The algorithm generalizes previous ones such as InRL, iterated best response,
double oracle, and ﬁctitious play. Then, we present a scalable implementation
which reduces the memory requirement using decoupled meta-solvers. Finally,
we demonstrate the generality of the resulting policies in two partially observable
settings: gridworld coordination games and poker.
1",0.6111111111111112,0.25882352941176473,0.3636363636363637,0.2112676056338028,0.08875739644970414,0.125,0.3472222222222222,0.14705882352941177,0.20661157024793386,0.025782366833116542,0.5699171
72,"the best generative models (as measured by log-likelihood) will be those without latents but a powerful decoder (such as PixelCNN), but their usefulness depends on the particular application the features are used in. we argue for learning discrete and useful latent variables, which we demonstrate on a variety of domains.","Learning useful representations without supervision remains a key challenge in
machine learning. In this paper, we propose a simple yet powerful generative
model that learns such discrete representations. Our model, the Vector Quantised-
Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the
encoder network outputs discrete, rather than continuous, codes; and the prior
is learnt rather than static. In order to learn a discrete latent representation, we
incorporate ideas from vector quantisation (VQ). Using the VQ method allows the
model to circumvent issues of “posterior collapse” -— where the latents are ignored
when they are paired with a powerful autoregressive decoder -— typically observed
in the VAE framework. Pairing these representations with an autoregressive prior,
the model can generate high quality images, videos, and speech as well as doing
high quality speaker conversion and unsupervised learning of phonemes, providing
further evidence of the utility of the learnt representations.
1",0.49019607843137253,0.16666666666666666,0.2487562189054726,0.04,0.013422818791946308,0.02010050251256281,0.2549019607843137,0.08666666666666667,0.1293532338308458,0.0018296008381545842,0.7136129
73,"inductive Logic Programming (ILP) is a collection of techniques for constructing logic pro- grams from examples. the learned program is an explicit sym- bolic structure that can be inspected, understood1 and verified. but even a complex machine-generated logic program will be easier to understand than a large tensor of floating point numbers.","Artiﬁcial Neural Networks are powerful function approximators capable of modelling
solutions to a wide variety of problems, both supervised and unsupervised. As their size and
expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous
overﬁtting problem. Although mitigated by a variety of model regularisation methods, the
common cure is to seek large amounts of training data—which is not necessarily easily
obtained—that suﬃciently approximates the data distribution of the domain we wish to
test on. In contrast, logic programming methods such as Inductive Logic Programming oﬀer
an extremely data-eﬃcient process by which models can be trained to reason on symbolic
domains. However, these methods are unable to deal with the variety of domains neural
networks can be applied to: they are not robust to noise in or mislabelling of inputs, and
perhaps more importantly, cannot be applied to non-symbolic domains where the data is
ambiguous, such as operating on raw pixels. In this paper, we propose a Diﬀerentiable
Inductive Logic framework, which can not only solve tasks which traditional ILP systems
are suited for, but shows a robustness to noise and error in the training data which ILP
cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood
objective, it can be hybridised by connecting it with neural networks over ambiguous data
in order to be applied to domains which ILP cannot address, while providing data eﬃciency
and generalisation beyond what neural networks on their own can achieve.",0.4716981132075472,0.09765625,0.16181229773462782,0.07692307692307693,0.01568627450980392,0.026058631921824105,0.22641509433962265,0.046875,0.07766990291262137,0.00038006714183386723,0.5604068
74,"we agree with Lake and colleagues on their list of ‘key ingredients’ for building humanlike intelligence. but we favor an approach that centers on one additional ingredient: autonomy. we aim toward agents that can both build and exploit their own internal models, with minimal human hand-engineering.","M. Botvinick, D.G.T.  Barrett, P. Battaglia, N. de Freitas, D. Kumaran, J. Z Leibo, T. 
Lillicrap, J. Modayil, S. Mohamed, N.C. Rabinowitz, D. J. Rezende, A. Santoro, T. 
Schaul, C. Summerfield, G. Wayne, T. Weber, D. Wierstra, S. Legg, and D. Hassabis",0.02127659574468085,0.022222222222222223,0.02173913043478261,0.0,0.0,0.0,0.02127659574468085,0.022222222222222223,0.02173913043478261,0.003997395604823099,-0.019273506
75,population Based Training of Neural Networks (PBT) uses parallel search and sequential optimisation. the method is capable of performing online adaptation of hyperparameters. this can be particularly important in problems with highly non-stationary learning dynamics.,"Neural networks dominate the modern machine learning landscape, but their training and
success still suffer from sensitivity to empirical choices of hyperparameters such as model
architecture, loss function, and optimisation algorithm. In this work we present Population
Based Training (PBT), a simple asynchronous optimisation algorithm which effectively
utilises a ﬁxed computational budget to jointly optimise a population of models and their
hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hy-
perparameter settings rather than following the generally sub-optimal strategy of trying to
ﬁnd a single ﬁxed set to use for the whole course of training. With just a small mod-
iﬁcation to a typical distributed hyperparameter training framework, our method allows
robust and reliable training of models. We demonstrate the effectiveness of PBT on deep
reinforcement learning problems, showing faster wall-clock convergence and higher ﬁnal
performance of agents by optimising over a suite of hyperparameters. In addition, we show
the same method can be applied to supervised learning for machine translation, where PBT
is used to maximise the BLEU score directly, and also to training of Generative Adversarial
Networks to maximise the Inception score of generated images. In all cases PBT results in
the automatic discovery of hyperparameter schedules and model selection which results in
stable training and better ﬁnal performance.
1",0.6666666666666666,0.11059907834101383,0.18972332015810278,0.17142857142857143,0.027777777777777776,0.047808764940239036,0.4166666666666667,0.06912442396313365,0.11857707509881424,0.0001538826495818264,0.535671
76,"nascent field of AI safety still lacks a general consensus on its research problems. there has been several recent efforts to turn these concerns into technical problems on which we can make direct progress. despite the simplicity of the environments, we have selected these challenges with the safety of very powerful artificial agents in mind.","We present a suite of reinforcement learning environments illustrating various
safety properties of intelligent agents. These problems include safe interruptibil-
ity, avoiding side effects, absent supervisor, reward gaming, safe exploration, as
well as robustness to self-modiﬁcation, distributional shift, and adversaries. To
measure compliance with the intended safe behavior, we equip each environment
with a performance function that is hidden from the agent. This allows us to cate-
gorize AI safety problems into robustness and speciﬁcation problems, depending
on whether the performance function corresponds to the observed reward function.
We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on
our environments and show that they are not able to solve them satisfactorily.
1",0.4,0.18803418803418803,0.2558139534883721,0.037037037037037035,0.017241379310344827,0.023529411764705882,0.2,0.09401709401709402,0.12790697674418605,0.004883500971933379,0.6803295
77,"WaveNet: Fast High-Fidelity Speech Synthesis Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu avdnoord. the recently published model achieves state-of-the-art results in speech synthesis, and significantly closes the gap with natural human speech. it is not well suited for real world deployment due to its prohibitive generation speed.","The recently-developed WaveNet architecture [27] is the current state of the art in
realistic speech synthesis, consistently rated as more natural sounding for many
different languages than any previous system. However, because WaveNet relies
on sequential generation of one audio sample at a time, it is poorly suited to today’s
massively parallel computers, and therefore hard to deploy in a real-time production
setting. This paper introduces Probability Density Distillation, a new method for
training a parallel feed-forward network from a trained WaveNet with no signiﬁcant
difference in quality. The resulting system is capable of generating high-ﬁdelity
speech samples at more than 20 times faster than real-time, and is deployed online
by Google Assistant, including serving multiple English and Japanese voices.
1",0.43103448275862066,0.1953125,0.26881720430107525,0.10526315789473684,0.047244094488188976,0.06521739130434782,0.27586206896551724,0.125,0.17204301075268816,0.004007576888356331,0.7969765
78,"online learning with Gated Linear Networks aims to combine techniques from the online learning and data compression communities for high dimensional density modeling. the current state of the art is almost exclusively dominated by various deep learning based approaches from the machine learning community. these methods are trained offline, can generate plausible looking samples, and generally offer ex- cellent empirical performance. but they also have some drawbacks.","This paper describes a family of probabilistic architectures designed for online learning un-
der the logarithmic loss. Rather than relying on non-linear transfer functions, our method
gains representational power by the use of data conditioning. We state under general con-
ditions a learnable capacity theorem that shows this approach can in principle learn any
bounded Borel-measurable function on a compact subset of euclidean space; the result is
stronger than many universality results for connectionist architectures because we provide
both the model and the learning procedure for which convergence is guaranteed.",0.31343283582089554,0.22826086956521738,0.2641509433962264,0.015151515151515152,0.01098901098901099,0.012738853503184714,0.16417910447761194,0.11956521739130435,0.13836477987421386,0.006741532493507962,0.49436408
79,ementary model for sequential prediction in the state of the art PAQ family of data compression algorithms. the analysis holds for a finite alphabet and expresses redundancy in terms of the total variation in probability mass of the stationary distributions of a Piecewise Stationary Source. a statistical model predicts a distribution p on the possible outcomes of the next letter (modeling phase),"Generalized Probability Smoothing
Christopher Mattern
DeepMind
London, United Kingdom
cmattern@google.com
Abstract. In this work we consider a generalized version of Probability Smoothing,
the core el",0.08064516129032258,0.18518518518518517,0.11235955056179775,0.0,0.0,0.0,0.08064516129032258,0.18518518518518517,0.11235955056179775,0.0038690034505641548,0.2993424
80,"cross-modal learning from images and audio has a long history in computer vision, principally in the form of images and text [1–4]. in particular, we use unlabelled video as our source material, and employ audio-visual correspondence (AVC) as the objective function. a network that can localize the sound source in an image can be learnt that enable both within-mode (e.g. audio-to-audio) and between-mode retrieval.","Objects that Sound
Relja Arandjelovi´c1 and Andrew Zisserman1,2
1 DeepMind
2 VGG, Department of Engineering Science, University of Oxford
Abstract. In this paper our objectives are, ﬁrst, networks tha",0.125,0.2903225806451613,0.17475728155339806,0.0,0.0,0.0,0.08333333333333333,0.1935483870967742,0.11650485436893204,0.004024431001434524,0.3068984
81,"a new task and dataset will test and reward artificial agents approaching this level of competence (Section 3). the dataset consists of stories, which are books and movie scripts, with human written questions and answers based solely on human-generated abstractive summaries. a reader usually can not reproduce Title: Ghostbusters II Question: How is Oscar related to Dana Barrett?","Reading comprehension (RC)—in contrast to
information retrieval—requires integrating in-
formation and reasoning about events, enti-
ties, and their relations across a full document.
Question answering is conventionally used to
assess RC ability, in both artiﬁcial agents and
children learning to read. However, existing
RC datasets and tasks are dominated by ques-
tions that can be solved by selecting answers
using superﬁcial information (e.g., local con-
text similarity or global term frequency); they
thus fail to test for the essential integrative as-
pect of RC. To encourage progress on deeper
comprehension of language, we present a new
dataset and set of tasks in which the reader
must answer questions about stories by reading
entire books or movie scripts. These tasks are
designed so that successfully answering their
questions requires understanding the underly-
ing narrative rather than relying on shallow
pattern matching or salience. We show that al-
though humans solve the tasks easily, standard
RC models struggle on the tasks presented here.
We provide an analysis of the dataset and the
challenges it presents.
1",0.5254237288135594,0.17318435754189945,0.2605042016806723,0.05172413793103448,0.016853932584269662,0.02542372881355932,0.23728813559322035,0.0782122905027933,0.11764705882352942,0.00201460610571501,0.53933525
82,"the DeepMind Control Suite is a set of stable, well-tested continuous control tasks that are easy to use and modify. the Control Suite has equivalent domains to all those in the Gym while adding many more1. this decade has seen rapid progress in the application of Reinforcement Learning techniques to difficult problem domains.","The DeepMind Control Suite is a set of continuous control tasks with a stan-
dardised structure and interpretable rewards, intended to serve as performance
benchmarks for reinforcement learning agents. The tasks are written in Python
and powered by the MuJoCo physics engine, making them easy to use and mod-
ify. We include benchmarks for several learning algorithms. The Control Suite
is publicly available at github.com/deepmind/dm_control. A video summary
of all tasks is available at youtu.be/rAai4QzcYbs.
1",0.5,0.32926829268292684,0.39705882352941174,0.2830188679245283,0.18518518518518517,0.22388059701492538,0.37037037037037035,0.24390243902439024,0.2941176470588235,0.12756857798826782,0.88427925
83,"Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents Joel Z. Leibo, Cyprien de Masson d’Autume, Daniel Zoran, David Amos, Charles Beattie, Keith Anderson, Antonio Garca Castaeda, Manuel Sanchez, Simon Green, Audrunas Gruslys, Shane Legg, Demis Hassabis, and Matthew M. Botvinick DeepMind, London, UK February 6, 2018 Introduction 2","Psychlab is a simulated psychology laboratory inside the ﬁrst-person 3D
game world of DeepMind Lab (Beattie et al., 2016). Psychlab enables im-
plementations of classical laboratory psychological experiments so that they
work with both human and artiﬁcial agents. Psychlab has a simple and ﬂex-
ible API that enables users to easily create their own tasks. As examples,
we are releasing Psychlab implementations of several classical experimen-
tal paradigms including visual search, change detection, random dot motion
discrimination, and multiple object tracking. We also contribute a study
of the visual psychophysics of a speciﬁc state-of-the-art deep reinforcement
learning agent: UNREAL (Jaderberg et al., 2016). This study leads to the
surprising conclusion that UNREAL learns more quickly about larger target
stimuli than it does about smaller stimuli. In turn, this insight motivates
a speciﬁc improvement in the form of a simple model of foveal vision that
turns out to signiﬁcantly boost UNREAL’s performance, both on Psychlab
tasks, and on standard DeepMind Lab tasks. By open-sourcing Psychlab we
hope to facilitate a range of future such studies that simultaneously advance
deep reinforcement learning and improve its links with cognitive science.
Contents
1",0.22,0.05527638190954774,0.08835341365461849,0.08163265306122448,0.020202020202020204,0.032388663967611336,0.2,0.05025125628140704,0.08032128514056226,0.00021507274167361019,0.7037097
84,"RL has been mainly explored in toy environments and video games. but in real-world applications, safety is a crucial con- cern. if safe operation is addressed thoroughly and en- sured from the first moment of deployment, RL is deemed incompatible.","We address the problem of deploying a reinforce-
ment learning (RL) agent on a physical system
such as a datacenter cooling unit or robot, where
critical constraints must never be violated. We
show how to exploit the typically smooth dynam-
ics of these systems and enable RL algorithms
to never violate constraints during learning. Our
technique is to directly add to the policy a safety
layer that analytically solves an action correc-
tion formulation per each state. The novelty of
obtaining an elegant closed-form solution is at-
tained due to a linearized model, learned on past
trajectories consisting of arbitrary actions. This
is to mimic the real-world circumstances where
data logs were generated with a behavior policy
that is implausible to describe mathematically;
such cases render the known safety-aware off-
policy methods inapplicable. We demonstrate the
efﬁcacy of our approach on new representative
physics-based environments, and prevail where
reward shaping fails by maintaining zero con-
straint violations.",0.4146341463414634,0.10493827160493827,0.16748768472906403,0.075,0.018633540372670808,0.02985074626865672,0.1951219512195122,0.04938271604938271,0.07881773399014778,0.0004615369462010428,0.32082903
85,IMPALA is capable of scaling to thousands of machines without sacri- ficing training stability or data efficiency. we propose the Importance Weighted Actor-Learner Architecture (IMPALA) shown in Figure 1. the earliest attempts to scale up deep reinforcement learn- ing relied on scalability.,"In this work we aim to solve a large collection
of tasks using a single reinforcement learning
agent with a single set of parameters. A key
challenge is to handle the increased amount of
data and extended training time. We have devel-
oped a new distributed agent IMPALA (Impor-
tance Weighted Actor-Learner Architecture) that
not only uses resources more efﬁciently in single-
machine training but also scales to thousands of
machines without sacriﬁcing data efﬁciency or
resource utilisation. We achieve stable learning at
high throughput by combining decoupled acting
and learning with a novel off-policy correction
method called V-trace. We demonstrate the effec-
tiveness of IMPALA for multi-task reinforcement
learning on DMLab-30 (a set of 30 tasks from
the DeepMind Lab environment (Beattie et al.,
2016)) and Atari-57 (all available Atari games in
Arcade Learning Environment (Bellemare et al.,
2013a)). Our results show that IMPALA is able to
achieve better performance than previous agents
with less data, and crucially exhibits positive trans-
fer between tasks as a result of its multi-task ap-
proach. The source code is publicly available at
github.com/deepmind/scalable agent.",0.627906976744186,0.13846153846153847,0.22689075630252106,0.2619047619047619,0.05670103092783505,0.09322033898305085,0.3953488372093023,0.08717948717948718,0.14285714285714285,0.004564523157230237,0.7517613
86,learning and Querying Fast Generative Models for Reinforcement Learning Lars Buesing 1 Th eophane Weber 1 S. M. Ali Eslami 1 D. Reichert 1. a critical drawback is the vast amount of experience required to achieve good performance. the promise of model-based reinforcement learning is to improve sample-efficiency.,"A key challenge in model-based reinforcement
learning (RL) is to synthesize computationally
efﬁcient and accurate environment models. We
show that carefully designed generative models
that learn and operate on compact state representa-
tions, so-called state-space models, substantially
reduce the computational costs for predicting out-
comes of sequences of actions. Extensive experi-
ments establish that state-space models accurately
capture the dynamics of Atari games from the
Arcade Learning Environment from raw pixels.
The computational speed-up of state-space mod-
els while maintaining high accuracy makes their
application in RL feasible: We demonstrate that
agents which query these models for decision
making outperform strong model-free baselines
on the game MS PACMAN, demonstrating the
potential of using learned environment models for
planning.",0.38,0.15079365079365079,0.21590909090909088,0.14285714285714285,0.056,0.08045977011494254,0.24,0.09523809523809523,0.13636363636363635,0.006892902660439656,0.6031642
87,"the problem of off-policy evaluation (OPE) has been *Equal contribution 1Georgia Institute of Technology 2Google DeepMind. the goal in OPE is to estimate the performance of an evaluation policy, given a log of data generated by the behavior policy(ies)","We study the problem of off-policy evaluation
(OPE) in reinforcement learning (RL), where the
goal is to estimate the performance of a policy
from the data generated by another policy(ies). In
particular, we focus on the doubly robust (DR)
estimators that consist of an importance sampling
(IS) component and a performance model, and
utilize the low (or zero) bias of IS and low vari-
ance of the model at the same time. Although the
accuracy of the model has a huge impact on the
overall performance of DR, most of the work on
using the DR estimators in OPE has been focused
on improving the IS part, and not much on how to
learn the model. In this paper, we propose alter-
native DR estimators, called more robust doubly
robust (MRDR), that learn the model parameter
by minimizing the variance of the DR estimator.
We ﬁrst present a formulation for learning the DR
model in RL. We then derive formulas for the
variance of the DR estimator in both contextual
bandits and RL, such that their gradients w.r.t. the
model parameters can be estimated from the sam-
ples, and propose methods to efﬁciently minimize
the variance. We prove that the MRDR estima-
tors are strongly consistent and asymptotically
optimal. Finally, we evaluate MRDR in bandits
and RL benchmark problems, and compare its
performance with the existing methods.",0.7560975609756098,0.1336206896551724,0.2271062271062271,0.475,0.08225108225108226,0.14022140221402213,0.5121951219512195,0.09051724137931035,0.15384615384615383,0.0026333679622412777,0.5320231
88,path Consistency Learning in Tsallis Entropy Regularized MDPs Ofir Nachum * 1 Yinlam Chow * 2 Mohamamd Ghavamzadeh * 2. the goal is to find a pol- icy with maximum long-term performance. a principled way of dealing with this issue is regularization.,"We study the sparse entropy-regularized rein-
forcement learning (ERL) problem in which the
entropy term is a special form of the Tsallis en-
tropy. The optimal policy of this formulation is
sparse, i.e., at each state, it has non-zero proba-
bility for only a small number of actions. This
addresses the main drawback of the standard
Shannon entropy-regularized RL (soft ERL) for-
mulation, in which the optimal policy is softmax,
and thus, may assign a non-negligible probability
mass to non-optimal actions. This problem is ag-
gravated as the number of actions is increased. In
this paper, we follow the work of Nachum et al.
(2017) in the soft ERL setting, and propose a
class of novel path consistency learning (PCL)
algorithms, called sparse PCL, for the sparse
ERL problem that can work with both on-policy
and off-policy data. We ﬁrst derive a sparse con-
sistency equation that speciﬁes a relationship be-
tween the optimal value function and policy of
the sparse ERL along any system trajectory. Cru-
cially, a weak form of the converse is also true,
and we quantify the sub-optimality of a policy
which satisﬁes sparse consistency, and show that
as we increase the number of actions, this sub-
optimality is better than that of the soft ERL op-
timal policy. We then use this result to derive the
sparse PCL algorithms. We empirically compare
sparse PCL with its soft counterpart, and show
its advantage, especially in problems with a large
number of actions.",0.5,0.07782101167315175,0.1346801346801347,0.07692307692307693,0.01171875,0.020338983050847456,0.35,0.054474708171206226,0.09427609427609426,6.604078575651638e-05,0.51233345
89,"learning to search with MCTSnets is based on the application of powerful tree search algorithms to chal- lenging planning problems (Samuel, 1959; Knuth & Moore, 1975; J unger et al., 2009). the performance can often be dramatically improved by modify the rules that select the trajectory to traverse, the states to expand, the evaluation function by which performance *Equal contribution 1DeepMind, London, UK.","Planning problems are among the most impor-
tant and well-studied problems in artiﬁcial intel-
ligence. They are most typically solved by tree
search algorithms that simulate ahead into the fu-
ture, evaluate future states, and back-up those eval-
uations to the root of a search tree. Among these
algorithms, Monte-Carlo tree search (MCTS) is
one of the most general, powerful and widely
used. A typical implementation of MCTS uses
cleverly designed rules, optimised to the partic-
ular characteristics of the domain. These rules
control where the simulation traverses, what to
evaluate in the states that are reached, and how
to back-up those evaluations. In this paper we
instead learn where, what and how to search. Our
architecture, which we call an MCTSnet, incorpo-
rates simulation-based search inside a neural net-
work, by expanding, evaluating and backing-up
a vector embedding. The parameters of the net-
work are trained end-to-end using gradient-based
optimisation. When applied to small searches
in the well-known planning problem Sokoban,
the learned search algorithm signiﬁcantly outper-
formed MCTS baselines.",0.5,0.16939890710382513,0.2530612244897959,0.08196721311475409,0.027472527472527472,0.0411522633744856,0.25806451612903225,0.08743169398907104,0.1306122448979592,0.004846822983574033,0.68450975
90,the Mechanics of n-Player Differentiable Games David Balduzzi 1 S ebastien Racaniere 1 James Martens 1 Jakob Foerster 2 Karl Tuyls 1 1. there is a rapidly growing set of powerful models that do not optimize a single objective. a basic result is that gradient descent converges to a local minimum of the objective under a broad range of conditions.,"The cornerstone underpinning deep learning is the
guarantee that gradient descent on an objective
converges to local minima. Unfortunately, this
guarantee fails in settings, such as generative ad-
versarial nets, where there are multiple interacting
losses. The behavior of gradient-based methods
in games is not well understood – and is becoming
increasingly important as adversarial and multi-
objective architectures proliferate. In this paper,
we develop new techniques to understand and con-
trol the dynamics in general games. The key result
is to decompose the second-order dynamics into
two components. The ﬁrst is related to potential
games, which reduce to gradient descent on an im-
plicit function; the second relates to Hamiltonian
games, a new class of games that obey a conser-
vation law, akin to conservation laws in classical
mechanical systems. The decomposition moti-
vates Symplectic Gradient Adjustment (SGA), a
new algorithm for ﬁnding stable ﬁxed points in
general games. Basic experiments show SGA is
competitive with recently proposed algorithms for
ﬁnding stable ﬁxed points in GANs – whilst at
the same time being applicable to – and having
guarantees in – much more general games.",0.4098360655737705,0.1358695652173913,0.2040816326530612,0.06666666666666667,0.02185792349726776,0.03292181069958848,0.2459016393442623,0.08152173913043478,0.12244897959183675,0.0035210934947133648,0.5558183
91,researchers have demonstrated that certain small perturba-tions to the input can make neural networks produce ex- tremely bad results. a key question remains unanswered: are these models free from any adver- sarial examples or are they simply robust to current attack methods?,"This paper investigates recently proposed ap-
proaches for defending against adversarial exam-
ples and evaluating adversarial robustness. We
motivate adversarial risk as an objective for
achieving models robust to worst-case inputs. We
then frame commonly used attacks and evaluation
metrics as deﬁning a tractable surrogate objective
to the true adversarial risk. This suggests that
models may optimize this surrogate rather than
the true adversarial risk. We formalize this notion
as obscurity to an adversary, and develop tools
and heuristics for identifying obscured models
and designing transparent models. We demon-
strate that this is a signiﬁcant problem in practice
by repurposing gradient-free optimization tech-
niques into adversarial attacks, which we use to
decrease the accuracy of several recently proposed
defenses to near zero. Our hope is that our formu-
lations and results will help researchers to develop
more powerful defenses.",0.27906976744186046,0.08391608391608392,0.12903225806451615,0.047619047619047616,0.014084507042253521,0.021739130434782608,0.13953488372093023,0.04195804195804196,0.06451612903225808,0.0015514118808820294,0.4977244
92,"-VAE is a popular method for un-supervised disentangling based on the Variational Autoen- coder (VAE) framework. it uses a modified ver- sion of the VAE objective with a larger weight ( > 1) on the KL divergence between the variational posterior and the prior, and has proven to be an effective and stable method.","We deﬁne and address the problem of unsuper-
vised learning of disentangled representations on
data generated from independent factors of varia-
tion. We propose FactorVAE, a method that dis-
entangles by encouraging the distribution of rep-
resentations to be factorial and hence independent
across the dimensions. We show that it improves
upon β-VAE by providing a better trade-off be-
tween disentanglement and reconstruction quality.
Moreover, we highlight the problems of a com-
monly used disentanglement metric and introduce
a new metric that does not suffer from them.",0.3584905660377358,0.21348314606741572,0.2676056338028169,0.019230769230769232,0.011363636363636364,0.014285714285714287,0.18867924528301888,0.11235955056179775,0.14084507042253522,0.0065398385202984355,0.60944027
93,"this paper focuses on the challenges of training Vari- ational Autoencoders (VAEs) to overcome the lim- itations of VAEs. it is natural to consider borrowing the strengths of another popular type of generative algorithm, Generative Adversarial networks (GANs)","With the increasingly widespread deployment
of generative models, there is a mounting
need for a deeper understanding of their be-
haviors and limitations. In this paper, we
expose the limitations of Variational Autoen-
coders (VAEs), which consistently fail to learn
marginal distributions in both latent and vis-
ible spaces.
We show this to be a conse-
quence of learning by matching conditional
distributions, and the limitations of explicit
model and posterior distributions. It is pop-
ular to consider Generative Adversarial Net-
works (GANs) as a means of overcoming these
limitations, leading to hybrids of VAEs and
GANs. We perform a large-scale evaluation
of several VAE-GAN hybrids and analyze the
implications of class probability estimation for
learning distributions. While promising, we
conclude that at present, VAE-GAN hybrids
have limited applicability: they are harder
to scale, evaluate, and use for inference com-
pared to VAEs; and they do not improve over
the generation quality of GANs.
1",0.5526315789473685,0.13291139240506328,0.21428571428571427,0.16216216216216217,0.03821656050955414,0.061855670103092786,0.42105263157894735,0.10126582278481013,0.16326530612244897,0.0010496668716258842,0.695434
94,"machine theory of mind is based on a system that learns to model other agents. we can make predictions about strangers’ future behaviour, and infer what information they have about the world, and establish efficient and effective communication. a prominent feature of these “understandings” of other agents is that they make little to no reference to the agents’ true underlying structure.","Theory of mind (ToM; Premack & Woodruff,
1978) broadly refers to humans’ ability to rep-
resent the mental states of others, including their
desires, beliefs, and intentions. We propose to
train a machine to build such models too. We de-
sign a Theory of Mind neural network – a ToM-
net – which uses meta-learning to build models
of the agents it encounters, from observations
of their behaviour alone. Through this process,
it acquires a strong prior model for agents’ be-
haviour, as well as the ability to bootstrap to
richer predictions about agents’ characteristics
and mental states using only a small number of
behavioural observations.
We apply the ToM-
net to agents behaving in simple gridworld en-
vironments, showing that it learns to model ran-
dom, algorithmic, and deep reinforcement learn-
ing agents from varied populations, and that it
passes classic ToM tasks such as the “Sally-
Anne” test (Wimmer & Perner, 1983; Baron-
Cohen et al., 1985) of recognising that others can
hold false beliefs about the world. We argue that
this system – which autonomously learns how to
model other agents in its world – is an impor-
tant step forward for developing multi-agent AI
systems, for building intermediating technology
for machine-human interaction, and for advanc-
ing the progress on interpretable AI.
*Corresponding author: ncr@google.com.",0.5901639344262295,0.16589861751152074,0.2589928057553957,0.2,0.05555555555555555,0.08695652173913045,0.32786885245901637,0.09216589861751152,0.14388489208633096,0.0032353175491639722,0.71655
95,"path-specific counterfactual fairness Silvia Chiappa 1 Thomas P. S. Gillam 1. but most often the training data contains bias that exists in our society. this bias can be absorbed or amplified by the systems, leading to decisions that are unfair.","We consider the problem of learning fair deci-
sion systems in complex scenarios in which a
sensitive attribute might affect the decision along
both fair and unfair pathways. We introduce a
causal approach to disregard effects along unfair
pathways that simpliﬁes and generalizes previous
literature. Our method corrects observations ad-
versely affected by the sensitive attribute, and uses
these to form a decision. This avoids disregarding
fair information, and does not require an often
intractable computation of the path-speciﬁc effect.
We leverage recent developments in deep learning
and approximate inference to achieve a solution
that is widely applicable to complex, non-linear
scenarios.",0.36585365853658536,0.14150943396226415,0.20408163265306123,0.025,0.009523809523809525,0.013793103448275862,0.21951219512195122,0.08490566037735849,0.12244897959183673,0.002933179020403145,0.61014813
96,"an ideal continual learning agent should be able to (A) solve multiple tasks, (B) exhibit synergies when tasks are related, and (C) cope with deep dependency structures among tasks. in a linear RL setting, ammar et al. trained an agent on a distribution of tasks that is explicitly designed to adapt to a new task with very little additional learning.","Some real-world domains are best characterized as a single task, but for others
this perspective is limiting. Instead, some tasks continually grow in complexity,
in tandem with the agent’s competence. In continual learning, also referred to as
lifelong learning, there are no explicit task boundaries or curricula. As learning
agents have become more powerful, continual learning remains one of the frontiers
that has resisted quick progress. To test continual learning capabilities we consider
a challenging 3D domain with an implicit sequence of tasks and sparse rewards.
We propose a novel agent architecture called Unicorn, which demonstrates strong
continual learning and outperforms several baseline agents on the proposed domain.
The agent achieves this by jointly representing and learning multiple policies
efﬁciently, using a parallel off-policy learning setup.
1",0.43333333333333335,0.1984732824427481,0.27225130890052357,0.06779661016949153,0.03076923076923077,0.042328042328042326,0.2,0.0916030534351145,0.1256544502617801,0.004558526353348139,0.68518454
97,the models learn the joint probability of the data by factorizing the distribution into a product of conditional probabilities over each sample. a sample can be generated only after samples on which it depends have been produced in accordance with the ordering. the serial aspect of the sampling process can make it slow and impractical to use these models to generate high-dimensional data like speech and video.,"Sequential models achieve state-of-the-art results
in audio, visual and textual domains with respect
to both estimating the data distribution and gener-
ating high-quality samples. Efﬁcient sampling for
this class of models has however remained an elu-
sive problem. With a focus on text-to-speech syn-
thesis, we describe a set of general techniques for
reducing sampling time while maintaining high
output quality. We ﬁrst describe a single-layer
recurrent neural network, the WaveRNN, with a
dual softmax layer that matches the quality of
the state-of-the-art WaveNet model. The compact
form of the network makes it possible to gener-
ate 24 kHz 16-bit audio 4× faster than real time
on a GPU. Second, we apply a weight pruning
technique to reduce the number of weights in the
WaveRNN. We ﬁnd that, for a constant number of
parameters, large sparse networks perform better
than small dense networks and this relationship
holds for sparsity levels beyond 96%. The small
number of weights in a Sparse WaveRNN makes
it possible to sample high-ﬁdelity audio on a mo-
bile CPU in real time. Finally, we propose a new
generation scheme based on subscaling that folds
a long sequence into a batch of shorter sequences
and allows one to generate multiple samples at
once. The Subscale WaveRNN produces 16 sam-
ples per step without loss of quality and offers
an orthogonal method for increasing sampling
efﬁciency.",0.5294117647058824,0.1487603305785124,0.23225806451612901,0.08955223880597014,0.024896265560165973,0.03896103896103896,0.3088235294117647,0.08677685950413223,0.13548387096774195,0.0015284162257693296,0.40976697
98,"we propose a model-free deep reinforcement learn- ing method that leverages a small amount of demonstration data to assist a reinforcement learning agent. in experiments, our reinforcement and imitation agent achieve significantly better performances than agents trained with reinforcement learning or imitation learning alone.","level. It needs to recognize the object
categories, perform successful grasps on diverse shapes, and
handle tasks with variable lengths.
C. Quantitative Evaluation
Our full model can solve all six tasks, with only occasional
failures, using the same policy network, the same training
algorithm, and a ﬁxed set of hyperparameters. On the contrary,
neither reinforcement nor imitation alone can solve all tasks.
We compare the full model with three baselines which cor-
respond to pure RL, pure GAIL, and RL w/o demonstration
curriculum. These baselines use the same setup as the full
model, except that we set λ = 0 for RL and λ = 1 for GAIL,
while our model uses a balanced contribution of the hybrid
reward, where λ = 0.5. In the third baseline, all training
episodes start from random initial states rather than resetting
to demonstration states. This is a standard RL setup.
We report the mean episode returns as a function of the
number of training iterations in Fig. 4. Our full model achieves
the highest returns in all six tasks. The only case where the
baseline model is on par with the full model is the block
lifting task, in which both the RL baseline and the full model
achieved similar levels of performance. We hypothesize that
this is due to the short length of the lifting task, where
random exploration can provide a sufﬁcient learning signal
without the aid of demonstrations. In the other ﬁve tasks, the
full model outperforms both the reinforcement learning and
imitation learning baselines by a large margin, demonstrating
the effectiveness of combining reinforcement and imitation for
learning complex tasks. Comparing the two variants of RL
with and without using demonstration as a curriculum, we
see a pronounced effect of altering the start state distribution.
We see that RL from scratch leads to very slow learning
progress; while initiating episodes along demonstration tra-
jectories enables the agent to train on states from different
stages of a task. As a result, it greatly reduces the burden
of exploration and improves the learning efﬁciency. We also
report the mean episode returns of human demonstrations in
0.0
0.5",0.6222222222222222,0.07865168539325842,0.1396508728179551,0.11363636363636363,0.014084507042253521,0.025062656641604012,0.35555555555555557,0.0449438202247191,0.0798004987531172,3.3755100734606554e-05,0.5698912
99,"a neural network performs badly on old tasks having been trained to perform well on a new task. this limits their application to life-long learning or dynamic environments and tasks. our work generalises these approaches and we present experimental results where we apply our model to both continual or incremental learning tasks, as well as language modelling.","Deep neural networks have excelled on a wide range of problems, from vision to
language and game playing. Neural networks very gradually incorporate informa-
tion into weights as they process data, requiring very low learning rates. If the
training distribution shifts, the network is slow to adapt, and when it does adapt, it
typically performs badly on the training distribution before the shift. Our method,
Memory-based Parameter Adaptation, stores examples in memory and then uses
a context-based lookup to directly modify the weights of a neural network. Much
higher learning rates can be used for this local adaptation, reneging the need for
many iterations over similar data before good predictions can be made. As our
method is memory-based, it alleviates several shortcomings of neural networks,
such as catastrophic forgetting, fast, stable acquisition of new knowledge, learning
with an imbalanced class labels, and fast learning during evaluation. We demon-
strate this on a range of supervised tasks: large-scale image classiﬁcation and
language modelling.
1",0.46551724137931033,0.16071428571428573,0.23893805309734512,0.10526315789473684,0.03592814371257485,0.05357142857142856,0.27586206896551724,0.09523809523809523,0.1415929203539823,0.00511102477306149,0.5762517
100,"learning by playing – Solving Sparse Reward Tasks from Scratch Martin Riedmiller * 1 Roland Hafner * 1 Thomas Lampe * 1 Michael Neunert * 1 Jonas Degrave 1 Tom Van de Wiele * 1 Tom van degrave * 2. learning by playing is based on the mastery of the agent to control its own sensory observations (e.g. images, proprioception, haptic sensors)","We propose Scheduled Auxiliary Control (SAC-
X), a new learning paradigm in the context of
Reinforcement Learning (RL). SAC-X enables
learning of complex behaviors – from scratch – in
the presence of multiple sparse reward signals. To
this end, the agent is equipped with a set of gen-
eral auxiliary tasks, that it attempts to learn simul-
taneously via off-policy RL. The key idea behind
our method is that active (learned) scheduling and
execution of auxiliary policies allows the agent to
efﬁciently explore its environment – enabling it
to excel at sparse reward RL. Our experiments in
several challenging robotic manipulation settings
demonstrate the power of our approach. A video
of the rich set of learned behaviours can be found
at https://youtu.be/mPKyvocNe M.",0.26785714285714285,0.11904761904761904,0.16483516483516486,0.09090909090909091,0.04,0.05555555555555555,0.19642857142857142,0.0873015873015873,0.12087912087912087,0.008330240809977907,0.54845494
101,"kickstarting deep reinforcement learning is a hallmark of human develop- ment. despite the prevalence of weight transfer in supervised deep learning, there has been limited success in leveraging previously trained ‘teacher’ agents in the reinforcement learning setting. the kickstarted student agent is encouraged to act in a similar manner.","We present a method for using previously-trained
‘teacher’ agents to kickstart the training of a
new ‘student’ agent.
To this end, we lever-
age ideas from policy distillation (Rusu et al.,
2015; Parisotto et al., 2015) and population based
training (Jaderberg et al., 2017).
Our method
places no constraints on the architecture of the
teacher or student agents, and it regulates itself
to allow the students to surpass their teachers in
performance.
We show that, on a challenging
and computationally-intensive multi-task bench-
mark (Beattie et al., 2016), kickstarted training
improves the data efﬁciency of new agents, mak-
ing it signiﬁcantly easier to iterate on their de-
sign. We also show that the same kickstarting
pipeline can allow a single student agent to lever-
age multiple ‘expert’ teachers which specialise
on individual tasks. In this setting kickstarting
yields surprisingly large gains, with the kick-
started agent matching the performance of an
agent trained from scratch in almost 10× fewer
steps, and surpassing its ﬁnal performance by
42%.
Kickstarting is conceptually simple and
can easily be incorporated into reinforcement
learning experiments.",0.46938775510204084,0.125,0.19742489270386265,0.10416666666666667,0.0273224043715847,0.04329004329004329,0.2857142857142857,0.07608695652173914,0.12017167381974249,0.0013054333723607002,0.67035127
102,a generalised method for Empirical Game Theoretic Analysis. works by Walsh and Wellman et al. have shown the great potential of using heuristic strategies and game theory to examine such interactions at a higher meta-level. a game theoretic analysis at the level of meta-strategies yields novel insights into the type and form of interactions in complex systems.,"This paper provides theoretical bounds for empirical game theo-
retical analysis of complex multi-agent interactions. We provide
insights in the empirical meta game showing that a Nash equilib-
rium of the meta-game is an approximate Nash equilibrium of the
true underlying game. We investigate and show how many data
samples are required to obtain a close enough approximation of the
underlying game. Additionally, we extend the meta-game analysis
methodology to asymmetric games. The state-of-the-art has only
considered empirical games in which agents have access to the
same strategy sets and the payoff structure is symmetric, implying
that agents are interchangeable. Finally, we carry out an empirical
illustration of the generalised method in several domains, illustrat-
ing the theory and evolutionary dynamics of several versions of the
AlphaGo algorithm (symmetric), the dynamics of the Colonel Blotto
game played by human players on Facebook (symmetric), and an
example of a meta-game in Leduc Poker (asymmetric), generated
by the PSRO multi-agent learning algorithm.",0.559322033898305,0.19642857142857142,0.29074889867841414,0.05172413793103448,0.017964071856287425,0.02666666666666667,0.3220338983050847,0.1130952380952381,0.16740088105726872,0.0020917145682667663,0.71817756
103,"ICLR 2018 on the IMPORTANCE OF SINGLE DIRECTIONS FOR GENERALIZATION Ari S. Morcos1, David G.T. Barrett, Neil C. Rabinowitz, & Matthew Botvinick DeepMind London, UK. these observations raise a key question: why do some networks generalize while others do not?","Despite their ability to memorize large datasets, deep neural networks often
achieve good generalization performance. However, the differences between the
learned solutions of networks which generalize and those which do not remain
unclear. Additionally, the tuning properties of single directions (deﬁned as the
activation of a single unit or some linear combination of units in response to some
input) have been highlighted, but their importance has not been evaluated. Here,
we connect these lines of inquiry to demonstrate that a network’s reliance on single
directions is a good predictor of its generalization performance, across networks
trained on datasets with different fractions of corrupted labels, across ensembles
of networks trained on datasets with unmodiﬁed labels, across different hyper-
parameters, and over the course of training. While dropout only regularizes this
quantity up to a point, batch normalization implicitly discourages single direction
reliance, in part by decreasing the class selectivity of individual units. Finally,
we ﬁnd that class selectivity is a poor predictor of task importance, suggesting
not only that networks which generalize well minimize their dependence on indi-
vidual units by reducing their selectivity, but also that individually selective units
may not be necessary for strong network performance.
1",0.425,0.0845771144278607,0.14107883817427386,0.07692307692307693,0.015,0.025104602510460254,0.25,0.04975124378109453,0.08298755186721993,0.00014963452641932848,0.6214369
104,inequity aversion improves cooperation in intertemporal social dilemmas. there is a tradeoff between short-term individual incentives and long-term collective interest. humans face such dilemmas when contributing to a collective food storage during the summer in preparation for a harsh winter.,"Groups of humans are often able to ﬁnd ways to cooperate with one another
in complex, temporally extended social dilemmas. Models based on behavioral
economics are only able to explain this phenomenon for unrealistic stateless matrix
games. Recently, multi-agent reinforcement learning has been applied to generalize
social dilemma problems to temporally and spatially extended Markov games.
However, this has not yet generated an agent that learns to cooperate in social
dilemmas as humans do. A key insight is that many, but not all, human individuals
have inequity averse social preferences. This promotes a particular resolution of
the matrix game social dilemma wherein inequity-averse individuals are personally
pro-social and punish defectors. Here we extend this idea to Markov games and
show that it promotes cooperation in several types of sequential social dilemma,
via a proﬁtable interaction with policy learnability. In particular, we ﬁnd that
inequity aversion improves temporal credit assignment for the important class
of intertemporal social dilemmas. These results help explain how large-scale
cooperation may emerge and persist.
1",0.47619047619047616,0.11494252873563218,0.1851851851851852,0.12195121951219512,0.028901734104046242,0.04672897196261683,0.2857142857142857,0.06896551724137931,0.1111111111111111,0.0021278496702460117,0.57333905
105,"in real-world settings, it is infeasible to model all meaningful aspects of a system and its environment by hand due to both complexity and size. instead, robots must be capable of learning and adapting to changes in their environment and task. one approach to such models proposes a massive network of interconnected and interdependent GVFs, which are incrementally added over time.","Accelerating Learning in Constructive Predictive
Frameworks with the Successor Representation
Craig Sherstan1, Marlos C. Machado1, Patrick M. Pilarski1
Abstract— Here we propose using the successor re",0.04838709677419355,0.11538461538461539,0.06818181818181818,0.0,0.0,0.0,0.04838709677419355,0.11538461538461539,0.06818181818181818,0.002989232719814957,0.25711814
106,"learning to navigate in cities without a map has been shown to be possible in some domains, by using deep reinforcement learning (RL) approaches that can learn from task rewards – for example, navigating to a destination. in this study, we propose an agent that learns a goal-dependent policy with a dual pathway, modular architecture with similarities to the interchangeable task-specific modules approach from.","Navigating through unstructured environments is a basic capability of intelligent
creatures, and thus is of fundamental interest in the study and development of
artiﬁcial intelligence. Long-range navigation is a complex cognitive task that re-
lies on developing an internal representation of space, grounded by recognisable
landmarks and robust visual processing, that can simultaneously support continu-
ous self-localisation (“I am here”) and a representation of the goal (“I am going
there”). Building upon recent research that applies deep reinforcement learning to
maze navigation problems, we present an end-to-end deep reinforcement learning
approach that can be applied on a city scale. Recognising that successful nav-
igation relies on integration of general policies with locale-speciﬁc knowledge,
we propose a dual pathway architecture that allows locale-speciﬁc features to be
encapsulated, while still enabling transfer to multiple cities. A key contribution of
this paper is an interactive navigation environment that uses Google Street View
for its photographic content and worldwide coverage. Our baselines demonstrate
that deep reinforcement learning agents can learn to navigate in multiple cities and
to traverse to target destinations that may be kilometres away. The project webpage
http://streetlearn.cc contains a video summarizing our research and show-
ing the trained agent in diverse city environments and on the transfer task, the form
to request the StreetLearn dataset and links to further resources. The StreetLearn en-
vironment code is available at https://github.com/deepmind/streetlearn.
1",0.676923076923077,0.18032786885245902,0.28478964401294504,0.203125,0.053497942386831275,0.08469055374592833,0.3230769230769231,0.0860655737704918,0.13592233009708737,0.0035139739437415173,0.762661
107,"we present a new approach for interpreting and generating images using Reinforced Adversarial Learning. a collection of actors (in our experiments, up to 64), asynchronously and continuously produce execution traces. the agent is re- warded by fooling a discriminator network, and is trained with distributed reinforcement learning without extra supervision.","Advances in deep generative networks have led
to impressive results in recent years. Neverthe-
less, such models can often waste their capacity
on the minutiae of datasets, presumably due to
weak inductive biases in their decoders. This is
where graphics engines may come in handy since
they abstract away low-level details and represent
images as high-level programs. Current methods
that combine deep learning and renderers are lim-
ited by hand-crafted likelihood or distance func-
tions, a need for large amounts of supervision, or
difﬁculties in scaling their inference algorithms
to richer datasets. To mitigate these issues, we
present SPIRAL, an adversarially trained agent
that generates a program which is executed by a
graphics engine to interpret and sample images.
The goal of this agent is to fool a discriminator
network that distinguishes between real and ren-
dered data, trained with a distributed reinforce-
ment learning setup without any supervision. A
surprising ﬁnding is that using the discrimina-
tor’s output as a reward signal is the key to allow
the agent to make meaningful progress at match-
ing the desired output rendering. To the best of
our knowledge, this is the ﬁrst demonstration of
an end-to-end, unsupervised and adversarial in-
verse graphics agent on challenging real world
(MNIST, OMNIGLOT, CELEBA) and synthetic
3D datasets. A video of the agent can be found at
https://youtu.be/iSyvwAwa7vk.",0.7,0.15021459227467812,0.24734982332155478,0.1836734693877551,0.03879310344827586,0.06405693950177936,0.46,0.09871244635193133,0.16254416961130744,0.0006921506574792882,0.6890436
108,"the KANERVA MACHINE: A GENERATIVE DISTRIBUTED MEMORY Yan Wu, Greg Wayne, Alex Graves, Timothy Lillicrap DeepMind yanwu,gregwayne,gravesa,countzero@google.com INTRODUCTION Recent work has examined a variety of novel ways to augment neural networks with fast memory stores. the basic problem of how to most efficiently use memory remains an open question.","We present an end-to-end trained memory system that quickly adapts to new
data and generates samples like them. Inspired by Kanerva’s sparse distributed
memory, it has a robust distributed reading and writing mechanism. The memory
is analytically tractable, which enables optimal on-line compression via a Bayesian
update-rule. We formulate it as a hierarchical conditional generative model, where
memory provides a rich data-dependent prior distribution. Consequently, the
top-down memory and bottom-up perception are combined to produce the code
representing an observation. Empirically, we demonstrate that the adaptive memory
signiﬁcantly improves generative models trained on both the Omniglot and CIFAR
datasets. Compared with the Differentiable Neural Computer (DNC) and its
variants, our memory model has greater capacity and is signiﬁcantly easier to train.
1",0.2962962962962963,0.12121212121212122,0.17204301075268816,0.018867924528301886,0.007633587786259542,0.010869565217391306,0.16666666666666666,0.06818181818181818,0.0967741935483871,0.0015011241148088236,0.6530949
109,"natural language processing (NLP) has been an active field of research for a long time. the introduction of deep learning enabled great progress in NLP tasks such as translation, image captioning, text generation and visual question answering. this approach will encourage agents to use languages grounded to task-related entities as well as communicate with other agents.","One of the distinguishing aspects of human language is its compositionality, which
allows us to describe complex environments with limited vocabulary. Previously, it
has been shown that neural network agents can learn to communicate in a highly
structured, possibly compositional language based on disentangled input (e.g. hand-
engineered features). Humans, however, do not learn to communicate based on
well-summarized features. In this work, we train neural agents to simultaneously
develop visual perception from raw image pixels, and learn to communicate with a
sequence of discrete symbols. The agents play an image description game where
the image contains factors such as colors and shapes. We train the agents using the
obverter technique where an agent introspects to generate messages that maximize
its own understanding. Through qualitative analysis, visualization and a zero-shot
test, we show that the agents can develop, out of raw image pixels, a language with
compositional properties, given a proper pressure from the environment.
1",0.45614035087719296,0.16352201257861634,0.24074074074074073,0.07142857142857142,0.02531645569620253,0.03738317757009346,0.22807017543859648,0.08176100628930817,0.12037037037037036,0.002910325400525569,0.55458087
110,"unsupervised learning is likely to be critical to the development of artificial intelligence, as it enables algorithms to exploit the vast amounts of data for which such signals are partially or completely lacking. it is hoped that unsupervised algorithms will be able to learn compact, transferable representations that will benefit the full spectrum of cognitive tasks.","This paper introduces Associative Compression
Networks (ACNs), a new framework for varia-
tional autoencoding with neural networks. The
system differs from existing variational autoen-
coders (VAEs) in that the prior distribution used
to model each code is conditioned on a similar
code from the dataset. In compression terms this
equates to sequentially transmitting the dataset
using an ordering determined by proximity in la-
tent space. Since the prior need only account for
local, rather than global variations in the latent
space, the coding cost is greatly reduced, leading
to rich, informative codes. Crucially, the codes
remain informative when powerful, autoregres-
sive decoders are used, which we argue is fun-
damentally difﬁcult with normal VAEs. Experi-
mental results on MNIST, CIFAR-10, ImageNet
and CelebA show that ACNs discover high-level
latent features such as object class, writing style,
pose and facial expression, which can be used to
cluster and classify the data, as well as to generate
diverse and convincing samples. We conclude
that ACNs are a promising new direction for rep-
resentation learning: one that steps away from
IID modelling, and towards learning a structured
description of the dataset as a whole.",0.375,0.10824742268041238,0.168,0.0,0.0,0.0,0.23214285714285715,0.06701030927835051,0.10400000000000001,0.0006115379344464844,0.43905035
111,"communication in the negotiation game (see Figure 1) is an established model of non-cooperative games in classical game theory (nash, 1950b; Neumann & Morgenstern, 1944; Nash, 1950a; 1951; Schelling, 1960; Binmore et al., 1986; Peters, 2008); and linguistic communication. a task-specific communication channel consisting of se- quences of arbitrary symbols similar to language can facilitate effective negotiation in prosocial agents.","Multi-agent reinforcement learning offers a way to study how communication
could emerge in communities of agents needing to solve speciﬁc problems. In this
paper, we study the emergence of communication in the negotiation environment,
a semi-cooperative model of agent interaction. We introduce two communication
protocols – one grounded in the semantics of the game, and one which is a priori
ungrounded and is a form of cheap talk. We show that self-interested agents can
use the pre-grounded communication channel to negotiate fairly, but are unable to
effectively use the ungrounded channel. However, prosocial agents do learn to use
cheap talk to ﬁnd an optimal negotiating strategy, suggesting that cooperation is
necessary for language to emerge. We also study communication behaviour in a
setting where one agent interacts with agents in a community with different levels
of prosociality and show how agent identiﬁability can aid negotiation.
1",0.4426229508196721,0.17880794701986755,0.25471698113207547,0.1,0.04,0.05714285714285714,0.29508196721311475,0.11920529801324503,0.16981132075471697,0.015335297787005755,0.7243712
112,"the study of emergent communication is important for two related problems in language develop- ment, both human and artificial. we focus on language evolution, the development of communication protocols from scratch, and language acquisition, the ability of an embodied agent to learn an existing language. this perspective especially motivates the study in cases where co-operative agents try to achieve shared goals in game scenarios.","The ability of algorithms to evolve or learn (compositional) communication pro-
tocols has traditionally been studied in the language evolution literature through
the use of emergent communication tasks. Here we scale up this research by us-
ing contemporary deep learning methods and by training reinforcement-learning
neural network agents on referential communication games. We extend previ-
ous work, in which agents were trained in symbolic environments, by developing
agents which are able to learn from raw pixel data, a more challenging and realis-
tic input representation. We ﬁnd that the degree of structure found in the input data
affects the nature of the emerged protocols, and thereby corroborate the hypoth-
esis that structured compositional language is most likely to emerge when agents
perceive the world as being structured.
1",0.5076923076923077,0.2558139534883721,0.3402061855670103,0.109375,0.0546875,0.07291666666666667,0.24615384615384617,0.12403100775193798,0.16494845360824742,0.010672126022112713,0.68271995
113,"aruderman, ncr, arimorcos, Ari S. Morcos and Daniel Zoran DeepMind, London, UK. the foundational intuitions informing convolutional neural networks (CNNs) for visual object recognition have been assumed to be common knowledge without empirical verification. despite this reasoning, recent models have abandoned interleaved pooling layers, achieving similar or greater success without them.","Many of our core assumptions about how neural networks operate remain empiri-
cally untested. One common assumption is that convolutional neural networks need
to be stable to small translations and deformations to solve image recognition tasks.
For many years, this stability was baked into CNN architectures by incorporating
interleaved pooling layers. Recently, however, interleaved pooling has largely been
abandoned. This raises a number of questions: Are our intuitions about deforma-
tion stability right at all? Is it important? Is pooling necessary for deformation
invariance? If not, how is deformation invariance achieved in its absence? In this
work, we rigorously test these questions, and ﬁnd that deformation stability in
convolutional networks is more nuanced than it ﬁrst appears: (1) Deformation
invariance is not a binary property, but rather that different tasks require different
degrees of deformation stability at different layers. (2) Deformation stability is
not a ﬁxed property of a network and is heavily adjusted over the course of train-
ing, largely through the smoothness of the convolutional ﬁlters. (3) Interleaved
pooling layers are neither necessary nor sufﬁcient for achieving the optimal form
of deformation stability for natural image classiﬁcation. (4) Pooling confers too
much deformation stability for image classiﬁcation at initialization, and during
training, networks have to learn to counteract this inductive bias. Together, these
ﬁndings provide new insights into the role of interleaved pooling and deformation
invariance in CNNs, and demonstrate the importance of rigorous empirical testing
of even our most basic assumptions about the working of neural networks.
1",0.43137254901960786,0.08661417322834646,0.14426229508196722,0.1,0.019762845849802372,0.03300330033003301,0.23529411764705882,0.047244094488188976,0.07868852459016394,0.0006557271025796713,0.6530508
114,progress & compress: a scalable framework for continual learning. based on the assumption that training examples are drawn i.i.d. from some fixed distribution. in many scenarios such a restriction is not of major concern. but it can prove to be an important limitation especially when a system needs to continuously adapt.,"We introduce a conceptually simple and scalable
framework for continual learning domains where
tasks are learned sequentially. Our method is con-
stant in the number of parameters and is designed
to preserve performance on previously encoun-
tered tasks while accelerating learning progress
on subsequent problems. This is achieved by train-
ing a network with two components: A knowledge
base, capable of solving previously encountered
problems, which is connected to an active column
that is employed to efﬁciently learn the current
task. After learning a new task, the active column
is distilled into the knowledge base, taking care
to protect any previously acquired skills. This
cycle of active learning (progression) followed by
consolidation (compression) requires no architec-
ture growth, no access to or storing of previous
data or tasks, and no task-speciﬁc parameters. We
demonstrate the progress & compress approach on
sequential classiﬁcation of handwritten alphabets
as well as two reinforcement learning domains:
Atari games and 3D maze navigation.",0.4230769230769231,0.13664596273291926,0.20657276995305168,0.09803921568627451,0.03125,0.047393364928909956,0.2692307692307692,0.08695652173913043,0.13145539906103285,0.008705619239093958,0.6795889
115,"pathwise gradient estimators are a core tool for stochastic estimation in machine learning and statistics. the reparameterization trick is easily used with distributions that have location-scale parameteriza-tions or tractable inverse cumulative distribution functions (CDFs), or are expressible as deterministic transformations of such distributions.","By providing a simple and efﬁcient way of computing low-variance gradients of
continuous random variables, the reparameterization trick has become the technique
of choice for training a variety of latent variable models. However, it is not
applicable to a number of important continuous distributions. We introduce an
alternative approach to computing reparameterization gradients based on implicit
differentiation and demonstrate its broader applicability by applying it to Gamma,
Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic
reparameterization trick. Our experiments show that the proposed approach is faster
and more accurate than the existing gradient estimators for these distributions.
1",0.35555555555555557,0.1523809523809524,0.21333333333333337,0.09090909090909091,0.038461538461538464,0.05405405405405406,0.2222222222222222,0.09523809523809523,0.13333333333333333,0.009186800977008828,0.6449284
116,"a popular approach for the training of such models is to maximize the log-likelihood of the training data. however, maximum likelihood is often intractable due to the presence of latent variables. Variational Bayes introduces per-sample approximate proposal distributions that need to be optimized using a process called variational inference.","The variational autoencoder (VAE) is a popular model for density estimation and
representation learning. Canonically, the variational principle suggests to prefer
an expressive inference model so that the variational approximation is accurate.
However, it is often overlooked that an overly-expressive inference model can be
detrimental to the test set performance of both the amortized posterior approximator
and, more importantly, the generative density estimator. In this paper, we leverage
the fact that VAEs rely on amortized inference and propose techniques for amortized
inference regularization (AIR) that control the smoothness of the inference model.
We demonstrate that, by applying AIR, it is possible to improve VAE generalization
on both inference and generative performance. Our paper challenges the belief that
amortized inference is simply a mechanism for approximating maximum likelihood
training and illustrates that regularization of the amortization family provides a
new direction for understanding and improving generalization in VAEs.
1",0.5686274509803921,0.19463087248322147,0.29,0.1,0.033783783783783786,0.0505050505050505,0.3333333333333333,0.11409395973154363,0.17,0.002891672413401229,0.5442968
117,"a conference paper at ICLR 2019 PUSHING THE BOUNDS OF DROPOUT Gábor Melis, Charles Blundell, Tomá Kocisk, Chris Dyer, Phil Blunsom DeepMind, London, UK. the dominant perspective today views dropout as either an implicit ensemble method (Warde-Farley et al. 2013) or averaging over an approximate Bayesian posterior (Gal & Ghahramani 2016a)","We show that dropout training is best understood as performing MAP estimation
concurrently for a family of conditional models whose objectives are themselves
lower bounded by the original dropout objective. This discovery allows us to pick
any model from this family after training, which leads to a substantial improvement
on regularisation-heavy language modelling. The family includes models that
compute a power mean over the sampled dropout masks, and their less stochastic
subvariants with tighter and higher lower bounds than the fully stochastic dropout
objective. We argue that since the deterministic subvariant’s bound is equal to its
objective, and the highest amongst these models, the predominant view of it as a
good approximation to MC averaging is misleading. Rather, deterministic dropout
is the best available approximation to the true objective.
1",0.23076923076923078,0.09090909090909091,0.13043478260869568,0.0,0.0,0.0,0.17307692307692307,0.06818181818181818,0.09782608695652174,0.0011948483002260993,0.57772535
118,a full classification for adversarial partial monitoring is proposed. the framework relaxes the relationship between feedback and loss. it offers a rich and elegant framework to study the exploration-exploitation dilemma beyond bandits. a learner and adversary secretly choose n outcomes from a finite set.,"Partial monitoring is a generalization of the well-known multi-armed bandit
framework where the loss is not directly observed by the learner. We complete the
classiﬁcation of ﬁnite adversarial partial monitoring to include all games, solving
an open problem posed by Bart´ok et al. [2014].
Along the way we simplify
and improve existing algorithms and correct errors in previous analyses. Our
second contribution is a new algorithm for the class of games studied by Bart´ok
[2013] where we prove upper and lower regret bounds that shed more light on the
dependence of the regret on the game structure.
1",0.4222222222222222,0.18446601941747573,0.25675675675675674,0.06818181818181818,0.029411764705882353,0.0410958904109589,0.24444444444444444,0.10679611650485436,0.14864864864864863,0.010047702910431056,0.8651129
119,"hyperbolic attention networks are referred to as heterogeneous, in the sense that they can be divided into sub-nodes which are themselves distinguishable from each other. the scale-free structure of natural data manifests itself as a power law distribution on the node degrees of the complex network that describes it. n-ary trees can be approximated with tree-like structures, such as taxonomies and dendro- grams.","We introduce hyperbolic attention networks to endow neural networks with enough
capacity to match the complexity of data with hierarchical and power-law structure.
A few recent approaches have successfully demonstrated the beneﬁts of imposing
hyperbolic geometry on the parameters of shallow networks. We extend this line of
work by imposing hyperbolic geometry on the activations of neural networks. This
allows us to exploit hyperbolic geometry to reason about embeddings produced
by deep networks. We achieve this by re-expressing the ubiquitous mechanism
of soft attention in terms of operations deﬁned for hyperboloid and Klein models.
Our method shows improvements in terms of generalization on neural machine
translation, learning on graphs and visual question answering tasks while keeping
the neural representations compact.
1",0.31343283582089554,0.168,0.21875,0.07575757575757576,0.04032258064516129,0.05263157894736842,0.22388059701492538,0.12,0.15625,0.00943350603191149,0.608989
120,"neural networks are increasingly being deployed in a wide variety of applications with great success. recent work has shown that the addition of small but carefully chosen deviations to the input, so-called adversarial perturbations, can cause the neural network to make incorrect predictions.","This paper proposes a new algorithmic framework, predictor-veriﬁer training,
to train neural networks that are veriﬁable, i.e., networks that provably satisfy
some desired input-output properties. The key idea is to simultaneously train two
networks: a predictor network that performs the task at hand, e.g., predicting
labels given inputs, and a veriﬁer network that computes a bound on how well
the predictor satisﬁes the properties being veriﬁed. Both networks can be trained
simultaneously to optimize a weighted combination of the standard data-ﬁtting
loss and a term that bounds the maximum violation of the property. Experiments
show that not only is the predictor-veriﬁer architecture able to train networks to
achieve state of the art veriﬁed robustness to adversarial examples with much
shorter training times (outperforming previous algorithms on small datasets like
MNIST and SVHN), but it can also be scaled to produce the ﬁrst known (to the
best of our knowledge) veriﬁably robust networks for CIFAR-10.
1",0.4772727272727273,0.12352941176470589,0.19626168224299065,0.06976744186046512,0.01775147928994083,0.02830188679245283,0.3409090909090909,0.08823529411764706,0.14018691588785046,0.0011765553810431243,0.61394083
121,a limit of classical planning algorithms is that one needs to know how to search for an optimal – or at least reasonable – solution for each instantiation of every possible type of plan. this is among the reasons why “learning to plan” has been an active research area to address these shortcomings.,"We present Value Propagation (VProp), a set of parameter-eﬃcient diﬀeren-
tiable planning modules built on Value Iteration which can successfully be
trained using reinforcement learning to solve unseen tasks, has the capability
to generalize to larger map sizes, and can learn to navigate in dynamic
environments. We show that the modules enable learning to plan when
the environment also includes stochastic elements, providing a cost-eﬃcient
learning system to build low-level size-invariant planners for a variety of
interactive navigation problems. We evaluate on static and dynamic conﬁg-
urations of MazeBase grid-worlds, with randomly generated environments
of several diﬀerent sizes, and on a StarCraft navigation scenario, with more
complex dynamics, and pixels as input.
1",0.29411764705882354,0.12195121951219512,0.17241379310344826,0.04,0.01639344262295082,0.023255813953488372,0.19607843137254902,0.08130081300813008,0.11494252873563218,0.0021141064888454573,0.514033
122,"despite the recent advancements in deep reinforcement learning algorithms and architectures, there are many “hard exploration” challenges characterized by particularly sparse environment rewards. despite these impressive results, there is two limitations of DQfD and related methods. they assume that there is no “domain gap” between the agent’s and demonstrator’s observation space.","Deep reinforcement learning methods traditionally struggle with tasks where en-
vironment rewards are particularly sparse. One successful method of guiding
exploration in these domains is to imitate trajectories provided by a human demon-
strator. However, these demonstrations are typically collected under artiﬁcial
conditions, i.e. with access to the agent’s exact environment setup and the demon-
strator’s action and reward trajectories. Here we propose a two-stage method that
overcomes these limitations by relying on noisy, unaligned footage without access
to such data. First, we learn to map unaligned videos from multiple sources to a
common representation using self-supervised objectives constructed over both time
and modality (i.e. vision and sound). Second, we embed a single YouTube video
in this representation to construct a reward function that encourages an agent to
imitate human gameplay. This method of one-shot imitation allows our agent to
convincingly exceed human-level performance on the infamously hard exploration
games MONTEZUMA’S REVENGE, PITFALL! and PRIVATE EYE for the ﬁrst time,
even if the agent is not presented with any environment rewards.
1",0.5660377358490566,0.16483516483516483,0.2553191489361702,0.1346153846153846,0.03867403314917127,0.060085836909871244,0.2830188679245283,0.08241758241758242,0.1276595744680851,0.0016582823979057763,0.5740622
123,deep reinforcement learning (RL) has led to artificial agents able to reach human-level control on a wide array of tasks such as some Atari 2600 games. the first challenge is processing diverse reward distributions. an algorithm must learn stably regardless of reward density and scale. this may change the set of optimal policies.,"Despite signiﬁcant advances in the ﬁeld of deep Reinforcement Learning (RL),
today’s algorithms still fail to learn human-level policies consistently over a set of
diverse tasks such as Atari 2600 games. We identify three key challenges that any
algorithm needs to master in order to perform well on all games: processing diverse
reward distributions, reasoning over long time horizons, and exploring efﬁciently.
In this paper, we propose an algorithm that addresses each of these challenges and
is able to learn human-level policies on nearly all Atari games. A new transformed
Bellman operator allows our algorithm to process rewards of varying densities
and scales; an auxiliary temporal consistency loss allows us to train stably using a
discount factor of γ = 0.999 (instead of γ = 0.99) extending the effective planning
horizon by an order of magnitude; and we ease the exploration problem by using
human demonstrations that guide the agent towards rewarding states. When tested
on a set of 42 Atari games, our algorithm exceeds the performance of an average
human on 40 games using a common set of hyper parameters. Furthermore, it is
the ﬁrst deep RL algorithm to solve the ﬁrst level of MONTEZUMA’S REVENGE.
1",0.7407407407407407,0.19801980198019803,0.3125,0.32075471698113206,0.0845771144278607,0.13385826771653542,0.5555555555555556,0.1485148514851485,0.23437499999999997,0.003846013604494924,0.8013022
124,"the distinc- tion lies in how we subjectively explain these systems, and identifies two ‘explanatory strategies’1: the physical stance, which dennett (2009) describes as the standard laborious method of the physical sciences. he describes the intentional stance as the strategy of interpreting the behavior of an entity (person, animal, arti fact, whatever) by treating it as if it were a rational agent.","According to Dennett, the same system may be de-
scribed using a ‘physical’ (mechanical) explanatory
stance, or using an ‘intentional’ (belief- and goal-
based) explanatory stance. Humans tend to ﬁnd the
physical stance more helpful for certain systems,
such as planets orbiting a star, and the intentional
stance for others, such as living animals. We de-
ﬁne a formal counterpart of physical and intentional
stances within computational theory: a description
of a system as either a device, or an agent, with the
key difference being that ‘devices’ are directly de-
scribed in terms of an input-output mapping, while
‘agents’ are described in terms of the function they
optimise. Bayes’ rule can then be applied to calcu-
late the subjective probability of a system being a
device or an agent, based only on its behaviour. We
illustrate this using the trajectories of an object in a
toy grid-world domain.
1",0.49206349206349204,0.2052980132450331,0.2897196261682243,0.0967741935483871,0.04,0.05660377358490566,0.31746031746031744,0.13245033112582782,0.18691588785046728,0.0074830739805816174,0.6248808
125,"can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. the combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail.","https://arxiv.org/abs/1806.01186
2019-3-11
Penalizing side effects using stepwise relative
reachability
Victoria Krakovna1, Laurent Orseau1, Ramana Kumar1, Miljan Martic1 and Shane Legg1
1DeepMind
How",0.05555555555555555,0.10344827586206896,0.07228915662650602,0.018867924528301886,0.03571428571428571,0.024691358024691357,0.05555555555555555,0.10344827586206896,0.07228915662650602,0.007929020238848623,0.29563662
126,"a key signature of human intelligence is the ability to make “infinite use of finite means”. this is, constructing new inferences, predictions, and behaviors from known building blocks. we use hierarchies to abstract away from fine-grained differences.","Artiﬁcial intelligence (AI) has undergone a renaissance recently, making major progress in
key domains such as vision, language, control, and decision-making. This has been due, in
part, to cheap data and cheap compute resources, which have ﬁt the natural strengths of deep
learning. However, many deﬁning characteristics of human intelligence, which developed under
much diﬀerent pressures, remain out of reach for current approaches. In particular, generalizing
beyond one’s experiences—a hallmark of human intelligence from infancy—remains a formidable
challenge for modern AI.
The following is part position paper, part review, and part uniﬁcation. We argue that
combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that
structured representations and computations are key to realizing this objective. Just as biology
uses nature and nurture cooperatively, we reject the false choice between “hand-engineering”
and “end-to-end” learning, and instead advocate for an approach which beneﬁts from their
complementary strengths. We explore how using relational inductive biases within deep learning
architectures can facilitate learning about entities, relations, and rules for composing them. We
present a new building block for the AI toolkit with a strong relational inductive bias—the graph
network—which generalizes and extends various approaches for neural networks that operate
on graphs, and provides a straightforward interface for manipulating structured knowledge and
producing structured behaviors. We discuss how graph networks can support relational reasoning
and combinatorial generalization, laying the foundation for more sophisticated, interpretable,
and ﬂexible patterns of reasoning. As a companion to this paper, we have also released an
open-source software library for building graph networks, with demonstrations of how to use
them in practice.
1",0.6052631578947368,0.0812720848056537,0.14330218068535824,0.08108108108108109,0.010638297872340425,0.018808777429467086,0.39473684210526316,0.053003533568904596,0.09345794392523366,7.751654859415287e-05,0.44259873
127,"mix & match – each box represents a policy, optimised with the true RL objective. the blue mm is the control policy, whilst the red policy is the final agent. we consider increasingly re- quire more powerful models, complex action spaces and challenging training regimes.","We introduce Mix & Match (M&M) – a train-
ing framework designed to facilitate rapid and
effective learning in RL agents, especially those
that would be too slow or too challenging to train
otherwise.
The key innovation is a procedure
that allows us to automatically form a curricu-
lum over agents. Through such a curriculum we
can progressively train more complex agents by,
effectively, bootstrapping from solutions found
by simpler agents. In contradistinction to typ-
ical curriculum learning approaches, we do not
gradually modify the tasks or environments pre-
sented, but instead use a process to gradually al-
ter how the policy is represented internally. We
show the broad applicability of our method by
demonstrating signiﬁcant performance gains in
three different experimental setups: (1) We train
an agent able to control more than 700 actions
in a challenging 3D ﬁrst-person task; using our
method to progress through an action-space cur-
riculum we achieve both faster training and better
ﬁnal performance than one obtains using tradi-
tional methods. (2) We further show that M&M
can be used successfully to progress through a
curriculum of architectural variants deﬁning an
agents internal state.
(3) Finally, we illustrate
how a variant of our method can be used to im-
prove agent performance in a multitask setting.
1",0.5348837209302325,0.10599078341013825,0.17692307692307693,0.07142857142857142,0.013888888888888888,0.023255813953488372,0.37209302325581395,0.07373271889400922,0.12307692307692307,0.0003531022057509995,0.64920056
128,"relationshipal reasoning is the process of understanding the ways in which entities are connected and using this understanding to accomplish some higher order goal. if previous hidden states are interpreted as entities, then computing a weighted sum of entities using attention helps to remove the locality bias present in vanilla RNNs.","Memory-based neural networks model temporal data by leveraging an ability to
remember information for long periods. It is unclear, however, whether they also
have an ability to perform complex relational reasoning with the information they
remember. Here, we ﬁrst conﬁrm our intuitions that standard memory architectures
may struggle at tasks that heavily involve an understanding of the ways in which
entities are connected – i.e., tasks involving relational reasoning. We then improve
upon these deﬁcits by using a new memory module – a Relational Memory Core
(RMC) – which employs multi-head dot product attention to allow memories to
interact. Finally, we test the RMC on a suite of tasks that may proﬁt from more
capable relational reasoning across sequential information, and show large gains
in RL domains (e.g. Mini PacMan), program evaluation, and language modeling,
achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and
GigaWord datasets.
1",0.45098039215686275,0.14838709677419354,0.2233009708737864,0.12,0.03896103896103896,0.058823529411764705,0.3137254901960784,0.1032258064516129,0.15533980582524273,0.022050127849691806,0.56584674
129,"a conference paper at ICLR 2019 LEARNING TO UNDERSTAND GOAL SPECIFICATIONS BY MODELLING REWARD Dzmitry Bahdanau Mila. in each of these cases, being able to reward an agent for success- fully completing a task specified by an instruction requires the implementation of a full interpreter of the instruction language. each of them can be interpreted as a result of successfully executing the instruction “build an L-like shape from","Recent work has shown that deep reinforcement-learning agents can learn to follow
language-like instructions from infrequent environment rewards. However, this
places on environment designers the onus of designing language-conditional reward
functions which may not be easily or tractably implemented as the complexity of
the environment and the language scales. To overcome this limitation, we present
a framework within which instruction-conditional RL agents are trained using
rewards obtained not from the environment, but from reward models which are
jointly trained from expert examples. As reward models improve, they learn to
accurately reward agents for completing tasks for environment conﬁgurations—and
for instructions—not present amongst the expert data. This framework effectively
separates the representation of what instructions require from how they can be
executed. In a simple grid world, it enables an agent to learn a range of commands
requiring interaction with blocks and understanding of spatial relations and under-
speciﬁed abstract arrangements. We further show the method allows our agent to
adapt to changes in the environment without requiring new expert examples.
1",0.5362318840579711,0.20670391061452514,0.29838709677419356,0.08823529411764706,0.033707865168539325,0.048780487804878044,0.2898550724637681,0.11173184357541899,0.16129032258064516,0.002980556271890301,0.5980964
130,"Spectral algorithms are central to machine learning and scientific computing. in machine learning, eigendecomposition and singular value decomposition are foundational tools, used for PCA as well as a wide variety of other models. this is the case in many applications in quantum physics.","We present Spectral Inference Networks, a framework for learning eigenfunctions
of linear operators by stochastic optimization. Spectral Inference Networks gen-
eralize Slow Feature Analysis to generic symmetric operators, and are closely
related to Variational Monte Carlo methods from computational physics. As such,
they can be a powerful tool for unsupervised representation learning from video or
graph-structured data. We cast training Spectral Inference Networks as a bilevel
optimization problem, which allows for online learning of multiple eigenfunc-
tions. We show results of training Spectral Inference Networks on problems in
quantum mechanics and feature learning for videos on synthetic datasets. Our
results demonstrate that Spectral Inference Networks accurately recover eigen-
functions of linear operators and can discover interpretable representations from
video in a fully unsupervised manner.
1",0.4186046511627907,0.14285714285714285,0.21301775147928995,0.047619047619047616,0.016,0.023952095808383235,0.2558139534883721,0.0873015873015873,0.1301775147928994,0.002604411667122206,0.5407351
131,"a new model and associated training algorithm, called Temporal Difference Variational Auto-Encoder (TD-VAE), meets all of the above requirements. the model should learn a belief state, i.e. a deterministic, coded representation of the filtering posterior of the state given all the observations up to a given time. it can be used to train consistent jumpy simulators of complex 3D environments.","To act and plan in complex environments, we posit that agents should have a mental
simulator of the world with three characteristics: (a) it should build an abstract state
representing the condition of the world; (b) it should form a belief which represents
uncertainty on the world; (c) it should go beyond simple step-by-step simulation,
and exhibit temporal abstraction. Motivated by the absence of a model satisfying all
these requirements, we propose TD-VAE, a generative sequence model that learns
representations containing explicit beliefs about states several steps into the future,
and that can be rolled out directly without single-step transitions. TD-VAE is
trained on pairs of temporally separated time points, using an analogue of temporal
difference learning used in reinforcement learning.
1",0.6031746031746031,0.2992125984251969,0.4,0.11290322580645161,0.05555555555555555,0.07446808510638298,0.23809523809523808,0.11811023622047244,0.15789473684210525,0.006083485924477786,0.6332383
132,"a class of causal video understanding mod- els aims to improve efficiency of video processing by maximising throughput, minimising latency, and reducing the number of clock cycles. the models are still very deep, with dozens of such operations being performed but in a pipelined fashion that enables depth-parallel computation. in sec. 4 we present our main experiments on two types of prediction tasks with different latency requirements.","and
to vary less over time [28], obeying the so-called slowness principle [35] – fast
varying observations can be explained by slow varying latent factors. For exam-
ple, when tracking a non-rigid moving object, the contours, which are shallow
features, change rapidly, but the identity of the object typically does not change
at all. Since not all features change at the same rate as the input rate, it is then
possible to reduce computation by reusing, and not recomputing, the deeper,
more abstract, features. This can be implemented by having multi-rate clocks:
whenever the clock of a layer does not tick, that layer does not compute activa-
tions, instead it reuses the existing ones. 3D ConvNets implement this principle
by using temporal strides but does not keep state and hence cannot eﬃciently
operate frame-by-frame. In our recurrent setting, multi-rate clocks can be imple-
mented by removing nodes from the unrolled graph and preserving an internal
state to cache outputs until the next slower-ticking layer can consume them. We
used a set of ﬁxed rates in our models, typically reducing clock rates by a factor
of two whenever spatial resolution is halved. Instead of just using identity to
create the internal state as we did, one could use any spatial recurrent module
(conv. versions of vanilla RNNs or LSTMs). This design is shown in ﬁg. 4 (d).
For pixelwise prediction tasks, the state tensors from the last layer of a given
spatial resolution are also passed through skip connections, bilinearly upsampled
and concatenated as input to the dense prediction head, similar to the skip
connections in FCN models [36], but arise from previous time steps 1.
1 More sophisticated trainable decoders, such as those in U-Nets [37], could also be
used in a similar pipelined fashion as the encoder.
Massively Parallel Video Networks
7
3.2
Temporal ﬁlters and feedback
The success of depth-parallelism and multi-rate clocks depends on the network
being able to learn to compensate for otherwise delayed, possibly stale inputs,
which may be feasible since videos are quite redundant and scene dynamics are
predictable over short temporal horizons. One way to make learning easier would
seem to be by using units with temporal ﬁlters. These have shown their worth
in a variety of video models [38,21,6]. We illustrate the use of temporal ﬁlters
in ﬁg. 4, (b) as temporalisation. Interestingly, depth-paralellisation by itself also
induces temporalisation in models with skip connections.
For dense predictions tasks, we experimented with adding a feedback con-
nection – the outputs of the previous frame are fed as inputs to the early layers
of the network (e.g. stacking them with the output of the ﬁrst conv. layer).
The idea is that previous outputs provide a simple starting solution with rich
semantics which can be reﬁned in few layers – similar to several recent pa-
pers [39,40,41,42,43]. This design is shown in ﬁg. 4, (c).
Fig. 4. Basic image models (left) can be extended along the temporal domain using dif-
ferent patterns of connectivity. Temporalisation adds additional inputs to the diﬀerent
computation nodes, increasing their temporal receptive ﬁeld. Feedback re-injects past
high-level activations to the bottom of the network. Both connectivity patterns aim to
improve the expressivity of the models. For increasing throughput, having multi-rate
clocks avoids always computing deeper activations (here shown for a temporal model),
and instead past activations are copied periodically.
I0
y0
I1
y1
I2
y2
I3
y3
I4
y4
(a) Basic image model
(b) Temporalisation
y0
y1
y2
y3
y4
I0
I1
I2
I3
I4
(c) Feedback
y0
y1
y2
y3
y4
I0
I1
I2
I3
I4
(d) Multi-rate Clocks
y0
y1
y2
y3
I0
I1
I2
I3
I4
3.3
Sequential-to-parallel “distillation”
The proposed parallel models reduce latency, but their computational depth for
the current frame at the moment where they produce an output is also reduced
compared to their fully sequential counterparts; additionally they are designed
to re-use features from previous states through the multi-rate clocks mechanism.
These properties typically make learning more diﬃcult. In order to improve the
accuracy of our parallel models, we adopt a strategy similar to distillation [44],
or to Ladder networks [45], wherein a teacher network is privileged relative to
8
Carreira, P˘atr˘aucean et al.
a student network, either due to having a greater capacity or (in the case of
Ladder networks) access to greater amounts of information.
In our case, we consider the sequential model as the teacher, since all of its
layers always have access to fresh features extracted from the current frame. We
ﬁrst train a causal fully-sequential model with the same overall architecture as
the parallel model. Then we modify the loss of the parallel model to encourage
its activations to match those of the sequential model for some given layers,
while still minimising the original classiﬁcation error, such that it predicts how
the abstract features would have looked, had the information from the current
frame been available. This is illustrated for one layer on the right side of ﬁg. 3.
In our experiment we used the average of this new loss over m = 3 layers. The
overall loss Ld with distillation is:
Ld = L(y, ygt) + λ
m
X
i=1
1
ni
ˆa(i) −a(i)
2
where L(y, ygt) is the initial cross-entropy loss between the predictions of the
parallel network y and the ground truth ygt, and the second term is the nor-
malised Euclidean distance between the activations of the pre-trained sequential
model ˆa(i) for layer i and the activation of the parallel model a(i) for the same
layer; ni denotes the number of feature channels of layer i. A parameter λ is
used to weight the two components of the new loss. We set λ = 1 for the dense
keypoint prediction and λ = 100 for action recognition.
4
Experiments
We applied the proposed principles starting from two popular image classiﬁca-
tion models: a 54 layer DenseNet [12] and Inception [11], which has 22 conv.
layers. We chose these models due to their diﬀerences in connectivity. Incep-
tion has some built-in parallelism due to the parallel branches in the Inception
blocks. DenseNet has no parallelism and instead has dense skip connections
within blocks, which helps reduce information latency when parallelised. Full
details on the architectures are provided in the supp. material.
We instantiated a number of model variations using the principles set in the
previous section. In all cases we are interested in the online, causal setting (i.e.
no peeking into the future), where eﬃciency matters the most. In the majority of
the experiments we trained models with 0 prediction latency (e.g. the output at
time t should correspond to the input at time t), the most challenging setting. We
name pipelined DenseNet models as Par-DenseNet and Inception-based models
as Par-Inception.
For evaluation, we considered two tasks having diﬀerent latency and through-
put requirements: (1) action classiﬁcation, where the network must output only
one label prediction for the entire video sequence, and (2) human keypoint lo-
calisation, where the network must output dense per-frame predictions for the
locations of human joints – in our case spatial heatmaps for the keypoints of
interest (see ﬁg. 5).
Massively Parallel Video Networks
9
Model
#Par. Subnets. Par-Inception Top-1 Par-Dense. Top-1
non-causal
1
71.8
-
sequential causal
1
71.4
67.6
semi-parallel causal
5 (7)
66.0
61.3
parallel causal
10 (14)
54.5
54.0
Table 1. Test accuracy as percentage for action recognition on the miniKinetics
dataset [46], using networks with multi-rate clocks and temporal ﬁlters. The num-
ber of parallel subnetworks is shown in the second column. For the semi-parallel case,
Par-Inception uses 5 parallel subnetworks and Par-DenseNet 7. The non-causal, single
subnetwork Par-Inception in the ﬁrst row is equivalent to the I3D model [6].
The dataset for training and evaluation in all cases was miniKinetics [46],
which has 80k training videos and 5k test videos. MiniKinetics is a subset of the
larger Kinetics [24], but more pratical when studying many factors of variation.
For heatmap estimation we populated miniKinetics automatically with poses
from a state-of-the-art 2D pose estimation method [47] – that we will call baseline
from now on – and used those as ground truth. This resulted in a total of 20
million training frames 2.
4.1
Action recognition
For this task we experimented with three levels of depth-parallelism for both
architectures: fully sequential, 5, and 10 parallel subnetworks for Par-Inception
models and fully sequential, 7, and 14 parallel subnetworks for Par-DenseNet
models. Table 1 presents the results in terms of Top-1 accuracy on miniKinetics.
The accuracy of the original I3D model [6] on miniKinetics is 78.3%, as reported
in [46]. This model is non-causal, but otherwise equivalent to the fully sequential
version of our Par-Inception 3.
There is a progressive degradation in performance as more depth-parallelism
is added, i.e. as the models become faster and faster, illustrating the trade-oﬀ
between speedup and accuracy. One possible explanation is the narrowing of the
temporal receptive ﬁeld, shown in ﬁg. 2. The activations of the last frames in
each training clip do not get to be processed by the last classiﬁer layer, which is
equivalent to training on shorter sequences – a factor known to impact negatively
the classiﬁcation accuracy. We intend to increase the length of the clips in future
work to explore this further. Promisingly, the loss in accuracy can be reduced
partially by just using distillation; see subsection 4.3.
2 This is far higher than the largest 2D pose video dataset, PoseTrack [48], which has
just 20k annotated frames, hardly suﬃcient for training large video models from
scratch (although cleanly annotated instead of automatically).
3 Note that this was pre-trained using ImageNet, hence it has a signiﬁcant advantage
over all our models that are trained from scratch.
10
Carreira, P˘atr˘aucean et al.
4.2
Human keypoint localisation
For this task we experimented with 5 diﬀerent levels of depth-parallelism for
Par-DenseNet: fully sequential and 2, 4, 7 and 14 parallel subnetworks. For Par-
Inception, we used three diﬀerent depth-parallelism levels: fully sequential, 5,
and 10 parallel subnetworks. We employed a weighted sigmoid cross-entropy
loss. Since the heatmaps contain mostly background (no-joint) pixels, we found
it essential to weight the importance of the keypoint pixels in the loss – we used
a factor of 10. For evaluation, we report results on the miniKinetics test set in
terms of weighted sigmoid cross-entropy loss.
Results using the pipelining connectivity with multi-rate clock models are
shown in ﬁg. 6, left. For both models, it can be observed that the performance
improves as more layers are allowed to execute in sequence. Par-Inception has
slightly better performance for higher degrees of parallelism, perhaps due to its
built-in parallelism; Par-DenseNet models become better as less parallelism is
used.
Since Par-DenseNet oﬀers more possibilities for parallelisation, we used it
to investigate more designs, i.e.: with/without multi-rate clocks, temporal ﬁlters
and feedback. The results are shown in ﬁg. 6, right. Versions with temporal ﬁlters
do better than without except for the most parallel models – these have intrinsi-
cally temporal receptive ﬁelds because of the skip connections in time, without
needing explicit temporal ﬁlters. Feedback helps slightly. Clocks degrade accu-
racy a little but provide big speedups (see subsection 4.6). We show predictions
for two test videos in ﬁg. 5.
4.3
Sequential to parallel distillation
As mentioned in section 3, we investigated training ﬁrst a sequential model, then
ﬁtting the parallel model to a subset of its activations in addition to the original
loss function. This led to signiﬁcant improvements for both models. The parallel
causal Par-Inception model obtains a relative improvement in accuracy of about
12%, from 54.5% to 61.2% for action recognition. The improvement for multi-
rate Par-DenseNet model on the keypoint localisation task is shown in ﬁg. 7.
4.4
Training speciﬁcally for depth-parallelism
Is it important to train a model speciﬁcally for operating in parallel mode or can
we rewire a pretrained sequential model and it will work just as well at inference
time? We ran an experiment where we initialiased Par-DenseNet models with
diﬀerent levels of parallelism with the weights from the DenseNet fully sequen-
tial model and ran inference on the miniKinetics test set. The results are shown
in ﬁg. 8, left, and indicate the importance of training with depth-parallelism
enabled, so the network learns to behave predictively. We similarly evaluated
the test loss of Par-DenseNet models with diﬀerent levels of parallelism when
initialised from a fully-parallel trained model. As expected, in this case the be-
haviour does not change much.
Massively Parallel Video Networks
11
Fig. 5. Example outputs on a subset of frames one second apart from two videos
of the miniKinetics test set. “Ground truth” keypoints from the model [47] used to
automatically annotate the dataset are shown as triangles, our models predictions are
shown as circles. Note that the parallel models exhibit some lag when the legs move
quickly on the video on the left. Best seen zoomed on a computer screen in color.
Par-DenseNet models with 14 parallel subnetworks, without clocks
Fully sequential Par-DenseNet model, without clocks
Par-DenseNet models with 14 parallel subnetworks, with clocks
Fully sequential Par-DenseNet model, with clocks
4.5
Eﬀect of higher prediction latency
All the results above were obtained when training for 0 frames of prediction
latency. However, if a parallel model is several times faster than a sequential one,
we can aﬀord to introduce a prediction latency greater than zero frames. Figure 8,
right, shows results for Par-DenseNet models in this setting. As expected, the
test loss decreases as the prediction latency increases, since more layers get to
process the input frame before a prediction needs to be made. Strikingly, by using
a predictive delay of 2 frames, models with up to 4 depth-parallel subnetworks
are as accurate as fully sequential models with 0 frame predictive latency.
4.6
Eﬃciency measurements
In this section, we present the eﬃciency improvements achieved by the proposed
models, comparing the cases with and without multi-rate clocks and with diﬀer-
12
Carreira, P˘atr˘aucean et al.
14(10)
7(5)
4
2
1(1)
Number of depth-parallel subnetworks
15
20
25
30
35
40
45
50
55
Test loss (x10−3)
Cl+T Par-Inception
Cl+T Par-DenseNet
14
7
4
2
1
Number of depth-parallel subnetworks
15
20
25
30
35
40
45
50
55
Test loss (x10−3)
Cl+T Par-DenseNet
Cl+FB Par-DenseNet
Cl+FB+T Par-DenseNet
T Par-DenseNet
FB+T Par-DenseNet
Fig. 6. Weighted sigmoid cross-entropy (lower is better) for human keypoint locali-
sation on miniKinetics test set for zero prediction latency. “Cl” denotes models with
multi-rate clocks, “T” – models with temporal ﬁlters, “FB” – models with feedback.
Left: Comparison between Par-Inception and Par-DenseNet for diﬀerent levels of par-
allelism. Note that in terms of number of sequential convolutions, 14 subnetworks for
Par-DenseNet are equivalent to 10 subnetworks for Par-Inception, and similar for 7(5).
Right: Variations of Par-DenseNet. In the absence of parallelisation (1 subnetwork),
the accuracy of the best models with multi-rate clocks is just slightly worse to that
of a much slower sequential model. Parallelisation penalises the accuracy of models
with clocks more. The basic Par-DenseNet can have up to 4 parallel subnetworks with
modest drop of accuracy.
14
7
4
2
1
Number of depth-parallel subnetworks
15
20
25
30
35
40
45
50
55
Test loss (x10−3)
Cl+FB+T Par-DenseNet
Cl+FB+T Par-DenseNet Distilled
Fig. 7. Comparison between the weighted sigmoid cross-entropy (lower is better) of
models with diﬀerent levels of parallelism and the same models distilled from sequential
for human keypoint localisation on miniKinetics test set for zero prediction latency.
Results presented for a DenseNet model with multi-rate clocks (“Cl”), temporal ﬁlters
(“T”), and feedback (“FB”). See text for details.
ent numbers of parallel subnetworks. Our parallel models improve eﬃciency un-
der the assumption that parallel computation resources are available. We bench-
mark our models on CPUs and GPUs by running inference on a CPU with 48
cores and on hosts with 2, 4, and 8 k40 GPUs, respectively. The GPUs were on
the same machine to avoid network latency. For benchmarking, each model is
run on 3000 frames and we average the time used to process each frame. Re-
sults are presented in table 2. A ﬁgure illustrating the loss in accuracy as the
throughput is increased can be found in the supp. material.
Massively Parallel Video Networks
13
14
7
4
2
1
Number of depth-parallel subnetworks
15
20
25
30
35
40
45
50
55
Test loss (x10−3)
Cl+FB+T Seq. weights
Cl+FB+T Par. weights
14
7
4
2
1
Number of depth-parallel subnetworks
16
18
20
22
24
26
28
30
Test loss (x10−3)
T - pred. latency 0
T - pred. latency 2
T - pred. latency 6
Fig. 8. Left: Seq. weights - Behaviour of Par-DenseNet with diﬀerent levels of par-
allelism at inference time when trained with sequential connectivity. Par. weights -
behaviour of Par-DenseNet with diﬀerent levels of parallelism at inference time when
trained with fully-parallel connectivity. Right: Test loss for Par-DenseNet when pre-
diction latency is allowed to be greater than zero.
Our models are implemented using TensorFlow (TF) [49], hence: (1) when
running on a multi-core CPU, we can run multiple operations in parallel and to
parallelise a single operation, e.g., for conv layers. This means that the sequential
model becomes faster with more cores, but only up to a certain point, when the
overhead cancels out the gain from parallelism. The proposed parallel models
beneﬁt far more from having many CPU cores. (2) Multiple operations cannot
run in parallel on the same GPU, hence there is little beneﬁt in running our
models on a single GPU. (3) A single operation cannot be split between GPUs.
This explains why the sequential image model performance does not improve
with more GPUs.
Par-DenseNet. Our Par-DenseNet architecture has a total of 4+8+8+6=26
miniblocks so when using 14 parallel subnetworks, each parallel subnetwork is
made of at most 2 miniblocks. When not using multi-rate clocks, 26 miniblocks
are executed for each frame resulting in 416 miniblocks executions for a sequence
of 16 frames. However when using multi-rate clocks, only 86 miniblocks are
executed for such a sequence, which theoretically results in a speedup of 4.8×. We
observe some smaller speedup but this is likely to be explained by the miniblocks
having diﬀerent sizes.
Par-Inception. Our models have 9 inception blocks. The most parallel version
uses 10 parallel subnetworks: one for the initial convolutions and one for each
inception block. For the sequential version, roughly a third of the time is spent
on these initial convolutions. This explains why we do not observe speedups
greater than 3 for the models without clocks when using more GPUs and we do
not see much diﬀerence between using 4 and 8 GPU. More details together with
execution timelines are included in the supp. material.
5
Conclusion
We introduced the paradigm of processing video sequences using networks that
are constrained in the amount of sequential processing they can perform, with
14
Carreira, P˘atr˘aucean et al.
Model
# Par. subnets 48 cores 2 GPUs 4 GPUs 8 GPUs
Par-DenseNet without multi-rate clocks
sequential
1",0.7941176470588235,0.016028495102404273,0.03142275240034914,0.23880597014925373,0.004750593824228029,0.009315866084425037,0.5441176470588235,0.010982487384980706,0.021530404422461444,3.1401180892086723e-22,0.5012846
133,"experience replay has become nearly ubiquitous in modern large-scale, deep reinforcement learning systems. ER maintains a buffer of transitions for replay, and Dyna a search-control queue composed of stored states and actions. it is not hard to imagine situ- ations where a Dyna-style approach could be better than ER.","Model-based strategies for control are critical to
obtain sample efﬁcient learning. Dyna is a plan-
ning paradigm that naturally interleaves learning
and planning, by simulating one-step experience
to update the action-value function. This elegant
planning strategy has been mostly explored in the
tabular setting. The aim of this paper is to revisit
sample-based planning, in stochastic and continu-
ous domains with learned models. We ﬁrst highlight
the ﬂexibility afforded by a model over Experience
Replay (ER). Replay-based methods can be seen as
stochastic planning methods that repeatedly sample
from a buffer of recent agent-environment interac-
tions and perform updates to improve data efﬁciency.
We show that a model, as opposed to a replay buffer,
is particularly useful for specifying which states to
sample from during planning, such as predecessor
states that propagate information in reverse from a
state more quickly. We introduce a semi-parametric
model learning approach, called Reweighted Ex-
perience Models (REMs), that makes it simple to
sample next states or predecessors. We demonstrate
that REM-Dyna exhibits similar advantages over
replay-based methods in learning in continuous state
problems, and that the performance gap grows when
moving to stochastic domains, of increasing size.
1",0.5,0.12682926829268293,0.20233463035019456,0.058823529411764705,0.014705882352941176,0.023529411764705882,0.2692307692307692,0.06829268292682927,0.10894941634241245,0.0015444882396222065,0.6037585
134,"the semantic segmentation task assigns a class label to each pixel in an image. a lesion might be clearly visible, but the information about whether it is cancer tissue or not might not be available from this image alone. similar ambiguities are present in photos.","Many real-world vision problems suffer from inherent ambiguities. In clinical
applications for example, it might not be clear from a CT scan alone which par-
ticular region is cancer tissue. Therefore a group of graders typically produces
a set of diverse but plausible segmentations. We consider the task of learning a
distribution over segmentations given an input. To this end we propose a generative
segmentation model based on a combination of a U-Net with a conditional vari-
ational autoencoder that is capable of efﬁciently producing an unlimited number
of plausible hypotheses. We show on a lung abnormalities segmentation task
and on a Cityscapes segmentation task that our model reproduces the possible
segmentation variants as well as the frequencies with which they occur, doing so
signiﬁcantly better than published approaches. These models could have a high
impact in real-world applications, such as being used as clinical decision-making
algorithms accounting for multiple plausible semantic segmentation hypotheses to
provide possible diagnoses and recommend further actions to resolve the present
ambiguities.
1",0.5555555555555556,0.14367816091954022,0.22831050228310504,0.13636363636363635,0.03468208092485549,0.055299539170506916,0.2,0.05172413793103448,0.0821917808219178,0.0026581695703654555,0.5057818
135,"Georg Ostrovski * 1 Will Dabney * 1 R emi Munos 1 1. Introduction There has been a staggering increase in progress on genera-tive modeling in recent years. these have led to breakthroughs in state-of-the-art generation of natural images (Karras et al., 2017) and variational inference (Kingma & Welling, 2013), and even been used for unsupervised learning of disentangled representations.","We introduce autoregressive implicit quantile
networks (AIQN), a fundamentally different ap-
proach to generative modeling than those com-
monly used, that implicitly captures the distribu-
tion using quantile regression. AIQN is able to
achieve superior perceptual quality and improve-
ments in evaluation metrics, without incurring a
loss of sample diversity. The method can be ap-
plied to many existing models and architectures.
In this work we extend the PixelCNN model with
AIQN and demonstrate results on CIFAR-10 and
ImageNet using Inception score, FID, non-cherry-
picked samples, and inpainting results. We con-
sistently observe that AIQN yields a highly stable
algorithm that improves perceptual quality while
maintaining a highly diverse distribution.",0.19672131147540983,0.10714285714285714,0.13872832369942195,0.0,0.0,0.0,0.14754098360655737,0.08035714285714286,0.10404624277456649,0.002222203875582979,0.48503798
136,"ICLR 2018 MAXIMUM A POSTERIORI POLICY OPTIMISATION Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Martin Riedmiller DeepMind, London, UK. these algorithms can acquire sophisticated behaviours by interacting with the environment while receiving simple rewards. they can require a large number of samples and – especially in continuous action spaces – suffer from high gradi- ent","We introduce a new algorithm for reinforcement learning called Maximum a-
posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-
entropy objective. We show that several existing methods can directly be related
to our derivation. We develop two off-policy algorithms and demonstrate that
they are competitive with the state-of-the-art in deep reinforcement learning. In
particular, for continuous control, our method outperforms existing methods with
respect to sample efﬁciency, premature convergence and robustness to hyperpa-
rameter settings.
1",0.2909090909090909,0.19047619047619047,0.23021582733812948,0.09259259259259259,0.060240963855421686,0.072992700729927,0.2,0.13095238095238096,0.15827338129496404,0.0067101675237047,0.5852768
137,"implicit quantile networks (IQN) focuses on intrinsic randomness of returns within the reinforcement learning framework. distributional RL aims to model the distribution over returns, whose mean is the traditional value function, and to use these distributions to evaluate and optimize a policy. dabney et al. (2018) propose an alternate pair of choices, parameterizing the distribution by a discrete set of quan- tiles.","In this work, we build on recent advances in dis-
tributional reinforcement learning to give a gener-
ally applicable, ﬂexible, and state-of-the-art dis-
tributional variant of DQN. We achieve this by
using quantile regression to approximate the full
quantile function for the state-action return distri-
bution. By reparameterizing a distribution over
the sample space, this yields an implicitly deﬁned
return distribution and gives rise to a large class of
risk-sensitive policies. We demonstrate improved
performance on the 57 Atari 2600 games in the
ALE, and use our algorithm’s implicitly deﬁned
distributions to study the effects of risk-sensitive
policies in Atari games.",0.46774193548387094,0.2636363636363636,0.3372093023255814,0.04918032786885246,0.027522935779816515,0.03529411764705882,0.24193548387096775,0.13636363636363635,0.1744186046511628,0.008201605591692899,0.72228426
138,"meta-learning achieves state of the art in few shot learning, for example by allowing a reinforcement learning algorithm to learn the optimal speed or direction of a simulated cheetah or four-legged robot. a similar effect was proposed by Mark Baldwin to explain how evolu-tion could deal with irreducibly complex adaptations without the need for Lamarckian information flow.","The scope of the Baldwin effect was recently called into question by two papers
that closely examined the seminal work of Hinton and Nowlan. To this date there
has been no demonstration of its necessity in empirically challenging tasks. Here
we show that the Baldwin effect is capable of evolving few-shot supervised and re-
inforcement learning mechanisms, by shaping the hyperparameters and the initial
parameters of deep learning algorithms. Furthermore it can genetically accom-
modate strong learning biases on the same set of problems as a recent machine
learning algorithm called MAML ”Model Agnostic Meta-Learning” which uses
second-order gradients instead of evolution to learn a set of reference parameters
(initial weights) that can allow rapid adaptation to tasks sampled from a distri-
bution. Whilst in simple cases MAML is more data efﬁcient than the Baldwin
effect, the Baldwin effect is more general in that it does not require gradients to
be backpropagated to the reference parameters or hyperparameters, and permits
effectively any number of gradient updates in the inner loop. The Baldwin effect
learns strong learning dependent biases, rather than purely genetically accommo-
dating ﬁxed behaviours in a learning independent manner.
1",0.45,0.1377551020408163,0.2109375,0.1016949152542373,0.03076923076923077,0.047244094488188976,0.3,0.09183673469387756,0.140625,0.0015957890098244447,0.59432876
139,ICLR 2019 DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH Hanxiao Liu CMU hanxiaol@cs.cmu.edu INTRODUCTION Discovering state-of-the-art neural network architectures requires substantial effort of human experts. the best existing architecture search algorithms are computationally demanding despite their remarkable performance.,"This paper addresses the scalability challenge of architecture search by formulating
the task in a differentiable manner. Unlike conventional approaches of applying evo-
lution or reinforcement learning over a discrete and non-differentiable search space,
our method is based on the continuous relaxation of the architecture representation,
allowing efﬁcient search of the architecture using gradient descent. Extensive
experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that
our algorithm excels in discovering high-performance convolutional architectures
for image classiﬁcation and recurrent architectures for language modeling, while
being orders of magnitude faster than state-of-the-art non-differentiable techniques.
Our implementation has been made publicly available to facilitate further research
on efﬁcient architecture search algorithms.
1",0.36585365853658536,0.12396694214876033,0.18518518518518517,0.15,0.05,0.075,0.2682926829268293,0.09090909090909091,0.13580246913580246,0.001734219479304732,0.67635524
140,"modeling raw audio signals is challenging, because they span many different orders of magnitude. the use of symbolic representations comes with limitations: the precise timing, timbre and volume of the notes played by a musician do not correspond exactly to those written in a score. this is usually very difficult and impractical for most instruments.","Realistic music generation is a challenging task. When building generative models
of music that are learnt from data, typically high-level representations such as scores
or MIDI are used that abstract away the idiosyncrasies of a particular performance.
But these nuances are very important for our perception of musicality and realism,
so in this work we embark on modelling music in the raw audio domain. It has
been shown that autoregressive models excel at generating raw audio waveforms
of speech, but when applied to music, we ﬁnd them biased towards capturing
local signal structure at the expense of modelling long-range correlations. This
is problematic because music exhibits structure at many different timescales. In
this work, we explore autoregressive discrete autoencoders (ADAs) as a means to
enable autoregressive models to capture long-range correlations in waveforms. We
ﬁnd that they allow us to unconditionally generate piano music directly in the raw
audio domain, which shows stylistic consistency across tens of seconds.
1",0.509090909090909,0.1728395061728395,0.25806451612903225,0.05555555555555555,0.018633540372670808,0.027906976744186046,0.2,0.06790123456790123,0.10138248847926266,0.00232390921640057,0.5810031
141,"learning Implicit Generative Models with the Method of Learned Moments (MoM) is an ancient principle of learning. given a model with parameters, estimate  such that the moments — or more generally feature averages — of the model match those of the data. other properties of the moment estimator are less desirable.","We propose a method of moments (MoM) algo-
rithm for training large-scale implicit generative
models. Moment estimation in this setting en-
counters two problems: it is often difﬁcult to de-
ﬁne the millions of moments needed to learn the
model parameters, and it is hard to determine
which properties are useful when specifying mo-
ments. To address the ﬁrst issue, we introduce a
moment network, and deﬁne the moments as the
network’s hidden units and the gradient of the net-
work’s output with respect to its parameters. To
tackle the second problem, we use asymptotic the-
ory to highlight desiderata for moments – namely
they should minimize the asymptotic variance of
estimated model parameters – and introduce an
objective to learn better moments. The sequence
of objectives created by this Method of Learned
Moments (MoLM) can train high-quality neural
image samplers. On CIFAR-10, we demonstrate
that MoLM-trained generators achieve signiﬁ-
cantly higher Inception Scores and lower Fr´echet
Inception Distances than those trained with gradi-
ent penalty-regularized and spectrally-normalized
adversarial objectives.
These generators also
achieve nearly perfect Multi-Scale Structural Sim-
ilarity Scores on CelebA, and can create high-
quality samples of 128×128 images.",0.7346938775510204,0.17733990147783252,0.2857142857142857,0.20833333333333334,0.04950495049504951,0.08,0.3469387755102041,0.08374384236453201,0.1349206349206349,0.0054499677968740685,0.6038967
142,"an agent can choose to impute “adversarial” or “friendly” qualities to an environment that it does not know well. an agent that is trained in a simulator could compensate for the innaccuracies by assuming that the real environment differs from the simulated one—but in an adversarial way, so as to devise countermeasures ahead of time. a bandit secretly chooses which one of the two arms will deliver the reward.","How can one detect friendly and adversarial behavior from raw data? Detecting
whether an environment is a friend, a foe, or anything in between, remains a poorly
understood yet desirable ability for safe and robust agents. This paper proposes
a deﬁnition of these environmental “attitudes” based on an characterization of
the environment’s ability to react to the agent’s private strategy. We deﬁne an
objective function for a one-shot game that allows deriving the environment’s
probability distribution under friendly and adversarial assumptions alongside the
agent’s optimal strategy. Furthermore, we present an algorithm to compute these
equilibrium strategies, and show experimentally that both friendly and adversarial
environments possess non-trivial optimal strategies.",0.45714285714285713,0.27350427350427353,0.3422459893048128,0.028985507246376812,0.017241379310344827,0.021621621621621623,0.22857142857142856,0.13675213675213677,0.1711229946524064,0.007046796556472073,0.58089286
143,conditional neural networks have enjoyed remarkable success in recent years. but they require large datasets for effective training. one approach to supervised problems is to randomly ini-tialize a parametric function g anew for each new task.,"Deep neural networks excel at function approxi-
mation, yet they are typically trained from scratch
for each new function.
On the other hand,
Bayesian methods, such as Gaussian Processes
(GPs), exploit prior knowledge to quickly infer
the shape of a new function at test time. Yet GPs
are computationally expensive, and it can be hard
to design appropriate priors. In this paper we
propose a family of neural models, Conditional
Neural Processes (CNPs), that combine the bene-
ﬁts of both. CNPs are inspired by the ﬂexibility
of stochastic processes such as GPs, but are struc-
tured as neural networks and trained via gradient
descent. CNPs make accurate predictions after
observing only a handful of training data points,
yet scale to complex functions and large datasets.
We demonstrate the performance and versatility
of the approach on a range of canonical machine
learning tasks, including regression, classiﬁcation
and image completion.",0.4864864864864865,0.12080536912751678,0.1935483870967742,0.1388888888888889,0.033783783783783786,0.05434782608695653,0.24324324324324326,0.06040268456375839,0.0967741935483871,0.0018156699801060406,0.57609755
144,natural language is notoriously ambiguous and difficult to process computationally. the lexicalisation of spatial concepts can vary widely across languages and cultures. a large dataset of 3D scenes has been created to validate our model. in this paper we introduce a multi-modal architecture that learns such representations.,"Natural language processing has made signiﬁcant inroads into learning the seman-
tics of words through distributional approaches, however representations learnt via
these methods fail to capture certain kinds of information implicit in the real world.
In particular, spatial relations are encoded in a way that is inconsistent with human
spatial reasoning and lacking invariance to viewpoint changes. We present a system
capable of capturing the semantics of spatial relations such as behind, left of, etc
from natural language. Our key contributions are a novel multi-modal objective
based on generating images of scenes from their textual descriptions, and a new
dataset on which to train it. We demonstrate that internal representations are robust
to meaning preserving transformations of descriptions (paraphrase invariance),
while viewpoint invariance is an emergent property of the system.
1",0.5625,0.20300751879699247,0.2983425414364641,0.06382978723404255,0.022727272727272728,0.0335195530726257,0.3333333333333333,0.12030075187969924,0.17679558011049723,0.0024614521430773037,0.74306804
145,"the ability to fill in gaps in high-dimensional data is a fundamental cognitive skill. a model is given frames f1,..., fn from a single video along with the arbitrary time-points t1 and tn at which those frames occurred. in a 40-frame video, our model can directly sample frames in the future, bypassing intermediate frames.","Stochastic video prediction models take in a sequence of image frames, and gener-
ate a sequence of consecutive future image frames. These models typically gener-
ate future frames in an autoregressive fashion, which is slow and requires the input
and output frames to be consecutive. We introduce a model that overcomes these
drawbacks by generating a latent representation from an arbitrary set of frames
that can then be used to simultaneously and efﬁciently sample temporally con-
sistent frames at arbitrary time-points. For example, our model can “jump” and
directly sample frames at the end of the video, without sampling intermediate
frames. Synthetic video evaluations conﬁrm substantial gains in speed and func-
tionality without loss in ﬁdelity. We also apply our framework to a 3D scene re-
construction dataset. Here, our model is conditioned on camera location and can
sample consistent sets of images for what an occluded region of a 3D scene might
look like, even if there are multiple possibilities for what that region might contain.
Reconstructions and videos are available at https://bit.ly/2O4Pc4R.
1",0.631578947368421,0.19889502762430938,0.3025210084033613,0.19642857142857142,0.06111111111111111,0.09322033898305085,0.3333333333333333,0.10497237569060773,0.15966386554621848,0.005583461096583426,0.56873274
146,"linear bandits are among the most adopted as they allow to take into account the structure 1DeepMind, London, UK 2Otto-Von-Guericke Universitt, Magdeburg, Germany 3Amazon, Germany. the authors ran multiple tests on propri- etary industrial datasets, providing a good example of how delays affect the performance of click-through rate estima-tion.","Stochastic linear bandits are a natural and
well-studied
model
for
structured
explo-
ration/exploitation problems and are widely used
in applications such as online marketing and
recommendation. One of the main challenges
faced by practitioners hoping to apply existing
algorithms is that usually the feedback is
randomly delayed and delays are only partially
observable. For example, while a purchase is
usually observable some time after the display,
the decision of not buying is never explicitly sent
to the system. In other words, the learner only
observes delayed positive events. We formalize
this problem as a novel stochastic delayed linear
bandit and propose OTFLinUCB and OTFLinTS,
two computationally efﬁcient algorithms able to
integrate new information as it becomes available
and to deal with the permanently censored
feedback. We prove optimal ˜O(d
√
T) bounds
on the regret of the ﬁrst algorithm and study the
dependency on delay-dependent parameters. Our
model, assumptions and results are validated by
experiments on simulated and real data.",0.3018867924528302,0.0975609756097561,0.14746543778801843,0.038461538461538464,0.012269938650306749,0.018604651162790697,0.20754716981132076,0.06707317073170732,0.10138248847926268,0.0028275240480346957,0.59014666
147,"Temporal Difference Learning with Neural Networks is a framework for studying sequential decision making equal contribution processes. it deals with the setup where an agent inter- acts with an environment and at each time step it sees the current state (or just an observation) and takes one action, which will determine the immediate reward Rt it will get in the next state. the goal is to learn the value func- tion of the current policy as accurately as possible.","Temporal-Difference learning (TD) [Sutton, 1988] with
function approximation can converge to solutions that are
worse than those obtained by Monte-Carlo regression,
even in the simple case of on-policy evaluation. To in-
crease our understanding of the problem, we investigate
the issue of approximation errors in areas of sharp discon-
tinuities of the value function being further propagated by
bootstrap updates. We show empirical evidence of this
leakage propagation, and show analytically that it must
occur, in a simple Markov chain, when function approxi-
mation errors are present. For reversible policies, the re-
sult can be interpreted as the tension between two terms of
the loss function that TD minimises, as recently described
by [Ollivier, 2018]. We show that the upper bounds from
[Tsitsiklis and Van Roy, 1997] hold, but they do not im-
ply that leakage propagation occurs and under what con-
ditions. Finally, we test whether the problem could be
mitigated with a better state representation, and whether
it can be learned in an unsupervised manner, without re-
wards or privileged information.
1",0.3670886075949367,0.1638418079096045,0.2265625,0.0641025641025641,0.028409090909090908,0.03937007874015747,0.17721518987341772,0.07909604519774012,0.10937500000000001,0.00349443223588228,0.48183852
148,"learning models for visual 3D localization with implicit mapping have been studied extensively for decades. one formulation, often referred to as simply ‘localization’, assumes that a map of the 3D scene is provided in advance and the goal is to localize any new image of the scene relative to this map. another formulation, commonly called ‘Simultaneous Localization and Mapping’ (SLAM), assumes there is no prespecified map of a scene, and that it should be estimated concurrently with","We propose a formulation of visual localization that does not require construction of
explicit maps in the form of point clouds or voxels. The goal is to learn an implicit
representation of the environment at a higher, more abstract level, for instance that
of objects. To study this approach we consider procedurally generated Minecraft
worlds, for which we can generate visually rich images along with camera pose
coordinates. We ﬁrst show that Generative Query Networks (GQNs) enhanced
with a novel attention mechanism can capture the visual structure of 3D scenes
in Minecraft, as evidenced by their samples. We then apply the models to the
localization problem, investigating both generative and discriminative approaches,
and compare the different ways in which they each capture task uncertainty. Our
results show that models with implicit mapping are able to capture the underlying
3D structure of visually complex scenes, and use this to accurately localize new
observations, paving the way towards future applications in sequential localization.
Supplementary video available at https://youtu.be/iHEXX5wXbCI.
1",0.5714285714285714,0.25882352941176473,0.35627530364372473,0.10526315789473684,0.047337278106508875,0.0653061224489796,0.2727272727272727,0.12352941176470589,0.17004048582995954,0.009939246858992737,0.66997075
149,"unsupervised learning is yet to see a breakthrough similar to supervised learning: modeling high-level representations from raw observations remains elusive. this idea of predictive coding is one of the oldest techniques in signal processing for data compression. in neuroscience, predictive coding theories suggest that the brain predicts observations at various levels of abstraction.","While supervised learning has enabled great progress in many applications, unsu-
pervised learning has not seen such widespread adoption, and remains an important
and challenging endeavor for artiﬁcial intelligence. In this work, we propose a
universal unsupervised learning approach to extract useful representations from
high-dimensional data, which we call Contrastive Predictive Coding. The key in-
sight of our model is to learn such representations by predicting the future in latent
space by using powerful autoregressive models. We use a probabilistic contrastive
loss which induces the latent space to capture information that is maximally useful
to predict future samples. It also makes the model tractable by using negative
sampling. While most prior work has focused on evaluating representations for
a particular modality, we demonstrate that our approach is able to learn useful
representations achieving strong performance on four distinct domains: speech,
images, text and reinforcement learning in 3D environments.
1",0.5,0.17880794701986755,0.2634146341463415,0.07547169811320754,0.02666666666666667,0.03940886699507389,0.2222222222222222,0.07947019867549669,0.11707317073170731,0.002492266282439701,0.54889125
150,the Universal Transformer is a parallel-in-time recurrent self-attentive sequence model. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer. the model uses a self-attention mechanism to compute a series of context-informed vector-space representations of the symbols in its input and output.,"Recurrent neural networks (RNNs) sequentially process data by updating their
state with each new data point, and have long been the de facto choice for sequence
modeling tasks. However, their inherently sequential computation makes them
slow to train. Feed-forward and convolutional architectures have recently been
shown to achieve superior results on some sequence modeling tasks such as machine
translation, with the added advantage that they concurrently process all inputs in
the sequence, leading to easy parallelization and faster training times. Despite these
successes, however, popular feed-forward sequence models like the Transformer
fail to generalize in many simple tasks that recurrent models handle with ease, e.g.
copying strings or even simple logical inference when the string or formula lengths
exceed those observed at training time. We propose the Universal Transformer
(UT), a parallel-in-time self-attentive recurrent sequence model which can be
cast as a generalization of the Transformer model and which addresses these
issues. UTs combine the parallelizability and global receptive ﬁeld of feed-forward
sequence models like the Transformer with the recurrent inductive bias of RNNs.
We also add a dynamic per-position halting mechanism and ﬁnd that it improves
accuracy on several tasks. In contrast to the standard Transformer, under certain
assumptions UTs can be shown to be Turing-complete. Our experiments show that
UTs outperform standard Transformers on a wide range of algorithmic and language
understanding tasks, including the challenging LAMBADA language modeling
task where UTs achieve a new state of the art, and machine translation where UTs
achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.
1",0.7407407407407407,0.14814814814814814,0.24691358024691354,0.39622641509433965,0.07806691449814127,0.13043478260869565,0.6481481481481481,0.12962962962962962,0.21604938271604934,0.003271593048665487,0.7136851
151,"a human’s capacity for abstract reasoning can be estimated *Equal contribution, ordered by surname. the premise behind RPMs is simple: one must reason about the relationships between perceptually obvious visual features – such as shape positions or line colors – to choose an image that completes the matrix.","Whether neural networks can learn abstract rea-
soning or whether they merely rely on superﬁcial
statistics is a topic of recent debate. Here, we
propose a dataset and challenge designed to probe
abstract reasoning, inspired by a well-known hu-
man IQ test. To succeed at this challenge, models
must cope with various generalisation ‘regimes’
in which the training and test data differ in clearly-
deﬁned ways. We show that popular models such
as ResNets perform poorly, even when the train-
ing and test sets differ only minimally, and we
present a novel architecture, with a structure de-
signed to encourage reasoning, that does signiﬁ-
cantly better. When we vary the way in which the
test questions and training data differ, we ﬁnd that
our model is notably proﬁcient at certain forms
of generalisation, but notably weak at others. We
further show that the model’s ability to generalise
improves markedly if it is trained to predict sym-
bolic explanations for its answers. Altogether,
we introduce and explore ways to both measure
and induce stronger abstract reasoning in neural
networks. Our freely-available dataset should mo-
tivate further progress in this direction.",0.3829787234042553,0.09230769230769231,0.1487603305785124,0.043478260869565216,0.010309278350515464,0.016666666666666666,0.23404255319148937,0.05641025641025641,0.09090909090909091,0.0008574722889699079,0.41182873
152,"researchers have long sought biologically plausible and empirically powerful learning algorithms that avoid these flaws. weaker objections included undesirable characteristics of artificial networks in general, such as their violation of Dale’s Law, their lack of cell-type variability, and the need for the gradient signals to be both positive and negative.","The backpropagation of error algorithm (BP) is impossible to implement in a real
brain. The recent success of deep networks in machine learning and AI, however,
has inspired proposals for understanding how the brain might learn across multiple
layers, and hence how it might approximate BP. As of yet, none of these proposals
have been rigorously evaluated on tasks where BP-guided deep learning has proved
critical, or in architectures more structured than simple fully-connected networks.
Here we present results on scaling up biologically motivated models of deep learn-
ing on datasets which need deep networks with appropriate architectures to achieve
good performance. We present results on the MNIST, CIFAR-10, and ImageNet
datasets and explore variants of target-propagation (TP) and feedback alignment
(FA) algorithms, and explore performance in both fully- and locally-connected
architectures. We also introduce weight-transport-free variants of difference target
propagation (DTP) modiﬁed to remove backpropagation from the penultimate layer.
Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet
we ﬁnd that TP and FA variants perform signiﬁcantly worse than BP, especially for
networks composed of locally connected units, opening questions about whether
new architectures and algorithms are required to scale these approaches. Our results
and implementation details help establish baselines for biologically motivated deep
learning schemes going forward.
1",0.40384615384615385,0.09417040358744394,0.1527272727272727,0.0196078431372549,0.0045045045045045045,0.007326007326007326,0.23076923076923078,0.053811659192825115,0.08727272727272728,0.0005189736446299023,0.42415705
153,"large-scale visual speech recognition is first to combine a deep learning-based phoneme recognition model with production-grade word-level decoding techniques. we propose a novel lipreading system, illustrated in Figure 1, which transforms raw video into a word sequence. this process takes as input raw video and annotated audio segments, filters and preprocesses them, and produces aligned phoneme and lip frame sequences.","This work presents a scalable solution to open-vocabulary visual speech recognition.
To achieve this, we constructed the largest existing visual speech recognition
dataset, consisting of pairs of text and video clips of faces speaking (3,886 hours
of video). In tandem, we designed and trained an integrated lipreading system,
consisting of a video processing pipeline that maps raw video to stable videos of lips
and sequences of phonemes, a scalable deep neural network that maps the lip videos
to sequences of phoneme distributions, and a production-level speech decoder that
outputs sequences of words. The proposed system achieves a word error rate (WER)
of 40.9% as measured on a held-out set. In comparison, professional lipreaders
achieve either 86.4% or 92.9% WER on the same dataset when having access to
additional types of contextual information. Our approach signiﬁcantly improves
on other lipreading approaches, including variants of LipNet and of Watch, Attend,
and Spell (WAS), which are only capable of 89.8% and 76.8% WER respectively.
1",0.5625,0.20930232558139536,0.30508474576271183,0.07936507936507936,0.029239766081871343,0.042735042735042736,0.28125,0.10465116279069768,0.15254237288135591,0.0057517143988845415,0.820351
154,"reinforcement learning has been shown to be effective at learning navigation policies from complex image input. but these algorithms tend to require large amounts of interaction with the environment, which can be prohibitively costly to obtain on robots in the real world. we propose multi-ple forms of computationally efficient stochastic augmentation to enable the learned policy to generalise beyond these precomputed embeddings.","Learning Deployable Navigation Policies at Kilometer
Scale from a Single Traversal
Jake Bruce
QUT, Brisbane
jacob.bruce@hdr.qut.edu.au
Niko Sünderhauf
QUT, Brisbane
niko.suenderhauf@qut.edu.au
Piotr M",0.07936507936507936,0.15151515151515152,0.10416666666666667,0.016129032258064516,0.03125,0.02127659574468085,0.06349206349206349,0.12121212121212122,0.08333333333333333,0.003496039269903171,0.41852236
155,"ICLR 2019 META-LEARNING WITH LATENT EMBEDDING OPTIMIZATION Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero & Raia Hadsell DeepMind, London, UK. few-shot learning tasks challenge models to learn a new concept or behaviour with very few exam- ples or limited experience.","Gradient-based meta-learning techniques are both widely applicable and proﬁ-
cient at solving challenging few-shot learning and fast adaptation problems. How-
ever, they have practical difﬁculties when operating on high-dimensional param-
eter spaces in extreme low-data regimes. We show that it is possible to bypass
these limitations by learning a data-dependent latent generative representation
of model parameters, and performing gradient-based meta-learning in this low-
dimensional latent space. The resulting approach, latent embedding optimization
(LEO), decouples the gradient-based adaptation procedure from the underlying
high-dimensional space of model parameters. Our evaluation shows that LEO
can achieve state-of-the-art performance on the competitive miniImageNet and
tieredImageNet few-shot classiﬁcation tasks. Further analysis indicates LEO is
able to capture uncertainty in the data, and can perform adaptation more effec-
tively by optimizing in latent space.
1",0.3404255319148936,0.1103448275862069,0.16666666666666666,0.13043478260869565,0.041666666666666664,0.06315789473684211,0.19148936170212766,0.06206896551724138,0.09375,0.0016545268681909535,0.7544465
156,"intermediate Observations and Outcomes: To exploit ISyms, we need to formalize their relation- ship to outcomes. the problem: in the ebook marketplace, ebook candidates are not generated by a fixed distribution. a useful scoring algorithm should be agnostic to the candidate retrieval process.","Optimizing for long term value is desirable in many practical applications, e.g. rec-
ommender systems. The most common approach for long term value optimization
is supervised learning using long term value as the target. Unfortunately, long term
metrics take a long time to measure (e.g., will customers ﬁnish reading an ebook?),
and vanilla forecasters cannot learn from examples until the outcome is observed.
In practical systems where new items arrive frequently, such delay can increase the
training-serving skew, thereby negatively affecting the model’s predictions for new
products. We argue that intermediate observations (e.g., if customers read a third of
the book in 24 hours) can improve a model’s predictions. We formalize the problem
as a semi-stochastic model, where instances are selected by an adversary but, given
an instance, the intermediate observation and the outcome are sampled from a
factored joint distribution. We propose an algorithm that exploits intermediate
observations and theoretically quantify how much it can outperform any prediction
method that ignores the intermediate observations. Motivated by the theoretical
analysis, we propose two neural network architectures: Factored Forecaster (FF)
which is ideal if our assumptions are satisﬁed, and Residual Factored Forecaster
(RFF) that is more robust to model mis-speciﬁcation. Experiments on two real
world datasets, a dataset derived from GitHub repositories and another dataset from
a popular marketplace, show that RFF outperforms both FF as well as an algorithm
that ignores intermediate observations.
1",0.5581395348837209,0.09836065573770492,0.16724738675958187,0.07142857142857142,0.012345679012345678,0.021052631578947368,0.27906976744186046,0.04918032786885246,0.08362369337979093,0.00010248170505892496,0.39011058
157,"an agent interacts with an environment in an episodic manner and attempts to maximize its return. this trade-off is called the exploration- exploitation dilemma. it measures how sub-optimal the rewards the agent has received are so far, relative to the (unknown) optimal policy.","In reinforcement learning the Q-values summarize the expected future rewards
that the agent will attain. However, they cannot capture the epistemic uncertainty
about those rewards. In this work we derive a new Bellman operator with associ-
ated ﬁxed point we call the ‘knowledge values’. These K-values compress both the
expected future rewards and the epistemic uncertainty into a single value, so that
high uncertainty, high reward, or both, can yield high K-values. The key principle
is to endow the agent with a risk-seeking utility function that is carefully tuned to
balance exploration and exploitation. When the agent follows a Boltzmann policy
over the K-values it yields a Bayes regret bound of ˜O(L
√
SAT), where L is the
time horizon, S is the total number of states, A is the number of actions, and T is
the number of elapsed timesteps. We show deep connections of this approach to
the soft-max and maximum-entropy strands of research in reinforcement learning.
1",0.4444444444444444,0.11976047904191617,0.18867924528301885,0.06818181818181818,0.018072289156626505,0.02857142857142857,0.26666666666666666,0.0718562874251497,0.11320754716981132,0.0011931968836874235,0.51862425
158,"the winning system of the Charades challenge 2017 obtained just around 21% accuracy on this per-frame classification task. resulting model outperforms all previous approaches, including all submissions to the AVA challenge at CVPR 2018. this includes various highly sophisticated solutions involving multiple input modalities like optical flow and audio.","We introduce a simple baseline for action localization on the AVA dataset. The
model builds upon the Faster R-CNN bounding box detection framework, adapted
to operate on pure spatiotemporal features – in our case produced exclusively by
an I3D model pretrained on Kinetics. This model obtains 21.9% average AP on
the validation set of AVA v2.1, up from 14.5% for the best RGB spatiotemporal
model used in the original AVA paper (which was pretrained on Kinetics and
ImageNet), and up from 11.3% of the publicly available baseline using a ResNet-
101 image feature extractor, that was pretrained on ImageNet. Our ﬁnal model
obtains 22.8%/21.9% mAP on the val/test sets and outperforms all submissions to
the AVA challenge at CVPR 2018.
1",0.38,0.14728682170542637,0.2122905027932961,0.20408163265306123,0.078125,0.11299435028248588,0.32,0.12403100775193798,0.1787709497206704,0.04386539785185488,0.45929924
159,"a new approach for hard attention has emerged as a promising strategy for achieving good performance on a recent-released visual question- answering dataset. the hard attention mechanism is thought to be non-differentiable, but the feature magnitudes correlate with semantic relevance. this is especially important for approaches that use non-local pairwise operations, where computational and memory costs are quadratic in the size of the set of features.","Learning Visual Question Answering by
Bootstrapping Hard Attention
Mateusz Malinowski, Carl Doersch, Adam Santoro, and Peter Battaglia
DeepMind, London, United Kingdom
Abstract. Attention mechanisms i",0.11594202898550725,0.32,0.1702127659574468,0.058823529411764705,0.16666666666666666,0.08695652173913045,0.08695652173913043,0.24,0.1276595744680851,0.0027574366655649983,0.7360556
160,"the ability to represent and manipulate numerical quantities is apparent in the behavior of many species, from insects to mammals to humans. this failure pattern indicates that the learned behavior is better characterized by memorization than by systematic ab- straction. a new module can be used in conjunction with standard neural network architectures (e.g., LSTMs or convnets) but which is biased to learn systematic numerical computation.","Neural networks can learn to represent and manipulate numerical information, but
they seldom generalize well outside of the range of numerical values encountered
during training. To encourage more systematic numerical extrapolation, we propose
an architecture that represents numerical quantities as linear activations which are
manipulated using primitive arithmetic operators, controlled by learned gates. We
call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic
logic unit in traditional processors. Experiments show that NALU-enhanced neural
networks can learn to track time, perform arithmetic over images of numbers,
translate numerical language into real-valued scalars, execute computer code, and
count objects in images. In contrast to conventional architectures, we obtain
substantially better generalization both inside and outside of the range of numerical
values encountered during training, often extrapolating orders of magnitude beyond
trained numerical ranges.
1",0.5074626865671642,0.2446043165467626,0.3300970873786408,0.10606060606060606,0.050724637681159424,0.06862745098039215,0.2537313432835821,0.1223021582733813,0.1650485436893204,0.03031558169926272,0.5369463
161,"Kinetics-600 represents a 50% increase in number of classes, from 400 to 600, and a 60% increase in the number of video clips, from around 300k to around 500k. the new version of the dataset, called Kinetic-600, fol- lows the same principles as the previous model.","We describe an extension of the DeepMind Kinetics hu-
man action dataset from 400 classes, each with at least 400
video clips, to 600 classes, each with at least 600 video
clips. In order to scale up the dataset we changed the data
collection process so it uses multiple queries per class, with
some of them in a language other than english – portuguese.
This paper details the changes between the two versions of
the dataset and includes a comprehensive set of statistics of
the new version as well as baseline results using the I3D
neural network architecture. The paper is a companion to
the release of the ground truth labels for the public test set.",0.5625,0.23478260869565218,0.33128834355828224,0.1702127659574468,0.07017543859649122,0.09937888198757763,0.3333333333333333,0.1391304347826087,0.19631901840490798,0.008938042073659317,0.4359846
162,"invasive mechanical ventilation (IMV) is a life-saving therapy, but when used for a long period, protracted IMV is associated with increased morbidities and mortalities. a random undersampling of examples from the majority class could have identified 71% of infants who failed extubation.","The objective of this study was to use automated, 
objective features of cardiorespiratory variability to predict 
extubation readiness using nonlinear machine learning tools 
in a database of 189 extremely preterm infants. The rest of 
the paper is organized as follows: Section II describes the 
APEX study and the signals acquired; Section III details the 
methods used for feature development; Section IV describes 
the machine learning methods used to develop a predictor; 
Section V reports the results of the work; and Section VI 
provides a discussion and concluding remarks. 
II. APEX STUDY DESIGN 
A. Infant Population 
The APEX study is an ongoing multicenter, prospective, 
observational study. Over a period of 4 years, data have been 
acquired from five NICUs: the Royal Victoria Hospital; the 
Montreal Children’s Hospital; the Jewish General Hospital",0.37209302325581395,0.12213740458015267,0.1839080459770115,0.023809523809523808,0.007692307692307693,0.011627906976744186,0.20930232558139536,0.06870229007633588,0.10344827586206896,0.0018249293615850933,0.43417472
163,a new dataset for evaluating ques- tion answering models is proposed. we evaluate a number of recent neural models with mem- ory augmentation. the models’ accuracy decreases when random sen- tences are introduced to the tasks at test.,"Evaluating Theory of Mind in Question Answering
Aida Nematzadeh
DeepMind
nematzadeh@google.com
Kaylee Burns
UC Berkeley
kayleeburns@berkeley.edu
Erin Grant
UC Berkeley
eringrant@berkeley.edu
Alison Go",0.07894736842105263,0.10344827586206896,0.08955223880597013,0.0,0.0,0.0,0.05263157894736842,0.06896551724137931,0.05970149253731343,0.0048742862874500996,0.4611525
164,"syntactic scaffold offers a substantial boost to state-of-the-art baselines for two SRL tasks (5) and corefer-ence resolution (6). a natu- ral parser can be used to label spans, in- cluding semantic role labeling, and named entity recognition.","We introduce the syntactic scaﬀold, an ap-
proach to incorporating syntactic informa-
tion into semantic tasks. Syntactic scaﬀolds
avoid expensive syntactic processing at run-
time, only making use of a treebank during
training, through a multitask objective. We im-
prove over strong baselines on PropBank se-
mantics, frame semantics, and coreference res-
olution, achieving competitive performance on
all three tasks.
1",0.2682926829268293,0.1746031746031746,0.21153846153846154,0.0,0.0,0.0,0.14634146341463414,0.09523809523809523,0.11538461538461538,0.004261447914433244,0.5440256
165,"a common risk-measure is the variance of the expected sum of rewards/costs and the mean-variance trade-offfunction. other risk-sensitive objectives have also been studied, for example, Borkar studied exponential utility functions, Tamar et al. experimented with the Sharpe Ratio measurement. a novel algorithm is proposed based on stochastic cyclic block coordinate descent.","Risk management in dynamic decision problems is a primary concern in many ﬁelds, including
ﬁnancial investment, autonomous driving, and healthcare. The mean-variance function is one
of the most widely used objective functions in risk management due to its simplicity and
interpretability. Existing algorithms for mean-variance optimization are based on multi-time-
scale stochastic approximation, whose learning rate schedules are often hard to tune, and
have only asymptotic convergence proof. In this paper, we develop a model-free policy search
framework for mean-variance optimization with ﬁnite-sample error bound analysis (to local
optima). Our starting point is a reformulation of the original mean-variance function with
its Fenchel dual, from which we propose a stochastic block coordinate ascent policy search
algorithm. Both the asymptotic convergence guarantee of the last iteration’s solution and
the convergence rate of the randomly picked solution are provided, and their applicability is
demonstrated on several benchmark domains.
1",0.5,0.18064516129032257,0.2654028436018957,0.10909090909090909,0.03896103896103896,0.057416267942583726,0.30357142857142855,0.10967741935483871,0.16113744075829384,0.00290926061057287,0.5810591
166,"a key view of TD learning is that it is learning predictive knowl- edge about the environment in the form of value functions, from which it can derive its behavior to address long-term se- quential decision-making problems. the agent’s horizon of interest, that is, how immediate or long a TD learns into the future, is adjusted through a discount rate parameter. TD allows learning to occur from raw experience in the absence of a model of the environment’s dynamics, like with Monte","Predicting Periodicity with Temporal Difference Learning
Kristopher De Asis, Brendan Bennett, Richard S. Sutton
Reinforcement Learning and Artiﬁcial Intelligence Laboratory, University of Alberta
{kld",0.06976744186046512,0.24,0.10810810810810809,0.0,0.0,0.0,0.046511627906976744,0.16,0.07207207207207207,0.002627262769368812,0.44108534
167,high-fidelity imitation is a powerful way to show agents how to solve a task. in one-shot we can closely imitate a demonstration from raw sensory input. this is sometimes called over-imitation. it is known that humans over-imitate more than other primates.,"Humans are experts at high-ﬁdelity imitation – closely mimicking a demonstration,
often in one attempt. Humans use this ability to quickly solve a task instance, and to
bootstrap learning of new tasks. Achieving these abilities in autonomous agents is an
open problem. In this paper, we introduce an off-policy RL algorithm (MetaMimic)
to narrow this gap. MetaMimic can learn both (i) policies for high-ﬁdelity one-shot
imitation of diverse novel skills, and (ii) policies that enable the agent to solve
tasks more efﬁciently than the demonstrators. MetaMimic relies on the principle
of storing all experiences in a memory and replaying these to learn massive deep
neural network policies by off-policy RL. This paper introduces, to the best of our
knowledge, the largest existing neural networks for deep RL and shows that larger
networks with normalization are needed to achieve one-shot high-ﬁdelity imitation
on a challenging manipulation task. The results also show that both types of policy
can be learned from vision, in spite of the task rewards being sparse, and without
access to demonstrator actions.
1",0.6,0.14835164835164835,0.2378854625550661,0.13636363636363635,0.03314917127071823,0.05333333333333334,0.3333333333333333,0.08241758241758242,0.13215859030837004,0.0007762253685771755,0.53211284
168,"the need for two systems reflects the typical trade-off between the sample efficiency and the computational complexity of a learning algorithm. we argue that the majority of contemporary deep reinforcement learning systems fall into the latter category: slow, gradient-based updates combined with incremental updates from Bellman backups result in systems that are good at generalising. the replay buffer stores previously seen tuples of experience: state, action, reward, and next state.","We propose Ephemeral Value Adjusments (EVA): a means of allowing deep re-
inforcement learning agents to rapidly adapt to experience in their replay buffer.
EVA shifts the value predicted by a neural network with an estimate of the value
function found by planning over experience tuples from the replay buffer near
the current state. EVA combines a number of recent ideas around combining
episodic memory-like structures into reinforcement learning agents: slot-based
storage, content-based retrieval, and memory-based planning. We show that EVA
is performant on a demonstration task and Atari games.
1",0.375,0.28421052631578947,0.32335329341317365,0.04225352112676056,0.031914893617021274,0.03636363636363637,0.16666666666666666,0.12631578947368421,0.1437125748502994,0.017530375411535678,0.7001475
169,we propose an intrinsic reward function designed for multi-agent RL (MARL) which awards agents for having a causal influence on other agents’ actions. 'defecting' (non-cooperative behavior) has the highest payoff. but the collective reward will be better if all agents choose to cooperate.,"We derive a new intrinsic social motivation for multi-agent reinforcement learning
(MARL), in which agents are rewarded for having causal influence over another agent’s
actions. Causal influence is assessed using counterfactual reasoning. The reward does
not depend on observing another agent’s reward function, and is thus a more realistic
approach to MARL than taken in previous work. We show that the causal influence
reward is related to maximizing the mutual information between agents’ actions. We test
the approach in challenging social dilemma environments, where it consistently leads to
enhanced cooperation between agents and higher collective reward. Moreover, we find
that rewarding influence can lead agents to develop emergent communication protocols.
We therefore employ influence to train agents to use an explicit communication channel,
and find that it leads to more effective communication and higher collective reward. Fi-
nally, we show that influence can be computed by equipping each agent with an internal
model that predicts the actions of other agents. This allows the social influence reward
to be computed without the use of a centralised controller, and as such represents a sig-
nificantly more general and scalable inductive bias for MARL with independent agents.
1",0.6222222222222222,0.1414141414141414,0.23045267489711932,0.18181818181818182,0.04060913705583756,0.06639004149377593,0.4666666666666667,0.10606060606060606,0.1728395061728395,0.0006958200128785964,0.71819025
170,"a paradigm driving progress is deep reinforcement learning (Deep RL), which uses deep learning to train function approximators that represent policies, reward estimates, or both, to learn directly from experience and rewards. the optimization problem is not as clearly defined as maximizing one’s own expected reward, because each agent’s policy affects the others’ optimization problems.","Optimization of parameterized policies for reinforcement learning (RL) is an impor-
tant and challenging problem in artiﬁcial intelligence. Among the most common
approaches are algorithms based on gradient ascent of a score function representing
discounted return. In this paper, we examine the role of these policy gradient and
actor-critic algorithms in partially-observable multiagent environments. We show
several candidate policy update rules and relate them to a foundation of regret
minimization and multiagent learning techniques for the one-shot and tabular cases,
leading to previously unknown convergence guarantees. We apply our method to
model-free multiagent reinforcement learning in adversarial sequential decision
problems (zero-sum imperfect information games), using RL-style function ap-
proximation. We evaluate on commonly used benchmark Poker domains, showing
performance against ﬁxed policies and empirical convergence to approximate Nash
equilibria in self-play with rates similar to or better than a baseline model-free
algorithm for zero-sum games, without any domain-speciﬁc state space reductions.
1",0.40350877192982454,0.1402439024390244,0.20814479638009048,0.017857142857142856,0.006134969325153374,0.009132420091324202,0.19298245614035087,0.06707317073170732,0.0995475113122172,0.0019603587363995656,0.52946
171,the aim of this paper is to learn a compact represen- tation of image sets for template-based face recognition. we propose a network architecture that aggregates and embeds the face descriptors produced by deep convolu- tional neural networks into a fixed-length representation. this representation requires minimal memory storage and enables ef- ficient similarity computation.,"GhostVLAD for set-based face recognition
Yujie Zhong1, Relja Arandjelovi´c2, and Andrew Zisserman1,2
1VGG, Department of Engineering Science, University of Oxford, UK
{yujie,az}@robots.ox.ac.uk
2DeepM",0.14285714285714285,0.25806451612903225,0.1839080459770115,0.03636363636363636,0.06666666666666667,0.047058823529411764,0.08928571428571429,0.16129032258064516,0.1149425287356322,0.00506619280504146,0.44638377
172,"CIIRC – Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague, Czechia. in recent years, significant effort has gone into developing trainable image representations for finding corre- spondences between images under strong appearance changes caused by viewpoint or illumination variations. but the currently dominant approach for finding image correspondence based on matching individual image features has been only modest at best.","We address the problem of ﬁnding reliable dense correspondences between a pair
of images. This is a challenging task due to strong appearance differences between
the corresponding scene elements and ambiguities generated by repetitive patterns.
The contributions of this work are threefold. First, inspired by the classic idea
of disambiguating feature matches using semi-local constraints, we develop an
end-to-end trainable convolutional neural network architecture that identiﬁes sets
of spatially consistent matches by analyzing neighbourhood consensus patterns
in the 4D space of all possible correspondences between a pair of images without
the need for a global geometric model. Second, we demonstrate that the model
can be trained effectively from weak supervision in the form of matching and
non-matching image pairs without the need for costly manual annotation of point
to point correspondences. Third, we show the proposed neighbourhood consensus
network can be applied to a range of matching tasks including both category- and
instance-level matching, obtaining the state-of-the-art results on the PF Pascal
dataset and the InLoc indoor visual localization benchmark.
1",0.3230769230769231,0.11666666666666667,0.17142857142857143,0.015625,0.00558659217877095,0.00823045267489712,0.18461538461538463,0.06666666666666667,0.09795918367346938,0.00202859213353452,0.5558269
173,"'tackling Sequence to Sequence Mapping Problems' with Neural Networks Lei Yu Mansfield College University of Oxford A thesis submitted for the degree of Doctor of Philosophy Trinity 2017 arXiv:1810.10802v1 [cs.CL] 25 Oct 2018 To my parents Shijie and Lijuan First and foremost, I am deeply grateful to my supervisors, Prof. Stephen Pul- man and Prof. Phil Blunsom.","In Natural Language Processing (NLP), it is important to detect the relationship
between two sequences or to generate a sequence of tokens given another ob-
served sequence. We call the type of problems on modelling sequence pairs as
sequence to sequence (seq2seq) mapping problems. A lot of research has been
devoted to ﬁnding ways of tackling these problems, with traditional approaches
relying on a combination of hand-crafted features, alignment models, segmen-
tation heuristics, and external linguistic resources. Although great progress has
been made, these traditional approaches suffer from various drawbacks, such as
complicated pipeline, laborious feature engineering, and the difﬁculty for do-
main adaptation. Recently, neural networks emerged as a promising solution
to many problems in NLP, speech recognition, and computer vision. Neural
models are powerful because they can be trained end to end, generalise well
to unseen examples, and the same framework can be easily adapted to a new
domain.
The aim of this thesis is to advance the state-of-the-art in seq2seq mapping
problems with neural networks. We explore solutions from three major as-
pects: investigating neural models for representing sequences, modelling inter-
actions between sequences, and using unpaired data to boost the performance
of neural models. For each aspect, we propose novel models and evaluate their
efﬁcacy on various tasks of seq2seq mapping.
Chapter 2 covers the relevant literature on neural networks. Following this, in
Chapter 3 we explore the usefulness of distributed sentence models in seq2seq
mapping problems by testing them in the task of answer sentence selection.
We also empirically compare the performance of distributed sentence mod-
els based on different types of neural networks. Chapter 4 presents a neural
sequence transduction model that learns to alternate between encoding and de-
coding segments of the input as it is read. The model not only outperforms the
encoder-decoder model signiﬁcantly on various tasks such as machine trans-
lation and sentence summarisation, but also is capable of predicting outputs
online during decoding. In Chapter 5, we propose to incorporate abundant
unpaired data using the noisy channel model—with the component models pa-
rameterised by recurrent neural networks—and present a tractable and effective
beam search decoder.
Contents
1",0.35,0.057065217391304345,0.09813084112149534,0.1016949152542373,0.01634877384196185,0.028169014084507043,0.31666666666666665,0.051630434782608696,0.08878504672897197,3.154623566778739e-05,0.64205474
174,"in cooperative, partially observable multi-agent settings, agents must learn to act in an environment that contains multiple learning agents, often under partial observability. the ability to learn such protocols is essential for many real-world tasks where agents have to interact and communicate seamlessly with others.","When observing the actions of others, humans
carry out inferences about why the others acted
as they did, and what this implies about their
view of the world. Humans also use the fact
that their actions will be interpreted in this man-
ner when observed by others, allowing them to
act informatively and thereby communicate ef-
ﬁciently with others. Although learning algo-
rithms have recently achieved superhuman per-
formance in a number of two-player, zero-sum
games, scalable multi-agent reinforcement learn-
ing algorithms that can discover effective strate-
gies and conventions in complex, partially observ-
able settings have proven elusive. We present
the Bayesian action decoder (BAD), a new multi-
agent learning method that uses an approximate
Bayesian update to obtain a public belief that con-
ditions on the actions taken by all agents in the
environment. Together with the public belief, this
Bayesian update effectively deﬁnes a new Markov
decision process, the public belief MDP, in which
the action space consists of deterministic partial
policies, parameterised by deep neural networks,
that can be sampled for a given public state. It
exploits the fact that an agent acting only on this
public belief state can still learn to use its pri-
vate information if the action space is augmented
to be over partial policies mapping private infor-
mation into environment actions. The Bayesian
update is also closely related to the theory of mind
reasoning that humans carry out when observ-
ing others’ actions. We ﬁrst validate BAD on a
proof-of-principle two-step matrix game, where
it outperforms traditional policy gradient meth-
ods. We then evaluate BAD on the challenging,
cooperative partial-information card game Han-
abi, where in the two-player setting the method
*Equal
contribution
1University
of
Oxford,
UK
2DeepMind,
London,
UK.
Correspondence
to:
Jakob
Foerster
<jakob.foerster@cs.ox.ac.uk>,
Francis
Song
<songf@google.com>.
surpasses all previously published learning and
hand-coded approaches.",0.7021276595744681,0.10248447204968944,0.17886178861788615,0.15217391304347827,0.021806853582554516,0.03814713896457765,0.3829787234042553,0.055900621118012424,0.0975609756097561,6.557380479334624e-05,0.54159975
175,"despite the success of recent exploration methods, it is still an open question on how to construct an optimal representation for exploration. there has been a surge of interest in developing effective exploration strategies for problems with high-dimensional state spaces and sparse rewards.","This paper investigates whether learning contingency-awareness and controllable
aspects of an environment can lead to better exploration in reinforcement learning.
To investigate this question, we consider an instantiation of this hypothesis evalu-
ated on the Arcade Learning Element (ALE). In this study, we develop an attentive
dynamics model (ADM) that discovers controllable elements of the observations,
which are often associated with the location of the character in Atari games. The
ADM is trained in a self-supervised fashion to predict the actions taken by the agent.
The learned contingency information is used as a part of the state representation
for exploration purposes. We demonstrate that combining A2C with count-based
exploration using our representation achieves impressive results on a set of noto-
riously challenging Atari games due to sparse rewards.1 For example, we report
a state-of-the-art score of >6600 points on MONTEZUMA’S REVENGE without
using expert demonstrations, explicit high-level information (e.g., RAM states), or
supervised data. Our experiments conﬁrm that indeed contingency-awareness is
an extremely powerful concept for tackling exploration problems in reinforcement
learning and opens up interesting research questions for further investigations.
1",0.6136363636363636,0.13917525773195877,0.22689075630252098,0.06976744186046512,0.015544041450777202,0.025423728813559324,0.3181818181818182,0.07216494845360824,0.1176470588235294,0.0006360407455524527,0.5973309
176,"nEURAL PHRASE-TO-PHRASE MACHINE TRANSLATION Jiangtao Feng Fudan University fengjt16@fudan.edu.cn Lingpeng Kong, Po-Sen Huang DeepMind lingpenk, dahua@google.com. linguistic structures can be understood as a form of inductive bias to the model, a key factor to its superior performance.","In this paper, we propose Neural Phrase-to-Phrase Machine Translation (NP2MT).
Our model uses a phrase attention mechanism to discover relevant input (source)
segments that are used by a decoder to generate output (target) phrases. We
also design an efﬁcient dynamic programming algorithm to decode segments that
allows the model to be trained faster than the existing neural phrase-based machine
translation method by Huang et al. (2018). Furthermore, our method can naturally
integrate with external phrase dictionaries during decoding. Empirical experiments
show that our method achieves comparable performance with the state-of-the art
methods on benchmark datasets. However, when the training and testing data are
from different distributions or domains, our method performs better.
1",0.37777777777777777,0.14285714285714285,0.20731707317073172,0.13636363636363635,0.05084745762711865,0.07407407407407408,0.26666666666666666,0.10084033613445378,0.14634146341463414,0.0010806756326230723,0.6766033
177,"a generic framework for privacy preserving deep learning is becoming increasingly popular as a way to perform operations in an untrusted environment without disclosing data. in the case of machine learning models, SMPC would protect the model weights while allowing multiple workers to take part in the training phase with their own datasets.","We detail a new framework for privacy preserving deep learning and discuss its
assets. The framework puts a premium on ownership and secure processing of data
and introduces a valuable representation based on chains of commands and tensors.
This abstraction allows one to implement complex privacy preserving constructs
such as Federated Learning, Secure Multiparty Computation, and Differential
Privacy while still exposing a familiar deep learning API to the end-user. We report
early results on the Boston Housing and Pima Indian Diabetes datasets. While
the privacy features apart from Differential Privacy do not impact the prediction
accuracy, the current implementation of the framework introduces a signiﬁcant
overhead in performance, which will be addressed at a later stage of the develop-
ment. We believe this work is an important milestone introducing the ﬁrst reliable,
general framework for privacy preserving deep learning.
1",0.4716981132075472,0.176056338028169,0.25641025641025644,0.11538461538461539,0.0425531914893617,0.0621761658031088,0.2830188679245283,0.1056338028169014,0.15384615384615383,0.022412448193644505,0.7498386
178,learning dynamics models that are accurate enough for planning has been a long-standing challenge. planning using learned models offers several benefits over model-free reinforcement learning. the success of such models has been limited to simple tasks such as balancing cartpoles and con- trolling arms from dense rewards.,"Planning has been very successful for control
tasks with known environment dynamics.
To
leverage planning in unknown environments,
the agent needs to learn the dynamics from
interactions with the world. However, learning
dynamics models that are accurate enough for
planning has been a long-standing challenge,
especially in image-based domains. We propose
the Deep Planning Network (PlaNet), a purely
model-based agent that learns the environment
dynamics from images and chooses actions
through fast online planning in latent space.
To achieve high performance, the dynamics
model must accurately predict the rewards
ahead for multiple time steps.
We approach
this problem using a latent dynamics model
with both deterministic and stochastic transition
components and a multi-step variational inference
objective that we call latent overshooting.
Using only pixel observations, our agent solves
continuous control tasks with contact dynamics,
partial observability, and sparse rewards, which
exceed the difﬁculty of tasks that were previously
solved by planning with learned models. PlaNet
uses substantially fewer episodes and reaches ﬁnal
performance close to and sometimes higher than
strong model-free algorithms.",0.673469387755102,0.1853932584269663,0.29074889867841414,0.375,0.1016949152542373,0.16,0.4897959183673469,0.1348314606741573,0.21145374449339205,0.021154378756236648,0.606415
179,"multi-armed bandits are a classical framework for sequential decision-making under uncertainty, where the actions of the learning agent are represented by arms. the agent does not know the expected rewards in advance and has to learn them by pulling the arms with the highest estimated reward thus far. -greedy policy is simple to implement and widely used in practice but is statistically suboptimal.","We propose a multi-armed bandit algorithm that explores based on randomizing its history. The key idea is to
estimate the value of the arm from the bootstrap sample of its history, where we add pseudo observations after each
pull of the arm. The pseudo observations seem to be harmful. But on the contrary, they guarantee that the bootstrap
sample is optimistic with a high probability. Because of this, we call our algorithm Giro, which is an abbreviation
for garbage in, reward out. We analyze Giro in a K-armed Bernoulli bandit and prove a O(K∆−1 log n) bound on
its n-round regret, where ∆denotes the difference in the expected rewards of the optimal and best suboptimal arms.
The main advantage of our exploration strategy is that it can be applied to any reward function generalization, such
as neural networks. We evaluate Giro and its contextual variant on multiple synthetic and real-world problems, and
observe that Giro is comparable to or better than state-of-the-art algorithms.
1",0.47692307692307695,0.1791907514450867,0.26050420168067223,0.109375,0.040697674418604654,0.05932203389830508,0.2923076923076923,0.10982658959537572,0.1596638655462185,0.005361420978382636,0.6430817
180,emergence of cooperation among self-interested agents has become an important topic in multi-agent deep reinforcement learning (MARL). evolution is based on an intrinsic reward function derived from behavioral eco- nomics. we propose evolution can be applied to remove the hand-crafting of intrin- sic motivation.,"Multi-agent cooperation is an important feature of the natural
world. Many tasks involve individual incentives that are misaligned
with the common good, yet a wide range of organisms from bac-
teria to insects and humans are able to overcome their differences
and collaborate. Therefore, the emergence of cooperative behav-
ior amongst self-interested individuals is an important question
for the fields of multi-agent reinforcement learning (MARL) and
evolutionary theory. Here, we study a particular class of multi-
agent problems called intertemporal social dilemmas (ISDs), where
the conflict between the individual and the group is particularly
sharp. By combining MARL with appropriately structured natu-
ral selection, we demonstrate that individual inductive biases for
cooperation can be learned in a model-free way. To achieve this,
we introduce an innovative modular architecture for deep rein-
forcement learning agents which supports multi-level selection.
We present results in two challenging environments, and interpret
these in the context of cultural and ecological evolution.",0.5319148936170213,0.15527950310559005,0.24038461538461536,0.17391304347826086,0.05,0.07766990291262137,0.40425531914893614,0.11801242236024845,0.1826923076923077,0.0017674795135616571,0.6908545
181,"CF-GPS aims to leverage possible advan- tages of counterfactual reasoning for learning decision making in the reinforcement learning framework. a mismatch between the model and the true environment can cause this approach to fail, resulting in policies that do not generalize to the real environment.","Learning policies on data synthesized by models can in principle quench the thirst
of reinforcement learning algorithms for large amounts of real experience, which
is often costly to acquire. However, simulating plausible experience de novo is a
hard problem for many complex environments, often resulting in biases for model-
based policy evaluation and search. Instead of de novo synthesis of data, here we
assume logged, real experience and model alternative outcomes of this experi-
ence under counterfactual actions, i.e. actions that were not actually taken. Based
on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algo-
rithm for learning policies in POMDPs from off-policy experience. It leverages
structural causal models for counterfactual evaluation of arbitrary policies on in-
dividual off-policy episodes. CF-GPS can improve on vanilla model-based RL al-
gorithms by making use of available logged data to de-bias model predictions. In
contrast to off-policy algorithms based on Importance Sampling which re-weight
data, CF-GPS leverages a model to explicitly consider alternative outcomes, al-
lowing the algorithm to make better use of experience data. We ﬁnd empiri-
cally that these advantages translate into improved policy evaluation and search
results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes
the previously proposed Guided Policy Search and that reparameterization-based
algorithms such Stochastic Value Gradient can be interpreted as counterfactual
methods.
1",0.6739130434782609,0.13191489361702127,0.22064056939501778,0.08888888888888889,0.017094017094017096,0.02867383512544803,0.34782608695652173,0.06808510638297872,0.11387900355871887,0.0004465319466611181,0.6218859
182,"unsupervised learning provides a generic framework which allows machines to learn independently of supervision or reinforcement. in most domains there exists a great wealth of information in the raw data alone. despite these successes, prior work mostly evaluate the quality of the learned representation indirectly through the performance in some supervised or RL task.","Unsupervised representation learning has succeeded with excellent results in many
applications. It is an especially powerful tool to learn a good representation of
environments with partial or noisy observations. In partially observable domains it
is important for the representation to encode a belief state—a sufﬁcient statistic of
the observations seen so far. In this paper, we investigate whether it is possible to
learn such a belief representation using modern neural architectures. Speciﬁcally,
we focus on one-step frame prediction and two variants of contrastive predictive
coding (CPC) as the objective functions to learn the representations. To evaluate
these learned representations, we test how well they can predict various pieces of
information about the underlying state of the environment, e.g., position of the
agent in a 3D maze. We show that all three methods are able to learn belief repre-
sentations of the environment—they encode not only the state information, but also
its uncertainty, a crucial aspect of belief states. We also ﬁnd that for CPC multi-step
predictions and action-conditioning are critical for accurate belief representations
in visually complex environments. The ability of neural representations to capture
the belief information has the potential to spur new advances for learning and
planning in partially observable domains, where leveraging uncertainty is essential
for optimal decision making.
1",0.42592592592592593,0.10407239819004525,0.1672727272727273,0.07547169811320754,0.01818181818181818,0.029304029304029304,0.37037037037037035,0.09049773755656108,0.14545454545454545,0.0009037935176955212,0.53345495
183,"the purpose of this report is to sketch a research outline, share some of the most important open issues we are facing. the content is based on some of our discussions during a week-long workshop held in Barbados in February 2018.","arXiv:1811.07004v1  [cs.AI]  16 Nov 2018
The Barbados 2018 List of
Open Issues in Continual Learning
Tom Schaul
Hado van Hasselt
Joseph Modayil
Martha White
Adam White
Pierre-Luc Bacon
Jean Harb
Shibl",0.16666666666666666,0.2,0.1818181818181818,0.024390243902439025,0.029411764705882353,0.02666666666666667,0.11904761904761904,0.14285714285714285,0.12987012987012989,0.005927305348705561,0.60447323
184,"the ultimate goal of machine learning (ML) research is to go beyond games and improve human lives. but performance on these and other real-world tasks is not easily measurable, since they do not come readily equipped with a reward function. we want ML to generate creative and brilliant solutions like AlphaGo’s Move 37 (Metz, 2016)","One obstacle to applying reinforcement learning algorithms to real-world problems
is the lack of suitable reward functions. Designing such reward functions is difﬁcult
in part because the user only has an implicit understanding of the task objective.
This gives rise to the agent alignment problem: how do we create agents that behave
in accordance with the user’s intentions? We outline a high-level research direction
to solve the agent alignment problem centered around reward modeling: learning a
reward function from interaction with the user and optimizing the learned reward
function with reinforcement learning. We discuss the key challenges we expect
to face when scaling reward modeling to complex and general domains, concrete
approaches to mitigate these challenges, and ways to establish trust in the resulting
agents.
1",0.40350877192982454,0.17692307692307693,0.24598930481283424,0.05357142857142857,0.023255813953488372,0.032432432432432434,0.24561403508771928,0.1076923076923077,0.1497326203208556,0.0033490722429135953,0.42153597
185,"generative adversarial nets (GANs) have proliferated substantially. they can effectively be viewed as differentiable games played by cooperating and competing agents. the difficulty is that each loss depends on all parameters, including those of others.","A growing number of learning methods are actually games which optimise mul-
tiple, interdependent objectives in parallel – from GANs and intrinsic curiosity
to multi-agent RL. Opponent shaping is a powerful approach to improve learning
dynamics in such games, accounting for the fact that the ‘environment’ includes
agents adapting to one another’s updates.
Learning with Opponent-Learning
Awareness (LOLA) is a recent algorithm which exploits this dynamic response
and encourages cooperation in settings like the Iterated Prisoner’s Dilemma.
Although experimentally successful, we show that LOLA can exhibit ‘arrogant’
behaviour directly at odds with convergence. In fact, remarkably few algorithms
have theoretical guarantees applying across all differentiable games. In this pa-
per we present Stable Opponent Shaping (SOS), a new method that interpolates
between LOLA and a stable variant named LookAhead. We prove that Look-
Ahead locally converges and avoids strict saddles in all differentiable games, the
strongest results in the ﬁeld so far. SOS inherits these desirable guarantees, while
also shaping the learning of opponents and consistently either matching or outper-
forming LOLA experimentally.
1",0.4,0.07909604519774012,0.13207547169811323,0.029411764705882353,0.005681818181818182,0.009523809523809523,0.22857142857142856,0.04519774011299435,0.07547169811320754,0.0001780031821617935,0.5265566
186,"researchers have found that deep networks are in some sense ‘brittle’, in that small changes to their inputs can result in wildly different outputs. these changes are often referred to as adversarial perturbations, in the sense that an adversary could craft a very small change to the input to create an undesirable outcome.","While deep learning has led to remarkable results on a number of challenging
problems, researchers have discovered a vulnerability of neural networks in ad-
versarial settings, where small but carefully chosen perturbations to the input can
make the models produce extremely inaccurate outputs. This makes these models
particularly unsuitable for safety-critical application domains (e.g. self-driving cars)
where robustness is extremely important. Recent work has shown that augmenting
training with adversarially generated data provides some degree of robustness
against test-time attacks. In this paper we investigate how this approach scales
as we increase the computational budget given to the defender. We show that
increasing the number of parameters in adversarially-trained models increases their
robustness, and in particular that ensembling smaller models while adversarially
training the entire ensemble as a single model is a more efﬁcient way of spending
said budget than simply using a larger single model. Crucially, we show that it is
the adversarial training of the ensemble, rather than the ensembling of adversarially
trained models, which provides robustness.
1",0.5471698113207547,0.16477272727272727,0.25327510917030566,0.07692307692307693,0.022857142857142857,0.035242290748898675,0.32075471698113206,0.09659090909090909,0.148471615720524,0.0035345563760992784,0.6476504
187,neural models are capable of inferring the existence of word boundaries solely based on statistical properties of the input. the strongest models in terms of word segmentation are far too far behind the current (non-neural) state-of-the-art to be a useful foundation.,"We propose a segmental neural language model that combines the representational
power of neural networks and the structure learning mechanism of Bayesian non-
parametrics, and show that it learns to discover semantically meaningful units
(e.g., morphemes and words) from unsegmented character sequences. The model
generates text as a sequence of segments, where each segment is generated either
character-by-character from a sequence model or as a single draw from a lexical
memory that stores multi-character units. Its parameters are ﬁt to maximize the
marginal likelihood of the training data, summing over all segmentations of the
input, and its hyperparameters are likewise set to optimize held-out marginal like-
lihood. To prevent the model from overusing the lexical memory, which leads
to poor generalization and bad segmentation, we introduce a differentiable reg-
ularizer that penalizes based on the expected length of each segment. To our
knowledge, this is the ﬁrst demonstration of neural networks that have predictive
distributions better than LSTM language models and also infer a segmentation into
word-like units that are competitive with the best existing word discovery models.
1",0.6,0.14594594594594595,0.2347826086956522,0.09090909090909091,0.021739130434782608,0.03508771929824561,0.35555555555555557,0.08648648648648649,0.1391304347826087,0.0006774557628096273,0.61369044
188,reinforcement learning (RL) is a major challenge for controlling bodies with a large number of degrees of freedom (DoFs) it is now possible to train high-dimensional vision-based policies from scratch. we build on recent advances in imitation learning to make flexible low-level motor controllers for high-DoF hu- manoids.,"We aim to build complex humanoid agents that integrate perception, motor con-
trol, and memory. In this work, we partly factor this problem into low-level motor
control from proprioception and high-level coordination of the low-level skills in-
formed by vision. We develop an architecture capable of surprisingly ﬂexible,
task-directed motor control of a relatively high-DoF humanoid body by combin-
ing pre-training of low-level motor controllers with a high-level, task-focused con-
troller that switches among low-level sub-policies. The resulting system is able to
control a physically-simulated humanoid body to solve tasks that require coupling
visual perception from an unstabilized egocentric RGB camera during locomotion
in the environment. Supplementary video link1
1",0.46153846153846156,0.19672131147540983,0.27586206896551724,0.09803921568627451,0.04132231404958678,0.05813953488372093,0.19230769230769232,0.08196721311475409,0.11494252873563218,0.00875022546157403,0.5664461
189,"we propose an efficient regularizer that encourages small curvatures. this is based on the implications of adversarial training on the curvature of the loss function and decision boundaries of the classifier. the aim of this paper is to provide an analysis of this phe- nomenon, and propose a regularization strategy (CURE)","State-of-the-art classiﬁers have been shown to be largely
vulnerable to adversarial perturbations. One of the most ef-
fective strategies to improve robustness is adversarial train-
ing. In this paper, we investigate the effect of adversarial
training on the geometry of the classiﬁcation landscape and
decision boundaries. We show in particular that adversar-
ial training leads to a signiﬁcant decrease in the curvature
of the loss surface with respect to inputs, leading to a dras-
tically more “linear” behaviour of the network. Using a
locally quadratic approximation, we provide theoretical ev-
idence on the existence of a strong relation between large
robustness and small curvature. To further show the impor-
tance of reduced curvature for improving the robustness, we
propose a new regularizer that directly minimizes curvature
of the loss surface, and leads to adversarial robustness that
is on par with adversarial training. Besides being a more
efﬁcient and principled alternative to adversarial training,
the proposed regularizer conﬁrms our claims on the impor-
tance of exhibiting quasi-linear behavior in the vicinity of
data points in order to achieve robustness.",0.7058823529411765,0.1925133689839572,0.3025210084033613,0.34,0.0913978494623656,0.1440677966101695,0.4117647058823529,0.11229946524064172,0.17647058823529413,0.01678387911385016,0.7940487
190,RNs represent a set of objects by aggregating representations of pairwise relations of all pairs of objects. the relationship between some word pairs is more meaningful than a relationship between others. a series of papers have explored improving sentence representations by incorporating structural information into the encoders.,"The meaning of a sentence is a function of the relations that hold between its
words. We instantiate this relational view of semantics in a series of neural models
based on variants of relation networks (RNs) which represent a set of objects (for
us, words forming a sentence) in terms of representations of pairs of objects. We
propose two extensions to the basic RN model for natural language. First, building
on the intuition that not all word pairs are equally informative about the meaning
of a sentence, we use constraints based on both supervised and unsupervised
dependency syntax to control which relations inﬂuence the representation. Second,
since higher-order relations are poorly captured by a sum of pairwise relations,
we use a recurrent extension of RNs to propagate information so as to form
representations of higher order relations. Experiments on sentence classiﬁcation,
sentence pair classiﬁcation, and machine translation reveal that, while basic RNs
are only modestly effective for sentence representation, recurrent RNs with latent
syntax are a reliably powerful representational device.
1",0.6170212765957447,0.1657142857142857,0.26126126126126126,0.2826086956521739,0.07471264367816093,0.11818181818181818,0.40425531914893614,0.10857142857142857,0.17117117117117117,0.011810526087047924,0.7467932
191,"single-agent policy tree search is based on Levin search [Levin, 1973] and we derive a strict upper bound on the number of nodes to search before finding the least-cost solution. we propose two different algorithms with different strengths and weaknesses.","We introduce two novel tree search algorithms that use a policy to guide search.
The ﬁrst algorithm is a best-ﬁrst enumeration that uses a cost function that allows
us to prove an upper bound on the number of nodes to be expanded before reaching
a goal state. We show that this best-ﬁrst algorithm is particularly well suited for
“needle-in-a-haystack” problems. The second algorithm is based on sampling and
we prove an upper bound on the expected number of nodes it expands before
reaching a set of goal states. We show that this algorithm is better suited for
problems where many paths lead to a goal. We validate these tree search algorithms
on 1,000 computer-generated levels of Sokoban, where the policy used to guide the
search comes from a neural network trained using A3C. Our results show that the
policy tree search algorithms we introduce are competitive with a state-of-the-art
domain-independent planner that uses heuristic search.
1",0.6190476190476191,0.15568862275449102,0.24880382775119617,0.2926829268292683,0.07228915662650602,0.11594202898550723,0.4523809523809524,0.11377245508982035,0.18181818181818182,0.014964555945900142,0.61876893
192,"policy optimization is a family of reinforcement learning algorithms aiming to directly optimize the parameters of a policy by maximizing discounted cumulative rewards. this often involves a difficult non-concave maximization problem, even when using a simple policy with a linear state-action mapping. these algorithms involve estimating a noisy gradient of the optimization objective using Monte-Carlo sampling to enable stochastic gradient ascent.","Entropy regularization is commonly used to im-
prove policy optimization in reinforcement learn-
ing. It is believed to help with exploration by
encouraging the selection of more stochastic poli-
cies. In this work, we analyze this claim using
new visualizations of the optimization landscape
based on randomly perturbing the loss function.
We ﬁrst show that even with access to the exact
gradient, policy optimization is difﬁcult due to the
geometry of the objective function. We then qual-
itatively show that in some environments, a policy
with higher entropy can make the optimization
landscape smoother, thereby connecting local op-
tima and enabling the use of larger learning rates.
This paper presents new tools for understanding
the optimization landscape, shows that policy en-
tropy serves as a regularizer, and highlights the
challenge of designing general-purpose policy op-
timization algorithms.",0.453125,0.20863309352517986,0.28571428571428575,0.1111111111111111,0.050724637681159424,0.06965174129353234,0.234375,0.1079136690647482,0.14778325123152708,0.01120262097122246,0.6601702
193,DISCERN learns to control an environment in an unsupervised way by learning purely from the stream of observations and actions. a goal achievement reward function r(s; sg) measures similarity in the space of controllable aspects of the environment. the goal-conditioned policy is able to deal with goal states that are not perfectly reachable from the current state.,"Learning to control an environment without hand-crafted rewards or expert data
remains challenging and is at the frontier of reinforcement learning research. We
present an unsupervised learning algorithm to train agents to achieve perceptually-
speciﬁed goals using only a stream of observations and actions. Our agent simulta-
neously learns a goal-conditioned policy and a goal achievement reward function
that measures how similar a state is to the goal state. This dual optimization leads
to a co-operative game, giving rise to a learned reward function that reﬂects simi-
larity in controllable aspects of the environment instead of distance in the space of
observations. We demonstrate the efﬁcacy of our agent to learn, in an unsupervised
manner, to reach a diverse set of goals on three domains – Atari, the DeepMind
Control Suite and DeepMind Lab.
1",0.711864406779661,0.3,0.4221105527638191,0.43103448275862066,0.17985611510791366,0.25380710659898476,0.5084745762711864,0.21428571428571427,0.3015075376884422,0.07438086096920166,0.7055304
194,"we build generative agents that output programs for diverse scenes that can be used to create virtual worlds, conditioned on an instruction. a single instruction corresponds to a diverse set of, yet consistent goal images. the agent learns to generate programs that draw the corresponding MNIST digits. this setting leads to more accurate and more diverse programs.","Advances in Deep Reinforcement Learning have led to agents that perform well
across a variety of sensory-motor domains. In this work, we study the setting in
which an agent must learn to generate programs for diverse scenes conditioned on
a given symbolic instruction. Final goals are speciﬁed to our agent via images of
the scenes. A symbolic instruction consistent with the goal images is used as the
conditioning input for our policies. Since a single instruction corresponds to a di-
verse set of different but still consistent end-goal images, the agent needs to learn
to generate a distribution over programs given an instruction. We demonstrate that
with simple changes to the reinforced adversarial learning [8] objective, we can
learn instruction conditioned policies to achieve the corresponding diverse set of
goals. Most importantly, our agent’s stochastic policy is shown to more accurately
capture the diversity in the goal distribution than a ﬁxed pixel-based reward func-
tion baseline. We demonstrate the efﬁcacy of our approach on two domains: (1)
drawing MNIST digits with a paint software conditioned on instructions and (2)
constructing scenes in a 3D editor that satisﬁes a certain instruction.
1",0.8245614035087719,0.23737373737373738,0.3686274509803922,0.4107142857142857,0.116751269035533,0.18181818181818182,0.5964912280701754,0.1717171717171717,0.26666666666666666,0.020044261704207252,0.72939724
195,"models can separate a single video into multiple layers, e.g. to consider the interior of the car or the shapes of the reflected trees in isolation. the model can learn to separate them in a more general data-driven way that does away with explicit assumptions about shape. in the visual domain this approximation is accurate when dealing with some reflections.","True video understanding requires making sense of non-
lambertian scenes where the color of light arriving at the
camera sensor encodes information about not just the last
object it collided with, but about multiple mediums – col-
ored windows, dirty mirrors, smoke or rain. Layered video
representations have the potential of accurately modelling
realistic scenes but have so far required stringent assump-
tions on motion, lighting and shape. Here we propose a
learning-based approach for multi-layered video represen-
tation: we introduce novel uncertainty-capturing 3D con-
volutional architectures and train them to separate blended
videos. We show that these models then generalize to single
videos, where they exhibit interesting abilities: color con-
stancy, factoring out shadows and separating reﬂections.
We present quantitative and qualitative results on real world
videos.",0.41935483870967744,0.1984732824427481,0.2694300518134715,0.03278688524590164,0.015384615384615385,0.020942408376963356,0.1935483870967742,0.0916030534351145,0.12435233160621761,0.003634222054512616,0.6081468
196,"behavioural sub-components can also be used as high-level actions in hierarchical decision-making. the event structure and composable representations must be discovered in an unsupervised manner, as sub-sequence labels are rarely available.","We introduce a framework for Compositional Imitation Learning and Execu-
tion (CompILE) of hierarchically-structured behavior. CompILE learns reusable,
variable-length segments of behavior from demonstration data using a novel un-
supervised, fully-differentiable sequence segmentation module. These learned
behaviors can then be re-composed and executed to perform new tasks. At training
time, CompILE auto-encodes observed behavior into a sequence of latent codes,
each corresponding to a variable-length segment in the input sequence. Once
trained, our model generalizes to sequences of longer length and from environ-
ment instances not seen during training. We evaluate our model in a challenging
2D multi-task environment and show that CompILE can ﬁnd correct task bound-
aries and event encodings in an unsupervised manner without requiring annotated
demonstration data. Latent codes and associated behavior policies discovered
by CompILE can be used by a hierarchical agent, where the high-level policy
selects actions in the latent code space, and the low-level, task-speciﬁc policies are
simply the learned decoders. We found that our agent could learn given only sparse
rewards, where agents without task-speciﬁc policies struggle.
1",0.6,0.1111111111111111,0.18750000000000003,0.17647058823529413,0.031914893617021274,0.05405405405405405,0.2857142857142857,0.05291005291005291,0.08928571428571429,0.0005019469662244675,0.44232526
197,a successful generative model has two core aspects: it produces targets that have high fidelity and it generalizes well on held-out data. the models trained by conventional maximum likelihood estimation (MLE) have produced superior scores on held out data across a wide range of domains. a second source of difficulties arises from the high dimensionality of large images.,"The unconditional generation of high ﬁdelity images is a longstanding benchmark
for testing the performance of image decoders. Autoregressive image models have
been able to generate small images unconditionally, but the extension of these
methods to large images where ﬁdelity can be more readily assessed has remained
an open problem. Among the major challenges are the capacity to encode the vast
previous context and the sheer difﬁculty of learning a distribution that preserves
both global semantic coherence and exactness of detail. To address the former
challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder
architecture that generates an image as a sequence of sub-images of equal size. The
SPN compactly captures image-wide spatial dependencies and requires a fraction
of the memory and the computation required by other fully autoregressive models.
To address the latter challenge, we propose to use Multidimensional Upscaling
to grow an image in both size and depth via intermediate stages utilising distinct
SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size
256 and of ImageNet from size 32 to 256. We achieve state-of-the-art likelihood
results in multiple settings, set up new benchmark results in previously unexplored
settings and are able to generate very high ﬁdelity large scale samples on the basis
of both datasets.
1",0.4406779661016949,0.1187214611872146,0.1870503597122302,0.017241379310344827,0.0045871559633027525,0.007246376811594203,0.2542372881355932,0.0684931506849315,0.1079136690647482,0.00046556129606740906,0.43959928
198,"a self-driving car company decides that the cost of a single accident where the car is at fault outweighs the benefits of 100 million miles of faultless operation. but with catastrophic failures, this may be prohibitively in-efficient. we develop a novel adversarial evaluation approach to overcome the above-mentioned problems.","This paper addresses the problem of evaluating learning systems in safety critical
domains such as autonomous driving, where failures can have catastrophic conse-
quences. We focus on two problems: searching for scenarios when learned agents
fail and assessing their probability of failure. The standard method for agent eval-
uation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely,
leading to the deployment of unsafe agents. We demonstrate this is an issue for
current agents, where even matching the compute used for training is sometimes
insufﬁcient for evaluation. To address this shortcoming, we draw upon the rare
event probability estimation literature and propose an adversarial evaluation ap-
proach. Our approach focuses evaluation on adversarially chosen situations, while
still providing unbiased estimates of failure probabilities. The key difﬁculty is in
identifying these adversarial situations – since failures are rare there is little sig-
nal to drive optimization. To solve this we propose a continuation approach that
learns failure modes in related but less robust agents. Our approach also allows
reuse of data already collected for training the agent. We demonstrate the efﬁcacy
of adversarial evaluation on two standard domains: humanoid control and simu-
lated driving. Experimental results show that our methods can ﬁnd catastrophic
failures and estimate failures rates of agents multiple orders of magnitude faster
than standard evaluation schemes, in minutes to hours rather than days.
1",0.4423076923076923,0.10043668122270742,0.16370106761565836,0.0392156862745098,0.008771929824561403,0.014336917562724014,0.28846153846153844,0.06550218340611354,0.10676156583629894,0.00036768335456935955,0.6254062
199,we are interested in methods that are suitable for transfer in the context of high-dimensional motor control problems. we introduce succcessor features (SF) and extend the GPI theorem to this case (max-ent GPI) we construct tabular and continuous action tasks where both fail to transfer well.,"Deep reinforcement learning (RL) algorithms have made great strides in recent
years. An important remaining challenge is the ability to quickly transfer existing
skills to novel tasks, and to combine existing skills with newly acquired ones.
In domains where tasks are solved by composing skills this capacity holds the
promise of dramatically reducing the data requirements of deep RL algorithms, and
hence increasing their applicability. Recent work has studied ways of composing
behaviors represented in the form of action-value functions. We analyze these
methods to highlight their strengths and weaknesses, and point out situations where
each of them is susceptible to poor performance. To perform this analysis we extend
generalized policy improvement to the max-entropy framework and introduce a
method for the practical implementation of successor features in continuous action
spaces. Then we propose a novel approach which, in principle, recovers the optimal
policy during transfer. This method works by explicitly learning the (discounted,
future) divergence between policies. We study this approach in the tabular case
and propose a scalable variant that is applicable in multi-dimensional continuous
action spaces. We compare our approach with existing ones on a range of non-
trivial continuous control problems with compositional structure, and demonstrate
qualitatively better performance despite not requiring simultaneous observation of
all task rewards.
1",0.6666666666666666,0.14814814814814814,0.24242424242424243,0.06382978723404255,0.013953488372093023,0.022900763358778626,0.3333333333333333,0.07407407407407407,0.12121212121212122,0.0005107375367458243,0.49942693
