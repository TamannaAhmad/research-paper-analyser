{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TamannaAhmad/research-paper-analyser/blob/main/image_captioning_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import login, snapshot_download\n",
        "\n",
        "# Access the token from Colab secrets\n",
        "login(token=os.environ.get(\"HF_TOKEN\"))"
      ],
      "metadata": {
        "id": "lNoo_DkfZqnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNNziuOcij8B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # download the SciCap dataset\n",
        "dataset_path = snapshot_download(repo_id=\"CrowdAILab/scicap\", repo_type='dataset')"
      ],
      "metadata": {
        "id": "APJaPrbTRR-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLeGdlhYUdRF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwsHM7dYij8N"
      },
      "outputs": [],
      "source": [
        "class SciCapDataset(Dataset):\n",
        "    def __init__(self, data_dir, split=\"train\", transform=None, max_samples=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        # SciCap dataset structure is different than expected\n",
        "        # Check for alternative file paths\n",
        "        annotations_path = None\n",
        "\n",
        "        # Try different possible annotation file locations\n",
        "        possible_paths = [\n",
        "            os.path.join(data_dir, f'{split}_captions.json'),  # Root directory\n",
        "            os.path.join(data_dir, split, f'{split}_captions.json'),  # Split subdirectory\n",
        "            os.path.join(data_dir, 'annotations', f'{split}_captions.json')  # Annotations subdirectory\n",
        "        ]\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                annotations_path = path\n",
        "                break\n",
        "\n",
        "        if not annotations_path:\n",
        "            raise FileNotFoundError(f\"Could not find annotation file for {split} split. Searched: {possible_paths}\")\n",
        "\n",
        "        print(f\"Using annotations from: {annotations_path}\")\n",
        "\n",
        "        # Load annotations\n",
        "        with open(annotations_path, 'r', encoding='utf-8') as f:\n",
        "            captions_data = json.load(f)\n",
        "\n",
        "        # Try to find image directory\n",
        "        img_dir = None\n",
        "        possible_img_dirs = [\n",
        "            os.path.join(data_dir, split, 'images'),\n",
        "            os.path.join(data_dir, 'images', split),\n",
        "            os.path.join(data_dir, 'images')\n",
        "        ]\n",
        "\n",
        "        for path in possible_img_dirs:\n",
        "            if os.path.exists(path):\n",
        "                img_dir = path\n",
        "                break\n",
        "\n",
        "        if not img_dir:\n",
        "            raise FileNotFoundError(f\"Could not find images directory for {split} split. Searched: {possible_img_dirs}\")\n",
        "\n",
        "        print(f\"Using images from: {img_dir}\")\n",
        "\n",
        "        # Process images based on the actual structure of captions_data\n",
        "        if isinstance(captions_data, dict):\n",
        "            # Format: {img_name: caption_info, ...}\n",
        "            for img_name, caption_info in captions_data.items():\n",
        "                img_path = os.path.join(img_dir, img_name)\n",
        "\n",
        "                if isinstance(caption_info, list):\n",
        "                    caption = caption_info[0]  # Use first caption (fixed typo)\n",
        "                elif isinstance(caption_info, dict) and 'caption' in caption_info:\n",
        "                    caption = caption_info['caption']  # Fixed typo\n",
        "                else:\n",
        "                    caption = str(caption_info)\n",
        "\n",
        "                if os.path.exists(img_path):\n",
        "                    self.samples.append((img_path, caption))\n",
        "\n",
        "        if max_samples and len(self.samples) > max_samples:  # Fixed typo\n",
        "            self.samples = self.samples[:max_samples]\n",
        "\n",
        "        print(f\"Dataset created with {len(self.samples)} image-caption pairs from {split} set\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, caption = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            # Only load image when requested during iteration\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return {'image': image, 'caption': caption, 'path': img_path}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            if self.transform:\n",
        "                placeholder = torch.zeros(3, 384, 384)\n",
        "            else:\n",
        "                placeholder = Image.new('RGB', (384, 384), color='black')\n",
        "\n",
        "            return {'image': placeholder, 'caption': \"Error loading image\", 'path': img_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BemoMVXfij8R"
      },
      "outputs": [],
      "source": [
        "# data preparation\n",
        "def create_dataloaders(data_dir, batch_size=4, image_size=384):\n",
        "    # image transformations\n",
        "    transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "    # create train and val datasets\n",
        "    train_dataset = SciCapDataset(data_dir, split = 'train', transform = transform)\n",
        "    val_dataset = SciCapDataset(data_dir, split = 'val', transform = transform)\n",
        "\n",
        "    # create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wBuNDdHij8U"
      },
      "outputs": [],
      "source": [
        "def train_model(train_loader, val_loader, learning_rate=5e-5, num_epochs=5, device=\"cuda\"):\n",
        "    # Load pre-trained model\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Set up optimizer with a smaller batch size\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Use gradient accumulation to simulate larger batches\n",
        "    gradient_accumulation_steps = 4\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        valid_batches = 0\n",
        "\n",
        "        # Use garbage collection more aggressively\n",
        "        import gc\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")):\n",
        "            try:\n",
        "                images = batch['image'].to(device)\n",
        "                captions = batch['caption']\n",
        "\n",
        "                if \"Error loading image\" in captions:\n",
        "                    continue\n",
        "\n",
        "                pixel_values = processor.image_processor(images, do_normalize=True, return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "                batch_loss = 0\n",
        "                for i, caption in enumerate(captions):\n",
        "                    # Process one sample at a time to reduce memory usage\n",
        "                    tokenized = processor.tokenizer(\n",
        "                        caption,\n",
        "                        padding=\"max_length\",\n",
        "                        truncation=True,\n",
        "                        max_length=75,\n",
        "                        return_tensors=\"pt\"\n",
        "                    ).to(device)\n",
        "\n",
        "                    outputs = model(\n",
        "                        pixel_values=pixel_values[i].unsqueeze(0),\n",
        "                        input_ids=tokenized.input_ids,\n",
        "                        attention_mask=tokenized.attention_mask,\n",
        "                        labels=tokenized.input_ids\n",
        "                    )\n",
        "\n",
        "                    if outputs.loss is not None:\n",
        "                        # Normalize loss by gradient accumulation steps\n",
        "                        loss = outputs.loss / gradient_accumulation_steps\n",
        "                        loss.backward()\n",
        "                        batch_loss += outputs.loss.item()\n",
        "\n",
        "                # Step optimizer only after accumulating gradients\n",
        "                if (batch_idx + 1) % gradient_accumulation_steps == 0 or batch_idx == len(train_loader) - 1:\n",
        "                    # Gradient clipping\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                if batch_loss > 0:\n",
        "                    epoch_loss += batch_loss / len(captions)\n",
        "                    valid_batches += 1\n",
        "\n",
        "                # Clear cache periodically\n",
        "                if batch_idx % 10 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during training: {e}\")\n",
        "                continue\n",
        "\n",
        "        # update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # calculate average loss if there were valid batches\n",
        "        avg_train_loss = epoch_loss / valid_batches if valid_batches > 0 else float('inf')\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        valid_val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "                try:\n",
        "                    images = batch['image'].to(device)\n",
        "                    captions = batch['caption']\n",
        "\n",
        "                    # skip batches with error placeholders\n",
        "                    if \"Error loading image\" in captions:\n",
        "                        continue\n",
        "\n",
        "                    pixel_values = processor.image_processor(images, do_normalize=True, return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "                    batch_inputs = []\n",
        "                    for caption in captions:\n",
        "                        tokenized = processor.tokenizer(\n",
        "                            caption,\n",
        "                            padding=\"max_length\",\n",
        "                            truncation=True,\n",
        "                            max_length=75,\n",
        "                            return_tensors=\"pt\"\n",
        "                        ).to(device)\n",
        "\n",
        "                        batch_inputs.append({\n",
        "                            \"input_ids\": tokenized.input_ids,\n",
        "                            \"attention_mask\": tokenized.attention_mask\n",
        "                        })\n",
        "\n",
        "                    # calculate validation loss\n",
        "                    batch_val_loss = 0\n",
        "                    for i, inputs in enumerate(batch_inputs):\n",
        "                        outputs = model(\n",
        "                            pixel_values=pixel_values[i].unsqueeze(0),\n",
        "                            input_ids=inputs[\"input_ids\"],\n",
        "                            attention_mask=inputs[\"attention_mask\"],\n",
        "                            labels=inputs[\"input_ids\"]\n",
        "                        )\n",
        "\n",
        "                        if outputs.loss is not None:\n",
        "                            batch_val_loss += outputs.loss.item()\n",
        "\n",
        "                    if batch_val_loss > 0:\n",
        "                        val_loss += batch_val_loss / len(batch_inputs)\n",
        "                        valid_val_batches += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during validation: {e}\")\n",
        "                    continue\n",
        "\n",
        "        avg_val_loss = val_loss / valid_val_batches if valid_val_batches > 0 else float('inf')\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        # savve the best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), \"best_blip_captioning_model.pth\")\n",
        "            print(f\"Model saved with validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    # plot training curves\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('training_curve.png')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc_n4Q7eij8e"
      },
      "outputs": [],
      "source": [
        "def generate_captions(model, processor, test_loader, device=\"cuda\", num_samples=5):\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            images = batch['image'].to(device)\n",
        "            original_captions = batch['caption']\n",
        "            image_paths = batch['path']\n",
        "\n",
        "            if \"Error loading image\" in original_captions:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # generate captions with sampling enabled\n",
        "                generated_ids = model.generate(\n",
        "                    pixel_values=images,\n",
        "                    max_length=100,\n",
        "                    num_beams=5,\n",
        "                    no_repeat_ngram_size=2,\n",
        "                    temperature=1.2,\n",
        "                    do_sample=True\n",
        "                )\n",
        "                generated_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "                for j, (img_path, orig_cap, gen_cap) in enumerate(zip(image_paths, original_captions, generated_captions)):\n",
        "                    pil_image = Image.open(img_path).convert('RBG')\n",
        "                    results.append({\n",
        "                        '\\nimage' : pil_image,\n",
        "                        '\\nimage_path': img_path,\n",
        "                        '\\noriginal_caption': orig_cap,\n",
        "                        '\\ngenerated_caption': gen_cap\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating captions: {e}\")\n",
        "                continue\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onX0jplfij8g"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # check if CUDA is available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # define parameters\n",
        "    data_dir = dataset_path\n",
        "    batch_size = 8\n",
        "    learning_rate = 5e-5\n",
        "    num_epochs = 5\n",
        "\n",
        "    try:\n",
        "        # create dataloaders\n",
        "        train_loader, val_loader = create_dataloaders(data_dir, batch_size=batch_size)\n",
        "\n",
        "        print(f\"Training on {len(train_loader.dataset)} samples, validating on {len(val_loader.dataset)} samples\")\n",
        "\n",
        "        # train model\n",
        "        model = train_model(train_loader, val_loader, learning_rate=learning_rate,\n",
        "                           num_epochs=num_epochs, device=device)\n",
        "\n",
        "        # Save the model to Google Drive\n",
        "        model_save_path = \"/content/drive/MyDrive/best_blip_captioning_model.pth\"\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Best model saved to {model_save_path}\")\n",
        "\n",
        "        # load processor for inference\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "\n",
        "        # generate some example captions\n",
        "        results = generate_captions(model, processor, val_loader, device=device, num_samples=5)\n",
        "\n",
        "        # display results\n",
        "        for item in results:\n",
        "          plt.figure(figsize=(10, 10))\n",
        "          plt.imshow(item['image'])\n",
        "          plt.axis('off')\n",
        "          plt.title(f\"Image: {os.path.basename(item['image_path'])}\")\n",
        "          plt.show()\n",
        "          print(f\"Original: {item['original_caption']}\")\n",
        "          print(f\"Generated: {item['generated_caption']}\")\n",
        "          print(\"-\" * 80)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred in the main function: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c7wwrF1ij8k",
        "outputId": "6f249e20-687d-4507-ae30-e639e3590f71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "An error occurred in the main function: Could not find annotation file for train split. Searched: ['/root/.cache/huggingface/hub/datasets--CrowdAILab--scicap/snapshots/60e504baa94423f63cda87d5442e73a696b953d3/train_captions.json', '/root/.cache/huggingface/hub/datasets--CrowdAILab--scicap/snapshots/60e504baa94423f63cda87d5442e73a696b953d3/train/train_captions.json', '/root/.cache/huggingface/hub/datasets--CrowdAILab--scicap/snapshots/60e504baa94423f63cda87d5442e73a696b953d3/annotations/train_captions.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-27-b0b52a2609ba>\", line 14, in main\n",
            "    train_loader, val_loader = create_dataloaders(data_dir, batch_size=batch_size)\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-24-b4da2fc243f0>\", line 10, in create_dataloaders\n",
            "    train_dataset = SciCapDataset(data_dir, split = 'train', transform = transform)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-23-f27a3e028ec7>\", line 24, in __init__\n",
            "    raise FileNotFoundError(f\"Could not find annotation file for {split} split. Searched: {possible_paths}\")\n",
            "FileNotFoundError: Could not find annotation file for train split. Searched: ['/root/.cache/huggingface/hub/datasets--CrowdAILab--scicap/snapshots/60e504baa94423f63cda87d5442e73a696b953d3/train_captions.json', '/root/.cache/huggingface/hub/datasets--CrowdAILab--scicap/snapshots/60e504baa94423f63cda87d5442e73a696b953d3/train/train_captions.json', '/root/.cache/huggingface/hub/datasets--CrowdAILab--scicap/snapshots/60e504baa94423f63cda87d5442e73a696b953d3/annotations/train_captions.json']\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gcsu_vMnXzA8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}