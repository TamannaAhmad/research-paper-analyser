Monte Carlo Gradient Estimation in Machine Learning Monte Carlo Gradient Estimation in Machine Learning Shakir Mohamed∗ shakir@google.com Mihaela Rosca∗ mihaelacr@google.com Michael Figurnov∗ mfigurnov@google.com Andriy Mnih∗ amnih@google.com ∗Equal contributions; DeepMind, London Abstract This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and supervised learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies—the pathwise, score function, and measure-valued gradient estimators— exploring their historical developments, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support. Keywords: gradient estimation, Monte Carlo, sensitivity analysis, score-function estimator, pathwise estimator, measure-valued estimator, variance reduction 1. Introduction Over the past five decades the problem of computing the gradient of an expectation of a function—a stochastic gradient—has repeatedly emerged as a fundamental tool in the advancement of the state of the art in the computational sciences. An ostensibly anodyne gradient lies invisibly within many of our everyday activities: within the management of modern supply-chains (Kapuscinski and Tayur, 1999; Siekman, 2000), in the pricing and hedging of financial derivatives (Glasserman, 2013), in the control of traffic lights (Rubinstein and Shapiro, 1993), and in the major milestones in the ongoing research in artificial intelligence (Silver et al., 2016). Yet, computing the stochastic gradient is not without complexity, and its fundamental importance requires that we deepen our understanding of them to sustain future progress. This is our aim in this paper: to provide a broad, accessible, and detailed understanding of the tools we have to compute gradients of stochastic functions. We also aim to describe their instantiations in other research fields, to consider the tradeoffs we face in choosing amongst the available solutions, and to consider questions for future research. 1 9102 nuJ 52 ]LM.tats[ 1v25601.6091:viXra
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih Our central question is the computation of a general probabilistic objective of the form: F (cid:90) (θ) := p(x; θ)f (x; φ)dx = E [f (x; φ)] . (1) p(x;θ) F Equation (1) is a mean-value analysis, in which a function f of an input variable x with structural parameters φ, is evaluated on average with respect to an input distribution p(x; θ) with distributional parameters θ. We will refer to f as the cost and to p(x; θ) as the measure, following the naming used in existing literature. We will make one restriction in this review, and that is to problems where the measure p is a probability distribution that is continuous in its domain and differentiable with respect to its distributional parameters. This is a general framework that allows us to cover problems from queueing theory and variational inference to portfolio design and supervised learning. The need to learn the distributional parameters θ makes the gradient of the function (1) of greater interest to us: η := (θ) = E [f (x; φ)] . (2) θ θ p(x;θ) ∇ F ∇ Equation (2) is the sensitivity analysis of , i.e. the gradient of the expectation with respect to the F distributional parameters. It is this gradient that lies at the core of tools for model explanation and optimisation. But this gradient problem is difficult in general: we will often not be able to evaluate the expectation in closed form; the integrals over x are typically high-dimensional making quadrature ineffective; it is common to request gradients with respect to high-dimensional parameters θ, easily in the order of tens-of-thousands of dimensions; and in the most general cases, the cost function may itself not be differentiable, or be a black-box function whose output is all we are able to observe. In addition, for applications in machine learning, we will need efficient, accurate and parallelisable solutions that minimise the number of evaluations of the cost. These are all challenges we can overcome by developing Monte Carlo estimators of these integrals and gradients. Overview. We develop a detailed understanding of Monte Carlo gradient estimation by first in- troducing the general principles and considerations for Monte Carlo methods in Section 2, and then showing how stochastic gradient estimation problems of the form of (2) emerge in five distinct re- search areas. We then develop three classes of gradient estimators: the score-function estimator, the pathwise estimator, and the measure-valued gradient estimator in Sections 4–6. We discuss methods to control the variance of these estimators in Section 7. Using a set of case studies in Section 8, we explore the behaviour of gradient estimators in different settings to make their application and design more concrete. We discuss the generalisation of the estimators developed and other methods for gradient estimation in section 9, and conclude in section 10 with guidance on choosing estima- tors and some of the opportunities for future research. This paper follows in the footsteps of several related reviews that have had an important influence on our thinking, specifically, Fu (1994), Pflug (1996), V´azquez-Abad (2000), and Glasserman (2013). Notation. Throughout the paper, bold symbols indicate vectors, otherwise variables are scalars. f (x) is function of variables x which may depend on structural parameters φ, although we will not explicitly write out this dependency since we will not consider these parameters in our exposition. We indicate a probability distribution using the symbol p, and use the notation p(x; θ) to represent the distribution over random vectors x with distributional parameters θ. represents the gradient θ ∇ operator that collects all the partial derivatives of a function with respect to parameters in θ, i.e. f = [ ∂f , . . . , ∂f ], for D-dimensional parameters; will also be used for scalar θ. By ∇θ ∂θ1 ∂θD ∇θ convention, we consider vectors to be columns and gradients as rows. We represent the sampling or simulation of variates xˆ from a distribution p(x) using the notation xˆ p(x). We use E [f ] and p V [f ] to denote the expectation and variance of the function f under the d∼ istribution p, respectively. p 2
Monte Carlo Gradient Estimation in Machine Learning Shorthand notation for distributions such as or for the Gaussian and double-sided Maxwell N M distribution, respectively, is defined in Appendix A. Reproducibility. Code to reproduce Figures 2 and 3 in the paper will later be available in a repository at www.github.com/deepmind/mc_gradients . 2. Monte Carlo Methods and Stochastic Optimisation This section briefly reviews the Monte Carlo method and the stochastic optimisation setting we rely on throughout the paper. But importantly, this section provides the motivation for why we consider the gradient estimation problem (2) to be so fundamental, by exploring what is an astounding breadth of research areas in which it appears. 2.1 Monte Carlo Estimators The Monte Carlo method is one of the most general tools we have for the computation of probabilities, integrals and summations. Consider the mean-value analysis problem (1), which evaluates the expected value of a general function f under a distribution p. In most problems, the integral (1) will not be known in closed-form, and not amenable to evaluation using numerical quadrature. However, by using the Monte Carlo method (Metropolis and Ulam, 1949) we can easily approximate the value of the integral. The Monte Carlo method says that we can numerically evaluate the integral by first drawing independent samples xˆ(1), . . . , xˆ(N) from the distribution p(x; θ), and then computing the average of the function evaluated at these samples: 1 (cid:88)N (cid:16) (cid:17) ¯ = f xˆ(n) , where xˆ(n) p(x; θ) for n = 1, ..., N. (3) N F N ∼ n=1 The quantity ¯ is a random variable, since it depends on the specific set of random variates N F xˆ(1), . . . , xˆ(n) used, and we can repeat this process many times by constructing multiple sets of { } such random variates. Equation (3) is a Monte Carlo estimator of the expectation (1). As long as we can write an integral in the form of equation (1)—as a product of a function and a distribution that we can easily sample from—we will be able to apply the Monte Carlo method and develop Monte Carlo estimators. This is the strategy we use throughout this paper. There are four properties we will always ask of a Monte Carlo estimator: Consistency. As we increase the number of samples N in (3), the estimate ¯ should converge to N the true value of the integral E [f (x)]. This usually follows from tF he strong law of large p(x;θ) numbers. Unbiasedness. If we repeat the estimation process many times, we should find that the estimate is centred on the actual value of the integral on average. The Monte Carlo estimators for (1) satisfy this property easily: (cid:34) (cid:35) E (cid:2) ¯ (cid:3) = E 1 (cid:88)N f (cid:16) x(n)(cid:17) = 1 (cid:88)N E (cid:104) f (cid:16) x(n)(cid:17)(cid:105) = E [f (x)] . (4) p(x;θ) FN p(x;θ) N N p(x;θ) p(x;θ) n=1 n=1 3
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih Input parameter Simulation output (✓) θ System or Environment F<<<<<lllllaaaaattttteeeeexxxxxiiiiittttt ssssshhhhhaaaaa11111_____bbbbbaaaaassssseeeee6666644444====="""""(((((nnnnnuuuuullllllllll)))))""""">>>>>(((((nnnnnuuuuullllllllll)))))<<<<</////lllllaaaaattttteeeeexxxxxiiiiittttt>>>>> Gradient estimate (✓) Simulation r<<<<<lllllaaaaattttteeeeexxxxxiiiiittttt ssssshhhhhaaaaa11111_____bbbbbaaaaassssseeeee6666644444====="""""(((((nnnnnuuuuullllllllll)))))""""">>>>>(((((nnnnnuuuuullllllllll)))))<<<<</////lllllaaaaattttteeeeexxxxxiiiiittttt>>>>> ✓F Optimisation Gradient-based New θ optimisation Figure 1: Stochastic optimisation loop comprising a simulation phase and an optimisation phase. The simulation phase produces a simulation of the stochastic system or interaction with the envi- ronment, as well as unbiased estimators of the gradient. Unbiasedness is always preferred because it allows us to guarantee the convergence of a stochas- tic optimisation procedure. Biased estimators can sometimes be useful but require more care in their use (Mark and Baram, 2001). Minimum variance. Because an estimator (3) is a random variable, if we compare two unbiased estimators, we will prefer the estimator that has lower variance. We will repeatedly emphasise the importance of low variance estimators for two reasons: the resulting gradient estimates are themselves more accurate, which is essential for problems in sensitivity analysis where the actual value of the gradient is the object of interest; and where the gradient is used for stochastic optimisation, low-variance gradients makes learning more efficient, allowing larger step-sizes (learning rates) to be used, potentially reducing the overall number of steps needed to reach convergence and hence resulting in faster training. Computational efficiency. We will always prefer an estimator that is computationally efficient, such as those that allow the expectation to be computed using the fewest number of samples, those that have a computational cost linear in the number of parameters, and those whose computations can be easily parallelised. We can typically assume that our estimators are consistent because of the generality of the law of large numbers. Therefore most of our effort will be directed towards characterising their unbiased- ness, variance and computational cost, since it is these properties that affect the choice we make between competing estimators. Monte Carlo methods are widely studied, and the books by Robert and Casella (2013) and Owen (2013) provide a deep coverage of their wider theoretical properties and practical considerations. 2.2 Stochastic Optimisation The gradient (2) supports at least two key computational tasks, that of explanation and optimisation. Because the gradient provides a computable value that characterises the behaviour of the cost—the cost’s sensitivity to changing settings—a gradient is directly useful as an tool with which to explain the behaviour of a probabilistic system. More importantly, the gradient (2) is the key quantity needed for optimisation of the distributional parameters θ. Figure 1 (adapted from Pflug (1996, sect. 1.2.5)) depicts the general stochastic optimisation loop, which consists of two phases: a simulation phase and an optimisation phase. This is a stochastic system because the system or the environment has elements of randomness, i.e. the input parameters θ influence the system in a probabilistic manner. We will consider several case studies in Section 4
Monte Carlo Gradient Estimation in Machine Learning 8 that all operate within this optimisation framework. Whereas in a typical optimisation we would make a call to the system for the function value, which is deterministic, in stochastic optimisation we make a call for random variables. Instead of calling for the function value, we request an estimate of the function value; instead of calling for the gradient, we ask for an estimate of the gradient; instead of calling for the Hessian, we will ask for an estimate of the Hessian. Making a clear separation between the simulation and optimisation phases allows us to focus our attention on developing the best estimators of gradients we can, knowing that when used with gradient-based optimisation methods available to us, we can guarantee convergence so long as the estimate is unbiased, i.e. the estimate of the gradient is correct on average (Kushner and Yin, 2003). If the optimisation phase is also used with stochastic approximation (Kushner and Yin, 2003; Robbins and Monro, 1951), such as stochastic gradient descent, as is widely-used in machine learning, then this loop can be described as a doubly-stochastic optimisation (Titsias and L´azaro-Gredilla, 2014). In this setting, there are now two sources of stochasticity. One source arises in the simulation phase from the use of Monte Carlo estimators of the gradient, which are random variables because of the repeated sampling from the measure, like those in (3). A second source of stochasticity arises in the optimisation phase from the use of the stochastic approximation method, which introduces a source of randomness through the subsampling of data points (mini-batching) when computing the gradient. In Section 8.3 we explore some of the performance effects from the interaction between these sources of stochasticity. 2.3 The Central Role of Gradient Estimation Across the breadth of research areas, whether in approximate inference, supervised learning, experimental design, or active learning, the need for accurate and efficient computation of stochastic gradients and their corresponding Monte Carlo estimators appears, making the gradient question one of the fundamental problems of statistical and machine learning research. We make the problem (2) more concrete by briefly describing its instantiation in five areas, each with independent and thriving research communities of their own. In them, we can see the central role of gradient estimation by matching the pattern of the problem that arises, and in so doing, see their shared foundations. Variational Inference. Variational inference is a general method for approximating complex and unknown distributions by the closest distribution within a tractable family. Wherever the need to approximate distributions appears in machine learning—in supervised, unsupervised or supervised learning—a form of variational inference can be used. Consider a generic probabilistic model p(x z)p(z) that defines a generative process in which observed data x is | generated using a set of unobserved variables z, using a model p(x z; φ), which may have addi- | tional side information or additional parameters φ, and a prior distribution p(z). In supervised learning, the unobserved variables might correspond to the weights of a regression problem, and in unsupervised learning to latent variables. The posterior distribution of this generative process p(z x) is unknown, and is approximated by a variational distribution q(z x; θ), which | | is a parameterised family of distributions with variational parameters θ, e.g., the mean and variance of a Gaussian distribution. Finding the distribution q(z x; θ) that is closest to p(z x) | | leads to an objective, the variational free-energy, that optimises an expected log-likelihood log p(x z) subject to a regularisation constraint that encourages closeness between the vari- | ational distribution q and the prior distribution p(z) (Blei et al., 2017; Jordan et al., 1999). Optimising the distribution q requires the gradient of the free energy with respect to the variational parameters θ: (cid:20) (cid:21) q(z x; θ) η = E log p(x z) log | . (5) ∇θ q(z|x;θ) | − p(z) 5
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih This is an objective that is in the form of equation (1): the cost is the difference between a log-likelihood and a log-ratio (of the variational distribution and the prior distribution), and the measure is the variational distribution q(z x; θ). This problem also appears in other re- | search areas, especially in statistical physics, information theory and utility theory (Honkela and Valpola, 2004). Many of the solutions that have been developed for scene understanding, representation learning, photo-realistic image generation, or the simulation of molecular struc- tures rely on variational inference (Eslami et al., 2018; Higgins et al., 2017; Kusner et al., 2017). In variational inference we find a thriving research area where the problem (2) of computing gradients of expectations lies at its core. supervised learning. Model-free policy search is an area of supervised learning where we learn a policy—a distribution over actions—that on average maximises the accumulation of long-term rewards. Through interaction in an environment, we can generate trajectories τ = (s , a , s , a , . . . , s , a ) that consist of pairs of states s and actions a for time period 1 1 2 2 T T t t t = 1, . . . , T . A policy is learnt by following the policy gradient (Sutton and Barto, 1998): (cid:34) (cid:35) T (cid:88) η = E γtr(s , a ) , (6) θ p(τ;θ) t t ∇ t=0 which again has the form of equation (1). The cost is the return over the trajectory, which is a weighted sum of rewards obtained at each time step r(s , a ), with the discount fac- t t tor γ [0, 1]. The measure is the joint distribution over states and actions p(τ ; θ) = (cid:81)T p(∈ s s , a )p(a s ; θ), which is the product of a state transition probability p(s s , a ) t=0 t+1 | t t t | t t+1 | t t and the policy distribution p(a s ; θ) with policy parameters θ. The Monte Carlo gradient t t | estimator used to compute this gradient with respect to the policy parameters leads to the policy gradient theorem, one of the key theorems in supervised learning, which lies at the heart of many successful applications, such as the AlphaGo system for complex board games (Silver et al., 2016) or in robotic control (Deisenroth et al., 2013). And so we find yet another thriving area of research where gradient estimation plays a fundamental role. Sensitivity Analysis. The field of sensitivity analysis is dedicated to the study of problems of the form of (2), and asks what the sensitivity (another term for gradient) of an expectation is to its input parameters. Computational finance is one area where sensitivity analysis is widely used to compare investments under different pricing and return assumptions, in order to choose a strategy that provides the best potential future payout. Knowing the value of the gradient gives information needed to understand the sensitivity of future payouts to different pricing assumptions, and provides a direct measure of the financial risk that an investment strategy will need to manage. In finance, these sensitivities, or gradients, are referred to as the greeks. A classic example of this problem is the Black-Scholes option pricing model, which can be written in the form of equation (1): the cost function is the discounted value of an option with price s at the time of maturity T , using discount factor γ; and the measure is a log-Normal T distribution over the price at maturity p(s ; s ), and is a function of the initial price s . The T 0 0 gradient with respect to the initial price is known as the Black-Scholes Delta (Chriss, 1996): ∆ = E (cid:2) e−γT (s K)+(cid:3) , (7) ∇s0 p(sT ;s0) T − where K is a minimum expected return on the investment and (x)+:= max(0, x). Delta is the risk measure that is actively minimised in delta-neutral hedging strategies. The Black-Scholes delta above can be computed in closed form, and the important greeks have closed or semi- closed form formulae in the Black-Scholes formulation. Gradient estimation methods are used when the cost function (the payoff) is more complicated, or in more realistic models where the measure is not a log-normal. In these more general settings, the gradient estimators we 6
Monte Carlo Gradient Estimation in Machine Learning review in this paper are the techniques that are still widely-used in financial computations today (Glasserman, 2013). Discrete Event Systems and Queuing Theory. An enduring problem in operations research is the study of queues, the waiting systems and lines that we all find ourselves in as we await our turn for a service. Such systems are often described by a stochastic recursion L = x + (L a )+, where L is the total system time (of people in the queue plus in ser- t t t−1 t t − vice), x is the service time for the tth customer, a is the inter-arrival time between the tth and t t the (t + 1)th customer, and the operation (z)+ := max(0, z) (Rubinstein et al., 1996; V´azquez- Abad, 2000). The number of customers served in this system is denoted by τ . For convenience we can write the service time and the inter-arrival time using the variable y = x , a , and t t t { } characterise it by a distribution p(y ; θ) = p(x ; θ )p(a ; θ ) with distributional parameters t t x t a θ = θ , θ . The expected steady-state behaviour of this system gives a problem of the form x a { } of (2), where the cost is the ratio of the average total system time to the average number of customers served, and the measure is the joint distribution over service times p(y ; θ) 1:T (Rubinstein, 1986): (cid:34) (cid:80)T L (y ) (cid:35) η = E t=1 t 1:t . (8) ∇θ p(y1:T ;θ) τ (y ) 1:T In Kendall’s notation, this is a general description for G/G/1 queues (Newell, 2013): queues with general distributions for the arrival rate, general distributions for the service time, and a single-server first-in-first-out queue. This problem also appears in many other areas and under many other names, particularly in regenerative and semi-Markov processes, and discrete event systems (Cassandras and Lafortune, 2009). In all these cases, the gradient of the expectation of a cost, described as a ratio, is needed to optimise a sequential process. Queues permeate all parts of our lives, hidden from us in global supply chains and in internet routing, and visible to us at traffic lights and in our online and offline shopping, all made efficient through the use of Monte Carlo gradient estimators. Experimental Design. In experimental design we are interested in finding the best designs—the inputs or settings to a possibly unknown system—that result in outputs that are optimal with respect to some utility or score. Designs are problem configurations, such as a hypothesis in an experiment, the locations of sensors on a device, or the hyperparameters of a statistical model (Chaloner and Verdinelli, 1995). We evaluate the system using a given design θ and measure its output y, usually in settings where the measurements are expensive to obtain and hence cannot be taken frequently. One such problem is the probability-of-improvement, which allows us to find designs θ that on average improve over a currently best-known outcome. This is an objective that can be written in the form of equation (1), where the cost is the score of a new design being better that the current best design, and the measure is a predictive function p(y; θ), which allows us to simulate the output y for an input design θ. To find the best design, we will need to compute the gradient of the probability-of-improvement with respect to the design θ: η = E (cid:2)1 (cid:3) , (9) ∇θ p(y|θ) {y<ybest} where the indicator 1 is one if the condition is met, and zero otherwise. There are y<ybest many such objectives, in areas such as Bayesian optimisation, active learning and bandits (Shahriari et al., 2016; Wilson et al., 2018), all of which involve computing the gradient of an expectation of a loss function, with wide use in computer graphics, model architecture search, automatic machine learning, and treatment design; again highlighting the central role that general-purpose gradient estimators play in modern applications. While these five areas are entire fields of their own, they are also important problems for which there is ongoing effort throughout machine learning. There are also many other problems where the need 7
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih for computing stochastic gradients appears, including systems modelling using stochastic differential equations, parameter learning of generative models in algorithms such as variational autoencoders, generative adversarial networks and generative stochastic networks (Rezende et al. (2014); Kingma and Welling (2014b); Goodfellow et al. (2014); Bengio et al. (2014)), in bandits and online learning (Hu et al., 2016), in econometrics and simulation-based estimation (Gourieroux et al., 1996), and in instrumental-variables estimation and counter-factual reasoning (Hartford et al., 2016). The ability to compute complicated gradients gives us the confidence to tackle increasingly more complicated and interesting problems. 3. Intuitive Analysis of Gradient Estimators The structure of the sensitivity analysis problem E [f (x)] (2) directly suggests that gradients θ p(x;θ) ∇ can be computed in two ways: Derivatives of Measure. The gradient can be computed by differentiation of the measure p(x; θ). Gradient estimators in this class include the score function estimator (Section 4) and the measure-valued gradient (Section 6). Derivatives of Paths. The gradient can be computed by differentiation of the cost f (x), which encodes the pathway from parameters θ, through the random variable x, to the cost value. In this class of estimators, we will find the pathwise gradient (Section 5), harmonic gradient estimators and finite differences (Section 9.5), and Malliavin-weighted estimators (Section 9.7). We focus our attention on three classes of gradient estimators: the score function, pathwise and measure-valued gradient estimators. All three estimators satisfy two desirable properties that we identified previously, they are consistent and unbiased ; but they differ in their variance behaviour and in their computational cost. Without knowing the mathematical descriptions of these three gradient estimators, we can compare the performance of the three estimation approaches in a sim- plified problem, to develop an intuitive view of the differences between methods with regards to performance, computational cost, differentiability, and variability of the cost function. Consider the stochastic gradient problem (2) that uses Gaussian measures for three simple families of cost functions, quadratics, exponentials and cosines: (cid:90) η = (x µ, σ2)f (x; k)dx; θ µ, σ ; f (x k)2, exp( kx2), cos(kx) . (10) θ ∇ N | ∈ { } ∈ { − − } We are interested in estimates of the gradient (10) with respect to the mean µ and the standard deviation σ of the Gaussian distribution. The cost functions vary with a parameter k, which allows us to explore how changes in the cost affect the gradient. In the graphs that follow, we use numerical integration to compute the variance of these gradients. To reproduce these graphs, see the note on code in the introduction. Quadratic costs. Figure 2 compares the variance of several gradient estimators, for the quadratic function f (x, k) = (x k)2. For this function we see that we could create a rule-of-thumb ranking: − the highest variance estimator is the score function, lower variance is obtained by the pathwise derivative, and the lowest variance estimator is the measure-valued derivative. But for the gradient with respect to the mean µ, we also see that this is not uniformly true, since there are quadratic functions, those with small or large offsets k, for which the variance of the pathwise estimator is lower than the measure-valued derivative. This lack of universal ranking is a general property of gradient estimators. 8
Monte Carlo Gradient Estimation in Machine Learning Score function Score function + variance reduction Pathwise Measure-valued + variance reduction Value of the cost Derivative of the cost 103 102 101 100 3 2 1 0 1 2 3 k µ rof rotamitse eht fo ecnairaV 103 102 101 100 3 2 1 0 1 2 3 k 5 0 2 5 0 5 5 0 5 5 0 5 σ rof rotamitse eht fo ecnairaV 5 0 2 5 0 5 5 0 5 5 0 5 Figure 2: Variance of the stochastic estimates of E (cid:2) (x k)2(cid:3) for µ = σ = 1 as a function θ N (x|µ,σ2) ∇ − of k for three different classes of gradient estimators. Left: θ = µ; right: θ = σ. The graphs in the bottom row show the function (solid) and its gradient (dashed) for k 3, 0, 3 . ∈ {− } The computational cost of the estimators in Figure 2 is the same for the score-function and pathwise estimators: they can both be computed using a single sample in the Monte Carlo estimator (N = 1 in (3)), even for multivariate distributions, making them computationally cheap. The measure-valued derivative estimator will require 2D evaluations of the cost function for D dimensional parameters, and for this the reason will typically not be preferred in high-dimensional settings. We will later find that if the cost function is not differentiable, then the pathwise gradient will not be applicable. These are considerations which will have a significant impact on the choice of gradient estimator for any particular problem. Exponential and Cosine costs. Figure 3 shows the variance behaviour for two other functions, exp( kx2) and cos(kx). As the parameter k of these functions varies, the functions can become − very sharp and peaked (see the black graphs at the bottom), and this change in the cost function can change the effectiveness of a gradient estimator: from being the lowest-variance estimator in one regime, to being the highest-variance one in another. The green curve in Figure 3 for the pathwise estimator shows its variance increasing as k becomes larger. The measure-valued derivative (red curve) has similar behaviour. The blue curve for the score function for the exponential cost shows that its variance can behave in the opposite way, and can be lower for higher values of k. We highlight this because, in machine learning applications, we usually learn the cost function (by optimising its structural parameters), and face a setting akin to varying k in these graphs. Therefore the process of learning influences the variance behaviour of an estimator differently at different times over the course of learning, and is an aspect we will need to take steps to control. Figures 2 and 3 also demonstrate the importance of variance reduction. The score function estimator is commonly used with a control variate, a way to reduce the variance of the gradient that we explore further in Section 7. We see a large decrease in variance by employing this technique. The variance of the measure-valued derivative estimator in these plots is also shown with a form of variance reduction (known as coupling), and for these simple cost functions, there are regimes of the function that allow corrections that drive the variance to zero; we can see this where the kink in the plot for the variance of the mean-gradient for the cosine cost function. 9
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih Score function Score function + variance reduction Pathwise Measure-valued + variance reduction Value of the cost Derivative of the cost 100 10-1 10-2 10-3 0.1 1 10 k µ rof rotamitse eht fo ecnairaV exp 100 10-1 10-2 0.1 1 10 k 1 0 1 3 0 3 3 0 3 3 0 3 σ rof rotamitse eht fo ecnairaV 1 0 1 3 0 3 3 0 3 3 0 3 101 100 10-1 10-2 0.5 1 2 5 k µ rof rotamitse eht fo ecnairaV cos 101 100 10-1 0.5 1 2 5 k 3 0 3 3 0 3 3 0 3 3 0 3 σ rof rotamitse eht fo ecnairaV 3 0 3 3 0 3 3 0 3 3 0 3 Figure 3: Variance of the stochastic estimates of E [f (x; k)] for µ = σ = 1 as a function θ N (x|µ,σ2) ∇ of k. Top: f (x; k) = exp( kx2), bottom: f (x; k) = cos kx. Left: θ = µ; right: θ = σ. The graphs − in the bottom row show the function (solid) and its gradient (dashed): for k 0.1, 1, 10 for the ∈ { } exponential function, and k 0.5, 1.58, 5. for the cosine function. ∈ { } From this initial exploration, we find that there are several criteria to be judged when choosing an unbiased gradient estimator: computational cost, implications on the use of differentiable and non-differentiable cost functions, the change in behaviour as the cost itself changes, e.g., during learning, and the availability of effective variance reduction techniques to achieve low variance. We will revisit these figures again in subsequent sections as we develop the precise description of these methods. We will assess each estimator based on these criteria, working towards building a deeper understanding of them and their implications for theory and practice. 4. Score Function Gradient Estimators The score function estimator is in the class of derivatives of measure, and is one of the most general types of gradient estimators available to us. Because of its widespread and general-purpose nature, it appears under various names throughout the literature, including the score function estimator (Kleijnen and Rubinstein, 1996; Rubinstein et al., 1996), the likelihood ratio method (Glynn, 1990), and the REINFORCE estimator (Williams, 1992). This section will derive the estimator, expose 10
Monte Carlo Gradient Estimation in Machine Learning some of its underlying assumptions and possible generalisations, explore its behaviour in various settings, and briefly review its historical development. 4.1 Score Functions A score function is the derivative of the log of a probability distribution log p(x; θ) with respect θ ∇ to its distributional parameters, which using the rule for the derivative of the logarithm can be expanded as: p(x; θ) log p(x; θ) = ∇θ . (11) ∇θ p(x; θ) This identity is useful since it relates the derivative of a probability to the derivative of a log- probability; for Monte Carlo estimators it will allows us to rewrite integrals of gradients in terms of expectations under the measure p. It is the appearance of the score function in the gradient estimator we develop in this section that will explain its name. The score function is important since, amongst other uses, it is the key quantity in maximum likelihood estimation (Stigler et al., 2007). One property of the score that will later be useful is that its expectation is zero: (cid:90) p(x; θ) (cid:90) E [ log p(x; θ)] = p(x; θ) ∇θ dx = p(x; θ)dx = 1 = 0. (12) p(x;θ) ∇θ p(x; θ) ∇θ ∇θ We show this in (12) by first replacing the score using the ratio given by the identity (11), then cancelling terms, interchanging the order of differentiation and integration, and finally recognising that probability distributions must integrate to one, resulting in a derivative of zero. A second important property of the score is that its variance, known as the Fisher information, is an important quantity in establishing the Cramer-Rao lower bound (Lehmann and Casella, 2006). 4.2 Deriving the Estimator Using knowledge of the score function, we can now derive a general-purpose estimator for the sen- sitivity analysis problem (2); its derivation is uncomplicated and insightful: (cid:90) (cid:90) η = E [f (x)] = p(x; θ)f (x)dx = f (x) p(x; θ)dx (13a) θ p(x;θ) θ θ ∇ ∇ ∇ (cid:90) = p(x; θ)f (x) log p(x; θ)dx (13b) θ ∇ = E [f (x) log p(x; θ)] (13c) p(x;θ) θ ∇ N 1 (cid:88) η¯ = f (xˆ(n)) log p(xˆ(n); θ); xˆ(n) p(x; θ). (13d) N N ∇θ ∼ n=1 In the first line (13a), we expanded the definition of the expectation as an integral and then exchanged the order of the integral and the derivative; we discuss the validity of this operation in section 4.3.1. In (13b), we use the score identity (11) to replace the gradient of the probability by the product of the probability and the gradient of the log-probability. Finally, we obtain an expectation (13c), which is in the form we need—a product of a distribution we can easily sample from and a function we can evaluate—to provide a Monte Carlo estimator of the gradient in (13d). 11
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih Equation (13c) is the most basic form in which we can write this gradient. One simple modification replaces the cost function with a shifted version of it: η = E [(f (x) β) log p(x; θ)] , (14) p(x;θ) θ − ∇ where β is a constant that we will call a baseline. For any value of β, we still obtain an unbiased estimator because the additional term it introduces has zero expectation due to the property (12) of the score. This baseline-corrected form should be preferred to (13c), because, as we will see in Section 7, it allows for a simple but effective form of variance reduction. 4.3 Estimator Properties and Applicability By inspecting the form (13c), we can intuitively see that the score-function estimator relates the overall gradient to the gradient of the measure reweighted by the value of the cost function. This intuitiveness is why the score function estimator was one of the first and most widely-used estimators for sensitivity analysis. But there are several properties of the score-function gradient to consider that have a deep impact on its use in practice. 4.3.1 Unbiasedness When the interchange between differentiation and integration in (13a) is valid, we will obtain an unbiased estimator of the gradient (L’Ecuyer, 1995). Intuitively, since differentiation is a process of limits, the validity of the interchange will relate to the conditions for which it is possible to exchange limits and integrals, in such cases most often relying on the use of the dominated convergence theorem or the Leibniz integral rule (Flanders, 1973; Grimmett and Stirzaker, 2001). The interchange will be valid if the following conditions are satisfied: • The measure p(x; θ) is continuously differentiable in its parameters θ. • The product f (x)p(x; θ) is both integrable and differentiable for all parameters θ. • There exists an integrable function g(x) such that sup f (x) p(x; θ) g(x) x. θ (cid:107) ∇θ (cid:107)1 ≤ ∀ These assumptions usually hold in machine learning applications, since probability distributions, especially the ones that appear most often in machine learning applications, are smooth functions of their parameters. L’Ecuyer (1995) provides an in depth discussion on the validity of interchanging integration and differentiation, and also develops additional tools to check if they are satisfied. 4.3.2 Absolute Continuity An important behaviour of the score function estimator is exposed by rewriting it in one other way. Here, we consider a scalar distributional parameter θ and look at its derivatives using first principles: (cid:90) E [f (x)] = p(x; θ)f (x)dx (15a) θ p(x;θ) θ ∇ ∇ (cid:90) p(x; θ + h) p(x; θ) = lim − f (x)dx (15b) h→0 h (cid:90) p(x; θ + h) p(x; θ) = lim − f (x)dx (15c) h→0 h 12
Monte Carlo Gradient Estimation in Machine Learning 1 (cid:90) p(x; θ + h) p(x; θ) = lim p(x; θ) − f (x)dx (15d) h→0 h p(x; θ) 1 (cid:90) (cid:18) p(x; θ + h) (cid:19) = lim p(x; θ) 1 f (x)dx (15e) h→0 h p(x; θ) − = lim 1 (cid:0)E [ω(θ, h)f (x)] E [f (x)](cid:1) . (15f) h→0 h p(x;θ) − p(x;θ) In the first line (15a), we again exchange the order of integration and differentiation, which we established is safe in most use-cases. We then expand the derivative in terms of its limit definition in (15b), and we swap the order of the limit and the integral in (15c). We introduce an identity term in (15d) to allow us to later rewrite the expression as an expectation with respect to the distribution p(x; θ). Finally, we simplify the expression separating it in two terms (15e), denoting the importance weight ω(θ, h) := p(x;θ+h) and rewrite the gradient using the expectation notation (15f). It is this p(x;θ) final expression that exposes a hidden requirement of the score function estimator. The ratio ω(θ, h) in (15f) is similar to that which appears in importance sampling (Robert and Casella, 2013). Like importance sampling, the estimator makes an implicit assumption of absolute continuity, where we require p(x; θ + h) > 0 for all points where p(x; θ) > 0. Not all distributions of interest satisfy this property, and failures of absolute continuity can result in a biased gradient. As one instance, absolute continuity is violated when the parameter θ defines the support of the distribution, such as the uniform distribution [0, θ]. U Example (Bounded support). Consider the score-function estimator for a cost f (x) = x and distribution p(x; θ) = 1 1 , which is differentiable in θ when x (0, θ); the score s(x) = θ {0<x<θ} ∈ 1/θ. This is a popular example also used by Glasserman (2013) and Pflug (1996), amongst others. − Comparing the two gradients: (cid:18) (cid:12)θ(cid:19) True gradient: E [x] = x2 (cid:12) = 1 . (16a) ∇θ p(x;θ) ∇θ 2θ (cid:12) 0 2 Score-function gradient: E (cid:2) x −1 (cid:3) = θ/2 = 1 . (16b) p(x;θ) θ − θ − 2 In this example, because p(x; θ) is not absolutely continuous with respect to θ at the boundary of the support, the estimator fails to provide the correct gradient. (cid:50) 4.3.3 Estimator Variance From the intuitive analysis in Section 3, we compared the score function estimator to other gradient estimation techniques (which we explore in the subsequent sections), and see that even in those simple univariate settings, the variance of the score-function estimator can vary widely as the cost function varies (blue curves in Figures 2 and 3). Starting from the estimator (13d) and denoting the estimator mean as µ(θ) := E [η¯ ], we can write the variance of the score function estimator as: p(x;θ) N V [η¯ ] = E (cid:104)(cid:0) f (x) log p(x; θ)(cid:1)2(cid:105) µ(θ)2, (17) p(x;θ) N p(x;θ) θ ∇ − which writes out the definition of the variance and allows us to see that the parameter dimensionality, through its appearance in the score, is an important part of the estimator’s variance. The alternative form of the gradient we explored in equation (15f) provides another way to characterise the variance: V [η¯ ] = lim 1 E (cid:2) (ω(θ, h) 1)2f (x)2(cid:3) µ(θ)2, (18) p(x;θ) N h→0 h p(x;θ) − − 13
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih which, for a fixed h, exposes the dependency of the variance on the importance weight ω. Although we will not explore it further, we find it instructive to connect these variance expressions to the variance bound for the estimator given by the Hammersley-Chapman-Robbins bound (Lehmann and Casella, 2006, ch 2.5): 2 2 (µ(θ + h) µ(θ)) (µ(θ + h) µ(θ)) V [η¯ ] sup − = sup − , (19) p(x;θ) N ≥ h E (cid:104) p(x;θ+h) 1(cid:105)2 h E p(x;θ) [ω(θ, h) 1]2 p(x;θ) p(x;θ) − − which is a generalisation of the more widely-known Cramer-Rao bound and describes the minimal variance achievable by the estimator. Our understanding of the gradient variance can then be built by exploring three sources of variance: contributions from the implicit importance ratio ω(θ, h) that entered due to the need for absolute continuity, contributions from the dimensionality of the parameters, and contributions from the variance of the cost function. These are hard to characterise exactly for general cases, but we can develop an intuitive understanding by unpacking specific terms of the gradient variance. Variance from the importance ratio ω. The variance characterisation from either equa- tion (18) or (19) shows that the importance ratio ω(θ, h) directly affects the variance. The simplest way to see this effect is to consider the contributions to the variance using the form of the gradient in equation (15e), for a fixed h. The first term in that equation is the quadratic term: E (cid:2) (ω(θ, h) 1)2f (x)2(cid:3) , (20) p(x;θ) − where ω(θ, h) is the importance ratio we identified in equation (15e). We will only obtain finite variance gradients when the integral in (20) is finite, i.e. when the conditions for absolute con- tinuity are satisfied. We saw previously that failures of absolute continuity can lead to biased gradients. In practice, complete failure will be rare and we will instead face near -failures in maintaining absolute continuity, which as the above expression shows, will lead to an increase in the variance of the Monte Carlo estimator of the gradient. Variance due to input dimensionality. Assume for simplicity that the distribution fac- torises over the dimensions of x RD so that p(x; θ) = (cid:81) p(x ; θ), and again consider a scalar ∈ d d parameter θ. In expectation, the importance weights have the property that for any dimension (cid:104) (cid:105) D, (cid:81)D E p(xd;θ+h) = 1. The importance weight and its logarithm are given by ω(θ, h) = d=1 p(xd;θ) p(xd;θ) (cid:81)D ω (θ, h) = (cid:81)D p(xd;θ+h) and log ω(θ, h) = (cid:80)D log ω (θ, h) = (cid:80)D log p(xd;θ+h) , which d=1 d d=1 p(xd;θ) d=1 d d=1 p(xd;θ) we can use to study the behaviour of the importance weight as the dimensionality of x changes. If we follow an argument due to Glynn and Iglehart (1989) and Glasserman (2013, p. 259), and assume that the expectation of the log ratio is is bounded, i.e., E [log ω (θ, h)] < p(x;θ) d , then the using the strong law of large numbers we can find that this expectation con- ∞ verges to a constant, E [log ω (θ, h)] = c. Using Jensen’s inequality, we know that c p(x;θ) d (cid:104) (cid:105) ≤ log E (cid:81)D p(xd;θ+h) = 0, with equality only when p(x ; θ + h) = p(x ; θ), meaning c < 0. p(x;θ) d=1 p(xd;θ) d d As a result, the sum of many such terms has a limiting behaviour of: (cid:88) p(x d; θ + h) (cid:89) p(x d; θ + h) lim log = = lim = 0, (21) d→∞ p(x d; θ) −∞ ⇒ d→∞ p(x d; θ) d d where we reach the limit in the first term since it is a sum of negative terms, and the second expression is obtained by exponentiation. As the dimensionality increases, we find that the 14
Monte Carlo Gradient Estimation in Machine Learning 104 103 102 101 100 10-1 10-2 10-3 0 200 400 600 800 1000 Dimensionality D σ rof rotamitse eht fo ecnairaV Constant 105 104 103 102 101 100 10-1 10-2 10-3 10-4 σ 0.01 0.1 1.0 10-5 0 200 400 600 800 1000 Dimensionality D σ rof rotamitse eht fo ecnairaV Linear σ 0.01 0.1 1.0 Figure 4: Variance of the score function estimator for a normal random variable, with a constant cost function f (x) = 100 and linear cost f (x) = (cid:80) x , under a Gaussian measure (x 1 , σ2I ). The y-axis is the estimate of the average variance V[d d f (x)] across parameter dimenN sion| s2 . D σ ∇ importance weights converge to zero, while at the same time its expectation is one for all dimen- sions. Therefore, in high dimensions, the importance ratio can become highly skewed, taking large values with small probabilities and leading to high variance as a consequence. Variance from the cost. The cost function appears in all forms of the variance we wrote (17)– (19) and itself is a significant contributor to the estimator variance, since it is a multiplicative (cid:80) term in the gradient. For example, if the cost function is a sum of D terms, f (x) = d(x ), k d whose individual variance we assume is bounded, then the variance of the score-function estimator V[ log p(x; θ)f (x)] will be of order O(D2). Because the cost function is a black-box as far as θ ∇ the estimator is concerned, it will typically contain many elements that do not directly influence the parameters, and hence will not affect the gradient. But every extra term contributes to its variance: ideally, before multiplying the cost with the score function, we would eliminate the parts of it that have no influence on the parameter whose derivative we are computing. Alternatively, we can try to do this automatically by using the gradient of the function itself (but only if it is available) to remove these terms—this approach is explored in the next section. To support this intuition, we explore the effect of the cost function and measure on variance properties of the score-function estimator with an example. Figure 4 shows the estimator variance for the gradient E [f (x)], for a constant cost f (x) = 100 and a linear cost that sums the dimensi∇ onσ s N of(x f|0 (x.5 ),σ =2ID(cid:80)) x for x RD. For both cost functions, the expected value d d ∈ of the cost under the measure has no dependence on the scale parameter σ. As we expected, for the linear cost, the variance scales quadratically as the number of terms in the cost increases. This variance analysis is meant to expose the importance of understanding the variance in our esti- mators. Whether because of the influence of the structure of the cost function, or the dimensionality of the implied importance weights in our gradients, we will need to counterbalance their effects and remove excess variance by some method of variance reduction. The question of variance reduction will apply to every type of Monte Carlo gradient estimator, but the specific solutions that are used will differ due to the different assumptions and factors that affect the variance. We will study variance reduction in more detail in Section 7. 15
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih 4.3.4 Higher-order Gradients Higher derivatives are conceptually simple to compute using the score function estimator. The score of higher orders is defined as: p(x; θ) (k) p(x; θ) s(1)(x) = ∇θ = log p(x; θ); s(k)(x) = ∇θ (22) p(x; θ) ∇θ p(x; θ) where (k) represents the kth-order gradient operator. Using this definition, the higher-order score- ∇ function gradient estimator is: (cid:104) (cid:105) η(k) = (k)E [f (x)] = E f (x)s(k)(x) . (23) ∇θ p(x;θ) p(x;θ) 4.3.5 Computational Considerations We can express the score-function gradient estimator (for single parameter) in one other way, as: η = E [f (x)] = Cov[f (x), log p(x; θ)], (24) θ p(x;θ) θ ∇ ∇ Cov[f (x), log p(x; θ)]2 V [f (x)]V [ log p(x; θ)]. (25) θ p(x;θ) p(x;θ) θ ∇ ≤ ∇ The first identity shows that the score function gradient can be interpreted as a measure of covariance between the cost function and the score function (and is true because the expectation of the score is zero, which will remove the second term that the covariance would introduce) (Pflug, 1996, pp. 234). The second identity is the Cauchy-Schwartz inequality, which bounds the squared covariance. Computationally, this shows that the variance of the cost function is related to the magnitude and range of the gradient. A highly-variable cost function can result in highly-variable gradients, which is undesirable for optimisation. It is for this reason that we will often constrain the values of the cost function by normalising or bounding its value in some way, e.g., by clipping. The score-function gradient is considered to be general-purpose because it is computed using only the final value of the cost in its computation. It makes no assumptions about the internal structure of the cost function, and can therefore be used with any function whose outcome we can simulate; many functions are then open to us for use: known differentiable functions, discrete functions, dynamical systems, or black-box simulators of physical systems or graphics engines. Overall the computational cost of the score function estimator is low, and will be of the order (N (D + L)), O for D-dimensional distributional parameters θ, plus the additional cost L of evaluating the cost function, and multiplying by the number of samples N used in the estimator. Taking into account the exposition of this section, points for consideration when using the score function estimator are: • Any type of cost function can be used, allowing for the use of simulators and other black-box systems, as long as we are able to evaluate them easily. • The measure must be differentiable with respect to its parameters, an assumption we make throughout this paper. • We must be able to easily sample from the measure, since the gradient estimator is computed using samples from it. • The estimator can be implemented using only a single sample if needed, i.e. using N = 1. Single-sample estimation is applicable in both univariate and multivariate cases, making the estimator computationally efficient, since we can deliberately control the number of times that the cost function is evaluated. 16
Monte Carlo Gradient Estimation in Machine Learning • Because there are many factors that affect the variance of the gradient estimator, it will be important to use some form of variance reduction to obtain competitive performance. 4.4 Research in Score Function Gradient Estimation We are the lucky inheritors of a rich body of work specifically devoted to the score function gradient estimator. Given its simplicity and generality, this estimator has found its way to many parts of computational science, especially within operations research, computational finance and machine learning. We describe a subset of this existing work, focusing on the papers that provide deeper theoretical insight and further context on the score-function estimator’s use in practice. Development of the estimator. The score function estimator is one of the first types of estima- tors to be derived, initially by several different groups in the 1960s, including Miller (1967) in the design of stable nuclear reactors, and by Rubinstein (1969) in the early development of Monte Carlo optimisation. Later the estimator was derived and applied by Rubinstein and Kreimer (1983) for discrete-event systems in operations research, where they began to refer to the estimator as the score function method, the name we use in this paper. The estimator was again developed by Glynn (1987, 1990) and by Reiman and Weiss (1989), where they referred to it as the likelihood ratio method, and part of the study of queueing systems and regenerative stochastic processes. Concise descriptions have entered several books, such as those by Rubinstein and Shapiro (1993) and Fu and Hu (2012), and in two in books we especially recommend by Pflug (1996, sect. 4.2.1) and Glasserman (2013, sect 7.3). Interchange of integration and differentiation. The two basic questions for the application of the score-function estimator are differentiability of the stochastic system that is being studied, and subsequently, the validity of the interchange of integration and differentiation. For many simple systems, differentiability is safely assumed, but in discrete event systems that are often studied in operations research, some effort is needed to establish differentiability, e.g., like that of the stochastic recursions by Glynn and L’Ecuyer (1995). The validity of the interchange of differentiation and inte- gration for score function estimators is discussed by Kleijnen and Rubinstein (1996) and specifically in the note by L’Ecuyer (1995). In machine learning. In supervised learning the score-function gradient estimator was de- veloped as the REINFORCE algorithm by Williams (1992). In such settings, the gradient is the fundamental quantity needed for policy search methods and is the basis of the policy gradient theo- rem and the subsequent development of actor-critic supervised learning methods (Sutton et al., 2000). Approximate Bayesian inference methods based on variational inference deployed the score function gradient estimator to enable a more general-purpose, black-box variational inference; one that did not require tedious manual derivation, but that could instead be more easily combined with automatic differentiation tools. The appeal and success of this approach has been shown by several authors, including Paisley et al. (2012), Wingate and Weber (2013), Ranganath et al. (2014) and Mnih and Gregor (2014). Variance reduction is essential for effective use of this estimator, and has been explored in several areas, by Greensmith et al. (2004) in supervised learning, Titsias and L´azaro-Gredilla (2015) in variational inference, Capriotti (2008) in computational finance, or more recently by Walder et al. (2019). We describe variance reduction techniques in more detail in Section 7. Finally, as machine learning has sought to automate the computation of these gradients, the implementation of the score function estimator within wider stochastic computational graphs 17
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih has been explored for the standard estimator (Schulman et al., 2015) and its higher-order gradients (Foerster et al., 2018); we will provide more discussion on computational graphs in Section 9. 5. Pathwise Gradient Estimators We can develop a very different type of gradient estimator if, instead of using only knowledge of the score function, we can know more of the structural characteristics of the problem (2). One such structural property is the specific sequence of transformations and operations that sources of randomness take as they pass through the measure and into the cost function, to affect the overall objective. Using this sampling path will lead us to a second estimator, the pathwise gradient estimator, which as its name implies, is in the class of derivatives of paths. Because we need information about the path underlying a probabilistic objective, this class of gradient estimator will be less general-purpose than the score-function estimator, but in losing this generality, we will gain several advantages, especially in terms of lower variance and ease of implementation. The pathwise derivative is as fundamental to sensitivity analysis and stochastic optimisation as the score function estimator is, and hence also appears under several names, including: the pro- cess derivative (Pflug, 1996), as the general area of perturbation analysis and specifically infinites- imal perturbation analysis (Glasserman and Ho, 1991; Ho and Cao, 2012), the pathwise derivative (Glasserman, 2013), and more recently as stochastic meta-optimization synthesis and the reparameterisation trick (Kingma and Welling, 2014b; Rezende et al., 2014; Titsias and L´azaro-Gredilla, 2014). An- other way to view the pathwise approach is as a process of pushing the parameters of interest, which are part of the measure, into the cost function, and then differentiating the newly modified cost function. For this reason, the pathwise estimator is also called the ‘push-in’ gradient method (Rubinstein, 1992). 5.1 Sampling Paths Continuous distributions have a simulation property that allows both a direct and an indirect way of drawing samples from them, making the following sampling processes equivalent: xˆ p(x; θ) xˆ = g(ˆ(cid:15), θ), ˆ(cid:15) p((cid:15)), (26) ∼ ≡ ∼ and states that an alternative way to generate samples xˆ from the distribution p(x; θ) is to sample first from a simpler base distribution p((cid:15)), which is independent of the parameters θ, and to then transform this variate through a deterministic path g((cid:15); θ); we can refer to this procedure as either a sampling path or sampling process. For invertible paths, this transformation is described by the rule for the change of variables for probability: p(x; θ) = p((cid:15)) g((cid:15); θ) −1 . (27) (cid:15) |∇ | There are several classes of transformation methods available (Devroye, 2006): • Inversion methods. For univariate distributions, we will always be able to find an equivalent base distribution and sampling path by using the uniform distribution and inverse cumulative distribution function (CDF), respectively. This method can often be difficult to use directly, however, since computing the inverse of the CDF and its derivative can be computationally difficult; restricting this approach to univariate settings. We will return to this method later, but instead explore methods that allow us to use the CDF instead of its inverse. 18
Monte Carlo Gradient Estimation in Machine Learning • Polar transformations. It is sometimes possible, and more efficient, to generate a pair of random variates (y, z) from the target distribution p(x). We can map this pair to a represen- tation in polar form (r cos θ, r sin θ), which exposes other mechanisms for sampling, e.g., the famous Box-Muller transform for sampling Gaussian variates is derived in this way. • One-liners. In many cases there are simple functions that transform a base distribution into a richer form. One widely-known example is sampling from the multivariate Gaussian p(x; θ) = (x µ, Σ), by first sampling from the standard Gaussian p((cid:15)) = (0, I), and then N | N applying the location-scale transformation g((cid:15), θ) = µ + L(cid:15), with LL(cid:62) = Σ. Many such transformations exist for the most common distributions, including the Dirichlet, Gamma, and Exponential. Devroye (1996) refers to these types of transformations as one-liners because they can often be implemented in one line of code. With knowledge of these transformation methods, we can invoke the Law of the Unconscious Statis- tician (LOTUS) (Grimmett and Stirzaker, 2001): E [f (x)] = E [f (g((cid:15); θ))] , (28) p(x;θ) p((cid:15)) which states that we can compute the expectation of a function of a random variable x without knowing its distribution, if we know its corresponding sampling path and base distribution. LOTUS tells us that in probabilistic objectives, we can simply replace expectations over any random variables x wherever they appear, by the transformation g((cid:15); θ) and expectations over the base distribution p((cid:15)). This is a way to reparameterise a probabilistic system; it is often used in Monte Carlo methods, where it is referred to as the non-centred parameterisation (Papaspiliopoulos et al., 2007) or as a reparameterisation trick (Kingma and Welling, 2014b). 5.2 Deriving the Estimator Equipped with the pathwise simulation property of continuous distributions and LOTUS, we can derive an alternative estimator for the sensitivity analysis problem (2) that exploits this additional knowledge of continuous distributions. Assume that we have a distribution p(x; θ) with known differentiable and invertible sampling path g((cid:15); θ) and base distribution p((cid:15)). The sensitivity analysis problem (2) can then be reformulated as: (cid:90) η = E [f (x)] = p(x; θ)f (x)dx (29a) θ p(x;θ) θ ∇ ∇ (cid:90) = p((cid:15))f (g((cid:15); θ))d(cid:15) (29b) θ ∇ = E [ f (g((cid:15); θ))] (29c) p((cid:15)) θ ∇ N 1 (cid:88) η¯ = f (g(ˆ(cid:15)(n); θ)); ˆ(cid:15)(n) p((cid:15)). (29d) N N ∇θ ∼ n=1 In equation (29a) we first expand the definition of the expectation. Then, using the law of the unconscious statistician, and knowledge of the sampling path g and the base distribution for p(x; θ), we reparameterise this integral (29b) as one over the variable (cid:15). The parameters θ have now been pushed into the function making the expectation free of the parameters. This allows us to, without concern, interchange the derivative and the integral (29c), resulting in the pathwise gradient estimator (29d). 19
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih 5.3 Estimator Properties and Applicability The gradient (29c) shows that we can compute gradients of expectations by first pushing the pa- rameters into the cost function and then using standard differentiation, applying the chain rule. This is a natural way to think about these gradients, since it aligns with our usual understanding of how deterministic gradients are computed, and is why this approach is so popular. Its simplicity, however, belies several distinct properties that impact its use in practice. 5.3.1 Decoupling Sampling and Gradient Computation The pathwise estimator (29c), as we derived it, is limited to those distributions for which we si- multaneously have a differentiable path, and use this same path to generate samples. We could not compute gradients with respect to parameters of a Gamma distribution in this way, because sam- pling from a Gamma distribution involves rejection sampling that does not provide a differentiable path. The process of sampling from a distribution and the process of computing gradients with respect to its parameters are coupled in the estimator (29c); we can expand the applicability of the pathwise gradient by decoupling these two processes. The pathwise estimator can be rewritten in a more general form as: (cid:90) η = ∇θE p(x;θ) [f (x)] = E p((cid:15)) [ ∇θf (x)](cid:12) (cid:12) x=g((cid:15);θ) = p((cid:15)) ∇xf (x) ∇θxd(cid:15) (30a) (cid:90) = f (x) x p(x; θ)dx = E [ f (x) x] . (30b) x θ p(x;θ) x θ ∇ ∇ ∇ ∇ In the first line (30a), we recall the gradient that was formed as a result of applying the law of the unconscious statistician in equation (29c), and then apply the chain rule to write the gradient w.r.t. θ. Using the LOTUS (28) again, this time for the expectation of f (x) = f (x) x and in the θ x θ ∇ ∇ ∇ reverse direction, we obtain (30b). This derivation shows that sampling and gradient estimation can be decoupled: we can generate samples using any sampler for the original distribution p(x; θ) (e.g., using a sampling path, rejection sampling or Markov chain Monte Carlo), and compute the gradient using the chain rule on the function by finding some way to compute the term x. θ ∇ One way to compute x is to use g((cid:15); θ) as we did for (29c). In practice, this form is not always θ θ ∇ ∇ convenient. For example, if the sampling path is a inverse cumulative density function (CDF), which is often computed with root-finding methods, then evaluating its derivative may require numerically unstable finite difference methods. Instead, we can find another way of writing x that makes θ ∇ use of the inverse of the path g−1(x; θ). We can think of g−1(x; θ) as the ‘standardisation path’ of the random variable—that is the transformation that removes the dependence of the sample on the distribution parameters, standardising it to a zero mean unit variance-like form. In the univariate case, instead of using the inverse CDF, we can use the standardisation path given by the CDF. Consider the equation (cid:15) = g−1(x; θ) as an implicit function for x. Evaluating the total derivative (TD) on both sides—using implicit differentiation—and expanding we find that: (cid:15) = g−1(x; θ) = TD(cid:15) = TDg−1(x; θ) (31a) ⇒ ∇θ ∇θ ∴ 0 = g−1(x; θ) x + g−1(x; θ) (31b) x θ θ ∇ ∇ ∇ x = ( g−1(x; θ))−1 g−1(x; θ). (31c) θ x θ ∇ − ∇ ∇ Equation (31c) gives us the expression needed to fully evaluate (30b). Equation (31c) is the form in which Ho and Cao (1983) initially introduced the estimator, and it corresponds to the strategy of 20
Monte Carlo Gradient Estimation in Machine Learning differentiating both sides of the transformation noted by Glasserman (2013). Figurnov et al. (2018) refer to this approach to as an implicit reparameterisation gradient, because of its use of implicit differentiation. In this form, we are now able to apply pathwise gradient estimation to a far wider set of distributions and paths, such as for the Beta, Gamma or Dirichlet distributions. In Section 8 we look at this decoupling in other settings, and in the discussion (Section 9.4) look at optimal transport as another way of computing x. θ ∇ Example (Univariate Gaussian). For univariate Gaussian distributions (x µ, σ2), with θ = N | µ, σ , the location-scale transform is the natural choice of path: x = g((cid:15); θ) = µ+σ(cid:15) for (cid:15) (0, 1). { The i} nverse path is then (cid:15) = g−1(x, θ) = x−µ . The standard pathwise derivatives are d∼ x =N 1 and σ dµ dx = (cid:15). Equation (31c) is then: dσ dx ∂g−1(x, θ)/∂µ 1/σ dx ∂g−1(x, θ)/∂σ (x − µ)/σ2 x µ = = − = 1; = = − = − = (cid:15). (32) dµ − ∂g−1(x, θ)/∂x − 1/σ dσ − ∂g−1(x, θ)/∂x − 1/σ σ We see that the two approaches provide the same gradient. (cid:50) Example (Univariate distributions). As we discussed, for univariate distributions p(x; θ) we can use the sampling path given by the inverse CDF: x = g((cid:15); θ) = F −1(x; θ), where (cid:15) [0, 1]. ∼ U Computing the derivative x = F −1(x; θ) is often complicated and expensive. We can obtain an θ θ ∇ ∇ alternative expression for x by considering the inverse path, which is given the CDF g−1(x; θ) = θ ∇ F (x; θ). From equation (31c) we have: F (x; θ) F (x; θ) θ θ x = ∇ = ∇ . (33) ∇θ − F (x; θ) − p(x; θ) x ∇ In the final step, we used the fact that the derivative of the CDF w.r.t. x is the density function. This expression allows efficient computation of pathwise gradients for a wide range of continuous univariate distributions, including Gamma, Beta, von Mises, Student’s t-distribution and univariate mixtures (Figurnov et al., 2018; Jankowiak and Obermeyer, 2018). (cid:50) 5.3.2 Bias and Variance Properties In deriving the pathwise estimator (29d) we again exploited an interchange of differentiation and integration. If this interchange is valid, then the resulting application of the chain rule to compute the gradient will be valid, and the resulting estimator will be unbiased. We can always ensure that this interchange applies by ensuring that the cost functions we use are differentiable. The implication of this is that if we have discontinuous cost functions, then we will be unable to use the pathwise gradient estimator. We can be more rigorous about the conditions for unbiasedness, and we defer to Glasserman (2013, sect. 7.2.2) for this more in-depth discussion. We can build an understanding of the variance of the estimator by considering the case of gradients (2) with Gaussian measures p((cid:15)) = ((cid:15) 0, I). If we denote the gradient (29d) as E [h((cid:15))] using p((cid:15)) N | h((cid:15)) := f (g((cid:15); θ)), then we can bound the variance of the Gaussian pathwise derivative as: θ ∇ V [h((cid:15))] = E (cid:104)(cid:0) h((cid:15)) E [h((cid:15))](cid:1)2(cid:105) κ2π2 , (34) p((cid:15)) p((cid:15)) − p((cid:15)) ≤ 4 where κ is the Lipschitz constant of the function h; and this type of expression can be reached in sev- eral ways. The approach used here characterises the zero-mean random variable (h((cid:15)) E [h((cid:15))]) p((cid:15)) − as a sub-Gaussian variable, and invokes the variance-bound property that is characteristic of sub- Gaussian random variables (Fan et al., 2015, sect. 10). Alternatively, the variance properties of 21
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih the pathwise gradient are asymptotically the same as that for finite difference methods, and we can bound the variation in the finite difference by the squared Lipschitz constant (Glasserman, 2013, sect. 7.2.2). Although this specific result is derived using a Gaussian assumption its conclusions apply to the more general situations where the gradient will be used. However we obtain this expression (34), several key features of the variance are revealed by it. The variance bound is independent of the dimensionality of the parameter space, meaning that we can expect to get low-variance gradient estimates, even in high-dimensional settings. Because we are able to differentiate through the cost function itself, only the specific path through which any individual parameter influences the cost function is included in the gradient; unlike the score- function estimator, the gradient does not sum over large numbers of terms for which the parameter has no effect, allowing the estimator variance to be much lower. Importantly, the variance is directly bounded by the Lipschitz constant. As Figures 2 and 3 show, as the cost function becomes highly-variable, i.e. its Lipschitz constant increases, we enter regimes where, even with the elementary functions we considered, the variance of the pathwise estimator can be higher than that of the score-function method. The pathwise gradient, when it is applicable, will not always have lower variance when compared to other methods. In such situations, variance reduction will again be a powerful accompaniment to the pathwise estimator. Since most of the functions we will work with will not be Lipschitz continuous, Xu et al. (2018) use a different set of simplifying assumptions to develop an understanding of the variance that reinforces the general intuition built here. 5.3.3 Higher-order Gradients Computation of higher-order gradients using the pathwise method is also possible and directly involves higher-order differentiation of the function, requiring cost functions that are differentiable at higher-orders. This approach has been explored by Fu and Hu (1993) and Fan et al. (2015). Another common strategy for computing higher-order gradients is to compute the first-order gradient using the pathwise methods, and the higher-order gradients using the score-function method. 5.3.4 Computational Considerations The pathwise gradient estimator is restricted to differentiable cost functions. This is still a large class of functions, yet is not as general as the scope of applicability of the score-function estimator. For some types of discontinuous functions it is possible to smooth the function over the discontinuity and maintain the correctness of the gradient; this approach is often referred to as smoothed perturbation analysis in the existing literature (Glasserman and Ho, 1991). There are often several competing approaches for computing the gradient. The choice between them will need to be made considering the associated computational costs and ease of implementation. The case of the univariate Gaussian measure (x µ, σ2) highlights this. This distribution has two N | equivalent sampling paths: one given by the location-scale transform: x = µ + σ(cid:15) , (cid:15) (0, 1) 1 1 ∼ N and another given by the inverse CDF transform: x = F −1((cid:15) ; µ, σ), (cid:15) [0, 1]. While there is 2 2 ∼ U no theoretical difference between the two paths, the first path is preferred in practice due to the simplicity of implementation. 22
Monte Carlo Gradient Estimation in Machine Learning The same analysis that allowed us to characterise the variance of the Gaussian gradient (34) provides us with a tail bound to analyse the convergence of the Monte Carlo estimator (Fan et al., 2015). We find that the estimator probability is: (cid:32)(cid:12) (cid:12) 1 (cid:88)N (cid:12) (cid:12) (cid:33) (cid:18) 2N t2 (cid:19) P (cid:12) h((cid:15)(n)) E [h((cid:15))](cid:12) > t 2 exp , (35) (cid:12) (cid:12) N − p((cid:15)) (cid:12) (cid:12) ≤ − π2κ2 n=1 where N is the number of Monte Carlo samples used, and κ is the Lipschitz constant of the function h. We can obtain rapid convergence even when using only a single sample to compute the gradient, as is often done in practice. But this will need to be balanced against the appearance of the Lipschitz constant, where a larger κ may require more samples to be used. This consideration is why we will find that regularisation that promotes smoothness of the functions we learn is an important part of successful applications. Overall, the computational cost of the pathwise estimator is the same as the score function estimator and is low, of the order (N (D + L)), for D-dimensional distributional O parameters θ, plus the additional cost L of evaluating the cost function and its gradient, and multiplying by the number of samples N used in the estimator. Taking into account the exposition of this section, points for consideration when using the pathwise derivative estimator are: • Only cost functions that are differentiable can be used. • When using the pathwise estimator (29c), we do not need to know the measure explicitly. Instead we must know its corresponding deterministic and differentiable sampling path and a base distribution that is easy to sample from. • If using the implicit form of the estimator, we require a method of sampling from the original distribution as well as a way of computing the derivative of the inverse path (standardisation) transformation. • The estimator can be implemented using only a single sample if needed, i.e. using N = 1. Single-sample estimation is applicable in both univariate and multivariate cases, making the estimator computationally efficient, since we can deliberately control the number of times that the cost function is evaluated. • We might need to control the smoothness of the function during learning to avoid large variance, and may need to employ variance reduction. 5.4 Research in Pathwise Derivative Estimation Basic development. This pathwise estimator was initially developed by Ho and Cao (1983) under the name of infinitesimal perturbation analysis (IPA), by which it is still commonly known. Its convergence properties were analysed by Heidelberger et al. (1988). Other variations of perturbation analysis expand its applicability, see Suri and Zazanis (1988) and the books by Glasserman and Ho (1991) and Ho and Cao (2012). The view as a push-in technique is expanded on by Rubinstein (1992). Again the books by Glasserman (2013, sect. 7.2) and Pflug (1996, sect. 4.2.3) provide a patient exploration of this method using the names of the pathwise derivative, and process derivative estimation, respectively. Appearance in machine learning. The pathwise approach to gradient estimation has seen wide application in machine learning, driven by work in variational inference. An early instance of gradients of Gaussian expectations in machine learning was developed by Opper and Archambeau (2009). Rezende et al. (2014) called their use of the pathwise estimator stochastic meta-optimization synthesis, 23
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih since the gradient reduced to the standard gradient computation by meta-optimization synthesis averaged over an independent source of noise. As we described in Section 2.2, Titsias and L´azaro-Gredilla (2014) used the pathwise estimator under the umbrella of doubly stochastic optimisation, to recognise that there are two distinct sources of stochasticity that enter when using the pathwise estimator with minibatch optimisation. The substitution of the random variable by the path in the estimator (29d) led Kingma and Welling (2014a,b) to refer to their use of the substitution as a reparametrisation trick, and by which it is commonly referred to at present in machine learning. Use in generative models and supervised learning. Recognising that the pathwise gra- dients do not require knowledge of the final density, but only a sampling path and base distribution, Rezende and Mohamed (2015) developed normalising flows for variational inference, which allows learning of complex probability distributions that exploit this property of the pathwise estimator. The optimisation of model parameters in implicit generative models, such those in generative ad- versarial networks (Goodfellow et al., 2014; Mohamed and Lakshminarayanan, 2016), relies on the use of the pathwise gradient estimation. To learn behaviour policies in continuous action spaces, the pathwise estimator has also found numerous uses in supervised learning for continuous control. Williams (1992, sect. 7.2) invoked this approach as an alternative to the score function when using Gaussian distributions, and has also been used for learning value gradients by Heess et al. (2015) and Lillicrap et al. (2015). We find that the pathwise derivative is now a key tool for comput- ing information-theoretic quantities like the mutual information (Alemi et al., 2017), in Bayesian optimisation (Wilson et al., 2018) and in probabilistic programming (Ritchie et al., 2016). Derivative generalisations. Gong and Ho (1987) introduced smoothed perturbation analysis as one variant of the pathwise derivative to address cases where the interchange of differentiation and integration is not applicable, such as when there are discontinuities in the cost function; and several other extensions of the estimator appear in the perturbation analysis literature (Glasserman and Ho, 1991). In the variational inference setting, Lee et al. (2018) also look an the non-differentiable case by splitting regions in to differentiable and non-differentiable components. The implicit reparam- eterisation gradients for univariate distributions were developed by Salimans and Knowles (2013). Hoffman and Blei (2015) used the implicit gradients to perform meta-optimization synthesis through the Gamma distribution using a finite difference approximation of the CDF derivative. Graves (2016) derives the implicit reparameterisation gradients for multivariate distributions with analytically tractable CDFs, such as mixtures. The form of implicit reparameterisation that we described here was devel- oped by Figurnov et al. (2018) and clarifies the connections to existing work and provides further practical insight. Other generalisations like that of Ruiz et al. (2016), Naesseth et al. (2017) and Parmas et al. (2018), combine the score function method with the pathwise methods; we come back to these hybrid methods in Section 8. 6. Measure-valued Gradients A third class of gradient estimators, which falls within the class of derivatives of measure, is known simply as the measure-valued gradient estimators, and has received little attention in machine learn- ing. By exploiting the underlying measure-theoretic properties of the probabilities in the sensitivity analysis problem (2)—the properties of signed-measures in particular—we will obtain a class of unbiased and general-purpose gradient estimators with favourable variance properties. This ap- proach is referred to interchangeably as either the weak derivative method (Pflug, 1996) or as the measure-valued derivative (Heidergott and V´azquez-Abad, 2000). 24
Monte Carlo Gradient Estimation in Machine Learning Table 1: Weak derivative triples (c , p+, p−) of common distributions; we use for the Gaussian θ N density, for the Weibull, for the Gamma, for the exponential, r for the Erlang, the W G E E M double-sided Maxwell, and for the Poisson. See appendix for the forms of these distributions. P Distribution p (x) Constant c Positive part p+(x) Negative part p−(x) θ θ Bernoulli(θ) 1 δ δ 1 0 Poisson(θ) 1 (θ) + 1 (θ) √ P P Normal(θ, σ) 1/σ 2π θ + σ (2, 0.5) θ σ (2, 0.5) W − W Normal(µ, θ2) 1/θ (µ, θ2) (µ, θ2) M N Exponential(θ) 1/θ (θ) θ−1 r(2) E E Gamma(a, θ) a/θ (a, θ) (a + 1, θ) G G Weibull(α, θ) 1/θ (α, θ) (2, θ)1/α W G 6.1 Weak Derivatives The derivative of a density p(x; θ) is itself not a density, since it may have negative values and θ ∇ does not integrate to one. However, we can always decompose this derivative into a difference of two densities multiplied by a constant (Pflug, 1989): p(x; θ) = c+p+(x; θ) c−p−(x; θ), (36) ∇θ θ − θ where p+, p− are densities, referred to as the positive and negative components of p, respectively. By integrating both sides of (36), we can see that c+ = c−: θ θ (cid:90) (cid:90) (cid:90) LHS: p(x; θ)dx = p(x; θ)dx = 0; RHS: (cid:0) c+p+(x; θ) c−p−(x; θ)(cid:1) dx = c+ c−. (37) ∇θ ∇θ θ − θ θ − θ Therefore, we can simply use one constant that we denote c , and the decomposition becomes: θ p(x; θ) = c (cid:0) p+(x; θ) p−(x; θ)(cid:1) . (38) θ θ ∇ − The triple (c , p+, p−) is referred to as the weak derivative of p(x; θ). We restrict our definition θ here to the univariate parameter case; the multivariate parameter case extends this definition to form a vector of triples, with one triple for each dimension. We list the weak derivatives for several widely-used distributions in Table 1, and defer to Heidergott et al. (2003) for their derivations. The derivative is weak because we do not require the density p to be differentiable on its domain, but rather require that integrals of the decomposition p+, p− against sets of test functions converge. For example, the mass functions of discrete distributions results in positive and negative components that are delta functions, which are integrable against continuous functions. The weak derivative is not unique, but always exists and can be obtained using Hahn-Jordan decomposition of a signed measure into two measures that have complementary support (Billingsley, 2008). Like the score function and the sampling path, the weak derivative is a tool we will use to study the sensitivity analysis problem (2), although it has uses in other settings. The connections between the weak derivative and functional analysis, as well as a general calculus for weak differentiability is described by Heidergott and Leahu (2010). 6.2 Deriving the Estimator Using our knowledge of weak derivatives of probability measures, we can now derive a third es- timator for the gradient problem (2). This will lead us to an unbiased gradient estimator, which 25
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih intuitively computes the gradient using a weighted difference of two expectations. For D-dimensional parameters θ, we can rewrite the gradient for the ith parameter θ as: i (cid:90) (cid:90) η = E [f (x)] = p(x; θ)f (x)dx = p(x; θ)f (x)dx (39a) i ∇θi p(x;θ) ∇θi ∇θi (cid:18)(cid:90) (cid:90) (cid:19) = c f (x)p+(x; θ)dx f (x)p−(x; θ)dx (39b) θi i − i (cid:16) (cid:17) = c E [f (x)] E [f (x)] (39c) θi p+ i (x;θ) − p− i (x;θ) (cid:32) (cid:33) N N η¯ = c θi (cid:88) f (x˙ (s)) (cid:88) f (x¨(s)) ; x˙ (s) p+(x; θ), x¨(s) p−(x; θ). (39d) i N − ∼ i ∼ i n=1 n=1 In the first line (39a) we expanded the expectation as an integral and then interchanged the order of differentiation and integration. In the second line (39b), we substituted the density-derivative with its weak derivative representation (38). We use the symbols p+, p− to remind ourselves that the i i positive and negative components may be different depending on which parameter of the measure the derivative is taken with respect to, and c to indicate that the constant will also change depending θi on the parameter being differentiated. We write the gradient using the expectation operators we have used throughout the paper (39c), and finally obtain an estimator in equation (39d). The triple (cid:0) c , p+(x; θ), p−(x; θ)(cid:1) is the measure-valued gradient of (2) (Heidergott et al., 2003; θi i i Pflug, 1989). We now have a third unbiased estimator of the gradient that applies to any type of cost functions f , differentiable or not, since no knowledge of the function is used other than our ability to evaluate it for different inputs. Example (Bernoulli measure-valued gradient). We can illustrate the generality of the measure- valued derivatives by considering the case where p(x) is the Bernoulli distribution. (cid:90) p(x; θ)f (x)dx = (θf (1) + (1 θ)f (0)) = f (1) f (0). (40) θ θ ∇ ∇ − − The weak derivative is given by the triple (1, δ , δ ), where δ denotes the Dirac measure. (cid:50) 1 0 x Example (Gaussian measure-valued gradient). We now derive the measure-valued derivative for the probabilistic objective (2) for Gaussian measures; we will do this from first principles, rather than relying on the result in Table 1. We will make use of the change of variables x = σy µ, with − differentials σdy = dx in this derivation. (cid:90) η = E [f (x)] = (x; µ, σ)f (x)dx (41a) µ µ N (x;µ,σ) µ ∇ ∇ N = (cid:90) f (x) √1 1 exp (cid:16) 1 (cid:0) x−µ (cid:1)2(cid:17) (cid:0)1 x−µ 1 µ−x (cid:1) dx (41b) σ 2π σ − 2 σ {x≥µ} σ − {x<µ} σ = √1 (cid:18)(cid:90) ∞ f (x) 1 exp (cid:16) 1 (cid:0) x−µ (cid:1)2(cid:17) x−µ dx (cid:90) µ f (x) 1 exp (cid:16) 1 (cid:0) µ−x (cid:1)2(cid:17) µ−x dx(cid:19) (41c) σ 2π σ − 2 σ σ − σ − 2 σ σ µ −∞ = √1 (cid:18)(cid:90) ∞ f (x) 1 exp (cid:16) 1 (cid:0) x−µ (cid:1)2(cid:17) x−µ dx (cid:90) ∞ f ( x) 1 exp (cid:16) 1 (cid:0) µ+x (cid:1)2(cid:17) µ+x dx(cid:19) (41d) σ 2π σ − 2 σ σ − − σ − 2 σ σ µ −µ (cid:18)(cid:90) ∞ (cid:16) (cid:17) (cid:90) ∞ (cid:16) (cid:17) (cid:19) = √1 f (µ + σy)y exp y2 dy f (µ σy)y exp y2 dy (41e) σ 2π − 2 − − − 2 0 0 = √1 (cid:0)E [f (µ + σy)] E [f (µ σy)](cid:1) . (41f) σ 2π W(y|2,0.5) − W(y|2,0.5) − 26
Monte Carlo Gradient Estimation in Machine Learning In the first line we exchange the order of integration and differentiation, and in the second line (41b) differentiate the Gaussian with respect to its mean µ. The derivative is written in a form that decomposes the integral around the mean µ, and in this way introduces the minus sign needed for the form of the weak derivative. In the third line, we split the integral over the domains implied by the decomposition. The fourth line changes the limits of the second integral to the domain of ( µ, ), which is used in the next line (41e) along with the change of variables to rewrite the − ∞ integral more compactly in terms of y. In the final line (41f) we recognise the Weibull distribution with zero-shift as the new measure under which the integral is evaluated and write the expression using the expectation notation we use throughout the paper. We give the form of the Weibull and other distributions in the appendix. Equation (41f) gives the measure-valued derivative as the difference of two expectations under the Weibull distribution weighted by a constant, and can be described by the triple and estimator: (cid:16) (cid:17) √1 , µ + σ (2, 1 ), µ σ (2, 1 ) ; σ 2π W 2 − W 2 η¯ = √1 (cid:80)N (f (µ + σy˙) f (µ σy¨)) ; y˙ (2, 1 ), y¨ (2, 1 ). (42) N N 2πσ n=1 − − ∼ W 2 ∼ W 2 In equation (42) the samples used for positive and negative parts are independent. A simple vari- ance reduction is possible by coupling the two Weibull distributions with a set of common random numbers; we explore variance reduction by coupling in more detail later. (cid:50) While we have restricted ourselves to scalar quantities in these derivations, the same properties extend to the vector case, where the measure-valued derivative is a vector of triples; as Pflug (1996) says, ‘the vector case is more complicated in notation, but not in concept’. In equation (39a), if the (cid:81) measure is a factorised distribution p(x; θ) = p(x θ ) then the positive and negative components d d | d of the weak derivative will itself factorise across dimensions. For the positive component, this decomposition will be p+(x; θ) = p(x )p+(x ; θ ), which is the product of the original density for i ¬i i i i all dimensions except the ith dimension, and the positive density decomposition; the same holds for the negative component. For the derivative with respect to the mean when the measure is a diagonal Gaussian in (41a), this decomposition is the product of a Weibull distribution for the ith component, and a Gaussian distribution for all other dimensions, which are easy to sample from when forming the estimator. Full-covariance Gaussian measures will follow a similar decomposition, sampling from the multivariate Gaussian and replacing the ith component with a Weibull variate. 6.3 Estimator Properties and Applicability 6.3.1 Domination The two derivatives-of-measure—the score function and the measure-valued derivative—differ in the ways that they establish the correctness of the interchange of differentiation and integration. For the score function estimator, we achieved this by invoking the dominated convergence theorem and assuming the gradient estimator is dominated (bounded above) by a constant. We explored one example where we were unable to ensure domination (using the of example of bounded support in Section 4.3), because no bounding constant applies at the boundaries of the domain. For the weak derivative, the correctness of the interchange of differentiation and integration is guaranteed by its definition: the fundamental property of weak derivatives states that if the triple (c, p+, p−) is a weak derivative of the probability measure p(x), then for every bounded continuous function f (Heidergott 27
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih and V´azquez-Abad, 2000; Pflug, 1996): (cid:90) (cid:20)(cid:90) (cid:90) (cid:21) f (x)p(x; θ)dx = c f (x)p+(x; θ)dx f (x)p−(x; θ)dx . (43) θ θ ∇ − We should therefore expect the pathology in the example on bounded support not to appear. Example (Bounded support revisited). We revisit our earlier example from Section 4.3.2 in which we compute the gradient of the objective (2) using the cost function f (x) = x and distribution p (x) = 1 1 0 < x < θ , which is differentiable in θ when x (0, θ). The measure-valued derivative θ θ { } ∈ is: (cid:32) (cid:33) (cid:90) 1 (cid:90) θ 1 1 (cid:90) θ f (x) (x)dx = f (x)dx = f (θ) f (x)dx (44a) ∇θ U[0,θ] ∇θ θ θ − θ2 0 0 (cid:18)(cid:90) (cid:90) (cid:19) 1 = f (x)δ (x)dx f (x) (x)dx . (44b) θ θ − U[0,θ] In the first line, we fixed the limits of integration using the support of the uniform distribution and then applied the product rule for differentiation to obtain the two terms. In the second line (44b), we rewrite the terms to reflect the triple form with which we have been communicating the weak derivative. The measure-valued derivative is given by the triple (cid:0) 1 , δ , (cid:1) ; the positive part is a θ θ U[0,θ] discrete measure whereas the negative part is continuous in the range (0, θ). Using this result for our specific gradient problem, we find: (cid:18) (cid:12)θ(cid:19) True gradient: E [x] = x2 (cid:12) = 1 ; (45a) ∇θ pθ(x) ∇θ 2θ (cid:12) 0 2 Measure-valued gradient: 1 (cid:0)E [x] E [x](cid:1) = 1 (cid:0) θ θ (cid:1) = 1 . (45b) θ δθ − U[0,θ] θ − 2 2 By not requiring domination, the measure-valued derivative achieves the correct value. (cid:50) 6.3.2 Bias and Variance Properties By using the fundamental property of weak derivatives, which requires bounded and continuous cost functions f , we can show that the measure-valued gradient (41f) provides an unbiased estimator of the gradient. Unbaisedness can also be shown for other types of cost functions, such as those that are unbounded, or those with sets of discontinuities that have measure zero under the positive and negative components (Pflug, 1996, sect 4.2). The variance of the measure-valued derivative estimator is: V [ f (x)] = V [f (x)] + V [f (x)] 2Cov [f (x(cid:48)), f (x)]. (46) p(x;θ) ∇θ p+(x;θ) p−(x;θ) − p+(x(cid:48);θ)p−(x;θ) From this, we see that the variance of the gradient estimator depends on the choice of decomposition of the weak derivative into its positive and negative components. An orthogonal decomposition using the Hahn-Jordan decomposition is suggested to give low variance in general (Pflug, 1989). Furthermore, we see that if the random variables can be ‘coupled’ in some way, where they share the same underlying source of randomness, this will reduce gradient variance by increasing the covariance term in (46). The most common coupling scheme is to sample the variables x˙ and x¨ using common random numbers—this is what we used in Figures 2 and 3. From the figures, we see that the measure-valued derivative estimator with coupling often provides lower variance than the alternative estimators. Pflug (1996, example 4.21) discusses variance reduction for measure-valued derivatives and approaches for coupling in detail, and we expand on this in Section 7.2. 28
Monte Carlo Gradient Estimation in Machine Learning 6.3.3 Higher-order Derivatives The problem of computing higher-order derivatives involves computing (cid:82) f (x) (k) p(x; θ)dx. The ∇θ second derivative 2p(x; θ) is also a signed measure, which means that it too has a weak derivative ∇θ representation. For higher-order derivatives we will again be able to compute the gradient using a weighted difference of expectations, using the weak-derivative triple (c(θ), p+(n) , p−(n) ), where p+(n) and p−(n) are the positive and negative components of the decomposition for the nth order density-derivative (n) p(x; θ). ∇θ 6.3.4 Computational Considerations Measure-valued gradients are much more computationally expensive than the score-function or path- wise gradients. This is because the gradient we computed in (39d) is the gradient for a single param- eter: for every parameter we require two evaluations of the cost function to compute its gradient. It is this structure of adapting the underlying sampling distributions for each parameter that leads to the low variance of the estimator but at the same time makes its application to high-dimensional parameter spaces prohibitive. For problems that do not have a large number of parameters, and for which we can evaluate the cost f frequently, e.g., being in the form of a simulator rather than a costly interactive system, this gradient may prove beneficial. Overall the computational cost is (2N DL), for N samples drawn from each of the positive and negative components of the weak O derivative, D-dimensional parameters θ, and a cost L of evaluating the cost function. Taking into account the exposition of this section, points for consideration when using the score function estimator are: • The measure-valued derivative can be used with any type of cost function, differentiable or not, but we will need to have the ability to evaluate it repeatedly for different inputs. • It is applicable to both discrete and continuous distributions, which adds to its generality. • The estimator is computationally expensive in high-dimensional parameter spaces since it re- quires two cost function evaluations for every parameter. • We will need methods to sample from the positive and negative measures, e.g., the double-sided Maxwell distribution for gradients of the Gaussian variance. • Using the weak derivative requires manual derivation of the decomposition at first, although for many common distributions the weak-derivative decompositions are known. 6.4 Research in Measure-valued Derivatives The weak derivative was initially introduced by Pflug (1989), with much subsequent work to establish its key properties, including behaviour under convex combinations, convolution, transformation, restriction, and the use of L differentiability of the gradient problem as a sufficient condition for 1 unbiasedness. In most papers, the details of how to derive the weak derivative were omitted; this important exposition was provided for many common distributions by Heidergott et al. (2003). The connections between perturbation analysis (pathwise estimation) and measure-valued differentiation are explored by Heidergott et al. (2003), with specific exploration of Markov processes and queuing theory applications. Heidergott et al. (2008) also provide a detailed exposition for derivatives with Gaussian measures, which is one of the most commonly encountered cases. In machine learning, 29
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih Buesing et al. (2016) discuss the connection between weak derivatives, finite differences and other Monte Carlo gradient estimation methods. 7. Variance Reduction Techniques From the outset, we listed low variance as one of the crucial properties of Monte Carlo estimators. The three classes of gradient estimators we explored each have different underlying conditions and properties that lead to their specific variance properties. But in all cases the estimator variance is something we should control, and ideally reduce, since it will be one of the principal sources of performance issues. As we search for the lowest variance gradient estimators, it is important to recognise a key tension that will develop, between a demand for computationally-efficient and effective variance reduction versus low-effort, generic, or black-box variance reduction tools. We focus on four common methods in this section—large-samples, coupling, conditioning, and control variates. These methods will be described separately, but they can also be combined and used in concert to further reduce the variance of an estimator. Variance reduction is one of the largest areas of study in Monte Carlo methods, with important expositions on this topic provided by Robert and Casella (2013, ch. 4), Owen (2013, ch. 8-10) and Glasserman (2013, ch. 4). 7.1 Using More Samples The simplest and easiest way to reduce the variance of a Monte Carlo estimator is to increase the number of samples N used to compute the average in the estimator (3). The variance of such es- timators shrinks as O(1/N), since the samples are independent, while the computational cost grows linearly in N . For a more complex variance reduction method to be appealing, it should compare favourably to this reference method in terms of efficiency of variance reduction. If evaluating the cost function involves only a computational system or simulator, then using more samples can be appealing, since the computational cost can be substantially reduced by parallelizing the computa- tion. For some problems, however, increasing the number of Monte Carlo samples will not be an option, typically in problems where evaluating the cost function involves a real-world experiment or interaction. We restricted our discussion to the standard Monte Carlo method, which evaluates the average (3) using independent sets of random variates, but other approaches can provide opportuni- ties for improved estimation. As an example, quasi-Monte Carlo methods generate a set of samples that form a minimum discrepancy sequence, which are both more efficient in the use of the samples and can lead to reduced variance (Glasserman (2013, chp. 5), Leobacher and Pillichshammer (2014), Buchholz et al. (2018)). 7.2 Coupling and Common Random Numbers We will often encounter problems that take the form of a difference between two expectations of a function f (x) under different but closely-related distributions p (x) and p (x): 1 2 η = E [f (x)] E [f (x)] . (47) p1(x) − p2(x) We saw this type of estimation problem in the derivation of the measure-valued gradient estima- tor (39d). The direct approach to computing the difference would be to estimate each expectation 30
Monte Carlo Gradient Estimation in Machine Learning separately using N independent samples and to then take their difference: η¯ = 1 (cid:88)N f (cid:16) xˆ(n)(cid:17) 1 (cid:88)N f (cid:16) xˆ(n)(cid:17) = 1 (cid:88)N (cid:16) f (cid:16) xˆ(n)(cid:17) f (cid:16) xˆ(n)(cid:17)(cid:17) , (48) ind N 1 − N 2 N 1 − 2 n=1 n=1 n=1 where xˆ(n) and xˆ(n) are i.i.d. samples from p (x) and p (x), respectively. We can achieve a simple 1 2 1 2 form of variance reduction by coupling xˆ(n) and xˆ(n) in equation (48), so that each pair (xˆ(n) , xˆ(n) ) 1 2 1 2 is sampled from some joint distribution p (x , x ) with marginals p (x) and p (x). We will denote 12 1 2 1 2 the coupled estimator as η¯ . The variance reduction due to coupling is: cpl V [η¯ ] V [f (x ) f (x )] p12(x1,x2) cpl = p12(x1,x2) 1 − 2 V [η¯ ] V [f (x ) f (x )] p1(x1)p2(x2) ind p1(x1)p2(x2) 1 − 2 V [f (x )] + V [f (x )] 2Cov [f (x ), f (x )] = p1(x1) 1 p2(x2) 2 − p12(x1,x2) 1 2 . (49) V [f (x )] + V [f (x )] p1(x1) 1 p2(x2) 2 Therefore, to reduce variance we need to choose a coupling p (x , x ) such that f (x ) and f (x ) are 12 1 2 1 2 positively correlated. In general, finding an optimal coupling can be difficult, even in the univariate case since it involves the inverse CDFs of p (x ) and p (x ) (see e.g. Pflug (1996, p. 225)). The use 1 1 2 2 of common random numbers is one popular and simple coupling technique that can be used when p (x ) and p (x ) are close or in a related family of distributions. Common random numbers involve 1 1 2 2 sharing, in some way, the underlying random numbers used in generating the random variates xˆ 1 and xˆ . For example, in the univariate case, we can do this by sampling u [0, 1] and applying 2 ∼ U the inverse CDF transformations: x = CDF −1(u) and x = CDF −1(u). 1 p1 2 p2 Example (Maxwell-Gaussian Coupling). Consider Gaussian measures (x µ, σ2) in the setting N | of equation (2) and the task of computing the gradient with respect to the standard deviation σ. The measure-valued gradient, using Table 1, is given by the triple (cid:0) 1 ; (x µ, σ2), (x µ, σ2)(cid:1) , where σ M | N | is the double-sided Maxwell distribution with location µ and scale σ2, and is the Gaussian M N distribution with mean µ and variance σ2; see appendix for the densities for these distributions. We can couple the Gaussian and the Maxwell distribution by exploiting their corresponding sampling paths with a common random number: if we can generate samples ε˙ (0, 1) then, by first ∼ M sampling from the Uniform distribution u˙ [0, 1] and reusing the Maxwell samples, we can generate ∼ U (0, 1) distributed samples ε¨ via ε¨ = ε˙u˙ . Then, we can perform a location-scale transform to obtain N the desired Maxwell and Normal samples: x˙ = µ + σε˙, x¨ = µ + σε¨. The distributions are coupled because they use the same underlying Maxwell-distributed variates (Heidergott et al., 2008). (cid:50) The main drawback of using coupling is that implementing it can require non-trivial changes to the code performing sampling, which can make it less appealing than variance reduction methods that do not require any such changes. 7.3 Variance Reduction by Conditioning Probabilistically conditioning our estimators on a subset of dimensions and integrating out the remaining dimensions analytically is a powerful form of variance reduction in Monte Carlo esti- mators, sometimes referred to as Rao-Blackwellization. Assume that the dimensions 1, ..., D { } of x are partitioned into a set and its complement c = 1, ..., D with the expectation g(x Sc) = E p(xS) [f (x) |x Sc] beingS analytically computablS e for a{ ll values} o\ f S x Sc. We can then esti- 31
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih mate E p(x) [f (x)] by performing Monte Carlo integration over a smaller space of x Sc: g¯ = 1 (cid:88)N g (cid:16) x(n)(cid:17) = 1 (cid:88)N E (cid:104) f (x)(cid:12) (cid:12)x(n)(cid:105) where x(n) p(x) for n = 1, ..., N. (50) N Sc N p(xS) (cid:12) Sc ∼ n=1 n=1 We can show that this conditioned estimator has lower variance than its unconditional counterpart. Following Owen (2013), by the law of total variance, we have: V p(x)[f (x)] = E p(xSc) (cid:2)V p(xS) [f (x) |x Sc](cid:3) + V p(xSc)[E p(xS) [f (x) |x Sc]] (51) = E p(xSc) (cid:2)V p(xS) [f (x) |x Sc](cid:3) + V p(xSc)[g(x Sc)] (52) ≥ V p(xSc)[g(x Sc)], (53) which shows that the conditional system has lower variance than than the unconditional system f¯. And by implication, the conditional estimator g¯ is guaranteed to have lower variance than f¯: V (cid:2) f¯(cid:3) = 1 V [f (x)] 1 V [g(x)] = V [g¯] . (54) p(x) N p(x) ≥ N p(x) p(x) Conditioning is the basis of several variance reduction schemes: smoothed perturbation analysis for the pathwise derivative (Gong and Ho, 1987); the use of leave-one-out estimation and Rao- Blackwellisation in hierarchical models by Ranganath (2017); and the local-expectation gradients developed by Titsias and L´azaro-Gredilla (2015) to improve on the variance properties of score- function and pathwise estimators in variational inference. This technique is useful in practice only if we can compute the conditional expectations involved efficiently, whether analytically or with numerical integration. If evaluating these expectations is costly, then simply increasing the number of samples in the Monte Carlo estimator over the original space may prove easier and as effective. 7.4 Control Variates We now consider a general-purpose technique for reducing the variance of any Monte Carlo method, again looking at the general problem E [f (x)] (1), which we can do since all the gradient p(x;θ) estimators we developed in the preceding sections were eventually of this form. The strategy we will take is to replace the function f (x) in the expectation (1) by a substitute function f˜(x) whose (cid:104) (cid:105) expectation E f˜(x) is the same as the original, but that has lower variance. If we have a p(x;θ) function h(x) whose expectation E [h(x)] is known, we can write this modified expectation as: p(x;θ) f˜(x) = f (x) β(h(x) E [h(x)]) (55) p(x;θ) − − η¯ = 1 (cid:80)N f˜(xˆ(n)) = f¯ β(h¯ E [h(x)]), (56) N n=1 − − p(x;θ) where xˆ(n) p(x; θ) and we use the shorthand f¯ and h¯ denote the sample averages. We refer to ∼ the function h(x) as a control variate, and the estimator (56) a control variate estimator. Control variates allow us to exploit information about quantities that we know and have access to, to reduce the variance of an unknown quantity. The observed error (h(x) E [h(x)]) serves as a control p(x;θ) in estimating E [f (x)], and β is a coefficient that affects t− he strength of the control variate. p(x;θ) We expand on the specific choice of h in Section 7.4.3. Glynn and Szechtman (2002) describe the connections between control variates and other variance reduction approaches such as antithetic and stratified sampling, conditional Monte Carlo and non-parametric maximum likelihood. 32
Monte Carlo Gradient Estimation in Machine Learning 7.4.1 Bias, Consistency and Variance We can show that both the unbiasedness and consistency of the estimator (56) are maintained: Unbiasedness: E (cid:104) f˜(x; β)(cid:105) = E (cid:2) f¯ β(h¯ E [h(x)])(cid:3) = E (cid:2) f¯(cid:3) = E [f (x)] (57) p(x;θ) p(x;θ) − − 1 (cid:88)N (cid:104) (cid:105) Consistency: lim f˜(x(n)) = E f˜(x) = E [f (x)] (58) n→∞ N p(x;θ) p(x;θ) n=1 Importantly, we can characterise the variance of the estimator as: V[f˜] = V[f (x) β(h(x) E [h(x)])] = V[f ] 2βCov[f, h] + β2V[h]. (59) p(x;θ) − − − By minimising (59) we can find that the optimal value of the coefficient is: (cid:115) Cov[f, h] V[f ] β∗ = = Corr(f, h), (60) V[h] V[h] where we expressed the optimal coefficient in terms of the covariance between f and h, as well as in terms of the correlation coefficient Corr(f, h). The effectiveness of the control variate can be characterised by the ratio of the variance of the control variate estimator to that of the original estimator: we can say our effort has been effective if the ratio is substantially less than 1. Using the optimal control coefficient in (59), we find that the potential variance reduction is: V[f˜(x)] = V[f (x) − β(h(x) − E p(x;θ) [h(x)])] = 1 Corr(f (x), h(x))2. (61) V[f (x)] V[f (x)] − Therefore, as long as f (x) and h(x) are not uncorrelated, we can always obtain a reduction in variance using control variates. For variance reduction to be substantial, we need control variates that are strongly correlated with the functions whose variance we wish to reduce. The sign of the correlation does not matter since it is absorbed into β∗, and the stronger the correlation, the larger the reduction in variance. In practice, the optimal β∗ will not be known and we may need to estimate it empirically. Empirical estimation of β¯ using the same N samples used to estimate h¯ will introduce a bias because β¯ N N and h¯ will no longer be independent. In practice, this bias is often small (Glasserman, 2013; Owen, 2013), and can be controlled since it decreases quickly as the number of samples N is increased. 7.4.2 Multiple and Non-linear Controls The control variates in (55) are linear control variates, and this form should be used as a default. A first generalisation of this is in allowing for multiple controls, where we use a set of D controls rather than one: f˜(x) = f (x) β(cid:62)(h(x) E [h(x)]), (62) p(x;θ) − − where β is now a D-dimensional vector of coefficients, with D usually kept low; the bias and variance analysis follows the single-control case above closely. The use of multiple controls is best suited to cases when the evaluation of the function f is expensive and can be approximated by a linear combination of inexpensive control variates. 33
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih We can also consider non-linear ways of introducing control variates. Three alternative (multiplica- tive) forms are: f˜ = f E p(x h;θ) [h] ; f˜ = f Ep(xh ;θ)[h] ; f˜ = f exp(E p(x;θ) [h] − h). (63) These multiplicative controls lead to consistent but biased estimates. The non-linear controls also do not require a coefficient β since this is implicitly determined by the relationship between h and f . When we use a large number of samples in the estimator, non-linear controls are no better than linear controls, since asymptotically it is the linear terms that matter (Glynn and Whitt, 1989, sect. 8). In most contemporary applications, we do not use large samples—often we use a single sample in the estimator—and it is in this small sample regime that non-linear controls may have some use. The practical guidance though, remains to use linear controls as the default. 7.4.3 Designing Control Variates The primary requirements for a control variate is that it must have a tractable expectation and, ideally, be strongly correlated with the target. While control variates are a very general technique that can be applied to any Monte Carlo estimator, designing effective control variates is an art, and the best control variates exploit the knowledge of the problem at hand. We can identify some general strategies for designing controls: Baselines. One simple and general way to reduce the variance of a score-function gradient estimator is to use the score function itself as a control variate, since its expectation under the measure is zero (eqn. (12)). This simplifies to subtracting the coefficient β from the cost in the score function estimator in (13d), giving the modified estimator: N 1 (cid:88) η¯ = (f (xˆ(n); φ) β) log p(x(n); θ); xˆ(n) p(x; θ). (64) N θ N − ∇ ∼ n=1 In supervised learning, β is called a baseline (Williams, 1992) and has historically been estimated with a running average of the cost. While this approach is easier to implement than optimising β to minimise variance, it is not optimal and does not guarantee lower variance compared to the vanilla score-function estimator (Weaver and Tao, 2001). Baselines are among the most popular variance reduction methods because they are easy to implement, computa- tionally cheap, and require minimal knowledge about the problem being solved. We explore the baseline approach in an empirical analysis in section 8.3. Bounds. We can use bounds on the cost function f as ways of specifying the form of the control variate h. This is intuitive since this maintains a correlation between f and h, and if chosen well, may be easily integrable against the measure and available in closed form. This approach requires more knowledge of the cost function, since we will need to characterise the cost analytically in some way to bound it. Local variational inference methods developed several bounding methods, such as Bohning and Jaakola’s bounds (Khan et al., 2010), and piecewise bounds (Marlin et al., 2011). Paisley et al. (2012) show specific use of bounds as control variates. In general, unless the bounds used are tight, they will not be effective as control variates, since the gap between the bound and the true function is not controllable and will not necessarily give the information needed for variance reduction. Delta Methods. We can create control variates by approximating the cost function using a Taylor expansion. This requires a cost function that is differentiable so that we can compute the 34
Monte Carlo Gradient Estimation in Machine Learning second-order Taylor expansion, but can be an effective and very general approach for variance reduction that allows easy implementation. We will develop a specific example in section 8.1. Learning Controls. If we estimate the gradients repeatedly as a part of an optimisation process, we can create a parameterised control variate function and learn it as part of the overall optimisation algorithm. This allows us to create a control variate that dynamically adapts over the course of optimisation to help better match the changing nature of the functions. Furthermore, this approach makes it easy to condition on any additional information that is relevant, e.g., by using a neural network to map the additional information to the parameters of the control variate. This strategy has been used by Mnih and Gregor (2014) to reduce the score-function gradient variance substantially in the context of variational inference, by defining the baseline as the output of a neural network that takes the observed data point as its input. This approach is general, since it does not make assumptions about the nature of the cost function, but introducing a new set of parameters for the control function can make the optimisation process more expensive. Even in the simpler case of estimating the gradient only once, we can still learn a control variate that results in non-trivial variance reduction as long as we generate sufficiently many samples to train it and use a sample-efficient learning algorithm. Oates et al. (2017) demonstrated the feasibility of this approach using kernel-based non-parametric control variates. 8. Case Studies in Gradient Estimation We now bring together the different approaches for gradient estimation in a set of case studies to explore how these ideas have been combined in practice. Because these gradients are so fundamen- tal, they appear in all the major sub-areas of machine learning—in supervised, unsupervised, and supervised learning. Not all combinations that we can now think of using what we have reviewed have been explored, and the gaps we might find point to interesting topics for future work. 8.1 Delta Method Control Variates This case study makes use of the delta method (Bickel and Doksum, 2015), which is a method for describing the asymptotic variance of a function of a random variable. By making use of Taylor expansions, the delta method allows us to design lower variance gradient estimators by using the differentiability, if available, of the cost function. It can be used for variance reduction in both the score-function estimator (Paisley et al., 2012) and the pathwise estimator (Miller et al., 2017), and shows us concretely one approach for designing control variates that was listed in section 7.4.3. We denote the gradient and Hessian of the cost function f (x) as γ(x) := f (x)(cid:62) and H(x) := x ∇ 2 f (x), respectively. The second-order Taylor expansion of a cost function f (x) expanded around ∇x point µ and its derivative is: h(x) = f (µ) + (x µ)(cid:62)γ(µ) + 1 (x µ)(cid:62)H(µ)(x µ), (65) − 2 − − h(x) = γ(µ)(cid:62) + (x µ)(cid:62)H(µ). (66) x ∇ − 35
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih We can use this expansion directly as a control variate for the score-function estimator, following the approach we took to obtain equation (55): (cid:104) (cid:105) (cid:104) (cid:105) η = E f˜(x) = E f (x) β(cid:62)h(x) + β(cid:62) E [h(x)] (67a) SF ∇θ p(x;θ) ∇θ p(x;θ) − ∇θ p(x;θ) (cid:104)(cid:16) (cid:17) (cid:105) = E f (x) β(cid:62)h(x) log p(x; θ) + β(cid:62) E [h(x)] . (67b) p(x;θ) − ∇θ ∇θ pθ(x) In the first line we again wrote the control-variate form for a Monte Carlo gradient, and in the second line we wrote out the gradient using the score-function approach. This is the control variate that is proposed by Paisley et al. (2012, eq. (12)) for Monte Carlo gradient estimation in variational infer- ence problems. In the Gaussian mean-field variational inference that Paisley et al. (2012) consider, the second term is known in closed-form and hence does not require Monte Carlo approximation. Like in equation (62), β is a multivariate control coefficient and is estimated separately. With the increased popularity of the pathwise gradient estimator in machine learning, Miller et al. (2017) showed how delta method control variates can be used with pathwise estimation, and also for problems in variational inference. The approach is not fundamentally different from that of Paisley et al. (2012), and we show here that they can be derived from each other. If we begin with equation (67a) and instead apply the pathwise estimation approach, using a sampling path x = g((cid:15); θ); (cid:15) p((cid:15)), we will obtain the gradient: ∼ (cid:104) (cid:105) η = E f˜(x) = E [f (x) βh(x)] + β E [h(x)] (68a) P D ∇θ p(x;θ) ∇θ p(x;θ) − ∇θ p(x;θ) = E [f (g((cid:15); θ)) βh(g((cid:15); θ))] + β E [h(x)] (68b) θ p((cid:15)) θ p(x) ∇ − ∇ = E [ f (x) g((cid:15); θ) β h(x) g((cid:15); θ)] + β E [h(x)] . (68c) p((cid:15)) x θ x θ θ p(x) ∇ ∇ − ∇ ∇ ∇ The first line recalls the linear control variate definition (55) and expands it. In the second line we use the LOTUS to reparameterise the expectation in terms of the sampling path g and the base distribution p((cid:15)), but again assume that the final term is known in closed-form and does not require stochastic approximation. In the final line (68c) we exchanged the order of integration and differentiation, and applied the chain rule to obtain the form of the gradient described by Miller et al. (2017), but obtained from this different starting point. As both Paisley et al. (2012) and Miller et al. (2017) demonstrate, we can achieve an appreciable variance reduction using the delta method, if we are able to exploit the generic differentiability of the cost function. Due to the widespread use of automatic differentiation, methods like these can be easily implemented and allow for a type of automatic control variate, since no manual derivation and implementation is needed. 8.2 Hybrids of Pathwise and Score Function Estimators Decoupling of gradient estimation and sampling was an aspect of the pathwise estimator that we expanded upon in section 5.3.1. This decoupling can be achieved in other ways than with implicit differentiation that we used previously, and we explore two other instantiations in this section. Up to this point, we have always demanded that the sampling path consists of a transformation x = g((cid:15); θ) and a corresponding base distribution p((cid:15)) that is independent of the parameters θ. But it is possible for us to find weaker sampling paths that consists of transformations g, but that instead have parameter-dependent base distributions p((cid:15); θ). In this setting, gradient estimators will 36
Monte Carlo Gradient Estimation in Machine Learning form a hybrid of the pathwise and score-function estimators: (cid:90) E [f (x)] = E [f (g((cid:15); θ))] = p ((cid:15))f (g((cid:15); θ))d(cid:15) (69a) ∇θ pθ(x) ∇θ pθ((cid:15)) ∇θ θ (cid:90) (cid:90) = p ((cid:15)) f (g((cid:15); θ))d(cid:15) + p ((cid:15))f (g((cid:15); θ))d(cid:15) (69b) θ θ θ θ ∇ ∇ = E [ f (g((cid:15); θ))] + E [ log p ((cid:15))f (g((cid:15); θ))] (69c) pθ((cid:15)) ∇θ pθ((cid:15)) ∇θ θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) pathwise gradient score function gradient In the first line we expanded the definition of the gradient-expectation under the new parameter- dependent base distribution p((cid:15); θ) and the path g((cid:15); θ). In the second line, we applied the chain rule for differentiation, which results in two terms involving a gradient of the function and a gradient of the distribution. In the final line (69c), we apply the identity for score functions (11) to replace the gradient of the probability with the gradient of the log-probability, which leaves us with two terms that we recognise as the two major gradient estimators we have explored in this paper. We now explore two approaches that fit within this framework to make this more concrete. Furthermore, both terms in (69c) can be designed with variance reduction using control variates. 8.2.1 Weak Reparameterisation Sampling paths of the type we just described, which have parameter-dependent base distributions p((cid:15); θ), are referred to by Ruiz et al. (2016) as weak reparameterisations. Example (Gamma Weak Reparameterisation). The Gamma distribution (x; α, β) with G shape parameter α and rate β can be represented using the weak reparameterisation (Ruiz et al., 2016): log(x) ψ(α) + log(β) (cid:15) = g−1(x; α, β) = −(cid:112) ; (70a) ψ (α) 1 (cid:112) eαψ(α) ψ 1(α) (cid:16) (cid:112) (cid:16) (cid:112) (cid:17)(cid:17) p((cid:15); α, β) = exp (cid:15)α ψ (α) exp (cid:15) ψ (α) + ψ(α) , (70b) 1 1 Γ(α) − where ψ is the digamma function, and ψ is its k-th derivative. The final density (70b) can be k derived by using change of variable formula (27), and shows what is meant by weak dependency, since it depends on shape parameter α but not on the rate parameter β . (cid:50) Using knowledge of the existence of a weak reparameterisation for the measure we can rewrite (69c). We use two shorthands: (cid:96)((cid:15); θ) := g((cid:15); θ) for the gradient of the weak-path with respect to θ ∇ the parameters, and u((cid:15); θ) := g((cid:15); θ) for the gradient of the Jacobian-determinant. The θ (cid:15) ∇ |∇ | pathwise term in (69c) becomes: (cid:90) (cid:90) p ((cid:15)) f (g((cid:15); θ))d(cid:15) = p (g((cid:15); θ)) g((cid:15); θ) f (g((cid:15); θ))d(cid:15) (71a) θ θ θ (cid:15) θ ∇ |∇ | ∇ (cid:90) (cid:90) = p (x) f (x)dx = p (x) f (x) xdx (71b) θ θ θ x θ ∇ ∇ ∇ = E (cid:2) f (x)(cid:96)(g−1(x, θ); θ)(cid:3) , (71c) pθ(x) ∇x where we first applied the rule for the change of variables to rewrite the integrals in terms of the variable x. Introducing the infinitesimals for dx cancels the determinant terms, after which we apply 37
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih the chain rule and finally rewrite it in the more familiar expectation form in the final line. Similarly, the score function term in (69c) can be written as: (cid:90) p ((cid:15)) log p ((cid:15))f (g((cid:15); θ))d(cid:15) (72a) θ θ θ ∇ (cid:90) = p (g((cid:15); θ)) g((cid:15); θ) (log p (g((cid:15); θ)) + log g((cid:15); θ) ) f (g((cid:15); θ))d(cid:15) (72b) θ (cid:15) θ θ (cid:15) |∇ | ∇ |∇ | (cid:90) = p (x) (cid:0) log p (x) + u(g−1(x, θ); θ)(cid:1) f (x)dx (72c) θ θ θ ∇ = E (cid:2)(cid:0) log p (x)(cid:96)(g−1(x, θ); θ) + u(g−1(x, θ); θ)(cid:1) f (x)(cid:3) , (72d) pθ(x) ∇x θ where we again use the rule for the change of variables in the second line wherever p((cid:15)) appears. Rewriting the integral in terms of dx will cancel the determinant terms and we simplify using the shorthand notation u in the third line, giving gradient contribution in the final line using the expectation notation. 8.2.2 Gradients with Rejection Sampling Rejection sampling offers another way of exploring the decoupling of sampling from gradient esti- mation in pathwise estimation. Naesseth et al. (2017) developed the use of rejection sampling in the setting of the hybrid gradient (69c), which we will now present. Using rejection sampling, we can always generate samples from a distribution p (x) using a proposal θ distribution r (x) for which p (x) < M r (x) for a finite constant M . If we also know a param- θ θ θ θ θ eterisation of the proposal distribution r (x) in terms of a sampling path x = g((cid:15); θ) and a base θ distribution (cid:15) p((cid:15)), we can generate samples using a simple procedure: draw sample (cid:15) p((cid:15)), ∼ ∼ evaluate x = g((cid:15); θ), draw uniform variates u [0, 1], and finally accept samples x if u < pθ(x) . ∼ U Mθrθ(x) The distribution π((cid:15)) for the samples (cid:15) generated by rejection sampling is: (cid:90) (cid:90) 1 (cid:20) p (g((cid:15); θ)) (cid:21) π((cid:15)) = π((cid:15), u)du = M p((cid:15))1 0 < u < θ du (73a) θ M r (g((cid:15); θ)) 0 θ θ p (g((cid:15); θ)) p (g((cid:15); θ)) = M p((cid:15)) θ = p((cid:15)) θ (73b) θ M r (g((cid:15); θ)) r (g((cid:15); θ)) θ θ θ Using (73b), we can show that we can instead compute gradients under π((cid:15); θ), since: (cid:90) (cid:90) p (g((cid:15); θ)) E [f (g((cid:15); θ))] = π((cid:15))f (g((cid:15); θ)) d(cid:15) = p((cid:15)) θ f (g((cid:15); θ))d(cid:15) = E [f (x)] . (74) π((cid:15)) r (g((cid:15); θ)) pθ(x) θ The gradient (69c) can be rewritten using the effective distribution π((cid:15)) instead, giving another approach for gradient estimation: (cid:90) E [f (x)] = E [f (g((cid:15); θ))] = π((cid:15))f (g((cid:15); θ))d(cid:15) (75a) ∇θ pθ(x) ∇θ π((cid:15)) ∇θ (cid:20) (cid:21) p (g((cid:15); θ)) = E [ f (g((cid:15); θ))] + E log θ f (g((cid:15); θ)) (75b) πθ((cid:15)) ∇θ πθ((cid:15)) ∇θ r (g((cid:15); θ)) (cid:124) (cid:123)(cid:122) (cid:125) θ pathwise estimation (cid:124) (cid:123)(cid:122) (cid:125) score function estimation The proposal distribution r (x) should lead to a high acceptance probability, to minimize the mag- θ nitude of the score function term. 38
Monte Carlo Gradient Estimation in Machine Learning 8.3 Empirical Comparisons Logistic regression remains one of the most widely-used statistical models, and we use it here as a test case to explore the empirical performance of gradient estimators and to comment on their implementation and interactions with the optimisation procedure. Bayesian logistic regression defines a probabilistic model for a target y 1, 1 given features n (or covariates) x RD for data points n = 1, . . . , N , using a set of param∈ et{ e− rs w.} We denote the n ∈ collection of all N data points as X, y . Using a Gaussian prior on the parameters and a Bernoulli { } likelihood, the probabilistic model is: p(w) = (w 0, cI); p(y x, w) = σ(y x(cid:62)w), (76) N | | n n −1 where σ(z) = (1 + exp( z)) is the logistic function, and c is a constant. We use variational − Bayesian logistic regression, which is an approximate Bayesian inference procedure, to determine the parameter posterior distribution p(w X, y) (Jaakkola and Jordan, 1997; Murphy, 2012). Varia- | tional inference was one of the five research areas that we reviewed in Section 2 to which gradient estimation is of central importance. Variational inference allows us to specify a bound on the inte- grated likelihood of the logistic regression model, and learning requires gradients of the form of (2). Variational inference introduces a variational distribution q(w; θ) = (w µ, Σ), with variational N | parameters θ = µ, Σ that are optimised using a variational lower bound objective (Jaakkola and { } Jordan, 1997): I (θ) = (cid:88) E (cid:2) log σ(y x(cid:62)w)(cid:3) KL [q(w µ, Σ) p(w)] . (77) L q(w|µ,Σ) i i − | (cid:107) i=1 The first expectation is a problem of the form we have studied throughout this paper, which is the expectation of a cost function, given by a log-likelihood under a Gaussian measure; and we are interested in Monte Carlo gradients of the variational parameters θ. The second term is the Kullback-Leibler (KL) divergence between the variational posterior q and the prior distribution p, which is known in closed-form for Gaussian distributions. We optimise the variational bound using stochastic gradient descent, and compare learning of the variational parameters using both the score- function and pathwise estimator. The objective function we use is a Monte Carlo estimator of eqn. (77): B N K (cid:88) 1 (cid:88) ¯(θ) = log σ(y x(cid:62)wˆ ) KL [ (w µ, Σ) (w 0, cI)] ; wˆ (w µ, Σ), (78) L B N i i n − N | (cid:107)N | n ∼ N | i=1 n=1 where B is the batch size, K is the size of the full data set, N the number of samples taken from the posterior distribution to evaluate the Monte Carlo gradient, and the posterior covariance is a diagonal matrix Σ = diag(s). We use stochastic gradient descent for optimisation, with cosine learning rate decay (Loshchilov and Hutter, 2016) unless stated otherwise. We use the UCI Women’s Breast Cancer dataset (Dua and Graff, 2017), which has K = 569 data points and D = 39 features. For evaluation, we always use the entire dataset and 1000 posterior samples. Using this setup, we tie together the considerations raised in the previous sections, comparing gradient estimators when used in the same problem setting, looking at the interaction between the Monte Carlo estimation of the gradient and the mini-batch optimisation, and reporting on the variance of estimators under different variance reduction schemes. Generalised regression using the pathwise and score function estimators have been explored in several papers including Kucukelbir et al. (2017); Miller et al. (2017); Paisley et al. (2012); Salimans and Knowles (2013); Wang et al. (2013). 39
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih 104 103 102 101 100 0 500 1000 1500 2000 Iteration sgol rof rotamitse fo ecnairaV i Pathwise 106 w1 = mean texture w5 = mean compactness w10 = radius error 105 w20 = worst radius 104 103 0 500 1000 1500 2000 Iteration sgol rof rotamitse fo ecnairaV i Score function w1 = mean texture w5 = mean compactness w10 = radius error w20 = worst radius Figure 5: Progress of gradient variance V [ f (w)] for different parameters i as training q(w;s) ∇log si progresses; batch size B = 32, the number of samples N = 50. The legend shows the feature index and its name in the dataset. 0.975 0.970 0.965 0.960 0.955 0.950 0.945 0.940 0 1000 2000 3000 4000 5000 Iteration ycaruccA -68 -70 -72 -74 Score function Score function (MA) -76 Score function (Delta) Pathwise -78 Pathwise (Delta) -80 0 1000 2000 3000 4000 5000 Iteration dnuoB lanoitairaV 106 105 104 103 102 101 100 0 1000 2000 3000 4000 5000 Iteration µ rof rotamitse eht fo ecnairaV 105 104 103 102 101 100 0 1000 2000 3000 4000 5000 Iteration sgol rof rotamitse eht fo ecnairaV Figure 6: Comparison between the score function estimator and pathwise estimators at a fixed learning rate (10−3). B = 32, M = 50. When a control variate is used, it is marked in paren- thesis: MA stands for moving average, Delta for the second-order delta method. In the second row, we show the variance of the estimator for mean V [ f (w)] and log-standard deviation q(w|µ,s) µ V [ f (w)], averaged over parameter dimensions. ∇ q(w|µ,s) ∇log si 40
Monte Carlo Gradient Estimation in Machine Learning 0.98 0.97 0.96 0.95 0.94 0.93 0.92 0.91 0.90 0 200 400 600 800 1000 Iteration ycaruccA -80 -100 -120 -140 -160 Score function Score function (MA) -180 Pathwise -200 0 200 400 600 800 1000 Iteration dnuoB lanoitairaV 107 106 105 104 103 102 101 0 200 400 600 800 1000 Iteration µ rof rotamitse eht fo ecnairaV 107 106 105 104 103 102 101 100 0 200 400 600 800 1000 Iteration sgol rof rotamitse eht fo ecnairaV Figure 7: Comparison between the score function estimator and pathwise estimators with a fixed batch size and number of posterior samples B = 32, N = 5. The pathwise estimator converges with a higher learning rate (10−3), while the score function requires a smaller learning rate (10−4) to avoid divergence due to higher gradient variance. We do not compare with control variates which require additional samples, such as the delta method control variate. In the second row, we show the vari- ance of the estimator for mean V [ f (w)] and log-standard deviation V [ f (w)], q(w|µ,s) µ q(w|µ,s) log s ∇ ∇ averaged over parameter dimensions. Decreasing variance over training. Figure 5 shows the variance of the two estimators for the gradient with respect to the log of the standard deviation log s for four input features. These d plots establish the basic behaviour of the optimisation: different dimensions have different gradient- variance levels; this variance decreases over the course of the optimisation as the parameters get closer to an optimum. The same behaviour is shown by both estimators, but the magnitude of the variance is very different between them, with the pathwise estimator having much lower variance compared to the score-function. Performance with fixed hyperparameters. We compare the score-function and pathwise es- timators by now fixing the hyperparameters of the optimisation algorithm, using a batch size of B = 32 data points, a learning rate of 10−3 and estimating the gradient using N = 50 samples from the measure. Figure 6 shows the variational lower bound and the F1-score that is attained in this setting. These graphs also show the control-variate forms of the gradient estimators. As 41
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih -70 -80 -90 -100 -110 -120 0 200 400 600 800 1000 Iteration dnuoB lanoitairaV Pathwise -70 -80 -90 Num Samples -100 1 10 -110 100 -120 0 200 400 600 800 1000 Iteration dnuoB lanoitairaV Score function -70 -80 -90 -100 Num Samples 50 -110 100 -120 0 200 400 600 800 1000 Iteration dnuoB lanoitairaV Score function (Delta) Num Samples 10 50 100 Figure 8: The effect of the number of posterior samples N on the evidence lower bound for different estimators. B = 32. -70 -80 -90 -100 -110 -120 0 200 400 600 800 1000 Iteration dnuoB lanoitairaV Pathwise -70 -80 -90 Batch Size -100 32 64 -110 128 -120 0 200 400 600 800 1000 Iteration dnuoB lanoitairaV Score function -70 -80 -90 Batch Size -100 32 64 -110 128 -120 0 200 400 600 800 1000 Iteration dnuoB lanoitairaV Score function (Delta) Batch Size 32 64 128 Figure 9: The effect of the number of the batch size B on the evidence lower bound for different estimators with N = 50. control variates, we use the delta method (Section 8.1) and the moving average (MA) baseline (Section 7.4.3). For the delta method control variate, we always use 25 samples from the measure to estimate the control variate coefficient using equation (60). Comparing the different estimators in Figure 6, the pathwise estimator has lower variance and faster convergence than the score function estimator. The score function estimator greatly benefits from the use of control variates, with both the moving average baseline and the delta method reducing the variance. The delta method control variate leads to greater variance reduction than a moving average baseline, and reduces variance both for the score function and pathwise estimators. We already have a sense from these plots that where the pathwise estimator is applicable, it forms a good default estimator since it achieves the same performance level but with lower variance and faster convergence. Using fewer samples. Figure 7 shows a similar setup to Figure 6, with a fixed batch size B = 32, but using fewer samples from the measure, N = 5. Using fewer samples affects the speed of convergence. The score function estimator converges slower than the pathwise estimator, since it requires smaller learning rates. And like in the previous figure, this instability at higher learning rates can addressed using more samples in the estimator, or more advanced variance reduction like the delta method. Effect of batch size and samples. We now vary the number of samples from the measure that is used in computing the gradient estimator, and the batch size, and show the comparative performance in Figures 8 and 9. In Figure 8 there is a small change in the rate of the convergence of the pathwise estimator as the number of samples N is increased. The score function estimator benefits much more 42
Monte Carlo Gradient Estimation in Machine Learning 108 107 106 105 104 103 102 101 100 10-1 0 200 400 600 800 1000 Iteration σgol rof rotamitse fo .raV i Score function (Delta) 108 control variate 107 no control variate 106 105 104 103 102 101 100 10-1 0 200 400 600 800 1000 Iteration σgol rof rotamitse fo .raV i Pathwise (Delta) 108 control variate 107 no control variate 106 105 104 103 102 101 100 10-1 0 200 400 600 800 1000 Iteration σgol rof rotamitse fo .raV i Score function (MA) control variate no control variate Figure 10: Quantifying the variance reduction effect of control variates. Here, each model is trained with a control variate, but at each iteration we assess the variance V [ f (w)] of the same q(w|µ,s) log s ∇ estimator for the current parameters with batch size B = 32, N = 50. from increasing the number of samples used. As the batch size is increased in Figure 9, we again see a limited gain for the pathwise estimator and more benefit for the score-function estimator. When control variates are used with these estimators, the score function estimator benefits less from an increase in the number posterior samples or an increase in batch size compared to the vanilla score function estimator. The performance of the vanilla score function estimator can match that of the score function estimator with a delta control variate, by increasing the number of posterior samples: using N = 100, B = 32 the score function estimator matches the variational lower bound obtained by the score function estimator with a delta control variate when N = 10, B = 32. This exposes a trade-off between implementation complexity and sample efficiency. Controlling comparisons. When working with control variates, we found it useful to record for each iteration what the gradient variance would have been for the current update had the control variate not been used, in order to check that variance reduction is working. We show the results in this regime for both pathwise and score function estimators in Figure 10. 9. Discussion and Extensions 9.1 Testing Gradient Implementations Implementing Monte Carlo gradient estimators can be an error-prone process, so it is important to verify the correctness of the implemented estimator. We consider unbiased gradient estimators, so a good check is to compare the Monte Carlo estimate using a large number of samples to the exact gradient in a test problem for which the analytic solution is known. As basic tests: if the mean µ of the measure is known, we can use f (x) = x ; if the covariance matrix of the measure is i also known, we can pick f (x) = (x µ )(x µ ). As an example, for the Gamma distribution i i j j p(x; θ) = (x θ, 1), its mean is θ and− the exac− t gradient E [x] = 1. Then, we approximate θ p(x|θ) G | ∇ the left hand side using N Monte Carlo samples of a stochastic gradient estimator and compare the result to 1 with some tolerance, choosing N = 104 samples is a good starting point. It is also useful to check that, in expectation over a large number of samples, different unbiased estimators that might be available produce the same gradients for a variety of functions. With this approach, we can also use functions for which we do not know the exact gradient of the expectation. 43
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih For methods that use control variates for variance reduction, it is useful to check that the stochastic estimates of the expectation of the control variate and its gradients are close to their analytically computed values. To assess the variance reduction effects of the control variate, it is useful to also track the variance for the gradient estimator without the control variate (like we do in Figure 10). 9.2 Stochastic Computational Graphs and Probabilistic Programming. Computational graphs (Bauer, 1974) are now the standard formalism with which we represent com- plex mathematical systems in practical systems, providing a modular tool with which to reason about the sequence of computations that transform a problem’s input variables into its outputs; a computational graph defines a directed acyclic graph that describes a sequence of computations, from inputs and parameters to final outputs and costs. This is the framework in which automatic differentiation systems operate. Computational graphs allow us to automatically track dependen- cies between individual steps within our computations and then, using this dependency structure, enable the sensitivity analysis of any parameters by backwards traversal of the graph (Baydin et al., 2018; Griewank et al., 1989). In standard computational graphs, all operations are represented by nodes that are deterministic, but in the problems that we have studied, some of these nodes will be random, giving rise to stochastic computational graphs. Stochastic computational graphs provide a framework for implementing systems with both deter- ministic and stochastic components, and efficiently and automatically computing gradients using the types of estimators that we reviewed in this paper (Kucukelbir et al., 2017; Schulman et al., 2015). With such a framework it is easy to create complex probabilistic objects that mix differentiable and non-differentiable cost functions, and that use variables with complex dependencies. Simpler approaches for rich computational graphs of this type first approximate all intractable integrals using the Laplace approximation and apply automatic differentiation to the resulting approxima- tion (Fournier et al., 2012). Stochastic variational message passing (Paquet and Koenigstein, 2013; Winn and Bishop, 2005) uses mean-field approximations, bounding methods, and natural parame- terisations of the distributions involved, to create another type of stochastic computational graph that uses more detailed information about the structural properties of the cost and the measure. Seeking greater automation, Schulman et al. (2015) exploit the score-function and pathwise deriva- tive estimators to automatically provide gradient estimates for non-differentiable and differentiable components of the graph, respectively. The framework of Schulman et al. (2015), Kucukelbir et al. (2017), Fournier et al. (2012), Parmas (2018) and others, shows that it is easy to combine the score-function and pathwise estimators and automatically deploy them in situations where they are naturally applicable. And this framework has been further expanded by incorporating methods that include variance reductions within the computational graph, and to allow for higher-order gradients to be computed more easily and with lower variance (Foerster et al., 2018; Mao et al., 2018; Parmas et al., 2018; Weber et al., 2019). There are as yet no automated computational frameworks that include the measure-valued gradients. But as Pflug (1996) mentions, since the weak derivative—and also the sampling mechanism for the posi- tive and negative density components—is known for most of distributions in common use (c.f. Table 1), it is possible to create a computational framework with sets of nodes that have knowledge of their corresponding weak-derivative to enable gradient computation. The score-function estima- tor is implemented in stochastic computational graphs using surrogate nodes and loss functions (c.f. Schulman et al. (2015)), and the measure-valued gradients could be realised through similar representations. 44
Monte Carlo Gradient Estimation in Machine Learning The framework of a stochastic computational graph enables us to more easily explore complex probabilistic systems, since we are freed from creating customised implementations for gradient estimation and optimisation. They also make developing probabilistic programming languages easier: programming languages in which we specify generative and probabilistic descriptions of systems and data, and then rely on automated methods for probabilistic inference. Many of the approaches for automated inference in probabilistic programs, like variational inference, rely on the automated computation of Monte Carlo gradient estimates (Kucukelbir et al., 2017; Ritchie et al., 2016; Wingate et al., 2011). 9.3 Gaussian Gradients The case of Gaussian measures is a widely-encountered special case for gradient estimation. If we know we will deal with a Gaussian then we can derive estimators that exploit its specific properties. Consider the multivariate Gaussian (x µ(θ), Σ(θ)) with mean and variance that are functions N | of parameters θ whose gradients we are interested in. The most common approach for gradient computation is by pathwise estimation using the location-scale transform for the Gaussian, µ + R(cid:15); (cid:15) (0, I) with the decomposition Σ = RR(cid:62): ∼ N E [f (x)] = E (cid:2) γ(cid:62) µ + Tr((cid:15)γ(cid:62) R)(cid:3) = E (cid:2) γ(cid:62) µ + γ(cid:62) R(cid:15)(cid:3) ∇θi N (x|µ(θ),Σ(θ)) N ((cid:15)|0,I) ∇θi ∇θi N ((cid:15)|0,I) ∇θi ∇θi where γ is the gradient of the cost function f evaluated at µ + R(cid:15). Since we are dealing specifically with Gaussian measures, we can instead use Bonnet’s theorem (Bonnet, 1964) and Price’s theorem (Price, 1958; Rezende et al., 2014) to directly compute the gradient with respect to the Gaussian mean and covariance, respectively: E [f (x)] = E [ f (x)] , (79) ∇µi N (x|µ,Σ) N (x|µ,Σ) ∇xi (cid:104) (cid:105) E [f (x)] = 1 E 2 f (x) , (80) ∇Σi,j N (x|µ,Σ) 2 N (x|µ,Σ) ∇xi,xj E [f (x)] = E (cid:2) γ(cid:62) µ + 1 Tr (H Σ)(cid:3) , (81) ∇θi N (x|µ(θ),Σ(θ)) N (x|µ(θ),Σ(θ)) ∇θi 2 ∇θi where γ and H are the gradient and Hessian of the cost function f (x). Using Price’s theorem is computationally more expensive, since it requires knowledge of the Hessian, which incurs a cubic computational cost, and will not be scalable in high-dimensional problems. By comparison, the location-scale path has linear cost in the number of parameters, but at the expense of higher variance. We can also extend Price’s theorem (80) to the second order (Fan et al., 2015): 2 E [f (x)] = 1 E (cid:2) f (x)(cid:3) , (82) ∇Σij,Σkl N (x|µ,Σ) 4 N (x|µ,Σ) ∇xi,xj,xk,xl where i, j, k, l are indices on the dimensionality of the parameters space; Σ is the (i, j)th element ij of the covariance matrix Σ and x is the ith element of the vector x. This shows that while we can i compute gradients in a form suitable for higher-order gradients, for Gaussian measures, this comes at the cost of needing up to 4th-order differentiability of the cost function. Additional types of gradient estimators that work specifically for the case of Gaussian measures are developed by Jankowiak and Karaletsos (2019), and explored for measure-valued derivatives by Heidergott et al. (2008). 45
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih 9.4 Gradients Estimators using Optimal Transport An interesting connection to the pathwise gradient estimators was identified by Jankowiak and Ober- meyer (2018) using the tools of optimal transport. Because probabilities are themselves conserved quantities, we will find that the probabilities we use, as we vary their parameters, must satisfy the continuity equation (Santambrogio, 2015, sect. 4.1.2): p(x; θ) + div (p(x; θ)υ(θ )) = 0, (83) ∇θi x i where div is the divergence operator with respect to variables x, and υ(θ ) is a velocity field x i for the parameter θ ; there is a different velocity field for each parameter θ for i = 1, . . . , D. In i i univariate cases, the velocity field υ(θ ) can be obtained by solving the continuity equation to obtain i υ(θ) = ∇θF (x;θ) , where F (x; θ) is the cumulative distribution function of p. Using this definition, − p(x;θ) the gradient (2) for the ith parameter can be computed as: η = E [f (x)] = E [ f (x)υ(θ )] , (84) i ∇θi p(x;θ) p(x;θ) ∇x i which can be derived by first interchanging the order of differentiation and integration, substituting the density derivative with the expression for the vector field given by the continuity equation and then simplifying the expression using the product rule for divergences. One immediate connection we see is that the solution for the velocity field is exactly the solution for the quantity x that θ ∇ we identified in equation (31c), when we use the cumulative distribution function as the inverse transformation (g−1). The form of the gradient that is identified here was also derived in a very different way by Cong et al. (2019). In the multivariate case we can obtain a continuous set of solutions of the continuity equation and then select the one with the lowest variance (Jankowiak and Karaletsos, 2019; Jankowiak and Obermeyer, 2018). This set of solutions can be equivalently viewed as a set of control variates for the pathwise gradient. A deeper exploration of these connections, and the set of tools that they make available from other areas of mathematics, stands to provide us with many other approaches for gradient estimation. 9.5 Harmonic Gradient Estimators We can add an additional class of gradient estimators to the set we considered by decomposing the stochastic gradient problem (2) in terms of a Fourier series, resulting in the frequency domain or harmonic gradient estimators (Jacobson et al., 1986; Jacobson and Schruben, 1999; V´azquez-Abad and Jacobson, 1994). The intuition is that by oscillating the parameters of the input measures sinusoidally, we can, at different frequencies, observe the sensitivity (gradient) of the cost with respect to the parameters of interest. Consider sinusoidally oscillating the distributional parameters θ of the measure p (x) at various θ frequencies using θ˜ = θ + α sin(2πωt), where ω is referred to as the driving frequency, t is an index parameter, and α is the oscillation amplitude. Using this perturbation of the distributional parameters, we obtain a gradient estimator (V´azquez-Abad and Jacobson, 1994): 2 (cid:90) 1 2 (cid:88)T (θ) = (θ + α sin(2πωt)) sin(2πωt)dt = (θ + α sin(2πωt)) sin(2πωt), (85) θ ∇ F α F αT F 0 t=0 for T , α 0. This approach turns out to be an efficient way of implementing a weighted → ∞ → finite-difference method with sinusoidally varying step-sizes (Jacobson, 1994; V´azquez-Abad and Jacobson, 1994). And this also connects us to many other weighted finite difference schemes that 46
Monte Carlo Gradient Estimation in Machine Learning might be applicable for the problem of gradient estimation, e.g., (Fu et al., 2016). In the context of Monte Carlo gradient estimation, the difficulty in choosing the index, driving frequencies and oscillation amplitudes (Fu, 1994) makes this the least generic of the estimators we have considered; it also has high computational complexity requiring many evaluations of the cost function. Despite the drawbacks of this construction, the general use of Fourier decompositions remains an interesting additional tool with which to exploit knowledge in our estimation problems. 9.6 Generalised Pathwise Gradients and Stein-type Estimators In many statistical problems, the gradient of the entropy of a probabilistic model is an oft-required informational quantity: H[p(x; θ)] = E [log p(x; θ)] = E [ log p(x) x] , (86) θ θ p(x;θ) p(x;θ) x θ ∇ −∇ − ∇ ∇ where we first expanded the definition of the entropy, and then applied the chain rule. Here, the log probability takes the role of the cost function in our exposition. Equation (86) is an expression in the more familiar form E [ f (x) x] that we encountered in the application of the pathwise p(x;θ) x θ ∇ ∇ estimator, particularly in the decoupling of sampling from gradient computation, whether using implicit differentiation (30b) or using the continuity equation (84). To apply the pathwise estimator, we must be able to differentiate the log-probability, which implies that it must be known. This will not always be the case, and one common setting where this is encountered in machine learning is when using likelihood-free or implicit probabilistic models (Mohamed and Lakshminarayanan, 2016). With implicit models, we can simulate data from a distribution, but will not know its log-probability. In such cases, the general strategy we take is to use a substitute function that fills the role of the unknown cost function, like the unknown log- probability, and that gives the gradient information we need, e.g., like the use of a classifier in generative adversarial networks (Goodfellow et al., 2014) or a moment network in the method of moments (Ravuri et al., 2018). In this way, we can create a more generalised pathwise estimator that can be used when the gradient of the cost function is unknown, but that will reduce to the familiar pathwise estimator from section 5 when it is. Stein’s identity gives us a general tool with which to do this. Stein’s identity (Gorham and Mackey, 2015; Liu et al., 2016) tells us that if a distribution p(x) is continuously differentiable then the gradient of its log-probability with respect to the random variable x can be related to the gradient of a set of test functions t(x) by the integral: E [t(x) log p(x) + t(x)] = 0 . (87) p(x) x x K×D ∇ ∇ We assume that test function t(x) has dimension K, the random variable x has dimension D, and that t is in the Stein class of p defined by Gorham and Mackey (2015). By inverting this expression, we can find a general method for evaluating the gradient of a log probability. This expression is applicable to both normalised and unnormalised distributions, making it very general, i.e. we can use Stein’s identity to provide the unknown component f (x). x ∇ By expanding Stein’s identity as a Monte Carlo estimator using several samples, Li and Turner (2017) use kernel substitution to develop a kernelised method for gradient estimation in implicit probabilistic models. Li and Turner (2017) also provide the connections to other methods for estimating gradients of log-probabilities, such as score-matching and denoising auto-encoders. An alternative Stein-type gradient estimator using the spectral decomposition of kernels was developed by Shi et al. (2018), and Stein’s identity was also used in variational inference by Ranganath et al. (2016). 47
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih 9.7 Generalised Score Functions and Malliavin-weighted Estimators Stein’s identity gave us a tool with which to develop generalised pathwise estimators, and in a similar vein we can create a generalised score function estimator. There are many problems where the explicit form of the density needed to compute the score is not known, such as when the density is defined by a continuous-time system or an implicit probabilistic model. In such cases, a generalised form of the score-function estimator (13c) is E [f (x)ψ(x, θ)], i.e. the expected value of the cost p(x;θ) function multiplied by a weighting function ψ. When the weighting function is the score-function we will recover the score-function gradient, and in other cases, it gives us the information needed about the unknown score. One approach for developing generalised score function estimators that is widely used for continuous- time systems in computational finance is through Malliavin calculus, which is the calculus of vari- ations for stochastic processes (Nualart, 2006). Computational finance provides one example of a setting where we will not know the density p(x; θ) explicitly, because the random variables are instead specified as a diffusion process. The generalised gradient methods based on weighting func- tions, called Malliavin weights, allows gradients to be computed in such settings, where the score function is replaced by a Skorohod integral (Chen and Glasserman, 2007). There are equivalent concepts of differentiation (Malliavin derivative) and integration (Skorohod integration) that allow for the interchange of differentiation and integration and the development of unbiased estimators in such settings. We defer to the papers by Fourni´e et al. (1999) and Benhamou (2003) in computa- tional finance, and, closer to machine learning, the papers by Gobet and Munos (2005) and Munos (2006) in optimal control and supervised learning for further exploration of these ideas. 9.8 Discrete Distributions We purposefully limited the scope of this paper to measures that are continuous distributions in their domain. There are many problems, such as those with discrete latent variables and in combinatorial optimisation, where the measure p(x; θ) will instead be a discrete distribution, such as the Bernoulli or Categorical. This is a large area of research in itself, but does have some overlap with our coverage in this paper. While the Bernoulli or Categorical are discrete distributions over their domain, they are continuous with respect to their parameters, which means that both the score- function estimator and the measure-valued derivative are applicable to such estimation problems. Coverage of this discrete distribution setting makes for an interesting review paper in itself, and existing work includes Glasserman and Ho (1991); Gu et al. (2016); Maddison et al. (2016); Tucker et al. (2017). 10. Conclusion We have been through a journey of more than fifty years of research in one of the fundamental problems of the computational sciences: computing the gradient of an expectation of a function. We found that this problem lies at the core of many applications with real-world impact, and a broad swathe of research areas; in applied statistics, queueing theory, machine learning, com- putational finance, experimental design, optimal transport, continuous-time stochastic processes, variational inference, supervised learning, Fourier analysis, causal inference, and representation learning, amongst many others subject areas. The gradients in all these areas fall into one of two 48
Monte Carlo Gradient Estimation in Machine Learning classes, gradients-of-measure or gradients-of-paths. We derived the score-function estimator and the measure-valued gradient estimator as instances of gradients of measure, both of which exploit the measure in the stochastic objective to derive the gradient. And we derived the pathwise estimator that uses knowledge of the sampling path to obtain the gradient. All these methods benefit from variance reduction techniques and we reviewed four approaches for variance reduction we might consider in practice. We further explored the use of these estimators through a set of case studies, and explored some of the other tools for gradient estimation that exist beyond the three principal estimators. 10.1 Guidance in Choosing Gradient Estimators With so many competing approaches, we offer our rules of thumb in choosing an estimator, which follow the intuition we developed throughout the paper: • If our estimation problem involves continuous functions and measures that are continuous in the domain, then using the pathwise estimator is a good default. It is relatively easy to implement and a default implementation, one without other variance reduction, will typically have variance that is low enough so as not to interfere with the optimisation. • If the cost function is not differentiable or a black-box function then the score-function or the measure-valued gradients are available. If the number of parameters is low, then the measure- valued gradient will typically have lower variance and would be preferred. But if we have a high-dimensional parameter set, then the score function estimator should be used. • If we have no control over the number of times we can evaluate a black-box cost function, effectively only allowing a single evaluation of it, then the score function is the only estimator of the three we reviewed that is applicable. • The score function estimator should, by default, always be implemented with at least a basic variance reduction. The simplest option is to use a baseline control variate estimated with a running average of the cost value. • When using the score-function estimator, some attention should be paid to the dynamic range of the cost function and its variance, and to find ways to keep its value bounded within a reasonable range, e.g., transforming the cost so that it is zero mean, or using a baseline. • For all estimators, track the variance of the gradients if possible and address high variance by using a larger number of samples from the measure, decreasing the learning rate, or clipping the gradient values. It may also be useful to restrict the range of some parameters to avoid extreme values, e.g., by clipping them to a desired interval. • The measure-valued gradient should be used with some coupling method for variance reduc- tion. Coupling strategies that exploit relationships between the positive and negative compo- nents of the density decomposition, and which have shared sampling paths, are known for the commonly-used distributions. • If we have several unbiased gradient estimators, a convex combination of them might have lower variance than any of the individual estimators. • If the measure is discrete on its domain then the score-function or measure-valued gradient are available. The choice will again depend on the dimensionality of the parameter space. • In all cases, we strongly recommend having a broad set of tests to verify the unbiasedness of the gradient estimator when implemented. 49
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih 10.2 Future Pathways With all the progress in gradient estimation that we have covered, there still remain many more directions for future work, and there is still the need for new types of gradient estimators. Gradient estimators for situations where we know the measure explicitly, or only implicitly though a diffusion, are needed. New tools for variance reduction are needed, especially in more complex systems like Markov processes. The connections between the score function and importance sampling remains to be more directly related, especially with variance reduction in mind. Coupling was a tool for variance reduction that we listed, and further exploration of its uses could prove fruitful. And there remain opportunities to combine our existing estimators to obtain more accurate and lower variance gradients. The measure-valued derivatives have not been applied in machine learning, and it will be valuable to understand the settings in which they might prove beneficial. Gradient estimators for continuous time settings like the Malliavin-weighted versions, multivariate estimators derived using optimal transport, the use of non-linear control covariates, the use of higher-order gradients, variants using the natural gradient and information geometry, and the utility of Fourier series are among many of the other questions that remain open. (cid:50) Good gradients lead to ever-more interesting applications. Ultimately, there remains much more to do in advancing the principles and practice of Monte Carlo gradient estimation. Acknowledgements We are grateful to many of our colleagues for their invaluable feedback during the preparation of this paper. In particular, we would like to thank Michalis Titsias, Cyprien de Masson d’Autume, Csaba Cspesvari, Fabio Viola, Suman Ravuri, Arnaud Doucet, Nando de Freitas, Ulrich Paquet, Danilo Rezende, Theophane Weber, Dan Horgan, Jonas Degrave, Marc Deisenroth, Louise Deason. Appendix A. Common Distributions The following table provides the definition of the distributions that were mentioned within the paper. Table 2: List of distributions and their densities. Name Domain Notation Probability Density/Mass Function Gaussian R (x µ, σ2) √ 1 exp (cid:16) 1 (cid:0) x−µ (cid:1)2(cid:17) N | 2πσ2 − 2 σ (cid:16) (cid:17) Double-sided Maxwell R (x µ, σ2) √1 (x µ)2 exp (x−µ)2 M | σ3 2π − − 2σ2 Weibull R+ (x α, β, µ) αβ(x µ)α−1 exp( β(x µ)α)1 {x≥0} W | − − − Poisson Z (x θ) exp( θ) (cid:80)∞ θj δ P | − j=0 j! j Erlang R+ r(x θ, λ) λθxθ−1 exp(−λx) E | (θ−1)! Gamma R+ (x α, β) βα xα−1 exp( xβ)1 G | Γ(α) − {x≥0} Exponential R+ (x λ) (1, λ) E | G 50
Monte Carlo Gradient Estimation in Machine Learning References A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. In International Conference on Learning Representations, 2017. F. L. Bauer. Computational graphs and rounding error. SIAM Journal on Numerical Analysis, 11 (1):87–96, 1974. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: A survey. Journal of Machine Learning Research, 18:1–43, 2018. Y. Bengio, E. Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks trainable by backprop. In International Conference on Machine Learning, 2014. E. Benhamou. Optimal Malliavin weighting function for the computation of the greeks. Mathematical Finance: An International Journal of Mathematics, Statistics and Financial Economics, 13(1): 37–53, 2003. P. J. Bickel and K. A. Doksum. Mathematical statistics: basic ideas and selected topics, volume I. CRC Press, 2015. P. Billingsley. Probability and Measure. John Wiley & Sons, 2008. D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859–877, 2017. G. Bonnet. Transformations des signaux al´eatoires a travers les systemes non lin´eaires sans m´emoire. In Annales des T´el´ecommunications, volume 19, pages 203–220. Springer, 1964. A. Buchholz, F. Wenzel, and S. Mandt. Quasi-Monte Carlo variational inference. In International Conference on Machine Learning, 2018. L. Buesing, T. Weber, and S. Mohamed. Stochastic gradient estimation with finite differences. In NIPS2016 Workshop on Advances in Approximate Inference, 2016. L. Capriotti. Reducing the variance of likelihood ratio greeks in Monte Carlo. In Winter Simulation Conference, 2008. C. G. Cassandras and S. Lafortune. Introduction to Discrete Event Systems. Springer Science & Business Media, 2009. K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science, pages 273–304, 1995. N. Chen and P. Glasserman. Malliavin greeks without Malliavin calculus. Stochastic Processes and their Applications, 117(11):1689–1723, 2007. N. Chriss. Black Scholes and beyond: option pricing models. McGraw-Hill, 1996. Y. Cong, M. Zhao, K. Bai, and L. Carin. GO gradient for expectation-based objectives. In Interna- tional Conference on Learning Representations, 2019. M. P. Deisenroth, G. Neumann, J. Peters, et al. A survey on policy search for robotics. Foundations and Trends in Robotics, 2(1–2):1–142, 2013. L. Devroye. Random variate generation in one line of code. In Conference on Winter simulation, 1996. 51
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih L. Devroye. Nonuniform random variate generation. Handbooks in operations research and manage- ment science, 13:83–121, 2006. D. Dua and C. Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ ml. S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo, A. Ruderman, A. A. Rusu, I. Danihelka, K. Gregor, et al. Neural scene representation and rendering. Science, 360 (6394):1204–1210, 2018. K. Fan, Z. Wang, J. Beck, J. Kwok, and K. A. Heller. Fast second order stochastic meta-optimization synthesis for variational inference. In Advances in Neural Information Processing Systems, pages 1387–1395, 2015. M. Figurnov, S. Mohamed, and A. Mnih. Implicit reparameterization gradients. In Advances in Neural Information Processing Systems, 2018. H. Flanders. Differentiation under the integral sign. The American Mathematical Monthly, 80(6): 615–627, 1973. J. Foerster, G. Farquhar, M. Al-Shedivat, T. Rockt¨aschel, E. P. Xing, and S. Whiteson. DiCE: The infinitely differentiable Monte Carlo estimator. In International Conference on Machine Learning, 2018. E. Fourni´e, J.-M. Lasry, J. Lebuchoux, P.-L. Lions, and N. Touzi. Applications of Malliavin calculus to Monte Carlo methods in finance. Finance and Stochastics, 3(4):391–412, 1999. D. A. Fournier, H. J. Skaug, J. Ancheta, J. Ianelli, A. Magnusson, M. N. Maunder, A. Nielsen, and J. Sibert. AD model builder: using automatic differentiation for statistical inference of highly parameterized complex nonlinear models. Optimization Methods & Software, 27(2):233–249, 2012. M. C. Fu. Optimization via simulation: A review. Annals of operations research, 53(1):199–247, 1994. M. C. Fu and J.-Q. Hu. Second derivative sample path estimators for the GI/G/M queue. Manage- ment Science, 39(3):359–383, 1993. M. C. Fu and J.-Q. Hu. Conditional Monte Carlo: Gradient estimation and optimization applica- tions, volume 392. Springer Science & Business Media, 2012. M. C. Fu, B. Heidergott, H. Leahu, and F. Vazquez-Abad. Differentiation via logarithmic expansions. arXiv preprint arXiv:1608.07770, 2016. P. Glasserman. Monte Carlo methods in financial engineering, volume 53. Springer Science & Business Media, 2013. P. Glasserman and Y.-C. Ho. Gradient estimation via perturbation analysis, volume 116. Springer Science & Business Media, 1991. P. W. Glynn. Likelilood ratio gradient estimation: an overview. In Conference on Winter simulation, 1987. P. W. Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10):75–84, 1990. P. W. Glynn and D. L. Iglehart. Importance sampling for stochastic simulations. Management Science, 35(11):1367–1392, 1989. 52
Monte Carlo Gradient Estimation in Machine Learning P. W. Glynn and P. L’Ecuyer. Likelihood ratio gradient estimation for stochastic recursions. Ad- vances in applied probability, 27(4):1019–1053, 1995. P. W. Glynn and R. Szechtman. Some new perspectives on the method of control variates. In Monte Carlo and Quasi-Monte Carlo Methods 2000, pages 27–49. Springer, 2002. P. W. Glynn and W. Whitt. Indirect estimation via l = λw. Operations Research, 37(1):82–103, 1989. E. Gobet and R. Munos. Sensitivity analysis using Itˆo–Malliavin calculus and martingales, and application to stochastic optimal control. SIAM Journal on Control and Optimization, 43(5): 1676–1713, 2005. W.-B. Gong and Y.-C. Ho. Smoothed (conditional) perturbation analysis of discrete event dynamical systems. IEEE Transactions on Automatic Control, 32(10):858–866, 1987. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672–2680, 2014. J. Gorham and L. Mackey. Measuring sample quality with stein’s method. In Advances in Neural Information Processing Systems, pages 226–234, 2015. C. Gourieroux, M. Gourieroux, A. Monfort, and D. A. Monfort. Simulation-based econometric methods. Oxford University Press, 1996. A. Graves. Stochastic meta-optimization synthesis through mixture density distributions. arXiv preprint arXiv:1607.05690, 2016. E. Greensmith, P. L. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in supervised learning. Journal of Machine Learning Research, 5:1471–1530, 2004. A. Griewank et al. On automatic differentiation. Mathematical Programming: recent developments and applications, 6(6):83–107, 1989. G. Grimmett and D. Stirzaker. Probability and random processes. Oxford University Press, 2001. S. Gu, S. Levine, I. Sutskever, and A. Mnih. Muprop: Unbiased meta-optimization synthesis for stochastic neural networks. In International Conference on Learning Representations, 2016. J. Hartford, G. Lewis, K. Leyton-Brown, and M. Taddy. Counterfactual prediction with deep instrumental variables networks. In International Conference on Machine Learning, 2016. N. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, 2015. P. Heidelberger, X.-R. Cao, M. A. Zazanis, and R. Suri. Convergence properties of infinitesimal perturbation analysis estimates. Management Science, 34(11):1281–1302, 1988. B. Heidergott and H. Leahu. Weak differentiability of product measures. Mathematics of Operations Research, 35(1):27–51, 2010. B. Heidergott and F. V´azquez-Abad. Measure-valued differentiation for stochastic processes: the finite horizon case. In EURANDOM report 2000-033, 2000. B. Heidergott, G. Pflug, and F. V´azquez-Abad. Measure-valued differentiation for stochastic systems: from simple distributions to Markov chains. 2003. 53
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih B. Heidergott, F. J. V´azquez-Abad, and W. Volk-Makarewicz. Sensitivity estimation for Gaussian systems. European Journal of Operational Research, 187(1):193–207, 2008. I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Ler- chner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017. Y. Ho and X. Cao. Optimization and perturbation analysis of queueing networks. Journal of Optimization Theory and Applications, 40(4):559–582, 1983. Y.-C. L. Ho and X.-R. Cao. Perturbation analysis of discrete event dynamic systems, volume 145. Springer Science & Business Media, 2012. M. Hoffman and D. Blei. Stochastic structured variational inference. In Artificial Intelligence and Statistics, 2015. A. Honkela and H. Valpola. Variational learning and bits-back coding: an information-theoretic view to Bayesian learning. IEEE Transactions on Neural Networks, 15(4):800–810, 2004. X. Hu, L. Prashanth, A. Gy¨orgy, and C. Szepesv´ari. (bandit) convex optimization with biased noisy gradient oracles. In Artificial Intelligence and Statistics, 2016. T. Jaakkola and M. Jordan. A variational approach to Bayesian logistic regression models and their extensions. In Artificial Intelligence and Statistics, 1997. S. Jacobson, A. Buss, and L. Schruben. Driving frequency selection for frequency domain sim- ulation experiments. Technical report, Cornell University Operations Research and Industrial Engineering, 1986. S. H. Jacobson. Optimal precision analysis of the harmonic gradient estimators. Journal of Optimization Theory and Applications, 80(3):573–590, 1994. S. H. Jacobson and L. Schruben. A harmonic analysis approach to simulation sensitivity analysis. IIE Transactions, 31(3):231–243, 1999. M. Jankowiak and T. Karaletsos. Pathwise derivatives for multivariate distributions. In International Conference on Artificial Intelligence and Statistics, 2019. M. Jankowiak and F. Obermeyer. Pathwise derivatives beyond the reparameterization trick. In International Conference on Machine Learning, 2018. M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183–233, 1999. R. Kapuscinski and S. Tayur. Optimal policies and simulation-based optimization for capacitated production inventory systems. In Quantitative Models for Supply Chain Management, pages 7–40. Springer, 1999. M. E. Khan, G. Bouchard, K. P. Murphy, and B. M. Marlin. Variational bounds for mixed-data factor analysis. In Advances in Neural Information Processing Systems, pages 1108–1116, 2010. D. Kingma and M. Welling. Efficient gradient-based inference through transformations between bayes nets and neural nets. In International Conference on Machine Learning, pages 1782–1790, 2014a. D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014b. 54
Monte Carlo Gradient Estimation in Machine Learning J. P. Kleijnen and R. Y. Rubinstein. Optimization and sensitivity analysis of computer simulation models by the score function method. European Journal of Operational Research, 88(3):413–427, 1996. A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and D. M. Blei. Automatic differentiation variational inference. Journal of Machine Learning Research, 18(1):430–474, 2017. H. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and Applications, volume 35. Springer Science & Business Media, 2003. M. J. Kusner, B. Paige, and J. M. Hern´andez-Lobato. Grammar variational autoencoder. In Pro- ceedings of the 34th International Conference on Machine Learning, volume 70, pages 1945–1954, 2017. P. L’Ecuyer. Note: On the interchange of derivative and expectation for likelihood ratio derivative estimators. Management Science, 41(4):738–747, 1995. W. Lee, H. Yu, and H. Yang. Reparameterization gradient for non-differentiable models. In Advances in Neural Information Processing Systems, pages 5553–5563, 2018. E. L. Lehmann and G. Casella. Theory of Point Estimation. Springer Science & Business Media, 2006. G. Leobacher and F. Pillichshammer. Introduction to quasi-Monte Carlo integration and applications. Springer, 2014. Y. Li and R. E. Turner. Gradient estimators for implicit models. In International Conference on Learning Representations, 2017. T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep supervised learning. arXiv preprint arXiv:1509.02971, 2015. Q. Liu, J. Lee, and M. Jordan. A kernelized stein discrepancy for goodness-of-fit tests. In Interna- tional Conference on Machine Learning, pages 276–284, 2016. I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. C. J. Maddison, A. Mnih, and Y. W. Teh. The Concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations, 2016. J. Mao, J. Foerster, T. Rockt¨aschel, G. Farquhar, M. Al-Shedivat, and S. Whiteson. A better baseline for second order gradient estimation in stochastic computation graphs. 2018. Z. Mark and Y. Baram. The bias-variance dilemma of the Monte Carlo method. In International Conference on Artificial Neural Networks, 2001. B. M. Marlin, M. E. Khan, and K. P. Murphy. Piecewise bounds for estimating bernoulli-logistic latent gaussian models. In ICML, pages 633–640, 2011. N. Metropolis and S. Ulam. The Monte Carlo method. Journal of the American Statistical Associ- ation, 44(247):335–341, 1949. A. Miller, N. Foti, A. D’Amour, and R. P. Adams. Reducing reparameterization gradient variance. Advances in Neural Information Processing Systems, pages 3711–3721, 2017. L. B. Miller. Monte Carlo analysis of reactivity coefficients in fast reactors general theory and applications. Technical report, Argonne National Laboratory, 1967. 55
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. Advances in Neural Information Processing Systems, 2014. S. Mohamed and B. Lakshminarayanan. Learning in implicit generative models. arXiv preprint arXiv:1610.03483, 2016. R. Munos. Policy gradient in continuous time. Journal of Machine Learning Research, 7:771–791, 2006. K. P. Murphy. Machine Learning: A Probabilistic Perspective. MIT press, 2012. C. Naesseth, F. Ruiz, S. Linderman, and D. Blei. Reparameterization gradients through acceptance- rejection sampling algorithms. Artificial Intelligence and Statistics, 2017. C. Newell. Applications of Queueing Theory, volume 4. Springer Science & Business Media, 2013. D. Nualart. The Malliavin Calculus and Related Topics, volume 1995. Springer, 2006. C. J. Oates, M. Girolami, and N. Chopin. Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):695–718, 2017. M. Opper and C. Archambeau. The variational Gaussian approximation revisited. Neural Compu- tation, 21(3):786–792, 2009. A. B. Owen. Monte Carlo theory, methods and examples. 2013. J. Paisley, D. Blei, and M. Jordan. Variational Bayesian inference with stochastic search. Interna- tional Conference in Machine Learning, 2012. O. Papaspiliopoulos, G. O. Roberts, and M. Sk¨old. A general framework for the parametrization of hierarchical models. Statistical Science, 22(1):59–73, 2007. U. Paquet and N. Koenigstein. One-class collaborative filtering with random graphs. In Proceedings of the 22nd international conference on World Wide Web, pages 999–1008. ACM, 2013. P. Parmas. Total stochastic gradient algorithms and applications in supervised learning. In Advances in Neural Information Processing Systems, pages 10204–10214, 2018. P. Parmas, C. E. Rasmussen, J. Peters, and K. Doya. PIPPS: Flexible model-based policy search robust to the curse of chaos. In Proceedings of the 35th International Conference on Machine Learning, 2018. G. C. Pflug. Sampling derivatives of probabilities. Computing, 42(4):315–328, 1989. G. C. Pflug. Optimization of stochastic models: The interface between simulation and optimization, volume 373. Springer Science & Business Media, 1996. R. Price. A useful theorem for nonlinear devices having Gaussian inputs. IRE Transactions on Information Theory, 4(2):69–72, 1958. R. Ranganath. Black Box Variational Inference: Scalable, Generic Bayesian Computation and its Applications. PhD thesis, Princeton University, 2017. R. Ranganath, S. Gerrish, and D. Blei. Black box variational inference. In Artificial Intelligence and Statistics, 2014. R. Ranganath, D. Tran, J. Altosaar, and D. Blei. Operator variational inference. In Advances in Neural Information Processing Systems, pages 496–504, 2016. 56
Monte Carlo Gradient Estimation in Machine Learning S. Ravuri, S. Mohamed, M. Rosca, and O. Vinyals. Learning implicit generative models with the method of learned moments. In International Conference on Machine Learning, 2018. M. I. Reiman and A. Weiss. Sensitivity analysis for simulations via likelihood ratios. Operations Research, 37(5):830–844, 1989. D. Rezende and S. Mohamed. Variational inference with normalizing flows. In International Con- ference on Machine Learning, 2015. D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic meta-optimization synthesis and approximate inference in deep generative models. In International Conference on Machine Learning, 2014. D. Ritchie, P. Horsfall, and N. D. Goodman. Deep amortized inference for probabilistic programs. arXiv preprint arXiv:1610.05735, 2016. H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical statis- tics, pages 400–407, 1951. C. Robert and G. Casella. Monte Carlo statistical methods. Springer Science & Business Media, 2013. R. Y. Rubinstein. Some Problems in Monte Carlo Optimization. PhD thesis, 1969. R. Y. Rubinstein. Monte Carlo optimization, simulation and sensitivity of queueing networks. John Wiley & Sons, 1986. R. Y. Rubinstein. Sensitivity analysis of discrete event systems by the push out method. Annals of Operations Research, 39(1):229–250, 1992. R. Y. Rubinstein and A. Shapiro. Discrete event systems: Sensitivity analysis and stochastic opti- mization by the score function method. John Wiley & Sons Inc, 1993. R. Y. Rubinstein, A. Shapiro, and S. Uryas´ev. The score function method. Encyclopedia of Man- agement Sciences, 1996. Y. R. Rubinstein and J. Kreimer. About one Monte Carlo method for solving linear equations. Mathematics and Computers in Simulation, 25(4):321–334, 1983. F. R. Ruiz, M. Titsias, and D. Blei. The generalized reparameterization gradient. Advances in Neural Information Processing Systems, 2016. T. Salimans and D. A. Knowles. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4):837–882, 2013. F. Santambrogio. Optimal transport for applied mathematicians. Springer, 2015. J. Schulman, N. Heess, T. Weber, and P. Abbeel. Gradient estimation using stochastic computation graphs. In Advances in Neural Information Processing Systems, 2015. B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. De Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016. J. Shi, S. Sun, and J. Zhu. A spectral approach to gradient estimation for implicit distributions. arXiv preprint arXiv:1806.02925, 2018. P. Siekman. New victories in the supply-chain revolution still looking for ways to tighten shipping, inventory, and even manufacturing costs at your company? here’s how four masters of supply- chain efficiency are doing it. Fortune Magazine, 2000. URL https://money.cnn.com/magazines/ fortune/fortune_archive/2000/10/30/290626/index.htm. 57
S. Mohamed, M. Rosca, M. Figurnov, A. Mnih D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484, 2016. S. M. Stigler et al. The epic story of maximum likelihood. Statistical Science, 22(4):598–620, 2007. R. Suri and M. A. Zazanis. Perturbation analysis gives strongly consistent sensitivity estimates for the M/G/1 queue. Management Science, 34(1):39–64, 1988. R. S. Sutton and A. Barto. supervised learning: An Introduction. MIT Press, 1998. R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for rein- forcement learning with function approximation. In Advances in Neural Information Processing Systems, 2000. M. Titsias and M. L´azaro-Gredilla. Doubly stochastic variational Bayes for non-conjugate inference. In International Conference on Machine Learning, 2014. M. Titsias and M. L´azaro-Gredilla. Local expectation gradients for black box variational inference. In Advances in Neural Information Processing Systems, 2015. G. Tucker, A. Mnih, C. J. Maddison, J. Lawson, and J. Sohl-Dickstein. REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models. In Advances in Neural Information Processing Systems, 2017. F. J. V´azquez-Abad. A course on sensitivity analysis for gradient estimation of DES performance measures. In Discrete Event Systems, pages 3–28. Springer, 2000. F. J. Va´zquez-Abad and S. H. Jacobson. Application of RPA and the harmonic gradient estimators to a priority queueing system. In Winter simulation Conference, 1994. C. J. Walder, R. Nock, C. S. Ong, and M. Sugiyama. New tricks for estimating gradients of expectations. arXiv preprint arXiv:1901.11311, 2019. C. Wang, X. Chen, A. J. Smola, and E. P. Xing. Variance reduction for stochastic gradient opti- mization. In Advances in Neural Information Processing Systems, 2013. L. Weaver and N. Tao. The optimal reward baseline for gradient-based supervised learning. In Uncertainty in Artificial Intelligence, pages 538–545, 2001. T. Weber, N. Heess, L. Buesing, and D. Silver. Credit assignment techniques in stochastic compu- tation graphs. In Artificial Intelligence and Statistics, 2019. R. J. Williams. Simple statistical gradient-following algorithms for connectionist supervised learning. supervised learning, pages 5–32, 1992. J. T. Wilson, F. Hutter, and M. P. Deisenroth. Maximizing acquisition functions for bayesian optimization. In Advances in Neural Information Processing Systems, 2018. D. Wingate and T. Weber. Automated variational inference in probabilistic programming. arXiv preprint arXiv:1301.1299, 2013. D. Wingate, N. Goodman, A. Stuhlmu¨ller, and J. M. Siskind. Nonstandard interpretations of prob- abilistic programs for efficient inference. In Advances in Neural Information Processing Systems, 2011. J. Winn and C. M. Bishop. Variational message passing. Journal of Machine Learning Research, 6 (Apr):661–694, 2005. 58
Monte Carlo Gradient Estimation in Machine Learning M. Xu, M. Quiroz, R. Kohn, and S. A. Sisson. Variance reduction properties of the reparameteriza- tion trick. arXiv preprint arXiv:1809.10330, 2018. 59
