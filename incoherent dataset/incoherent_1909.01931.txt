Efron-Stein PAC-Bayesian Inequalities Ilja Kuzborskij and Csaba Szepesvári DeepMind Abstract We prove semi-empirical concentration inequalities for random variables which are given as possibly nonlinear functions of independent random variables. These inequalities describe concentration of random variable in terms of the data/distribution-dependent Efron-Stein (ES) estimate of its variance and they do not require any additional assumptions on the moments. In particular, this allows us to state semi- empirical Bernstein type inequalities for general functions of unbounded random variables, which gives user-friendly concentration bounds for cases where related methods (e.g. bounded differences) might be more challenging to apply. We extend these results to Efron-Stein PAC-Bayesian inequalities which hold for arbitrary probability kernels that define a random, data-dependent choice of the function of interest. Finally, we demonstrate a number of applications, including PAC-Bayesian generalization bounds for unbounded loss functions, empirical Bernstein type generalization bounds, new truncation-free bounds for off-policy evaluation with Weighted Importance Sampling (WIS), and off-policy PAC-Bayesian learning with WIS. 1 Introduction In the following we will be concerned with bounds on the upper tail probability of ∆ = f (S) − E[f (S)] , where S = (X , X , . . . , X ) composed from independent random elements is distributed according to some 1 2 n probability measure D ∈ M (Z)1, and Z = Z × · · · × Z is some space and f : Z → R is either a fixed 1 1 n measurable function, or is a function that is randomly chosen as a function of S. We first consider a simpler case of such concentration inequalities, when f is a fixed function and the user can choose from a number of different ways to study behavior of ∆ (see [Boucheron et al., 2013] for a comprehen- sive survey on the topic). Perhaps the most popular two methods used in learning theory are the martingale method [Azuma, 1967, McDiarmid, 1998] and the information-theoretic entropy method [Boucheron et al., 2003, Maurer, 2019]. Both of these give many well-known and useful inequalities: the first family includes the celebrated Azuma-Hoeffding and so-called bounded-differences inequalities popularized by McDiarmid [1998], while the second family is mostly known for powerful exponential Efron-Stein inequality which allows to state many prominent concentration inequalities as its special case (for instance, inequalities for self-bounding functions and Talagrand’s convex distance inequality). 1We use notation M (A) to denote a family of probability measures supported on a set A. 1 1 0202 beF 3 ]GL.sc[ 2v13910.9091:viXra
Roughly speaking, a common feature of both families is that they relate concentration of ∆ around zero to the sensitivity of f to coordinatewise perturbations, expressed through the Efron-Stein (ES) variance proxy (cid:34) n (cid:12) (cid:35) (cid:88) (cid:12) V ES = E (f (S) − f (S(k)))2 (cid:12) S , (1) + (cid:12) (cid:12) k=1 where (s) = max {0, s} and notation S(k) means that the kth element of S is replaced by X(cid:48) , where + k S(cid:48) = (X(cid:48) , X(cid:48) , . . . , X(cid:48) ) is an independent copy of S. 1 2 n For example, an inequality closely related to the well-known bounded-differences (or McDiarmid’s) inequality follows from a conservative upper bound on (1): assuming that V ES ≤ c almost surely for some positive constant c, one has (cid:16) √ (cid:17) P |∆| ≤ 2cx ≥ 1 − e−x , x ≥ 0 . This inequality is of course rather pessimistic since it neglects information about moments of ∆. A tangible step forward in proving less conservative inequalities was done in the context of so-called entropy method. In particular, one of the central achievements of the entropy method is the following ‘exponential ES inequality’:2 (cid:104) (cid:105) λ (cid:104) (cid:105) ln E eλ∆ ≤ ln E eλV ES , λ ∈ (0, 1) . (2) 1 − λ This inequality bounds the Moment-Generating Function (MGF) of ∆ through the MGF of its ES variance proxy, implying that by controlling the latter, one can obtain tail bounds involving moments. For instance, if for any choice of the distribution D, f satisfies V ES ≤ af (S) + b for some constants a, b > 0, it is called a weakly self-bounding function and we can employ Eq. (2) to show that the first moment of f and constants a, b control the tail behavior of ∆: (cid:16) (cid:112) (cid:17) P ∆ ≤ 2 (a E[f (S)] + b) x + 2ax ≥ 1 − e−x , x ≥ 0 . (3) Thus, whenever a is decreasing in n (for instance, when f is an average of random variables), one gets a dominating lower-order term when the first moment is small enough. This behavior is reminiscent of the classical Bernstein’s inequality and proved to be useful in a number of applications, such as generalization bounds with localization [Bartlett et al., 2002, Srebro et al., 2010, Catoni, 2007] and empirical Bernstein-type inequalities [Maurer and Pontil, 2009, Tolstikhin and Seldin, 2013]. It is natural to ask whether we can get similar inequalities with higher order moments. Indeed, a recent line of work by Maurer [2019], Maurer and Pontil [2018] introduced Bernstein-type concentration inequalities for general functions where in place of the variance, one has an expected ES variance proxy (note that one still controls the variance of f indirectly thanks to the Efron-Stein inequality Var(f ) ≤ E[V ES]). However, this comes at a cost of controlling the first and the second moments of V ES. Formally, if for any choice of distribution D, f satisfies3 max f (S) − E [f (S)] ≤ b , sup (cid:88) (cid:16) (cid:0) f (s) − f (s(k))(cid:1) − (cid:0) f (s(j)) − f (s(k,j))(cid:1)(cid:17)2 ≤ a2 −k k∈[n] s,s(cid:48)∈Z k,j:k(cid:54)=j 2Recall that the concentration inequality follows from the Chernoff bound, i.e. P(∆ ≥ t) ≤ inf E[eλ∆−λt]. λ∈(0,1) 3 Subscript −k in E [·] stands for conditioning on everything except for X . −k k Notation s(k) stands for replacement of kth element of s with the kth element of s(cid:48) and (k, j) stands for replacement of both kth and jth elements with their counterparts in s(cid:48). 2
almost surely for some a, b > 0, we have a tail bound P (cid:16) ∆ ≤ (cid:112) 2 E [V ES] x + (cid:0)√ 2a + 2 b(cid:1) x(cid:17) ≥ 1 − e−x x ≥ 0 . (4) 3 Thus, to have a Bernstein-type behavior of the bound, a and b should be of a lower order. Note the connection of Eq. (4) to the exponential ES inequality 2: the condition on f outlined above is sufficient to control the second and higher order moments of V ES, thus giving a concentration inequality for ∆. Despite their generality, all of these bounds implicitly control moments of V ES, which makes them difficult to apply in some cases. The pair (a, b) cannot depend on the sample and typically one would require boundedness of f to obtain a well-behaved (a, b). One way to avoid these limitations is to revisit exponential ES inequality and analyze MGF of ES variance proxy in an application-specific way (for example, to assume a subexponential behavior of V ES) [Abou-Moustafa and Szepesvári, 2019]. However, in general this requires knowledge of additional parameters (such as scale and variance factor of the underlying subexponential distribution). 1.1 Our Contribution Semi-empirical Efron-Stein Inequalities. In this paper we prove concentration inequalities without mak- ing apriori assumptions on the moments of the ES variance proxy and instead we state bounds on the upper tail probability in terms of the semi-empirical ES variance proxy n (cid:88) (cid:104) (cid:12) (cid:105) V = E (f (S) − f (S(k)))2 (cid:12) X , . . . , X . (5) (cid:12) 1 k k=1 Note that V is semi-empirical since it depends on both distribution D and sample S. Another property of V is that it is asymmetric w.r.t. the sample S — in general V depends on the order of elements in the sequence (X , X , . . . , X ), due to conditional expectation. However, as we discuss in the following section (see 1 2 n applications), this does not affect sums and weakly affects normalized sums. Our first result (Theorem 1) gives an exponential bound (cid:32) (cid:114) (cid:33) (cid:16) (cid:112) (cid:17) P |∆| ≤ 2(V + y) 1 + ln( 1 + V /y) x ≥ 1 − e−x , x ≥ 2, y > 0 . (6) This inequality does not require boundedness of random variables, nor of f — the only crucial assumption is independence of elements in S from each other. Observe that Eq. (6) essentially depends on V and a positive free parameter y, which must be selected by the user. For instance, a problem agnostic choice of y = 1/n2 for any x ≥ 2 gives us w.p. (with probability) at least 1 − e−x, (cid:114) (cid:16) (cid:112) (cid:17) |∆| ≤ 2(V + 1/n2) 1 + ln( 1 + n2V ) x . This recovers a Bernstein-type behavior, that is, the dominance of the lower-order term whenever V (a variance proxy) is small enough. The price we pay for such a simple choice of y is a logarithmic term. In general, one can achieve even sharper bound if the range of V is known (or can be guessed) — in this case, we can take a union bound over some discretized range of y, and select y minimizing the bound. In addition, we show a version of the bound that does not involve y and thus it is scale-free. This version of our inequality, however, depends on E[V ]: (cid:16) (cid:112) (cid:17) √ P |∆| ≤ 2 (V + E[V ])x ≥ 1 − 2e−x , x ≥ 0 . 3
PAC-Bayesian Semi-Empirical Efron-Stein Inequalities. So far we have presented concentration in- equalities which hold for fixed functions f . However, in many learning-theoretic applications we are interested in concentration w.r.t. the class of functions, for example when f potentially depends on the data. In the following we extend our results to the class F (Θ) ≡ {f : Z → R | ϑ ∈ Θ}, where Θ is some param- ϑ eter space. We focus on the stochastic, PAC-Bayesian model, where functions are parameterized by a random variable θ ∼ pˆ given some probability kernel pˆ from Z to Θ 4. For example, in the statistical learning S setting, the predictor is parameterized by θ sampled from pˆ called the posterior, while f represents an S θ empirical loss of the predictor (we discuss this in the upcoming section). In particular, defining a θ-dependent deviation (cid:90) ∆ = f (S) − f (s) D(ds) θ θ θ and a semi-empirical ES variance proxy n (cid:88) (cid:104) (cid:12) (cid:105) V = E (f (S) − f (S(k)))2 (cid:12) θ, X , . . . , X , θ θ θ (cid:12) 1 k k=1 we show (in Theorem 3, Section 4.2) that for an arbitrary probability kernel pˆ from Z to Θ and an arbitrary probability measure p0 on Θ called the prior, w.p. at least 1 − e−x for any x ≥ 2, y > 0, (cid:114) (cid:16) (cid:16)(cid:112) (cid:17)(cid:17) | E[∆ | S]| ≤ 2 (E[V | S] + y) KL (pˆ || p0) + x + x ln 1 + E[V | S]/y , (7) θ θ S θ where the Kullback-Liebler (KL) divergence between pˆ and p0 (assuming that pˆ (cid:28) p0) captures the S S effective capacity of Θ under respective measures. Similarly as before, we also have a y-free version, which holds w.p. at least 1 − 2e−x for any x ≥ 0: (cid:112) | E[∆ | S]| ≤ 2(E[V ] + E[V | S]) (KL (pˆ || p0) + 2x) . θ θ θ S Once again, these results do not require boundedness of random variables, nor of f ∈ F (Θ), and the concentration is essentially controlled by the expected variance-proxy E[V | S]. Next, we discuss several θ specializations of our results and note several key connections to the literature on the PAC-Bayesian analysis. 1.2 Applications Now we discuss several applications of our inequalities. Throughout this section we assume that inequalities hold for any x ≥ 2 and any y > 0, unless stated otherwise. Generalization bounds. PAC-Bayesian literature often discusses bounds on the generalization gap, which is a special case covered by our results. In this scenario, f is defined as an average of some non-negative θ loss functions, incurred by the predictor of interest parameterized by θ on a given example. Here, θ is sampled from the posterior pˆ (a density over the parameter space Θ) chosen by the learner. In particular, let S Z = · · · = Z and let (cid:96) : Θ × Z → R be some loss function with co-domain R ⊆ R . Then, we define 1 n 1 + the population loss and the empirical loss as n 1 (cid:88) L(θ) = E[(cid:96)(θ, X(cid:48) )] and Lˆ (θ) = (cid:96)(θ, X ) , 1 S n k k=1 4Given z ∈ Z, we will use abbreviation pˆ (·) to denote pˆ(·|z). z 4
respectively, and taking f (S) = Lˆ (θ), the generalization gap is defined as ∆ = Lˆ (θ) − L(θ). θ S θ S The vast majority of PAC-Bayesian literature, e.g. [McAllester, 1998, Seeger, 2002, Langford and Shawe- Taylor, 2003, Maurer, 2004] assume that the loss function is bounded, i.e. w.l.o.g. R ≡ [0, 1]. In such case, V ≤ 1/n and taking y = 1/n, Eq. (7) immediately implies that w.p. at least 1 − e−x, θ (cid:114) 1 (cid:16) √ (cid:17) | E[∆ | S]| ≤ 2 KL (pˆ || p0) + x(1 + ln( 2)) . θ S n √ This basic corollary tightens classical results by replacing term ln(2 n) with a universal constant, but slightly looses in terms of a multiplicative constant. The technical assumption on boundedness of the loss is not easy to avoid and the usual way to circumvent this would be to resort to a sub-exponential behavior of the relevant quantities, such as an empirical loss or a generalization gap [Alquier et al., 2016, Germain et al., 2016]. Recently, few works have also looked into the PAC-Bayesian analysis for heavy-tailed losses: Alquier and Guedj [2018] proposed a polynomial moment-dependent bound with f -divergence, while Holland [2019] devised an exponential bound which assumes that the second (uncentered) moment of the loss is bounded by a constant. Here, without any of those assumptions, we obtain (Corollary 1) a high-probability semi-empirical general- ization bound for unbounded loss functions 5 (R ≡ [0, ∞)), (cid:118) (cid:117) (cid:34) n (cid:12) (cid:35) | E[∆ θ | S]| =O(cid:101) (cid:117) (cid:116) n1 E n1 (cid:88) (cid:16) (cid:96)(θ, X k)2 + (cid:96)(θ, X k(cid:48) )2(cid:17) (cid:12) (cid:12) (cid:12) S KL(pˆ S || p0) + n1 , (8) (cid:12) k=1 where E[V θ | S] ≤ (1/n2) E (cid:2)(cid:80)n k=1 (cid:96)(θ, X k)2 + (cid:96)(θ, X k(cid:48) )2 (cid:12) (cid:12) S(cid:3) . This result is close in spirit to the localized bounds of Catoni [2007], Langford and Shawe-Taylor [2003], Tolstikhin and Seldin [2013]: for a small variance proxy (here sum of squared losses) we get the dominance of a lower-order term 1/n. Finally, while the bound of Eq. (8) is semi-empirical (note that we condition only on X ), by additionally k assuming boundedness of the loss, it implies a fully empirical result (Theorem 5): (cid:118) | E[∆ θ | S]| =O(cid:101) (cid:117) (cid:117) (cid:116) n1 E (cid:34) n1 (cid:88)n (cid:96)(θ, X k)2 (cid:12) (cid:12) (cid:12) (cid:12) S(cid:35) KL (pˆ S || p0) + KL (cid:0) pˆ nS || p0(cid:1) + n1 . (cid:12) k=1 Such empirical Bernstein bounds [Audibert et al., 2007, Maurer and Pontil, 2009] in PAC-Bayesian setting were first investigated by Tolstikhin and Seldin [2013]. The bound we present here is similar to the one of Tolstikhin and Seldin [2013], but slightly differs since we consider the sum of squared losses (with co-domain [0, 1]) rather than the sample variance. Nevertheless, we recover a similar behavior, that is a “fast” order (1 + KL(pˆ || p0))/n for the small enough variance proxy. S Off-policy Evaluation with Weighted Importance Sampling (WIS). Consider the stochastic decision making model where the pair of random variables called the action-reward pair is distributed according to some unknown joint probability measure D ∈ M ([K] × [0, 1]). In such model, also known as a stochastic 1 bandit feedback model (see [Lattimore and Szepesvári, 2018] for a detailed treatment on the subject), an agent takes action A by sampling it from a discrete probability distribution called the target policy π ∈ M ([K]) 1 and observes a realization of reward R ∼ D(· | A). In the off-policy setting of this model, the learner only 5Notation =O(cid:101) suppresses universal constants and logarithmic factors, while =O supresses only universal constants. 5
observes a tuple of actions and rewards S = ((A , R ), . . . , (A , R )) generated by sampling each action 1 1 n n A from another fixed discrete probability distribution π ∈ M ([K]) called the behavior policy, while i b 1 corresponding rewards are distributed as R ∼ D(· | A ). k k In the off-policy evaluation problem, our goal is to estimate an expected reward, or the value of a fixed target policy π, (cid:88) v(π) = π(a) E[R|A = a] , a∈[K] by relying on S, where actions and rewards are collected by policy π . Since observations are collected by b another policy, we face a distribution mismatch problem and an estimator of the value is typically designed using a variant of an importance sampling, while aiming to maintain a good bias and variance trade-off. In this paper we study Weighted Importance Sampling (WIS) (or self-normalized importance sampling) estimator (cid:80)n W R vˆWIS(π) = k=1 k k (cid:80)n W i=1 i where importance weights W are defined as a ratio of policies given an action, W = π(A )/π (A ). WIS k k k b k estimator is known for a relatively low variance in practice [Hesterberg, 1995], yet it concentrates well even when importance weights are unbounded, since all of its moments are bounded, which makes it appealing for confidence estimation. In this paper we show a lower bound on the value of the target policy when employing WIS, which partially captures the variance of an estimator. We prove (Theorem 7) that w.p. at least 1 − (n + 1)e−x, (cid:32) (cid:114) (cid:33) v(π) ≥ N x(n) vˆWIS(π) − 2(2V WIS + y) (cid:16) 1 + ln (cid:16)(cid:112) 1 + 2V WIS/y(cid:17)(cid:17) x n + where V WIS = 1 (cid:88)n (cid:0) W 2 + E[W 2](cid:1) and N (n) = (cid:18) n − (cid:113) 2xn E (cid:2) W 2(cid:3)(cid:19) . N (n)2 k k x 1 x k=1 + Here V WIS acts as a variance proxy and can be easily computed since the distribution of the importance weights is known. Note that the bound can be further improved by taking a tighter variance proxy (Theo- rem 6, Proposition 1) at an additional computational cost. Computationally efficient version of V WIS we discuss here states the rate of concentration in terms of the variance of the importance weights. Presented high-probability results do not require boundedness of importance weights, nor any form of truncation, prevalent in the literature on (weighted) importance sampling [Swaminathan and Joachims, 2015, Bottou et al., 2013, Thomas et al., 2015a]. Indeed, it is not hard to see that by truncating the weights we can apply standard concentration inequalities. However, truncation biases the estimator and in practice requires to carefully tune the level of truncation to guarantee a good bias-variance trade-off. We avoid this precisely because our concentration inequalities do not require control of higher moments through boundedness. While another general Bernstein-type concentration inequalities (e.g. Eqs. (3) and (4)) could be used for such problems, they would require truncation of importance weights. Off-policy Learning with WIS. An off-policy learning problem is a natural extension of the evaluation problem discussed earlier. Here, instead of the evaluation of a fixed target policy, our goal is to select a policy from a given class, which maximizes the value. In this paper we propose a PAC-Bayesian lower bound on the value by specializing our ES PAC-Bayesian inequalities. In particular, we consider a class of parametric target policies {π ∈ M ([K]) : ϑ ∈ Θ} for some parameter space Θ. Similarly as before, we assume that ϑ 1 6
the parameter θ is sampled from the posterior pˆ (typically, chosen by the learner after observing the data), S where pˆ is some probability kernel pˆ from ([K] × [0, 1])n to Θ. Note that the probability measure pˆ depends S on the tuple of observed action-reward pairs S generated as described before, and importance weights are now defined w.r.t. the random parameter θ, that is W = π (A )/π (A ). θ,k θ k b k We show (Theorem 8) that for an arbitrary probability kernel pˆ from ([K] × [0, 1])n to Θ and an arbitrary probability measure p0 over Θ, w.p. at least 1 − 2e−x, E[v(π θ) | S] ≥ (cid:18) E[vˆWIS(π θ) | S] − E (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) N n (n) − 1(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) S(cid:21) − (cid:113) 2 (cid:0) y + E[V θWIS | S](cid:1) C x,y(S)(cid:19) θ,x + where the effective capacity of the policy class is represented by the KL divergence in (cid:16)(cid:113) (cid:17) C (S) = KL (cid:0) pˆ || p0(cid:1) + x + x ln 1 + E[V WIS | S]/y , x,y S θ and the variance proxy of an estimator is V WIS = (cid:80)n E[W˜ 2 + U˜ 2 | θ, A , . . . , A ] , θ k=1 θ,k θ,k 1 k where W˜ = W /(W + · · · + W ) , U˜ = W (cid:48) /(W + · · · + W (cid:48) + · · · + W ) . θ,k θ,k θ,1 θ,n θ,k θ,k θ,1 θ,k θ,n Here E[|n/N (n) − 1| | S] captures the bias of an estimator, while V WIS is the variance proxy. Similarly θ,x θ as in case of evaluation, the variance proxy is not fully empirical, however it can be computed exactly since the distribution of importance weights is known (it is given by the behavior policy π and the target policy b π ). The variance proxy presented here is also closely related to the Effective Sample Size (ESS) encountered θ in the Monte-Carlo theory [Owen, 2013, Chap.9], [Elvira et al., 2018], which provides a problem-dependent convergence rate of the WIS estimator. When all importance weights are equal to one (perfectly matching policies), ESS is of order 1/n; on the other hand ESS approaches 1 when importance weights concentrate on a single action. The role of ESS in a variance-dependent off-policy problems was also observed by Metelli et al. [2018], although in a context of polynomial bounds for fixed target policies. Thus, maximizing presented lower bound w.r.t. a (parametric) probability measure pˆ gives a way to S learn a target policy maximizing the value, while maintaining a bias-variance trade-off of WIS estimator. This idea is well-known in the off-policy learning literature — typically this is done through empirical Bernstein bounds by employing importance sampling estimator [Thomas et al., 2015b] and sometimes WIS estimator [Swaminathan and Joachims, 2015], however, all these techniques require some form of weight truncation. As in the off-policy evaluation case, the trade-off between the bias and the variance has to be carefully controlled by tuning the level of truncation. Our results provide an alternative route, free from an additional tuning. A truncation-free uniform convergence bounds for importance sampling were also derived by Cortes et al. [2010], however here we explore a PAC-Bayesian analysis approach. 1.3 Proof Ideas Concentration inequalities shown in this paper to some extent are based on the inequalities for self-normalized estimators by de la Peña et al. [2008]. In particular, we use inequalities derived through the method of mixtures [de la Peña et al., 2008, Chap. 12.2.1], which hold for the pair of random variables A ∈ R, B > 0 satisfying the condition sup E (cid:2) exp (cid:0) λA − λ2B2/2(cid:1)(cid:3) ≤ 1 . Such random variables are called a canonical λ∈R √ pair and our semi-empirical ES inequalities follow by proving that (∆, V ) indeed forms a canonical pair. We do so by applying exponential decomposition inspired by the proof of the Azuma-Hoeffding’s inequality to the condition stated above, while ∆ is represented by the Doob martingale difference sequence. Note that 7
a similar technique was also applied by Rakhlin and Sridharan [2017] in the context of the self-normalized martingales. Our PAC-Bayesian inequalities follow a more involved argument. As in the classical PAC-Bayesian analysis we start from the Donsker-Varadhan change-of-measure inequality applied to the function f (θ) = λ E[∆ − θ (λ2/2)V | S] for λ ∈ R, and note that the log-MGF of f at the prior parameter is bounded by 1 due to the θ canonical pair argument. The rest of the proof is dedicated to the tuning of λ. One possibility here would be to apply the union bound argument of Seldin et al. [2012], however, since λ is unbounded, this allows us to take a more straightforward path. In particular, we employ the method of mixtures (used in [de la Peña et al., 2008, Chap. 12.2.1] to derive the aforementioned inequalities) to achieve analytic tuning of the bound w.r.t. λ. The idea behind the method of mixtures is to integrate the parameter of interest under some analytically-integrable probability measure. The choice of the Gaussian density with variance y2 (recall that y is a free parameter) and the Gaussian integration w.r.t. λ leads to the concentration inequalities. To the best of our knowledge, this is the first application of the method of mixtures in PAC-Bayesian analysis, which is an alternative technique to analytical (union bound) [Seldin et al., 2012, Tolstikhin and Seldin, 2013] and empirical tuning of λ as in [Thiemann et al., 2016]. Finally, described applications follow from the analysis of the semi-empirical ES variance proxy. In case of the generalization error, such analysis is straightforward — our main bounds are obtained by observing multiple cancellations in Lˆ (θ) − Lˆ (θ). The case of WIS, this comes by the stability analysis of the S S(k) estimator — given the removal of the k-th importance weight, the difference of estimators is bounded by W /(W + · · · + W ). k 1 n 1.4 Additional Related Work Our PAC-Bayesian bounds are related to the martingale bounds of Seldin et al. [2012]. In particular, our results need to be compared to the PAC-Bayes-Bernstein inequality for martingales [Seldin et al., 2012, Theorem 8]. In principle, their inequality could be applied to the Doob martingale difference sequence to prove a concentration bound. However, this would hold only for the bounded difference sequences, restricted family of probability kernels, and would yield inequality with a different ’less empirical’ ES variance proxy (with conditioning up to k − 1 elements in expectation). The technique of Seldin et al. [2012], Tolstikhin and Seldin [2013], Thiemann et al. [2016] at its heart relies on the self-bounding property of the variance proxy to control the log-MGF term arising due to the PAC-Bayesian analysis. This control is possible thanks to inequalities obtained through the entropy method. On the other hand, self-bounding property in these cases holds for a limited range of λ, and the method of mixtures applied in our proofs cannot be used here without introduction of superfluous error terms (because of the clipped Gaussian integration). Another direction in controlling log-MGF, related to the empirical Bernstein inequalities for martingales was explored in the online learning literature [Cesa-Bianchi et al., 2007, Wintenberger, 2017] by linearization of x → ex−x2 . PAC-Bayesian analysis in learning theory is not restricted to the generalization gap discussed in Section 1.2. Several works [Maurer, 2004, Seeger, 2002] have investigated generalization by giving upper bounds on the KL divergence of a Bernoulli variable (assuming that loss function is bounded on [0, 1]), which are clearly tighter than the difference bounds (due to the Pinsker’s inequality). In this paper we forego this setting for the sake of generality, however we suspect that KL-Bernoulli ES bounds can be derived for the bounded loss functions. A number of works have also looked into PAC-Bayesian bounds (and bounds for the closely related Gibbs predictors) on the excess risk E[L(θ) | S] − inf L(ϑ), e.g. [Catoni, 2007, Alquier et al., 2016, Kuzborskij ϑ∈Θ et al., 2019, Grünwald and Mehta, 2019]. A tantalizing line of research would be to investigate the use of our semi-empirical results in the context of an excess risk analysis. 8
Finally, in recent years many works [Dziugaite and Roy, 2017, Neyshabur et al., 2018, Rivasplata et al., 2018, Mhammedi et al., 2019] have observed that PAC-Bayesian bounds tend to give less conservative numerical estimates of the generalization gap for neural networks compared to the alternative techniques based on the concentration of empirical processes. Semi-empirical bounds proposed in this paper offer opportunities for sharpening these results in a data-dependent fashion. Organization. In Section 3 we prove concentration inequalities for the fixed function, where the proof crucially relies on Lemma 1, whose proof deferred to the Appendix A. In Section 4.2 we present the PAC- Bayesian extension of the concentration inequalities and present a major part of its proof, which is one of the main contributions of this paper. Finally, proofs for generalization bounds are presented in Appendix B.1, while proofs for the WIS value bounds are presented in Appendix B.3 and Appendix B.4. 2 Preliminaries O Throughout this paper, we use f = g to indicate that there exists a universal constant C > 0 such that f ≤ Cg holds uniformly over all arguments. We use =O(cid:101) to indicate a version of =O which also supresses logarithmic factors. Notation (s) is used to denote the positive part of the real number s ∈ R. We + use notation M (A) to denote a family of probability measures supported on a set A. If p and q are 1 densities over Θ such that p (cid:28) q and X ∼ p, the Kullback-Liebler (KL) divergence between p and q is defined as KL(p, q) = E (cid:2) ln (cid:0) p(X)/q(X)(cid:1)(cid:3) . We say that a random variable X is ν-subgaussian if ln E (cid:2) eλ(X−E[X])(cid:3) ≤ λ2ν for every λ ∈ R. 3 Semi-Empirical Concentration Inequalities In this section we prove semi-empirical concentration inequalities. Recall that we focus on measurable functions f : Z → R of a random tuple S = (X , X , . . . , X ) ∼ D ∈ M (Z), where elements of S are 1 2 n 1 distributed independently from each other and Z = Z × · · · × Z . In this section we prove bounds on the 1 n upper tail probability of ∆ = f (S) − E[f (S)]. Theorem 1. Let the semi-empirical Efron-Stein variance proxy be defined as n (cid:88) (cid:104) (cid:12) (cid:105) V = E (f (S) − f (S(k)))2 (cid:12) X , . . . , X . (9) (cid:12) 1 k k=1 Then, for any x ≥ 2, with probability at least 1 − e−x and any y > 0, (cid:115) (cid:18) (cid:18) (cid:19)(cid:19) 1 V |∆| ≤ 2(V + y) 1 + ln 1 + x . 2 y √ In addition, for any x > 0, with probability at least 1 − 2e−x we have (cid:112) |∆| ≤ 2 (V + E[V ])x . Note the similarity to the Efron-Stein inequality, which bounds the variance of f (S) with E[V ]. The proof of Theorem 1 combines the argument used in the proof of McDiarmid’s/Azuma-Hoeffding’s inequality with a 9
concentration inequality due to de la Peña et al. [2008]. To state this inequality, recall that a pair (A, B) of random variables is called a canonical pair if B ≥ 0 and (cid:20) (cid:18) λ2 (cid:19)(cid:21) sup E exp λA − B2 ≤ 1 . (10) λ∈R 2 The result of de la Peña et al. shows that if (A, B) is a canonical pair then |A| has a (random) subgaussian behavior with variance proxy B2: Theorem 2 (Theorem 12.4 of de la Peña et al. [2008]). Let (A, B) be a canonical pair. Then, for any t > 0, (cid:32) (cid:33) P (cid:112) B2 +|A (| E[B])2 ≥ t ≤ √ 2e− t 42 . √ In addition, for all t ≥ 2 and y > 0,   P  (cid:16) |A| (cid:16) (cid:17)(cid:17) ≥ t ≤ e− t 22 . (B2 + y) 1 + 1 ln 1 + B2 2 y √ Thus, provided that (∆, V ) is a canonical pair, it is easy to see that Theorem 1 follows from Theorem 2 √ √ applied to (∆, V ). Thus, it remains to be seen that (∆, V ) is a canonical pair, which is established by the following lemma, whose proof is presented in Appendix A: √ Lemma 1. (∆, V ) is a canonical pair. 4 PAC-Bayesian Bounds In this section we prove a PAC-Bayesian version of Theorem 2, which will consequently imply a PAC- Bayesian bound for classes of functions. √ Let Θ denote an index set. We call the collection (∆ , V ) of pairs of random variables indexed by θ √θ θ∈Θ elements of Θ a canonical family, if for each θ ∈ Θ, (∆ , V ) is a canonical pair. In this section we prove θ θ new PAC-Bayesian inequalities for families of canonical pairs that arise as functions of a common random element. Below we let pˆ to denote a probability kernel from Z to Θ. Also, for brevity, given z ∈ Z, we will also use pˆ (·) to denote pˆ(·|z). z √ Theorem 3. For some space Θ, let (∆ , V ) be a canonical family and S ∼ D, jointly distributed with √ θ θ θ∈Θ (∆ , V ) and taking values in Z. Fix an arbitrary probability kernel pˆ from Z to Θ and an arbitrary θ θ θ∈Θ probability measure p0 over Θ, and let θ ∼ pˆ . Then, for any x ≥ 0, with probability at least 1 − 2e−x we S have that (cid:112) | E[∆ | S]| ≤ 2(E[V ] + E[V | S]) (KL (pˆ || p0) + 2x) . (11) θ θ θ S In addition, for all y > 0 and x ≥ 2 with probability at least 1 − e−x we have (cid:115) (cid:18) (cid:18) (cid:19)(cid:19) x 1 | E[∆ | S]| ≤ 2 (y + E[V | S]) KL (pˆ || p0) + x + ln 1 + E[V | S] . (12) θ θ S θ 2 y 10
The proof (given in Appendix A) largely relies on the following theorem, which allows to bound a moment- (cid:113) generating function of a random variable (E[∆ | S]2/ (E[V ] + E[V | S]) − 2KL (pˆ || p0)) . Note that the θ θ θ S + crucial part is to show that this random variable is subgaussian (as show in (14)). Theorem 4. Under the same conditions as in Theorem 3, for any y > 0, we have (cid:34) (cid:35) E y exp (cid:18) E[∆ θ | S]2 − KL (cid:0) pˆ || p0(cid:1)(cid:19) ≤ 1 . (13) (cid:112) y2 + E[V θ | S] 2(y2 + E[V θ | S]) S Furthermore, for all x ≥ 0, (cid:34) (cid:32) (cid:115) (cid:33)(cid:35) (cid:18) E[∆ | S]2 (cid:19) E exp x θ − 2KL (pˆ || p0) ≤ 2ex2 . (14) E[V ] + E[V | S] S θ θ + The proof combines PAC-Bayesian ideas with the method of mixtures as described by de la Peña et al. [2008] [Section 12.2.1]. 4.1 Proof of Theorem 4 We start by applying the following change-of-measure lemma, which is the basis of the PAC-Bayesian analysis. Lemma 2 (Donsker and Varadhan [1975], Dupuis and R. S. Ellis [1997], Gray [2011]). Let p, q be probability measures on Θ and let X ∼ p, Y ∼ q. Then, for any measurable function f : Θ → R we have (cid:104) (cid:105) E[f (X)] ≤ KL(p || q) + ln E ef(Y ) . The lemma with p = pˆ , q = p0, and f (θ) = λ∆ − λ2 V for a fixed S implies S θ 2 θ E (cid:20) λ∆ θ − λ2 V θ (cid:12) (cid:12) (cid:12) S(cid:21) ≤ KL (cid:0) pˆ S || p0(cid:1) + ln E (cid:20) eλ∆ θ0− λ 22 V θ0 (cid:12) (cid:12) (cid:12) S(cid:21) . 2 (cid:12) (cid:12) Exponentiation of both sides gives eE(cid:104) λ∆ θ− λ 22 V θ (cid:12) (cid:12) (cid:12) S(cid:105) −KL(pˆS || p0) ≤ E (cid:20) eλ∆ θ0− λ 22 V θ0 (cid:12) (cid:12) (cid:12) S(cid:21) (cid:12) and taking expectation we have E (cid:20) eE(cid:104) λ∆ θ− λ 22 V θ (cid:12) (cid:12) (cid:12) S(cid:105) −KL(pˆS || p0)(cid:21) ≤ E (cid:20) eλ∆ θ0− λ 22 V θ0 (cid:21) = E (cid:20) E (cid:20) eλ∆ θ0− λ 22 V θ0 (cid:12) (cid:12) (cid:12) θ0(cid:21)(cid:21) ≤ 1 , (cid:12) √ since (∆ , V ) is a canonical pair for a fixed θ0 by assumption. Now we apply the method of mixtures θ0 θ0 √ with respect to the Gaussian distribution. Multiplying both sides by e−λ2y2/2y/ 2π for some y > 0, integrating w.r.t. λ ∈ R, and applying Fubini’s theorem gives E (cid:20) e−KL(pˆS || p0) √y (cid:90) ∞ eλ E[∆ θ | S]− λ 22 E[V θ | S]− λ 22 y2 dλ(cid:21) ≤ 1 . 2π −∞ 11
We perform Gaussian integration and arrive at (cid:34) (cid:35) E y exp (cid:18) E[∆ θ | S]2 − KL (cid:0) pˆ || p0(cid:1)(cid:19) ≤ 1 , (15) (cid:112) y2 + E[V θ | S] 2(y2 + E[V θ | S]) S which finishes the proof of Eq. (13). For the second part, we consider the following standard lemma: Lemma 3. Let U be a nonnegative valued random variable such that a = E (cid:2) exp(U 2/4)(cid:3) is finite. Then, for any x ≥ 0, E [exp(xU )] ≤ aex2 holds. (cid:113) Setting U = (E[∆ | S]2/ (E[V ] + E[V | S]) − 2KL (pˆ || p0)) , the lemma gives Eq. (14) provided that θ θ θ S + we show that E (cid:2) exp (cid:0) U 2/4(cid:1)(cid:3) ≤ 2. For this, let y > 0. Introduce the abbreviations A = y/(cid:112) y2 + E[V | S], θ B = E[∆ | S]2/(2y2 + 2 E[V | S]) − KL (cid:0) pˆ || p0(cid:1) so that E (cid:2) exp (cid:0) U 2/4(cid:1)(cid:3) = E (cid:2) exp((B) /2)(cid:3) . Note θ θ S + that A > 0. By Cauchy-Schwartz, E (cid:2) exp((B) /2)(cid:3) = E (cid:104) exp (cid:0) (B) /2(cid:1) A1/2A−1/2(cid:105) ≤ (cid:113) E (cid:2) A exp (cid:0) (B) (cid:1)(cid:3) (cid:112) E [A−1] . (16) + + + Observe that A ∈ (0, 1] a.s. and that E[A exp(B)] ≤ 1 by Eq. (13). Now, we have (cid:115) (cid:115) (cid:115) (cid:20) 1 (cid:21) (cid:16) (cid:17) (cid:20) 1 (cid:21) (cid:20) 2 (cid:21) E [A exp ((B) )] E = E [A I {B ≥ 0} exp (B)] + E [A I {B < 0}] E ≤ E , + A A A √ and finally, by subadditivity of · and Jensen’s inequality, (cid:115) (cid:20) 2 (cid:21) (cid:118) (cid:117) (cid:117) (cid:34)(cid:115) y2 + E[V | S] (cid:35) (cid:115) (cid:112)E[V ] E = (cid:116)2 E θ ≤ 2 + 2 θ ≤ 2 , A y2 y where the last inequality follows by taking y = (cid:112)E[V ]. Thus, applying Lemma 3 with a = 2 completes the θ proof. 4.2 PAC-Bayesian Efron-Stein Inequalities Now, we apply Theorem 3 to get concentration inequalities for classes of functions. In particular, consider the class of functions parametrized by some space Θ, that is F (Θ) ≡ {f : Z → R | ϑ ∈ Θ}. Furthermore, ϑ + we will assume that θ ∼ pˆ(·|S), where pˆ is a probability kernel from Z to Θ and that θ0 ∼ p0 where p0 ∈ M (Θ) is a probability distribution over Θ. We let S(cid:48) be a random element that shares a common 1 distribution with S and which is independent of (S, θ). Finally, we are interested in bounds on the deviation E[∆ | S], where θ (cid:90) ∆ = f (S) − f (s) D(ds) , θ θ θ which hold simultaneously for any choice of pˆ and p0, and which are controlled by the θ-dependent version S of a semi-empirical Efron-Stein variance proxy n (cid:88) (cid:104) (cid:12) (cid:105) V = E (f (S) − f (S(k)))2 (cid:12) θ, X , . . . , X . θ θ θ (cid:12) 1 k k=1 √ √ Then, by Lemma 1, (∆ , V ) is a canonical pair for any fixed θ0. Hence, (∆ , V ) form a canonical θ0 θ0 θ θ θ∈Θ family and the conclusions of the previous section hold for it. 12
Acknowledgements We are grateful to András György for many insightful comments. References S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A nonasymptotic theory of indepen- dence. Oxford University Press, 2013. K. Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical Journal, Second Series, 19(3):357–367, 1967. C. McDiarmid. Concentration. In Probabilistic methods for algorithmic discrete mathematics, pages 195–248. Springer, 1998. S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities using the entropy method. The Annals of Probability, 31(3):1583–1614, 2003. A. Maurer. A bernstein-type inequality for functions of bounded interaction. Bernoulli, 25(2):1451–1471, 2019. P. L. Bartlett, O. Bousquet, and S. Mendelson. Localized rademacher complexities. In Conference on Computational Learning Theory (COLT). Springer, 2002. N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates. In Conference on Neural Information Processing Systems (NIPS), 2010. O. Catoni. Pac-bayesian supervised classification: The thermodynamics of statistical learning. Lecture Notes-Monograph Series, 56:i–163, 2007. ISSN 07492170. A. Maurer and M. Pontil. Empirical bernstein bounds and sample variance penalization. arXiv:0907.3740, 2009. I. O. Tolstikhin and Y. Seldin. Pac-bayes-empirical-bernstein inequality. In Conference on Neural Information Processing Systems (NIPS), 2013. A. Maurer and M. Pontil. Empirical bounds for functions with weak interactions. In Conference on Computational Learning Theory (COLT), 2018. K. Abou-Moustafa and C. Szepesvári. An exponential tail bound for lq stable learning rules. In Algorithmic Learning Theory (ALT), 2019. D. A. McAllester. Some PAC-Bayesian theorems. In Conference on Computational Learning Theory (COLT), 1998. M. Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classification. Journal of Machine Learning Research, 3(Oct):233–269, 2002. J. Langford and J. Shawe-Taylor. Pac-bayes & margins. In Conference on Neural Information Processing Systems (NIPS), 2003. A. Maurer. A note on the pac bayesian theorem. arXiv:0411099, 2004. 13
P. Alquier, J. Ridgway, and N. Chopin. On the properties of variational approximations of Gibbs posteriors. Journal of Machine Learning Research, 17(1):8374–8414, 2016. P. Germain, F. Bach, A. Lacoste, and S. Lacoste-Julien. Pac-bayesian theory meets bayesian inference. In Conference on Neural Information Processing Systems (NIPS), 2016. P. Alquier and B. Guedj. Simpler pac-bayesian bounds for hostile data. Machine Learning, 107(5):887–902, May 2018. M. Holland. Pac-bayes under potentially heavy tails. In Conference on Neural Information Processing Systems (NeurIPS), 2019. J.-Y. Audibert, R. Munos, and C. Szepesvári. Tuning bandit algorithms in stochastic environments. In Algorithmic Learning Theory (ALT), 2007. T. Lattimore and C. Szepesvári. Bandit algorithms. Cambridge University Press, 2018. T. Hesterberg. Weighted average importance sampling and defensive mixture distributions. Technometrics, 37(2):185–194, 1995. A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learning. In Conference on Neural Information Processing Systems (NIPS), 2015. L. Bottou, J. Peters, J. Quiñonero Candela, D. X. Charles, M. Chickering, E. Portugaly, D. Ray, P. Y. Simard, and E. Snelson. Counterfactual reasoning and learning systems: the example of computational advertising. Journal of Machine Learning Research, 14(1):3207–3260, 2013. P. S. Thomas, G. Theocharous, and M. Ghavamzadeh. High-confidence off-policy evaluation. In Conference on Artificial Intelligence (AAAI), 2015a. A. B. Owen. Monte Carlo theory, methods and examples. 2013. V. Elvira, L. Martino, and C. P. Robert. Rethinking the effective sample size. arXiv:1809.04129, 2018. A. M. Metelli, M. Papini, F. Faccio, and M. Restelli. Policy optimization via importance sampling. In Conference on Neural Information Processing Systems (NeurIPS), 2018. P. S. Thomas, G. Theocharous, and M. Ghavamzadeh. High confidence policy improvement. In International Conference on Machine Learing (ICML), 2015b. C. Cortes, Y. Mansour, and M. Mohri. Learning bounds for importance weighting. In Conference on Neural Information Processing Systems (NIPS), 2010. V. H. de la Peña, T. L. Lai, and Q.-M. Shao. Self-normalized processes: Limit theory and Statistical Applications. Springer Science & Business Media, 2008. A. Rakhlin and K. Sridharan. On equivalence of martingale tail bounds and deterministic regret inequalities. In Conference on Computational Learning Theory (COLT), 2017. Y. Seldin, F. Laviolette, N. Cesa-Bianchi, J. Shawe-Taylor, and P. Auer. Pac-bayesian inequalities for martingales. IEEE Transactions on Information Theory, 58(12):7086–7093, 2012. 14
N. Thiemann, C. Igel, O. Wintenberger, and Y. Seldin. A strongly quasiconvex pac-bayesian bound. Algorithmic Learning Theory (ALT), 2016. N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. Machine Learning, 66(2-3):321–352, 2007. O. Wintenberger. Optimal learning with bernstein online aggregation. Machine Learning, 106(1):119–141, 2017. I. Kuzborskij, N. Cesa-Bianchi, and C. Szepesvári. Distribution-dependent analysis of gibbs-erm principle. In Conference on Computational Learning Theory (COLT), 2019. P. D. Grünwald and N. A. Mehta. A tight excess risk bound via a unified pac-bayesian–rademacher–shtarkov– mdl complexity. In Algorithmic Learning Theory (ALT), 2019. G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Uncertainty in Artificial Intelligence (UAI), 2017. B. Neyshabur, S. Bhojanapalli, and N. Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations (ICLR), 2018. O. Rivasplata, E. Parrado-Hernandez, J. S. Shawe-Taylor, S. Sun, and C. Szepesvári. Pac-bayes bounds for stable algorithms with instance-dependent priors. In Conference on Neural Information Processing Systems (NeurIPS), 2018. Z. Mhammedi, P. D. Grünwald, and B. Guedj. Pac-bayes un-expected bernstein inequality. In Conference on Neural Information Processing Systems (NeurIPS), 2019. M. D. Donsker and S. S. Varadhan. Asymptotic evaluation of certain Markov process expectations for large time. Communications on Pure and Applied Mathematics, 28, 1975. P. Dupuis and R. S. R. S. Ellis. A Weak Convergence Approach to the Theory of Large Deviations. Wiley- Interscience, 1997. R. M. Gray. Entropy and Information Theory. Springer, 2 edition, 2011. A. Maurer. A bound on the deviation probability for sums of non-negative random variables. J. Inequalities in Pure and Applied Mathematics, 4(1):15, 2003. A Proofs for semi-empirical concentration inequalities √ Lemma 1 (restated). (∆, V ) is a canonical pair. Proof. Let E [·] stand for E[· | X , . . . , X ]. The Doob martingale decomposition of f (S) − E[f (S)] gives k 1 k n (cid:88) f (S) − E[f (S)] = D , k k=1 15
where D = E [f (S)]−E [f (S)] = E [f (S)−f (S(k))] and the last equality follows from the elementary k k k−1 k identity E [f (S)] = E [f (S(k))]. k−1 k Observe that n n (cid:88) (cid:88) ∆ = D and V = V k k k=1 k=1 (cid:104) (cid:105) and where V = E (cid:0) f (S) − f (S(k))(cid:1)2 . Assume for now that for k ∈ [n], the inequalities k k (cid:20) (cid:18) λ2 (cid:19)(cid:21) E exp λD − V ≤ 1 (17) k−1 k k 2 hold. Then, using an argument similar to the proof of McDiarmid’s inequality, we get   (cid:20) (cid:18) λ2 (cid:19)(cid:21)  (cid:20) (cid:18) λ2 (cid:19)(cid:21) n (cid:89)−1 (cid:18) λ2 (cid:19) E exp λA − B2 = E E exp λD − V exp λD − V  2  n−1 n 2 n k 2 k    k=1 (cid:124) (cid:123)(cid:122) (cid:125) ≤1    (cid:20) (cid:18) λ2 (cid:19)(cid:21) n (cid:89)−2 (cid:18) λ2 (cid:19) ≤ E E exp λD − V exp λD − V   n−2 n−1 2 n−1 k 2 k    k=1 (cid:124) (cid:123)(cid:122) (cid:125) ≤1 ≤ · · · ≤ 1 . Thus, it remains to prove Eq. (17). For this, fix k ∈ [n] and introduce a Rademacher variable ε ∈ {−1, +1} such that P(ε = 1) = P(ε = −1) = 1 and ε is independent of S, S(cid:48). Let ∆ = f (S) − f (S(k)). Using that 2 k λD − λ2 V = E [λ∆ − λ2 ∆2], we get k 2 k k k 2 k (cid:20) (cid:18) λ2 (cid:19)(cid:21) (cid:20) (cid:18) λ2 (cid:19)(cid:21) E exp λD − V ≤ E exp λ∆ − ∆2 (Jensen’s w.r.t. E ) k−1 k 2 k k−1 k 2 k k (cid:20) (cid:20) (cid:18) λ2 (cid:19) (cid:12) (cid:21)(cid:21) = E E E exp ελ∆ − (ε∆ )2 (cid:12) S, S(cid:48) , (18) k−1 −k k 2 k (cid:12) where we recall that the subscript −k in E [·] stands for conditioning on S without X , and we get the −k k last equality thanks to our independence assumption, that is given X , . . . , X , X , . . . , X , X and 1 k−1 k+1 n k X(cid:48) are identically distributed and hence so are ∆ and −∆ . Since xε is x2/2-subgaussian for x ∈ R, the k k k innermost expectation in Eq. (18) is upper-bounded by one, thus, finishing the proof of Eq. (17) and also the theorem. Lemma 3 (restated). Let U be a nonnegative valued random variable such that a = E (cid:2) exp(U 2/4)(cid:3) is finite. Then, for any x ≥ 0, E [exp(xU )] ≤ aex2 holds. √ Proof. Fix x ≥ 0 and let α > 0. Using the inequality ab ≤ (a2 + b2)/2 with a = x/ 2α we have x √ x2 xU = √ 2αU 2 ≤ + αU 2 . 2α 4α Setting α = 1/4, exponentiating both sides and taking expectations the result follows. 16
√ Theorem 3 (restated). For some space Θ, let (∆ , V ) be a canonical family and S ∼ D, jointly √ θ θ θ∈Θ distributed with (∆ , V ) and taking values in Z. Fix an arbitrary probability kernel pˆ from Z to Θ θ θ θ∈Θ and an arbitrary probability measure p0 over Θ, and let θ ∼ pˆ . Then, for any x ≥ 0, with probability at S least 1 − 2e−x we have that (cid:112) | E[∆ | S]| ≤ 2(E[V ] + E[V | S]) (KL (pˆ || p0) + 2x) . (19) θ θ θ S In addition, for all y > 0 and x ≥ 2 with probability at least 1 − e−x we have (cid:115) (cid:18) (cid:18) (cid:19)(cid:19) x 1 | E[∆ | S]| ≤ 2 (y + E[V | S]) KL (pˆ || p0) + x + ln 1 + E[V | S] . (20) θ θ S θ 2 y Proof. Applying Chernoff’s bounding technique with Eq. (14) gives (cid:32)(cid:115) (cid:33) P (cid:18) E[VE θ][∆ +θ E| [S V] θ2 | S] − 2KL (pˆ S || p0)(cid:19) + ≥ t ≤ 2 xin ≥f 0 ex2−tx = 2e− t 42 . This implies that with probability at least 1 − 2e−x, (cid:18) E[∆ θ | S]2 − 2KL (cid:0) pˆ || p0(cid:1)(cid:19) ≤ 4x . E[V ] + E[V | S] S θ θ + From this, after algebra we get (cid:112) | E[∆ | S]| ≤ (E[V ] + E[V | S]) (2KL (pˆ || p0) + 4x) , θ θ θ S showing Eq. (19). √ Observing that for t ≥ 2 and y > 0, P (cid:18) E[∆ θ | S]2 − KL (cid:0) pˆ || p0(cid:1) ≥ t2 (cid:18) 1 + 1 ln (cid:18) 1 + E[V θ | S] (cid:19)(cid:19)(cid:19) 2(y2 + E[V | S]) S 2 2 y2 θ ≤ P (cid:18) E[∆ θ | S]2 − KL (cid:0) pˆ || p0(cid:1) ≥ t2 + 1 ln (cid:18) 1 + E[V θ | S] (cid:19)(cid:19) 2(y2 + E[V | S]) S 2 2 y2 θ = P (cid:18) E[∆ θ | S]2 − KL (cid:0) pˆ || p0(cid:1) − 1 ln (cid:18) 1 + E[V θ | S] (cid:19) ≥ t2 (cid:19) 2(y2 + E[V | S]) S 2 y2 2 θ (cid:34)(cid:115) (cid:35) ≤ E E[V |y S2 ] + y2 exp (cid:18) 2(yE 2 [ +∆ Eθ [| VS]2 | S]) − KL(pˆ S || p0)(cid:19) e− t 22 θ θ ≤ e− t 22 . where the last two inequalities follow from Chernoff bound and Eq. (13). This implies that with probability at least 1 − e−x for all x ≥ 2, E[∆ θ | S]2 − KL (cid:0) pˆ || p0(cid:1) 2(y2+E[V | S]) S θ ≤ x (cid:16) (cid:17) 1 + 1 ln 1 + E[V θ | S] 2 y2 17
and rearranging we get (cid:115) (cid:18) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19) 1 1 | E[∆ | S]| ≤ 2 (y2 + E[V | S]) KL (pˆ || p0) + x 1 + ln 1 + E[V | S] . θ θ S 2 y2 θ This concludes the proof of Eq. (20), which is stated with y in place of y2, which does not change the result since y is a free variable. The proof is now concluded. B Proofs for applications In this sections we discuss some of the implications of our bounds. B.1 Bernstein-type Generalization Bounds for Unbounded Losses gLet Z = · · · = Z and let (cid:96) : Θ × Z → [0, ∞) be some loss function. Recall that the population loss 1 n 1 and the empirical loss is defined as n 1 (cid:88) L(θ) = E[(cid:96)(θ, X(cid:48) )] and Lˆ (θ) = (cid:96)(θ, X ) , 1 S n k k=1 respectively. Then, Theorem 3 with n (cid:20) (cid:12) (cid:21) V θ = (cid:88) E (cid:16) Lˆ S(θ) − Lˆ S(k)(θ)(cid:17)2 (cid:12) (cid:12) (cid:12) θ, X 1, . . . , X k k=1 implies the following semi-empirical PAC-Bayesian generalization bound: Corollary 1. Assume that the elements of S = (X , . . . , X ) ∈ Z are sampled independently from each 1 n other. Let pˆ be a probability kernel from Z to Θ and let p0 ∈ M (Θ) be a probability distribution over Θ. 1 Then, for any x ≥ 2, with probability at least 1 − e−x, we have (cid:115) (cid:12) (cid:104) (cid:12) (cid:105)(cid:12) (cid:18) x (cid:18) 1 (cid:19)(cid:19) (cid:12)E Lˆ (θ) − L(θ) (cid:12) S (cid:12) ≤ 2 (y + E[V | S]) KL (pˆ || p0) + x + ln 1 + E[V | S] (cid:12) S (cid:12) (cid:12) θ S 2 y θ where (cid:34) n (cid:12) (cid:35) 1 (cid:88) (cid:12) E[V | S] ≤ E (cid:96)(θ, X )2 + (cid:96)(θ, X(cid:48) )2 (cid:12) S . (21) θ n2 k k (cid:12) (cid:12) k=1 Proof. According to notation of Section 4.2 we choose f (z) = Lˆ (θ), and note that θ z (cid:20) (cid:90) (cid:12) (cid:21) E[∆ θ | S] = E Lˆ S(θ) − Lˆ s(θ) D(ds) (cid:12) (cid:12) S (cid:12) (cid:104) (cid:105) = E Lˆ (θ) | S − E [L(θ) | S] S 18
√ and that (∆ , V ) form a canonical family as described in Section 4.2. Thus, we can apply Eq. (20) of θ θ θ∈Θ Theorem 3. At the same time, Lˆ (θ) − Lˆ (θ) = 1 ((cid:96)(θ, X ) − (cid:96)(θ, X(cid:48) )), and so S S(k) n k k n (cid:20) (cid:12) (cid:21) V θ = (cid:88) E (cid:16) Lˆ S(θ) − Lˆ S(k)(θ)(cid:17)2 (cid:12) (cid:12) (cid:12) θ, X 1, . . . , X k k=1 n = 1 (cid:88) E (cid:104) (cid:0) (cid:96)(θ, X ) − (cid:96)(θ, X(cid:48) )(cid:1)2 (cid:12) (cid:12) θ, X , . . . , X (cid:105) n2 k k (cid:12) 1 k k=1 (cid:32) n n (cid:33) ≤ 1 (cid:88) (cid:96)(θ, X )2 + (cid:88) E (cid:2) (cid:96)(θ, X(cid:48) )2 | θ(cid:3) , n2 k k k=1 k=1 where the inequality used that X(cid:48) is independent of θ and S. Taking expectations of both sides, conditioned k on S, and plugging into Eq. (20) completes the proof. Note that one simple data-independent choice for y is y = 1/n2 which gives a bound  (cid:118) (cid:117) (cid:34) n (cid:12) (cid:35)  (cid:12) (cid:12) (cid:12)E (cid:104) Lˆ S(θ) − L(θ) (cid:12) (cid:12) (cid:12) S(cid:105)(cid:12) (cid:12) (cid:12) =O(cid:101) n1 1 + (cid:117) (cid:116)E (cid:88) (cid:96)(θ, X k)2 + (cid:96)(θ, X k(cid:48) )2 (cid:12) (cid:12) (cid:12) S KL(pˆ S || p0) . (cid:12) k=1 Thus we pay a logarithmic term for an agnostic choice of y. Of course, when we have some idea about the range of E[V | S] (or when the loss function is bounded), we can follow a more refined argument and tune y θ optimizing the bound over a quantized range for y. Then, the final bound can be obtained by taking a union bound. B.2 Bounded Losses In this section we consider a simple case when the loss function is bounded, i.e. (cid:96) : Θ × Z → [0, 1], 1 and apply Corollary 1 with the choice y = 1/n2. In this case we will have a simple variance proxy σˆ2 = 1 (cid:80)n (cid:96)(θ, X )2 controlling the generalization gap. This is enough to get fully empirical bounds: θ n k=1 k Theorem 5. For any x ≥ 2, with probability at least 1 − 2e−x we have (cid:115) (cid:12) (cid:104) (cid:12) (cid:105)(cid:12) (cid:18) 1 2 (cid:19) (cid:12)E Lˆ (θ) − L(θ) (cid:12) S (cid:12) ≤ 2 + U C (S) (cid:12) S (cid:12) (cid:12) n2 n S x where √ C (S) = KL (cid:0) pˆ || p0(cid:1) + x + x ln (cid:0) 1 + n(cid:1) x S and U S = E (cid:2) σˆ θ2 (cid:12) (cid:12) S(cid:3) + (cid:114) n2 E (cid:2) σˆ θ2 (cid:12) (cid:12) S(cid:3) C x(S) + n1 (cid:18) 2C x(S) + √ 4 2(cid:112) C x(S) + √1 2 (cid:19) . The theorem implies that with high probability, (cid:115) (cid:12) (cid:104) (cid:12) (cid:105)(cid:12) E[σˆ2 | S] KL (pˆ || p0) KL (cid:0) pˆ || p0(cid:1) 1 (cid:12)E Lˆ (θ) − L(θ) (cid:12) S (cid:12) =O(cid:101) θ S + S + , (cid:12) S (cid:12) (cid:12) n n n 19
which is not hard to see noting that (cid:112) C (S) ≤ C (S) =O(cid:101) KL (cid:0) pˆ || p0(cid:1) (since x ≥ 2) and by completing the x x S square U /n =O(cid:101) (cid:16)(cid:113) E[σˆ2 | S]/n + (cid:112) C (S)/n(cid:17)2 . S θ x Observe that the bound becomes of order KL (cid:0) pˆ || p0(cid:1) /n for the small enough variance proxy term E[σˆ2 | S θ S]. Proof of Theorem 5. Fix x ≥ 2. Noting that E[V | S] ≤ 1/n a.s. by boundedness of the loss and taking θ y = 1/n2, Corollary 1 implies that with probability at least 1 − e−x, (cid:115) (cid:12) (cid:104) (cid:12) (cid:105)(cid:12) (cid:18) 1 (cid:19) (cid:12)E Lˆ (θ) − L(θ) (cid:12) S (cid:12) ≤ 2 + E[V | S] C (S) . (cid:12) S (cid:12) (cid:12) n2 θ x All that is left is to give an upper bound E[V | S] in terms of empirical quantities. Introduce notation θ n 1 (cid:88) σ2(s) = (cid:96)(θ, z )2 , s = (z , . . . , z ) ∈ Z , θ n k 1 n k=1 and so σˆ2 = σ2(S). Thus, by Eq. (21), θ θ E[V θ | S] ≤ n1 E (cid:2) σ θ2(S) (cid:12) (cid:12) S(cid:3) + n1 E (cid:2) σ θ2(S(cid:48)) (cid:12) (cid:12) S(cid:3) (22) and we only need to upper bound E (cid:2) σ2(S(cid:48)) (cid:12) (cid:12) S(cid:3) , which we will do apply our PAC-Bayesian concentration θ inequality. In particular we apply Theorem 3 to σˆ2. Taking f (s) = σ2(s) and denoting a deviation by θ θ θ (cid:90) ∆(cid:48) = σ2(S) − σ2(s)D(ds) θ θ θ we observe that E[∆(cid:48) | S] = E[σ2(S) | S] − E[σ2(S(cid:48)) | S] and that an ES variance proxy of σˆ2 is θ θ θ n V (cid:48) = 1 (cid:88) (cid:0) (cid:96)(θ, X )2 − (cid:96)(θ, X(cid:48) )2(cid:1)2 θ n2 k k k=1 n ≤ 2 (cid:88) (cid:0) (cid:96)(θ, X ) − (cid:96)(θ, X(cid:48) )(cid:1)2 = 2V n2 k k θ k=1 by boundedness of the loss. By Lemma 1, (∆(cid:48) , (cid:112) V (cid:48)) forms a canonical family, and so by Eq. (20) of θ θ θ∈Θ Theorem 3 with y = 1/n2 and having that E[V | S] ≤ 1/n, this gives that for all x ≥ 2, with probability at θ least 1 − e−x, (cid:115) (cid:18) (cid:19) E[σ θ2(S(cid:48)) | S] − E (cid:2) σ θ2(S) (cid:12) (cid:12) S(cid:3) ≤ 2 n1 2 + E[V θ(cid:48) | S] (cid:16) KL (pˆ S || p0) + x + x 2 ln (1 + n)(cid:17) (cid:115) (cid:18) (cid:19) 1 ≤ 2 + 2 E[V | S] C (S) . n2 θ x √ This combined with Eq. (22) and using subadditivity of · gives √ E[V θ | S] ≤ n2 E (cid:2) σ θ2(S) (cid:12) (cid:12) S(cid:3) + n22 + n2 (cid:112) E [V θ | S] C x(S) . 20
√ √ Using the fact that for a, b, c ≥ 0, a ≤ b + c a ⇒ a ≤ b + c2 + bc, we get √ E[V θ | S] ≤ n2 E (cid:2) σ θ2(S) (cid:12) (cid:12) S(cid:3) + n22 + n4 2 C x(S) (cid:115) √ (cid:18) (cid:19) + n2 E (cid:2) σ θ2(S) (cid:12) (cid:12) S(cid:3) + n22 n2 (cid:112) C x(S) ≤ n2 E (cid:2) σ θ2(S) (cid:12) (cid:12) S(cid:3) + n2 2 (cid:18) √1 2 + 2C x(S) + √ 4 2(cid:112) C x(S)(cid:19) (cid:114) + 2 n2 3 E (cid:2) σ θ2(S) (cid:12) (cid:12) S(cid:3) C x(S) . Simplifying and taking a union bound completes the proof. B.3 Concentration of Weighted Averages In this section we cover concentration and PAC-Bayesian results on weighted averages (also known as self-normalized estimators). Our results are particularly handy for unbounded weights since bounds we presented in previous sections hold for functions of independent, but not necessarily bounded random variables. Formally, let X = (W , R ) ∈ [0, ∞) × [0, 1], i ∈ [n] be a sequence of random pairs (not i i i necessarily independent from each other) and let S = (X , . . . , X ) and S(cid:48) = (X(cid:48) , . . . , X(cid:48) ) be sampled as 1 n 1 n before. Throughout this section we look at the weighted averages of the form (cid:80)n W R f WA(S) = i=1 i i . (23) (cid:80)n W j=1 j We call W the weight of the ith data R . The first goal we pursue here is to derive high probability tail bounds i i on f WA(S) − E[f WA(S)]. In these bounds we may assume that the distribution of W is known. Note that i weights are unbounded from above. This is important, as in some applications the support of the distribution of W is indeed unbounded, while in other applications where W is bounded, tail inequalities that use almost i i sure upper bounds on W become very lose. For instance, in a Weighted Importance Sampling estimator, W i i is a ratio of two probability densities, and therefore it is often indeed unbounded from above. As a corollary to Theorem 2, we obtain the following result: Theorem 6. For any y > 0 and any x ≥ 2, with probability at least 1 − e−x we have (cid:114) (cid:16) (cid:16)(cid:112) (cid:17)(cid:17) |f WA(S) − E[f WA(S)]| ≤ 2(2V WA + y) 1 + ln 1 + 2V WA/y x where n (cid:88) (cid:104) (cid:12) (cid:105) V WA = E W˜ 2 + U˜ 2 (cid:12) W , . . . , W (24) k k (cid:12) 1 k k=1 and W W (cid:48) W˜ = k U˜ = k k (cid:80)n W k W (cid:48) + (cid:80) W j=1 j k j(cid:54)=k j for k ∈ [n], and W (cid:48) shares the distribution of W and is independent of (W , . . . , W ). k k 1 n 21
Note that W˜ , U˜ are nonnegative and sum to one. As before, y can be chosen from some feasible range k k (through the union bound), for instance a quantized range [0, 2] (since V WA ≤ 2). Further, if the distribution of (W ) available, V WA can be computed exactly. However, sometimes this can be computationally prohibitive. i i In the following we propose two computationally-amenable bounds on V WA, which exploit the fact that weights concentrate well in the vicinity of 0. The following lemma (with proof given in the appendix) captures this fact. Lemma 4. Assume that non-negative random variables W , W , . . . , W are distributed i.i.d. Then, for any 1 2 n t ∈ [0, n E[W ]), 1 P (cid:32) (cid:88)n W ≤ t(cid:33) ≤ exp (cid:32) − (t − n E [W 1])2 (cid:33) . i 2n E (cid:2) W 2(cid:3) i=1 1 Also, with probability at least 1 − e−x for x > 0, (cid:88)n W ≥ N (n) where N (n) = (cid:18) n E[W ] − (cid:113) 2xn E (cid:2) W 2(cid:3)(cid:19) . (25) i x x 1 1 i=1 + This lemma implies the following bounds on V WA. Proposition 1. Let V WA be defined as in (24) and suppose that weights W , W , . . . , W are distributed i.i.d. 1 2 n Then for any x > 0 with probability at least 1 − ne−x we have   (cid:88)n W 2 E[W (cid:48) 2] V WA ≤  k + k  . (26)  (cid:16) (cid:17)2 (cid:16) (cid:17)2  k=1 (cid:80)k i=1 W i + N x(n − k) (cid:80)k i=− 11 W i + N x(n − k + 1) Under the same conditions we also have that n V WA ≤ 1 (cid:88) (cid:16) W 2 + E[W (cid:48) 2 ](cid:17) . (27) N (n)2 k k x k=1 Proof. Observe that   W 2 W 2 E [W˜ 2] = E  k  ≤ k k k k  (cid:16) (cid:17)2  (cid:16) (cid:17)2 (cid:80)k W + (cid:80)n W (cid:80)k W + N (n − k) i=1 i i=k+1 i i=1 i x where the last inequalities holds w.p. at least 1 − ne−x for x > 0 by taking union bound. Similarly   W (cid:48) 2 E[W (cid:48) 2] E [U˜ 2] = E  k  ≤ k . k k k  (cid:16) (cid:17)2  (cid:16) (cid:17)2 (cid:80)k−1 W + W (cid:48) + (cid:80)n W (cid:80)k−1 W + N (n − k + 1) i=1 i k i=k+2 i i=1 i x Obviously, the last result follows by concentrating the entire sum in the denominator. Combined with Theorem 6 through the union bound, these yield computationally efficient concentration bounds for f WA. 22
Proof of Theorem 6. The proof boils down to application of Theorem 1. First we show a basic remove-one stability property of a self-normalized average. Let f WA(S\k) = (cid:80) i(cid:54)=k WiRi be the remove-one version of k (cid:80) i(cid:54)=k Wi f WA(S), where S\k = (X , . . . , X , X , . . . , X ). 1 k−1 k+1 n Proposition 2 (Remove-One Stability). Let E = R − f WA(S\k) denote a pointwise remove-one (or k k k leave-one-out) error. Then, with f defined by (23), for any k ∈ [n], W E f WA(S) − f WA(S\k) = k k . k (cid:80)n W j=1 j Proof. The statement follows from simple algebra. As before, let E [·] stand for E[· | X , . . . , X ]. We need to upper bound k 1 k n (cid:88) (cid:104) (cid:105) V = E (f WA(S) − f WA(S(k)))2 k k=1 and its expectation. We have f WA(S) − f WA(S(k)) = f WA(S) − f WA(S\k) + f WA(S\k) − f WA(S(k)). Squaring k k both sides and using (a + b)2 ≤ 2(a2 + b2), we see that we need to bound E [(f WA(S) − f WA(S\k))2] k k and E [(f WA(S(k)) − f WA(S\k))2]. Recall that W˜ = W / (cid:80) W is the k-th normalized weight. We can k k k k j j directly use Proposition 2 to bound the first of these terms by E [W˜ 2] (using that E2 ≤ 1). The second term k k k can be bound the same way, except now we start from f WA(S(k)) − f WA(S\k) = W (cid:48) E(cid:48) /(W (cid:48) + (cid:80) W ), k k k k j(cid:54)=k j where E(cid:48) = R(cid:48) − f WA(S\k). Recall that U˜ = W (cid:48) /(W (cid:48) + (cid:80) W ). Putting things together, we have k k k k k k j(cid:54)=k j n (cid:88) (cid:16) (cid:17) V ≤ 2V WA = 2 E [W˜ 2] + E [U˜ 2] . k k k k k=1 Applying the first result of Theorem 1 completes the proof. B.3.1 Off-policy Evaluation through Weighted Importance Sampling Recall that in the setting of off-policy evaluation we assume that action-reward pairs are distributed according to some joint probability measure D ∈ M ([K] × [0, 1]), and observations (A , R ), . . . , (A , R ) are 1 1 1 n n generated by sampling actions A ∼ π , where π ∈ M ([K]) is called the behavior policy and R ∼ i b b 1 i D(R | A ). Now given another distribution π ∈ M ([K]) called the target policy, we want to estimate its i 1 expected reward, or the value function (cid:88) v(π) = π(a) E[R|A = a] . a∈[K] This can be done by employing WIS estimator, which is a special case of the weighted average f WA with weights W = π(A )/π (A ). Since in this case, the weights W are not necessarily bounded, the tools we i i b i i have developed in previous section allows to do exactly that. In the following, for the choice of such weights we denote f WA(S) by vˆWIS and its variance proxy V WA by V WIS. 23
Theorem 7. For any y > 0 and x ≥ 2, with probability at least 1 − (n + 1)e−x we have (cid:32) (cid:114) (cid:33) v(π) ≥ N x(n) vˆWIS − 2(2V WIS + y) (cid:16) 1 + ln (cid:16)(cid:112) 1 + 2V WIS/y(cid:17)(cid:17) x . n + where V WIS is defined as in (24) and N (n) is defined as in (25). x Proof. Introduce decomposition v(π) − vˆWIS = v(π) − E[vˆWIS] + E[vˆWIS] − vˆWIS . (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Bias of WIS estimator Concentration of WIS estimator Observe that concentration is readibly given by the Theorem 6, thus we pay attention to the bias. We apply Lemma 4 getting lower bound on the sum of weights (note that weights are independent from each other) to get E[vˆWIS] = E (cid:20) (cid:80)n i=1 W iR i (cid:21) ≤ 1 E (cid:34) (cid:88)n W R (cid:35) (cid:80)n W N (n) i i i=1 i x i=1 n (cid:90) 1 (cid:88) π(a) n = r π (a)D(r | a) = v(π) . b N (n) π (a) N (n) x 0 b x a∈[K] Thus, bias is bounded as (cid:18) (cid:19) n v(π) − E[vˆWIS] ≥ v(π) 1 − . N (n) x Combining this with the concentration result of Theorem 6 according to the decomposition, with probability at least 1 − 2e−x for x ≥ 0 gives (cid:18) (cid:19) n v(π) ≥ vˆWIS + v(π) 1 − − C (S) x N (n) x (cid:114) (cid:16) (cid:16)(cid:112) (cid:17)(cid:17) where C (S) = 2(2V WIS + y) 1 + ln 1 + 2V WIS/y x and rearranging we get a desired result x N (n) v(π) ≥ x (vˆWIS − C (S)) x n which completes the proof. Note that V WIS can be bounded according to Proposition 1. B.4 PAC-Bayesian Bound for Weighted Importance Sampling In this section we specialize the framework discussed in Section 4.2 and consider a class of parametric target policies {π ∈ M ([K]) : ϑ ∈ Θ} where Θ is a parameter space, and as in Section 4, parameter ϑ 1 θ ∼ pˆ ∈ M (Θ). Note that density pˆ depends on the tuple of observed action and rewards S = S 1 S ((A , R ), . . . , (A , R )) generated as described in Section B.3.1 (S(cid:48) is still sampled independently from 1 1 n n S). The importance weights are now defined w.r.t. the random parameter θ, that is W = π (A )/π (A ), θ,i θ i b i i ∈ [n]. Note that unlike the case of evaluation, importance weights W are not independent from each other θ,i 24
anymore, because θ is a function of S (the independence, however, still holds for weights W (cid:48) ). However, θ,i θ-dependent WIS estimator vˆWIS is a function of S (action-reward pairs independent from each other), so all θ our concentration arguments still apply. In the following N stands for its counterpart N with weights θ,x x depending on θ. Theorem 8. Fix an arbitrary probability kernel pˆ from Z to Θ and an arbitrary probability measure p0 over Θ. Then for any y > 0 and any n ≥ 1, x ≥ 2 that satisfy N (n) > 0, with probability at least 1 − 2e−x, x E[v(π θ) | S] ≥ (cid:18) E[vˆ θWIS | S] − min (cid:26) 1, E (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) N n (n) − 1(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) S(cid:21)(cid:27) − (cid:113) 2 (cid:0) y + E[V θWIS | S](cid:1) C x,y(S)(cid:19) θ,x + where C (S) = KL (cid:0) pˆ || p0(cid:1) + x + x ln (cid:0)(cid:112) 1 + E[V WIS | S]/y(cid:1) and x,y S θ n (cid:88) (cid:104) (cid:12) (cid:105) V WIS = E W˜ 2 + U˜ 2 (cid:12) θ, A , . . . , A (28) θ θ,k θ,k (cid:12) 1 k k=1 and W W (cid:48) W˜ = θ,k U˜ = θ,k . θ,k (cid:80)n W θ,k W (cid:48) + (cid:80) W j=1 θ,j θ,k j(cid:54)=k θ,j Proof. Throughout the proof we use notation vˆWIS(S) to indicate that the estimator is evaluated on S. We θ start from decomposition E[v(π ) | S] − E[vˆWIS(S) | S] = E[v(π ) | S] − E[vˆWIS(S(cid:48)) | S] + E[vˆWIS(S(cid:48)) | S] − E[vˆWIS(S) | S] θ θ θ θ θ θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Bias Concentration and first handle the bias term. Since elements of W (cid:48) , i ∈ [n] are independent from each other, Eq. (25) gives θ,i us that n (cid:88) W (cid:48) ≥ N (n) with probability at least 1 − e−x for any x ≥ 0 θ,i θ,x i=1 and thus (cid:34) (cid:80)n W (cid:48) R(cid:48) (cid:12) (cid:12) (cid:35) E[vˆWIS(S(cid:48)) | S] = E i=1 θ,i i (cid:12) S θ (cid:80)n W (cid:48) (cid:12) i=1 θ,i (cid:12) (cid:34) n (cid:12) (cid:35) 1 (cid:88) (cid:12) ≤ E W (cid:48) R(cid:48) (cid:12) S N (n) θ,i i (cid:12) θ,x (cid:12) i=1 (cid:20) (cid:12) (cid:21) = E n E (cid:2) W (cid:48) R(cid:48) | θ(cid:3) (cid:12) (cid:12) S ((A(cid:48) , R(cid:48) ) i ∈ [n] distributed identically) N (n) θ,1 1 (cid:12) i i θ,x (cid:20) (cid:12) (cid:21) = E n v(π θ) (cid:12) (cid:12) S . N (n) (cid:12) θ,x Observing that rewards are bounded by 1, we have a minimum of two bounds (cid:26) (cid:20) (cid:12) (cid:21)(cid:27) E[vˆ θWIS(S(cid:48)) | S] ≤ min 1, E N n (n) v(π θ) (cid:12) (cid:12) (cid:12) S . θ,x 25
This shows that the bias is bounded as (cid:26) (cid:20)(cid:18) (cid:19) (cid:12) (cid:21)(cid:27) E[vˆ θWIS(S(cid:48)) | S] − E[v(π θ) | S] ≤ min 1 − E[v(π θ) | S], E N n (n) − 1 v(π θ) (cid:12) (cid:12) (cid:12) S θ,x (cid:26) (cid:20)(cid:12) (cid:12) (cid:12) (cid:21)(cid:27) ≤ min 1, E (cid:12) (cid:12) n − 1(cid:12) (cid:12) (cid:12) (cid:12) S (cid:12) N (n) (cid:12) (cid:12) θ,x The concentration term then follows from Theorem 3 where V is a semi-empirical Efron-Stein variance θ proxy, that is (cid:113) E[vˆWIS(S) | S] − E[vˆWIS(S(cid:48)) | S] ≤ 2 (cid:0) y + E[V WIS | S](cid:1) C (S) . θ θ θ x,y This completes the proof. C Other Proofs Proof of Lemma 4 (see also [Maurer, 2003]). Chernoff bound readily gives a bound on the lower tail (cid:32) n (cid:33) P (cid:88) X i ≤ t ≤ inf eλt E (cid:104) e−λ (cid:80)n i=1 Xi(cid:105) . λ>0 i=1 By independence of X i (cid:89)n E (cid:104) e−λXi(cid:105) ≤ (cid:89)n (cid:18) 1 − λ E [X ] + λ2 E (cid:2) X2(cid:3)(cid:19) (e−x ≤ 1 − x + 1 x2 for x ≥ 0) i 2 i 2 i=1 i=1 ≤ e−λn E[X1]+ λ2 2n E[X 12] (1 + x ≤ ex for x ∈ R and i.i.d. assumption) Getting back to the Chernoff bound gives, (cid:40) (cid:41) n E [X ] − t 1 λ = max , 0 . n E (cid:2) X2(cid:3) 1 This proves the first result. The second result comes by inverting the bound and solving a quadratic equation. 26
