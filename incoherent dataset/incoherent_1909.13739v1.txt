Equivariant Hamiltonian Flows Danilo J. Rezende* Sébastien Racanière* Irina Higgins* Peter Toth* *{danilor, sracaniere, irinah, petertoth}@google.com Abstract This paper introduces equivariant hamiltonian flows, a method for learning expressive densities that are invariant with respect to a known Lie-algebra of local symmetry transformations while providing an equivariant representation of the data. We provide proof of principle demonstrations of how such flows can be learnt, as well as how the addition of symmetry invariance constraints can improve data efficiency and generalisation. Finally, we make connections to disentangled repre- sentation learning and show how this work relates to a recently proposed definition. 1 Introduction Learning generative models with structured latent representations is important for interpretability, data efficiency, and generalisation [2, 7]. One kind of structural bias that has been shown to work well in the past is that of invariance or equivariance with respect to a group of transformations. For example, con- volutional neural networks [20] are invariant to the group of translations by construction, while disen- tangled representations learn equivariance to more general transformations [10]. In order to learn such structured latent representations, however, often a trade-off has to be made in terms of latent expressiv- ity. For example, most of the disentangling models to date use a unit Gaussian prior [9, 14, 3]. Normal- izing flows [26, 11, 5, 18, 4, 24, 16] provide a simple mechanism to build expressive density estimators in problems without much domain knowledge. But it remains a challenge to embed flow-based models with domain knowledge such as data on specific manifolds [6] and known symmetries or factorisation of the target density. In this paper we focus on problems where there is domain knowledge in the form of known invariances of a target density which we wish to learn. This is the typical case in many mod- elling problems, such as density over molecular structures with various rotational SO(n) symmetries, lattice-QCD [1] with internal gauge symmetries (e.g. U(n) and SU(n)), etc. The challenge we address is the following: suppose that we start from a base distribution π(z) that is invariant with respect to a group of transformations. If we transform this density via a generic normalizing flow f (z), there will be no guarantees that the transformed density p(z) = π(f −1(z))/det|Jac| will be invariant as well. Our main contributions are: (i) Introduce equivariant Hamiltonian flows; (ii) Propose a general algorithm for enforcing equivariance in learned Hamiltonian flows with respect to any connected Lie-Group; and (iii) Prove a simple lemma that allow us to construct invariant densities from equivariant flows. The result of this are flows that transform a simple density that is invariant with respect to the actions of a known symmetry group, into another invariant density that is arbitrarily complex. We demonstrate that learning latent representations with the known equivariance structure helps with data efficiency and generalisation. Finally, we connect this work to the recent definition of disentangled representations [10]. 2 Methods We first revise the notion of Hamiltonian dynamics from physics [8]. In physics the Hamiltonian for- malism is used to model energy conserving continuous dynamics in an abstract state space s = (q,p) Preprint. Under review. 9102 peS 03 ]LM.tats[ 1v93731.9091:viXra
Figure 1: Model diagrams. From left to right: (i) Equivalence between flows with swapped T dt and T (cid:15), as a result H g of the equivariance of Hamiltonian flows with respect to generators of symmetries of the Hamiltonian function; (ii) Generative model diagram. The last momentum variable p (shaded node) is dropped during sampling; (iii) n Inference mode diagram for computing the ELBO, where h(p |q ) is an encoder model. The last momentum n n variable p is inferred during training. n of generalised position q and momentum p. Formal connections can be made between Hamiltonian dynamics and the actions of continuous symmetry generators – those transformations that leave the dy- namics of the system modelled by the Hamiltonian unchanged throughout the path traversed. Indeed, the roles of the Hamiltonian and its symmetry generators can be interchangeable, whereby a symmetry generator of one system can be a Hamiltonian for a different system, and both Hamiltonian and sym- metry transformations commute with each other. The commutativity of transformations is measured by an operator known as the Poisson bracket. Hence, we use the Poisson bracket between the Hamilto- nian and the known symmetry generators as a regularizer to restrict the form of the learnt Hamiltonian flow, whereby it models the final data density well, while also being invariant to the known symmetries. Hamiltonian Flow Generative Model A Hamiltonian flow is a continuous-time normalizing flow induced by the Hamiltonian dynamics in the state-space s = (q, p) ∈ R2d via the ODE (q˙,p˙) = {(q,p), H(q,p)} = ( ∂H ,− ∂H ), where {,} is a skew-symmetric differential operator known ∂p ∂q as the Poisson bracket or commutator and H : R2d → R is a scalar function known as Hamiltonian. For two scalar functions f , g we set {f (q,p), g(q,p)} = (cid:80) ∂f(q,p) ∂g(q,p) − ∂f(q,p) ∂g(q,p) . If f or g i ∂qi ∂pi ∂pi ∂qi is a vector valued function, we extend the previous definition component wise so that {f, g} is also a vector value function. In what follows, we introduce the notation T (cid:15)(x) = x + (cid:15){x, f (x)} to indicate an infinitesimal f transformation induced by the Poisson bracket with a function f . With this notation, the Euler discretization of the Hamiltonian ODE is written as s = T dt(s ) = s + dt{s ,H(s )}. We can t+dt H t t t t construct a more complex flow by chaining several transformations s(cid:48) = T dt ◦ ... ◦ T dt (s). The Hn H1 inverse of this flow is obtained by replacing dt by −dt and reversing the order of the transformations, s = T −dt ◦...◦T −dt(s(cid:48))+O(dt2). H1 Hn We can create an expressive probability density p (s ) on s = (q ,p ) by starting from an initial dis- θ n n n n tribution π(s ), and transforming this density via the flow s = T dt ◦...◦T dt (s ), where the Hamil- 0 n Hn H1 0 tonians H are scalar functions parametrized by neural networks with parameters θ. This constitutes i a more structured form of Neural ODE flow, [4]. There are a few notable differences between Hamil- tonian flows and general ODE flows: (i) The Hamiltonian ODE is volume-preserving 1, which makes the computation of log-likelihood cheaper than for a general ODE flow. We show in Figure 2 that this does not necessarily reduce the expressivity of p(q ) even if p(s ) may be restricted; (ii) General ODE n n flows are only invertible in the limit dt → 0, whereas for some Hamiltonians, we can use simplectic integrators (such as Leap-Frog, [22]) that are both invertible and volume-preserving for any dt > 0. The resulting density p (s ) is given by lnp(s ) = lnπ(s ) = lnπ(T −dt ◦...◦T −dt(s ))+O(dt2). θ n n 0 H1 Hn n The structure s = (q,p) on the state-space imposed by the Hamiltonian dynamics can be constraining from the point of view of density estimation, since this would require an artificial split of the data vectors in two disjoint sets. We could consider different mechanisms to address this: (i) Exploit the splits and use alternating masks similar to [5]; (ii) Treat the momentum variables p as latent variables. The latter is the same interpretation as in HMC [22, 25, 21]. It is also more elegant as it does not require an artificial split of the data. This results in a density p(q ) of the form n p(q ) = (cid:82) dp p(q ,p ) = (cid:82) dp π(T −dt ◦...◦T −dt(q ,p )). This integral is intractable, but the model n n n n n H1 Hn n n can still be trained via variational methods where we introduce a variational density h (p |q ) with φ n n 1 This can be seen by computing the determinant of the Jacobian of the infinitesimal transformation T dt, H (cid:34) (cid:32) ∂2H − ∂2H (cid:33)(cid:35) (cid:32) ∂2H − ∂2H (cid:33) det(Jac) = det I+dt ∂qi∂pj ∂qi∂qj = 1+dt Tr ∂qi∂pj ∂qi∂qj +O(dt2) = 1+O(dt2). ∂2H − ∂2H ∂2H − ∂2H ∂pi∂pj ∂pi∂qj ∂pi∂pj ∂pi∂qj 2
parameters φ and optimise the ELBO, ELBO(q ) = E [lnπ(T −dt ◦...◦T −dt(q ,p ))−lnh (p |q )] ≤ lnp(q ), (1) n hφ(pn|qn) H1 Hn n n φ n n n instead. Note that, in contrast to VAEs ([17, 13]), the ELBO (1) is not explicitly in the form of a reconstruction error term plus a KL term. Equivariant Hamiltonian Flows and Symmetries Let’s say we want to learn a density p (x) and θ we also want it to be invariant with respect to a set of transformations x(cid:48) = T (x) induced by the ω elements ω of a symmetry group G. That is, p(x(cid:48)) = p(T (x)) = p(x),∀ω ∈ G. In the following ω we will assume that G is a connected Lie group2. This will allow us to introduce infinitesimal transformations, and being invariant with respect to transformations in G will be equivalent to being invariant with respect to those infinitesimal transformations. The Hamiltonian formalism provides a natural language to manipulate symmetries and, in particular, it provides a connection between invariance of the Hamiltonian function to the equivariance of the Hamiltonian flow. We can parametrize the action of an infinitesimal symmetry transformation on the state-space s = (q,p) by a scalar function g(s) via the Poisson bracket as s(cid:48) = s + (cid:15){s,g}, where (cid:15) > 0, (cid:15) << 1. This can also be represented as s(cid:48) = T (cid:15)(s). In this context, a transformation T (cid:15) is g g defined to be an infinitesimal symmetry if T (cid:15) ◦ T dt = T dt ◦ T (cid:15) + O((cid:15)dt2 + (cid:15)2dt). That is, we can g H H g interchange the composition order of the Hamiltonian flow by the symmetry transformation. This is illustrated in Figure 1(left). This means that all intermediate states s of the Hamiltonian flow 1,...,n will also transform in the same manner as the starting state, that is, they will form an equivariant or covariant representation of the starting state. When T (cid:15) is a symmetry, the function g is called g a symmetry generator. By Noether’s first Theorem [23], if T (cid:15) is a symmetry then the generator g g commutes with the Hamiltonian function, that is {H, g} = 0 3. This implies that the numerical value of the function g will remain constant throughout the Hamiltonian flow, for this reason the symmetry generators g are also referred to as conserved charges in physics. Based on this reasoning, we say that a Hamiltonian flow is equivariant if {H, g} = 0. The set of all symmetry generators is closed under the Poisson bracket. That is, if two functions f and g are symmetry generators then {f,g} is a symmetry generator. This means that the set of all generators forms a Lie algebra. If we can express our domain knowledge about the target density as: (i) a set of symmetry generators g , k = 1,...,N and (ii) an initial simple invariant density π, we can construct a new density p via k g Hamiltonian flow that is also invariant by learning a Hamiltonian such that {g , H} = 0. This is k formalised in Lemma 1. Lemma 1. Given a Hamiltonian function H : R2d → R, a set of symmetry generators g : R2d → R k and a base density π : R2d → R+, the density p : R2d → R+ induced by the Hamiltonian flow T dt will H be invariant with respect to the generators g if {g , H} = {g , π} = 0, ∀k. k k k Proof. From Noether’s theorem, {g , H} = 0 implies the equivariance of the flow k T (cid:15) ◦T dt = T dt ◦T (cid:15) +O((cid:15)dt2 +(cid:15)2dt). It remains to prove that π(T (cid:15)(s)) = π(s)+O((cid:15)2). Expanding to g H H g g first order in (cid:15), we have π(T (cid:15)(s)) = π(s)+(cid:15)∇πT {s,g (s)}+O((cid:15)2) = π(s)+(cid:15){π,g (s)}+O((cid:15)2) = g k k π(s)+O((cid:15)2). We can enforce {g , H} = 0 via constrained optimisation, where we perform a min-max optimisation k of the Lagrangian L(θ,φ,λ) instead, (cid:88) (cid:88) (θ(cid:63),φ(cid:63),λ(cid:63)) = min max L(θ,φ,λ); L(θ,φ,λ) = − ELBO(x)+ λ E [{g (s),H(s)}2 −κ], k π k θ,φ λ≥0 x∈Data k where λ is a Lagrange multiplier for the kth generator and κ controls the precision of the constraint. k In lemma 1, we found how to get the density over the full state s to be invariant under the symmetries. What we want though, is to have the marginal distribution over q to be invariant. This will happen 2See [19] for an introduction to Lie groups and their infinitesimal counterparts, the Lie algebras. Readers unfamiliar with these concepts should think of Lie groups as abstract equivalent of groups of matrices. For example, the group of orientation preserving rotations SO(n) is a Lie group, and its infinitesimal counterpart (its Lie algebra) is the space of anti-symmetric matrices. 3This can be easily proven by observing that T (cid:15) ◦T dt −T dt ◦T (cid:15) = (cid:15)dt{g,H}+O((cid:15)dt2 +(cid:15)2dt). g H H g 3
Train Test A B atad etinifnI atad etiniF Target density Learned density Target density Learned density Single step Kinetic energy Potential energy Kinetic energy Potential energy Integrated flow Figure 2: Learning with Hamiltonians H(q, p) = K(p) + U(q). A: Learning a density with known SO(2) symmetry. First two columns: training and test log-likelihoods of models with different constraint precision κ in the regime of infinite and finite data. Adding the symmetry constraint increases both train and test data efficiency. In the finite data regime it prevents over-fitting. Second two columns: KDE estimators of the target and learnt densities; learned kinetic energy K(p) and potential energy U(q). B: Multimodal density learning. First two columns: KDE estimators of the target and learnt densities; learned kinetic energy K(p) and potential energy U(q). Last column: single Leap-Frog step and integrated flow. The potential energy learned multiple attractors, also clearly visible in the integrated flow plot. The basins of attraction are centred at the modes of the data. when the Hamiltonian H(q, p) is factored as a "kinetic" K(p), and "potential" U (q) energy terms, H(q,p) = K(p)+U (q).4 Connections to disentangling: Given a decomposition of a continuous group into a direct product of subgroups, the actions of the generators of these subgroups can be modelled by independent generators T (cid:15) and T (cid:15) such that {T (cid:15) ,T (cid:15)} = 0. By definition in [10], these will act on independent subspaces of w g w g a disentangled q. Since equivariant flows preserve the action of the generators, they will preserve the disentanglement of the representation. 3 Experiments For all experiments the Hamitonian was of the form H(q,p) = K(p)+U (q). The kinetic energy term K and the potential energy term U are soft-plus MLPs5 with layer-sizes [d,128,128,1] where d is the dimension of the data. The encoder network was parametrized as h(p|q) = N (p;µ(q),σ(q)), where µ and σ are relu MLPs with size [d,128,128,d]. The Hamiltonian flow T dt, was approximated using H a Leap-Frog integrator [22] since it preserves volume and is invertible for any dt. We found that only two Leap-Frog steps where sufficient for all our examples. Parameters were optimised using Adam [15] (learning rate 3e-4) and Lagrange multipliers were optimised using the same method as [12]. The initial density π was chosen separately for each dataset. Domain knowledge via generators: In this experiment, we test the main idea of this paper where we want to learn a target density which is symmetric with respect to the SO(2) group of 2D rotations. In Figure 2A we show the target density. The Lie-algebra of the SO(2) group has a single generator which we express as g(q,p) = q p −q p . For this experiment, the base density is a spherical normal 1 2 2 1 π(s) = N (s; 0, I) since it has SO(2) symmetry. Our results demonstrate that the inclusion of the symmetry constraint increases the data-efficiency of the model in the infinite data regime and, most importantly, substantially reduces overfitting in the finite data-regime. Multimodal density Learning: In this experiment we to assess the expressivity of Hamiltonian flows. We demonstrate in Figure 2B that it can transform a soft-uniform6 π(s;σ,β) into a new density with arbitrary number of modes. Furthermore, the resulting model is nicely interpretable: The learned potential energy U (q) learned to have several local minima, one for each mode of the data. As a consequence, the trajectory of the initial samples through the flow has attractors at the modes of the data. 4This condition puts a constraint on the Hamiltonian to ensure that any learnt generator will induce an action on q such that the marginal distribution is invariant under that action. Alternatively, we could have put a constraint on the type of generators (namely only consider those g such that ∂2g = 0, ∀i,j) such that any learnt Hamiltonian ∂pi∂pj will do. This condition is also satisfied in our experiments. 5Due to the Poisson bracket, optimisation of Hamiltonian flows involves second-order derivatives of the MLPs used for Hamiltonians and generators, so relu non-linearities are not suitable. 6The soft-uniform density π(s;σ,β) ∝ f(β(s + σ 1 ))f(−β(s − σ 1 )), where f is the sigmoid function was 2 2 chosen to make it easier to visualise the learned attractors. The experiment also work if we start from other densities such as a Normal. 4
4 Conclusions We have demonstrated an effective method for learning invariant probability densities using Hamil- tonian flows. The proposed method opens the doors to many potential applications of ML in physics, which we plan to explore in future work. 5
References [1] M. Albergo, G. Kanwar, and P. Shanahan. Flow-based generative models for markov chain monte carlo in lattice field theory. arXiv preprint arXiv:1904.12072, 2019. [2] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspec- tives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013. [3] T. Q. Chen, X. Li, R. Grosse, and D. Duvenaud. Isolating sources of disentanglement in variational autoencoders. NIPS, 2018. [4] T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Processing Systems 31, 2018. [5] L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016. [6] M. C. Gemici, D. Jimenez Rezende, and S. Mohamed. Normalizing flows on riemannian manifolds. arXiv preprint arXiv:1611.02304, 2016. [7] R. Gens and P. M. Domingos. Deep symmetry networks. NIPS, 2014. [8] H. Goldstein. Classical Mechanics. Addison-Wesley Pub. Co, Reading, Mass., 1980. [9] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. β-vae: Learning basic visual concepts with a constrained variational framework. ICLR, 2017. [10] I. Higgins, D. Amos, D. Pfau, S. Racaniere, L. Matthey, D. J. Rezende, and A. Lerchner. Towards a definition of disentangled representations. arxiv, 2018. [11] D. Jimenez Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015. [12] D. Jimenez Rezende and F. Viola. Taming vaes. arXiv preprint arXiv:1810.00597, 2018. [13] D. Jimenez Rezende, S. Mohamed, and D. Wierstra. Stochastic meta-optimization synthesis and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014. [14] H. Kim and A. Mnih. Disentangling by factorising. ICLR, 2018. [15] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [16] D. P. Kingma and P. Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems, pages 10215–10224, 2018. [17] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [18] D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Improved variational inference with inverse autoregressive flow. In Advances in neural information processing systems, pages 4743–4751, 2016. [19] A. A. Kirillov. An introduction to Lie groups and Lie algebras, volume 113. Cambridge University Press, 2008. [20] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. meta-optimization synthesis applied to handwritten zip code recognition. Neural Computation, 1(4): 541–551, 1989. [21] D. Levy, M. D. Hoffman, and J. Sohl-Dickstein. Generalizing hamiltonian monte carlo with neural networks. arXiv preprint arXiv:1711.09268, 2017. [22] R. M. Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11):2, 2011. 6
[23] E. Noether. Invariant variation problems. Transport Theory and Statistical Physics, 1(3): 186–207, 1971. [24] G. Papamakarios, T. Pavlakou, and I. Murray. Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems, pages 2338–2347, 2017. [25] T. Salimans, D. Kingma, and M. Welling. Markov chain monte carlo and variational inference: Bridging the gap. In International Conference on Machine Learning, pages 1218–1226, 2015. [26] E. G. Tabak, E. Vanden-Eijnden, et al. Density estimation by dual ascent of the log-likelihood. Communications in Mathematical Sciences, 8(1):217–233, 2010. 7
