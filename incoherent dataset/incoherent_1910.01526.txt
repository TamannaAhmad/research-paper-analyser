Gated Linear Networks Joel Veness * 1 Tor Lattimore * 1 David Budden * 1 Avishkar Bhoopchand * 1 Christopher Mattern 1 Agnieszka Grabska-Barwinska 1 Eren Sezener 1 Jianan Wang 1 Peter Toth 1 Simon Schmitt 1 Marcus Hutter 1 Abstract learnt tasks. Although various meta-learning [OWR+19] algorithms such as Elastic Weight Consolidation [KPR+17, This paper presents a new family of EWC] have been effective in compensating for these limita- meta-optimization synthesis-free neural architectures, tions, it is interesting to explore whether alternative methods Gated Linear Networks (GLNs). What dis- of credit assignment can give rise to complementary neural tinguishes GLNs from contemporary neural models with different strengths and weaknesses. networks is the distributed and local nature of their credit assignment mechanism; each neuron This paper introduces one alternative model family, Gated directly predicts the target, forgoing the ability Linear Networks (GLNs), and studies their contrasting prop- to learn feature representations in favor of rapid erties. The distinguishing feature of a GLN is its distributed online learning. Individual neurons can model and local credit assignment mechanism. This technique is a nonlinear functions via the use of data-dependent generalization of the PAQ family [Mah00, Mah05, Mah13] gating in conjunction with online convex opti- of online neural network models, which are well-known in mization. We show that this architecture gives the data compression community for their excellent sample rise to universal learning capabilities in the limit, efficiency [Mah13, Kno17]. By interpreting these systems with effective model capacity increasing as a within an online convex programming [Zin03] framework function of network size in a manner comparable as a sequence of data dependent linear networks coupled with deep ReLU networks. Furthermore, we with a choice of gating function, we are able to provide a demonstrate that the GLN learning mechanism new algorithm and gating mechanism that opens up their possesses extraordinary resilience to catastrophic usage to the wider machine learning community. forgetting, performing comparably to a MLP with GLNs have a number of desirable properties. Their local dropout and Elastic Weight Consolidation on credit assignment mechanism is derived by associating a standard benchmarks. These desirable theoretical separate convex loss function to each neuron, which greatly and empirical properties position GLNs as a simplifies parameter initialization and optimization, and pro- complementary technique to contemporary vides significant sample efficiency benefits when learning offline deep learning methods. online. Importantly, we show that these benefits do not come at the expense of capacity in practice, which adds fur- 1. Introduction ther weight to previously obtained asymptotic universality results [VLB+17]. meta-optimization synthesis has long been the de-facto credit assign- GLNs possess excellent online learning capabilities, which ment technique underlying the successful training of popular we demonstrate by showing performance competitive with neural network architectures such as convolutional neural batch-trained MLPs on a variety of standard classification, networks and multilayer perceptions (MLPs). It is well regression and density modeling tasks, using only a single known that meta-optimization synthesis enables these networks to learn online pass through the data. In terms of interpretibility, we highly-relevant task-specific features. However, this method show how the data-dependent linearity of the predictions can is not without its limitations. Contemporary neural networks be exploited to trivialise the process of constructing mean- trained via meta-optimization synthesis require many epochs of train- ingful saliency maps, which can be of great reassurance to ing over massive datasets, limiting their effectiveness for practitioners that the model is predicting well for the right data-efficient online learning. Interpretibility limitations can reasons. Perhaps most interestingly, we demonstrate that also prevent their application in domains where a human our credit assignment mechanism is extraordinarily resilient understandable solution is a mandatory requirement. Their to catastrophic forgetting, achieving performance competi- effectiveness is further limited in the continual learning set- tive with EWC on a standard continual learning benchmark ting by their tendency to catastrophically forget previously with no knowledge of the task boundaries. *Equal contribution 1DeepMind. 0202 nuJ 11 ]GL.sc[ 2v62510.0191:viXra
Gated Linear Networks 2. Background function of w [Mat13] and that the gradient of the loss with respect to w is given by In this section we review some necessary background on geometric mixing, a parametrised way of combining prob- ∇(cid:96) tGEO(w) = (GEOw(1; p t) − x t) logit(p t). (2) abilitstic forecasts, and show how to adapt its parameters using online convex programming. Later we will combine Furthermore we can bound the 2 √-norm of the gradient of this method with a gating mechanism to define a single the loss with (cid:107)∇(cid:96)GEO(w)(cid:107) ≤ d log (cid:0) 1 (cid:1) provided that t 2 ε neuron within a GLN. p ∈ [ε, 1 − ε]d for some ε ∈ (0, 1/2) for every time t. t These properties of the sequence of loss functions make it Geometric Mixing. Geometric Mixing is a simple and possible to apply one of the many different online convex well studied ensemble technique for combining probabilis- programming techniques to adapt w at the end of each round. tic forecasts. It has seen extensive application in statistical In this paper we restrict our attention to Online Gradient data compression [Mat12, Mat13]. Given p , p , . . . , p Descent [Zin03], with W equal to some choice of hyper- 1 2 d input probabilities predicting the occurrence of a single bi- cube, √for reasons of computational efficiency. This gives nary event, geometric mixing predicts σ(w(cid:62)σ−1(p)), where a O( T ) regret bound with respect to the best w∗ ∈ W σ(x) := 1/(1 + e−x) denotes the sigmoid function, σ−1 chosen in hindsight provided an appropriate schedule of defines the logit function, p := (p , . . . , p ) and w ∈ Rd decaying learning rates is used. 1 d is the weight vector which controls the relative importance of the input forecasts. One can easily show the following 3. Gated Geometric Mixing identity: We define the GLN neuron as a gated geometric mixer, σ (cid:0) w(cid:62)σ−1(p)(cid:1) = (cid:81)d i=1 pw i i , which we obtain by adding a contextual gating procedure to (cid:81)d i=1 pw i i + (cid:81)d i=1(1 − p i)wi geometric mixing. Here, contextual gating has the intuitive meaning of mapping particular input examples to particular which makes it clear that that geometric mixing implements sets of weights. The key change compared with normal a type of product of experts [Hin02] operation. This leads geometric mixing is that now our neuron will also take in to the following interesting properties: setting w i = 1/d an additional type of input, side information, which will is equivalent to taking the geometric mean of the d input be used by the contextual gating procedure to determine an probabilities; if the jth component of w j is 0 then the con- active subset of the neurons weights to use for a given exam- tribution of p j is ignored, and if w = 0 then the geometric ple. In typical applications the side information will simply mixture predicts 1/2; and finally, due to the product formu- be the input features associated with a given example. lation, every forecaster has “the right of veto”, in the sense that a single p close to 0 coupled with a w > 0 drives the More formally, associated with each neuron is a context i i function c : Z → C, where Z is the set of possible side geometric mixture prediction close to zero. information and C = {0, . . . , k − 1} for some k ∈ N is the context space. Given a convex set W ⊂ Rd, each neuron is Online Convex Programming Formulation. We now (cid:2) (cid:3)(cid:62) describe how to adapt the geometric mixing parameters parametrized by a matrix W = w 0 . . . w k−1 with each using online convex programming [Zin03, Haz16]. Let row vector w i ∈ W for 0 ≤ i < k. The context function c B := {0, 1}. As we are interested in probabilistic predic- is responsible for mapping a given piece of side information tion, we assume a standard online learning framework for z t ∈ Z to a particular row w c(zt) of W , which we then use the logarithmic loss, where at each round t ∈ N a predictor with standard geometric mixing. outputs a binary distribution q : B → [0, 1], with the t In other words, a Gated Geometric Mixer can be defined in environment responding with an observation x ∈ B, caus- t terms of geometric mixing as ing the predictor to suffer a loss (cid:96) (q , x ) = − log q (x ) t t t t t before moving onto round t + 1. GEOc W (x t ; p t, z t) := GEOwc(zt)(x t ; p t), (3) In the case of geometric mixing, we first define our param- eter space to be a non-empty convex set W ⊂ Rd. As the with the associated loss function − log (GEOc W (x t ; p t, z t)) inheriting all the properties needed to apply Online Convex prediction depends on both the d dimensional input predic- Programming directly from Equation 1. The key intuition tions p and the parameter vector w ∈ W, we abbreviate the t behind gating is that it allows each neuron to be able to loss at time t, given target x , using parameters w by t specialize its weighting of input predictions based on some (cid:96) tGEO(w) := − log (GEOw(x t ; p t)) , (1) particular property of the side information. with GEOw(1; p t) := σ(w(cid:62)σ−1(p t)) and GEOw(0 ; p t) := Universal context functions. We now introduce a half- 1 − GEOw(1 ; p t). One can show that (cid:96) tGEO(w) is a convex space gating mechanism that is tailored towards machine
Gated Linear Networks p p p p 20 21 22 2(K 2 -1) c c c Layer 2 Bias 21 22 ... 2K 2 p p p p 10 11 12 1(K 1 - 1) c c c Layer 1 Bias 11 12 ... 1K 1 p p p p 00 01 02 0(K 0 - 1) Layer 0 Bias Base Base ... Base model model model Side information z Figure 1. A graphical depiction of a Gated Linear Network. Each neuron receives inputs from the previous layer as well as the broadcasted side information z. The side information is passed through all the gating functions, whose outputs s j = c (z) determine the active i ij weight vectors (shown in blue). learning applications whose input features lie in Rd. Al- 4. Gated Linear Networks though not the focus of this work, its worth noting that We now introduce Gated Linear Networks, which are feed- this choice gives rise to universal approximation capabili- forward networks composed of many layers of gated geo- ties for sufficiently large GLNs [VLB+17]. Once we are metric mixing neurons as shown in Figure 1. Each neuron in a position to describe the learning dynamics of multiple in a given layer outputs a gated geometric mixture of the interacting neurons, the rationale for this class of context predictions from the previous layer, with the final layer functions will become more clear. Exploring alternative consisting of just a single neuron. In a supervised learn- gating mechanisms is an exciting area for future work. ing setting, a GLN is trained on (side information, base predictions, label) triplets (z , p , x ) derived from t t t t=1,2,3,... Halfspace gating. Given a normal v ∈ Rd and offset input-label pairs (z , x ). There are two types of input to t t b ∈ R, consider the associated affine hyperplane {x ∈ Rd : neurons in the network: the first is the side information z , t x · v = b}. This divides Rd in two, giving rise to two which can be thought of as the input features; the second half-spaces, one of which we denote is the input to the neuron, which will be the predictions output by the previous layer, or in the case of layer 0, some H v,b = {x ∈ Rd : x · v ≥ b}. (optionally) provided base predictions p t that typically will be a function of z . Each neuron will also take in a constant t bias prediction, which helps empirically and is essential for The associated half-space context function is then given by 1 (z), where 1 (s) := 1 if s ∈ S and 0 otherwise. universality guarantees [VLB+17]. Hv,b S GLN architecture. A GLN is a network of gated ge- Context composition. Richer notions of context can be ometric mixers organized in L + 1 layers indexed by created by composition. In particular, any finite set of m i ∈ {0, . . . , L}, with K models in each layer. Neurons context functions {c : Z → C }m with associated context i i i i=1 are indexed by their position in the network when laid out spaces C , . . . , C can be composed into a single higher 1 m on a grid; for example, neuron (i, k) will refer to the kth order context function c : Z → C, where C = C ×...×C ∼ = 1 d neuron of the i layer and p will refer to the output of neu- {0, ..., |C| − 1} by defining c(z) = (c (z), ..., c (z)). ik 1 d ron (i, k). The output of layer i will be denoted by p . The i For example, we could combine m = 4 different halfs- zeroth layer of the network is called the base layer, whose pace context functions into a single context function with output p will typically be instantiated via scaling or squash- 0 a context space containing |C| = 16 elements. From here ing each component of the current side information z to lie onwards, whenever this technique is used, we will refer to within [ε, 1 − ε]. The nonzero layers are composed of gated the choice of m as the context dimension. geometric mixing neurons. Associated to each of these will
Gated Linear Networks be a fixed context function c : Z → C that determines the Algorithm 1 GLN(Θ, z, p, x, η, update). ik behavior of the gating at neuron (i, k). In addition to the Perform a forward pass and optionally update weights. context function, for each context c ∈ C and each neuron (i, k) there is an associated weight vector w ikc ∈ RKi−1 1: Input: GLN weights Θ ≡ {w ijc} which is used to geometrically mix the inputs whenever 2: Input: side info z, base predictions p ∈ [ε; 1 − ε]K0−1 active. The bias outputs p for 0 ≤ i ≤ L can be set to 3: Input: binary target x, learning rate η ∈ (0, 1) i0 be any constant β ∈ [ε, 1 − ε] \ {0.5}. Given a z ∈ Z, a 4: Input: boolean update (controls if we learn or not) weight vector for each neuron is determined by evaluating 5: Output: estimate of P[x = 1 | z, p] its associated context function. For layers i ≥ 1, the kth 6: p 0 ← (β, p 1, p 2, . . . , p K0−1) node in the ith layer receives as input the vector p of i−1 7: for i ∈ {1, . . . , L} do {loops over layers} dimension K of predictions of the preceding layer. i−1 8: p i0 ← β 9: for j ∈ {1, . . . , K i} do {loops over neurons} G loL ssN os f a gr ee ned raa lt ia ty,d he ep re en wde en at ssli un mea er thn ae tt tw ho er nk es t. woW rkit ih so eu st - 10: p ij ← CLIP1 ε−ε (cid:2) σ (cid:0) w ijcij(z) · σ−1(p i−1)(cid:1)(cid:3) 11: if update then timating the probability of the target being positive. The 12: ∆ ij ← −η (p ij − x) σ−1(p i−1) o inu ptp uu tst wof ita h s ri en sg pl ee ctne tour aon sei ts ot fhe wg ee igo hm tset tr hic atm di ex pt eu nre d o of nt ih te s 13: w ijcij(z) ← CLIPb −b[w ijcij(z) + ∆ ij] 14: end if context, namely 15: end for p (z) = σ (cid:0) w · σ−1 (p (z))(cid:1) . 16: end for ik ikcik(z) i−1 17: return p L1 The output of layer i can be written in matrix form as p (z) = σ(W (z) σ−1(p (z))) , (4) i i i−1 weight space W. The gradient step can be trivially obtained where W i(z) ∈ RKi×Ki−1 is the matrix with kth row equal from Equation 2. It is well known that the projection step to w (z) = w . Iterating Equation 4 once gives can be implemented via clipping if the convex set W is ik ikcik(z) a scaled hypercube. In our case this can be achieved if p (z) = σ (cid:0) W (z) σ−1 (cid:0) σ (cid:0) W σ−1 (p (z))(cid:1)(cid:1)(cid:1) . we force every component of each weight vector, for each i i i−1 i−2 neuron, to lie within [−b, b] for some constant b > 1. Observing that the logit and sigmoid functions cancel, sim- plifying the ith iteration of Equation 4 gives Weight initialization. One benefit of a convex loss is that (cid:16) (cid:17) p (z) = σ W (z)W (z) . . . W (z) σ−1(p (z)) , (5) weight initialization is less important in determining overall i i i−1 1 0 model performance, and one can safely recommend deter- which shows the network behaves like a linear network ministic initialization schemes that favor reproducibility of [BH89, SMG13], but with weight matrices that are data- results. While other choices are possible, we found the ini- dependent. Without the data dependent gating, the product tialization w ikc = 1/K i−1 for all i, k, c to be a good choice of matrices would collapse to a single linear mapping and empirically, which causes geometric mixing to initially com- provide no additional modeling power over a single neuron pute a geometric average of its input. [MP69]. Algorithm. A single prediction step, as well as a single Local learning in GLNs. We now describe how the step of learning using Online Gradient Descent, can be weights are learnt in a Gated Linear Network using Online implemented via a single forward pass of the network as Gradient Descent (OGD) [Zin03] locally at each neuron. shown in Algorithm 1. Here we make use of a subrou- They key observation is that as each neuron (i, k) in layers tine CLIP1−ε[x] := min {max(x, ε), 1 − ε}. Generating a ε i > 0 is itself a gated geometric mixture, all of these neu- prediction requires computing the active contexts from the rons can be thought of as individually predicting the target. given side information for each neuron, and then performing Thus given side information z and from Equations 1 and 3, L matrix-vector products. Under the assumption that multi- each neuron (i, k) suffers a loss convex in its active weights plying a m × n by n × 1 pair of matrices takes O(mn) work, u := w of ikcik(z) the total time complexity to generate a single prediction is O((cid:80)L K K ) for the matrix-vector products, which in (cid:96) t(u) := − log (GEOu (x t ; p i−1)) . typicai l= c1 asei s wi− il1 l dominate the overall runtime. Note that Algorithmically, a single step of OGD consists of two parts: updating the weights does not affect this complexity. a gradient step, and then a projection back into some convex
Gated Linear Networks 1 0 3 2 1 0 1 2 3 z )z|x(P Figure 2. Output of a four layer network with random half-space contexts after training to convergence. Each box represents a non-bias neuron in the network, the function to fit is shown in black, and the output distribution learnt by each neuron is shown in colour (for example, red for the first layer and purple for the top-most neuron). All axes are identical, as labeled in the bottom left neuron. The dashed coloured lines represent the sampled hyperplane for each neuron. Random halfspace sampling. Here we describe how we formance. Below we will outline how (good) convergence generate a diverse set of halfspace context functions in prac- rates may be obtained for fixed finite sized GLNs to the best tice. As we are interested in higher dimensional applications, locally learnable approximation. Precisely obtaining such it is necessary to sample hyperplanes in a manner that ad- bounds is outside the scope of this paper. dresses the curse of dimensionality. Consider a halfspace For a single gated neuron, one can show [VLB+17] that On- context function: c(z; v, b) = 1 if z · v ≥ b; or 0 other- line Gradient Descent (OGD) [Zin03] with a learning rate wise. To sample v, we first generate an i.i.d. random vector √ √ proportional to 1/ t has total regret of O( T ) with respect x = (x , ..., x ) of dimension d, with each component of x 1 d to the best w∗ ∈ W chosen in hindsight. The loss func- distributed according to the unit normal N (0, 1), and then tion (cid:96) is exp-concave, so Online Newton Step [HAK07] divide by its 2-norm, giving us a vector v = x/||x|| . This t 2 can improve the regret to O(log T ), but is computationally scheme uniformly samples points from the surface of a unit more expensive. If the expected loss (cid:96)(w) := E[(cid:96) (w)] sphere. The scalar b is sampled directly from a standard t were strongly convex, then Stochastic Gradient Descent normal distribution. (SGD) with i.i.d. sampling and a learning rate proportional The motivation for this approach is two-fold. First, With to 1/t would also achieve a regret of O(log T ), Unfortu- large d, the hyperplanes defining each half-space are orthog- nately (cid:96) is flat in all directions orthogonal to logit(p ), t t onal with high probability; i.e. this choice should help to hence not strongly convex. But since (cid:96) is exp-concave t chop the data up in complementary ways given a limited (strongly convex in gradient direction), this makes (cid:96)(w) number of gates. Second, suppose we have a set of m differ- strongly convex in the linear subspace of W ⊂ Rd spanned ent gating functions c (z; v , b ) for 1 to m. Now consider by S := Span(logit(p ), ..., logit(p )). For sample size n i i i 1 n the binary vector: g = (c (z; v , b ), ..., c (z; v , b )). larger than d it is plausible that S = Rd. Even if not, all (cid:96) 1 1 1 m m m t This signature vector g of input z has the property [Cha02] are exactly constant in directions orthogonal to S, hence the that different z’s which are close in terms of cosine similar- gradient lies in S. Since (cid:96) is strongly convex in S, SGD will ity will map to similar signatures. For a GLN, this gives rise still achieve a regret of O(log T ). to the desirable property that inputs close in cosine distance The above implies that the time-averaged weights w¯ have will map to similar products of data dependent matrices, i.e. T an instantaneous regret of O(log T /T ), and even O(1/T ) they will predict similarly. can be achieved [B+15, Thm.6.2]. In general, OGD algo- rithms can be converted to achieve these rates even for the On convergence properties and rates for GLNs. current weight w t [Cut19], and, indeed, SGD achieves the Asymptotic convergence results for GLNs on i.i.d. data can former even unmodified [SZ13]. By strong convexity on S, be proven. On-average within each context cell, the predic- this implies that the (time-averaged) weights (or at least the tion converges to the true expected output/probability, and a outputs) converge with a rate of O˜(t−1/2). sufficiently large GLN can represent the true target proba- Hence after time O˜(1/ε2) the output of the first layer has bilities arbitrarily well [VLB+17]. For example, Figure 2 converged within O(ε), after which the input to the next shows the converged predictions when using a small GLN layer becomes approximately i.i.d. A similar analysis should to fit a simple parametrized density function. Of course then be possible for the second layer, and so on. With while asymptotic convergence is a useful sanity check for an appropriately delayed learning rate decay, this should any model, it tells us little about practical finite-time per-
Gated Linear Networks 1.0 0.8 0.6 0.4 0.2 0.0 ycarucca dataset = mnist | num_layers = 1 dataset = mnist | num_layers = 2 1.0 0.8 0.6 0.4 0.2 0.0 0 25 50 75 100 epochs ycarucca Figure 4. The effect of a sin- gle noisy XOR update (circled) on the decision boundaries of a halfspace gated GLN. Sampled hyperplanes for dataset = noise | num_layers = 1 dataset = noise | num_layers = 2 each gate are GLN (1) GLN (2) shown in white. GLN (4) MLP both selected by conducting a sweep over learning rates from 10−1 to 10−6. It is evident from Figure 3 that GLNs have comparable capacity to an equivalently sized MLP in practice, with their ability to memorize training data scaling 0 25 50 75 100 in both the number of neurons and context dimension. epochs Figure 3. Empirical capacity of GLNs (context dimension showed 6. Linear Interpretability of GLNs in parentheses) versus an MLP baseline. Top row represents the MNIST dataset with shuffled labels. Bottom row represents a In the case where we have an L layer GLN, with K neurons dataset of uniform noise of the same size and shape. i on layer i, and with input p of dimension K and side 0 0 lead to an overall time bound of O(L/ε2) to achieve ε- information z, the RHS of Equation 5 can be written as approximation. For these reasons we use gradient descent (cid:16) (cid:17) with a learning rate proportional to 1/t in our experiments σ W (z) W (z) . . . W (z) logit(p ) , L L−1 1 0 described in Section 8. (cid:124) (cid:123)(cid:122) (cid:125) multilinear polynomial of degree L 5. Empirical Capacity of GLNs where each matrix W i(z) is of dimension K i × K i−1, with the jth row constituting the active weights (as determined Contemporary neural networks have the desirable capabil- by the gating) for the jth neuron in layer i. This formula- ity to approximate arbitrary continuous functions given al- tion is convenient for many reasons. First, it allows us to most any reasonable activation function [Hor91, and others]. reason about the inductive bias of GLNs by observing that GLNs share this property so long as the context capacity the product of matrices collapses to a multilinear polyno- is sufficiently expressive. Moreover, [VLB+17] prove that mial in the learnt weights, i.e. the depth and shape of the this capacity is effective in the sense that gradient descent network directly influences how a GLN will generalize. A will eventually find the best feasible approximation. This visual example of the change in decision boundaries result- property is not shared by neural networks trained by back- ing from a single halfspace gated GLN update is shown in propagation; it is possible to demonstrate the existence of Figure 4 for the noisy XOR problem. The magnitude of such weights, but not to guarantee that gradient descent the change is largest within the convex polytope containing (or any other practical algorithm) will find them. Here we the training point, and decays with respect to the remaining demonstrate the capacity of GLNs in practice by measuring convex polytopes according to how many halfspaces they their ability to fit random labelled data. share with the containing convex polytope. This makes intu- itive sense, as since the weight update is local, each row of We ran two sets of experiments: first, using the standard W (z) is pushed in the direction to better explain the data MNIST dataset with shuffled labels; and second, replacing i independently of each other. Therefore one should think of the MNIST images with uniform noise of the same shape a halfspace gated GLN as a smoothing technique – input and dataset length. These results are presented in Figure 3 points which cause similar gating activation patterns must compared to an MLP baseline in an equivalent one-vs-all have similar outputs. configuration. For GLNs, we select a fixed layer width of 128 and vary both the context dimension and number of Aside from reasoning about the inductive bias, the above for- layers. For the MLP, we select the number of neurons such mulation provides a convenient mechanism for interpreting that the total number of weights in the network is equivalent the learnt weights of a trained GLN. Contemporary neural to a GLN with context dimension 4 (the largest considered). networks have been criticized by some as “black boxes” that The GLN was trained with learning rate 10−4 and the MLP are notoriously difficult to interpret [YCN+15, ZZ18]. De- using the Adam optimizer [KB14] with learning rate 10−5, spite their high discrimination power, this can prove prob-
Gated Linear Networks Unlike contemporary neural networks, we demonstrate that 0 1 2 3 4 1.00 the halfspace-gated GLN architecture and learning rule is 0.75 naturally robust to catastrophic forgetting without any mod- 0.50 ifications or knowledge of task boundaries. We focus on 0.25 the pixel-permuted MNIST continual learning benchmark 0.00 5 6 7 8 9 0.25 of [GMX+13, KPR+17], which involves training on a se- 0.50 quence of different tasks where each task is obtained from a 0.75 different random permutation of the input pixels. We com- 1.00 pare the learning and retention characteristics of a GLN Figure 5. Saliency maps for constituent GLN binary classifiers of against an MLP baseline (of equal number of neurons, using one-vs-all MNIST classifier after a single training epoch. dropout as per the original paper) with and without elastic lematic for learning and debugging efficiently at the se- weight consolidation (EWC) [KPR+17], which is a highly- mantic level as well as for deployment in safety-critical effective method explicitly designed to prevent catastrophic real-world applications. This has led to the development forgetting by storing parameters of previously seen tasks. of gradient-based methods for post-hoc network analy- Our results are presented in Figure 6. As we train our mod- sis [SVZ13]. Such methods are not necessary for GLNs; els on a growing number of sequential tasks (rows), the for a given input, the collapsed multilinear polynomial of performance on all previously learnt tasks (columns) is eval- degree L is a weight vector of the same dimension (since uated. Note that the plotted task indices are not contiguous. W (z) has 1 row and W (z) has K columns) as the in- L 1 0 It is evident that the GLN outperforms EWC in terms of puts and provides a natural formulation for intuitive saliency both initial single-task learning (diagonal) and retention maps without any further computational expense. An exam- when both are trained for a single pass. Only when EWC is ple of the obtained saliency maps are provided in Figure 5 trained for multiple (ten) passes over the data does it exhibit for a one-versus-all GLN trained as an MNIST classifier. superior performance to a vanilla GLN. In all tests, the GLN One can clearly see that the characteristic shape of each substantially outperforms the standard MLP without EWC. hand-written character is preserved. To gain some intuition as to why GLNs are resilient to 7. Resilience to Catastrophic Forgetting catastrophic interference, recall from Section 4 that inputs close in terms of cosine similarity will give rise to similar Humans are able to acquire new skills throughout life seem- data dependent weight matrices. Since each task-specific ingly without compromising their ability to solve previously cluster of examples is far from each other in signature space, learnt tasks. Contemporary neural networks do not share the amount of interference between tasks is significantly this ability; if a network is trained on a task A and these reduced, with the gating essentially acting as an implicit weights are used to initialize training for a new task B, the weight hashing mechanism. ability to solve A rapidly degrades as training progresses on B. This phenomenon of “catastrophic forgetting” has 8. Online Benchmarking been well studied for decades [CG88, MC89, Rob95] but continues to limit the applicability of neural networks in MNIST Classification. First we explore the use of GLNs continual or lifelong learning scenarios. for online (single-pass) classification of the deskewed MNIST dataset [LBBH98, GW17]. We use 10 GLNs to Similar to the problem of model interpretability, many construct a one-vs-all classifier, each consisting of 128 neu- algorithms have been developed that augment standard rons per layer with context dimension 4. The learning rate training by meta-optimization synthesis to address catastrophic for- at each step t was set to min{100/t, 0.01}. We find that the getting. These methods typically fall into two main GLN is capable of impressive online performance, achiev- categories. The first approach involves replaying previ- ing 98% F1-score in a single pass of the training data. ously seen tasks during training using one of many heuris- tics [Rob95, Car97, RKSL17]. The other common category involves explicitly maintaining additional sets of model UCI Dataset Classification. We next compare GLNs to parameters related to previously learnt tasks. Examples a variety of general purpose batch learning techniques include freezing a subset of weights [DJV+13, RASC14], (SVMs, Gradient Boosting for Classification, MLPs) in dynamically adjusting learning rates [GDDM14] or aug- small data regimes on a selection of standard UCI datasets. menting the loss with regularization terms with respect to A 1000-500 neuron GLN with context-dimension 8 was past parameters [KPR+17, ZPG17, SLC+18]. A limitation trained with a single pass over 80% of instances and eval- of these approaches (aside from additional algorithmic and uated with frozen weights on the remainder. The compari- computational complexity) is that they require task bound- son MLP used ReLU activations and the same number of aries to be provided or accurately inferred. weights, and was trained for 100 epochs using the Adam
Gated Linear Networks Figure 6. Retention results for permuted MNIST. Models are trained sequentially on 8 tasks (rows) and evaluated on all previously encountered tasks (columns). For example, the top-right plot indicates performance on Task 1 after being trained sequentially on Tasks 1 to 8 inclusive (not all tasks shown). Each model only trains for one epoch per task, with the exception of “EWC 10 pass” and “MLP 10 pass” (shrunken 10-fold on x axis). Error bars denote 95% confidence levels over 10 random seeds. h a b e r m a sn e g m e n t b ag ll aa s n s c e - s c a l e iri bs a n k n o t e v mo t ue s h r o o m ycarucca tset model SVM GBC MLP GLN probability, we constructed an autoregressive density model over the 28 × 28 dimensional binary space by using 784 1.0 GLNs to model the conditional distribution for each pixel; 0.8 a row-major ordering was used to linearize the two dimen- sional pixel locations. Running our method online (i.e. a 0.6 single pass of the concatenated training, validation and test sets) gave an average loss of 79.0 nats per image across the 0.4 test data, and 80.74 nats per image if we held the parameters fixed upon reaching the test set. These results are close to 0.2 state of the art [VDOKK16] of any batch trained density model which outputs exact probabilities. 0.0 From an MDL or compression perspective, our density mod- elling results are significantly stronger in the sense that we could couple our model to an adaptive arithmetic decoder Figure 7. Online (single-pass) GLN classification F1-score on a and reproduce the original data from a file much smaller selection UCI datasets, compared to three contemporary batch than the original input. Contemporary batch trained density methods (Support Vector Machine, Gradient Boosting for Classifi- models do not have this property; to make a fair compar- cation, Multi-Layer Perceptron) trained for 100 epochs. ison, they would need to first encode the parameters of the model before encoding the subsequent compressed data, optimizer [KB14] with learning rate 0.001 and batch size and state-of-the-art batch train density models typically have 32. The SVM classifier used a radial basis function kernel K(x, x(cid:48)) = exp{−γ (cid:107)x − x(cid:48)(cid:107)2} with γ = 1/d, where d is compressed size many orders of magnitude larger than the original data. the input dimension. The GBC classifier was an ensemble of 100 trees of maximum depth 3 with a learning rate of 0.1. The mean and stderr over 100 random train/test splits 9. Conclusion are shown in the leftmost graph of Figure 7. Here we see We have introduced a new family of general purpose neu- that the single-pass GLN is competitive with the best of the ral architectures, Gated Linear Networks, and studied the batch learning results on each domain. desirable characteristics that follow from their use of data- dependent gating and local credit assignment. Their fast MNIST Density Modelling. Our final result is to use online learning properties, easy interpretability, and excel- GLNs and image specific gating to construct an online im- lent robustness to catastrophic forgetting in continual learn- age density model for the binarized MNIST dataset [LM11], ing settings makes them an interesting and complementary a standard benchmark for image density modeling. By ex- ploiting the chain rule P(X ) = (cid:81)d P(X | X ) of alternative to contemporary deep learning approaches. 1:d i=1 i <i
Gated Linear Networks References [GW17] Dibya Ghosh and Alvin Wan, 2017. https://fsix.github.io/mnist/ [B+15] Se´bastien Bubeck et al. Convex optimization: Al- gorithms and complexity. Foundations and Trends® [HAK07] Elad Hazan, Amit Agarwal, and Satyen Kale. in Machine Learning, 8(3-4):231–357, 2015. Logarithmic regret algorithms for online convex op- timization. Machine Learning, 69:169–192, 2007. [BFH+18] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal [Haz16] Elad Hazan. Introduction to online convex opti- Maclaurin, and Skye Wanderman-Milne. JAX: com- mization. Foundations and Trends in Optimization, posable transformations of Python+NumPy pro- 2(3-4):157–325, 2016. grams, 2018. [HCNB20] Tom Hennigan, Trevor Cai, Tamara Norman, [BH89] P. Baldi and K. Hornik. Neural networks and prin- and Igor Babuschkin. Haiku: Sonnet for JAX, 2020. cipal component analysis: Learning from examples [Hin02] Geoffrey E. Hinton. Training products of experts without local minima. Neural Networks, 2(1):53– by minimizing contrastive divergence. Neural Com- 58, January 1989. putation, 14(8):1771–1800, August 2002. [BHQK20] David Budden, Matteo Hessel, John Quan, and [Hor91] Kurt Hornik. Approximation capabilities of mul- Steven Kapturowski. RLax: Reinforcement Learn- tilayer feedforward networks. Neural networks, ing in JAX, 2020. 4(2):251–257, 1991. [Car97] Rich Caruana. Multitask learning. Machine Learn- [KB14] Diederik P Kingma and Jimmy Ba. Adam: A ing, 28(1):41–75, Jul 1997. method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [CG88] G. A. Carpenter and S. Grossberg. The art of adap- tive pattern recognition by a self-organizing neural [Kno17] Byron Knoll, 2017. network. Computer, 21(3):77–88, March 1988. http://www.byronknoll.com/cmix.html [Cha02] M.S. Charikar. Similarity estimation techniques [KPR+17] James Kirkpatrick, Razvan Pascanu, Neil Ra- from rounding algorithms. Conference Proceed- binowitz, Joel Veness, Guillaume Desjardins, An- ings of the Annual ACM Symposium on Theory of drei A. Rusu, Kieran Milan, John Quan, Tiago Ra- Computing, pages 380–388, 01 2002. malho, Agnieszka Grabska-Barwinska, Demis Has- sabis, Claudia Clopath, Dharshan Kumaran, and [Cut19] Ashok Cutkosky. Anytime Online-to-Batch, opti- Raia Hadsell. Overcoming catastrophic forgetting mism and acceleration. In Kamalika Chaudhuri in neural networks. Proceedings of the National and Ruslan Salakhutdinov, editors, Proceedings Academy of Sciences, 114(13):3521–3526, 2017. of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine [LBBH98] Yann Lecun, Lon Bottou, Yoshua Bengio, and Learning Research, pages 1446–1454, Long Beach, Patrick Haffner. Gradient-based learning applied to California, USA, June 2019. PMLR. document recognition. In Proceedings of the IEEE, pages 2278–2324, 1998. [DJV+13] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor [LM11] Hugo Larochelle and Iain Murray. The neural Darrell. Decaf: A deep convolutional activa- autoregressive distribution estimator. In Geoffrey tion feature for generic visual recognition. CoRR, Gordon, David Dunson, and Miroslav Dudk, edi- abs/1310.1531, 2013. tors, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, [GDDM14] Ross Girshick, Jeff Donahue, Trevor Darrell, volume 15 of Proceedings of Machine Learning and Jitendra Malik. Rich feature hierarchies for Research, pages 29–37, Fort Lauderdale, FL, USA, accurate object detection and semantic segmenta- 11–13 Apr 2011. PMLR. tion. 2014 IEEE Conference on Computer Vision and Pattern Recognition, Jun 2014. [Mah00] Matthew Mahoney. Fast text compression with neural networks. AAAI, 2000. [GMX+13] Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical [Mah05] Matthew Mahoney. Adaptive weighing of context investigation of catastrophic forgetting in gradient- models for lossless data compression. Technical based neural networks, 2013. Report, Florida Institute of Technology CS, 2005.
Gated Linear Networks [Mah13] Matthew Mahoney. Data Compression Explained. [SMG13] Andrew M. Saxe, James L. McClelland, and Dell, Inc, 2013. Surya Ganguli. Exact solutions to the nonlinear dy- namics of learning in deep linear neural networks. [Mat12] Christopher Mattern. Mixing strategies in data CoRR, abs/1312.6120, 2013. compression. In 2012 Data Compression Confer- ence, Snowbird, UT, USA, April 10-12, pages 337– [SVZ13] Karen Simonyan, Andrea Vedaldi, and Andrew 346, 2012. Zisserman. Deep inside convolutional networks: Vi- sualising image classification models and saliency [Mat13] Christopher Mattern. Linear and geometric mix- maps. arXiv preprint arXiv:1312.6034, 2013. tures - analysis. In 2013 Data Compression Confer- ence, DCC 2013, Snowbird, UT, USA, March 20-22, [SZ13] Ohad Shamir and Tong Zhang. Stochastic gradient 2013, pages 301–310, 2013. descent for non-smooth optimization: Convergence results and optimal averaging schemes. In Inter- [MC89] Michael McCloskey and Neal J. Cohen. Catas- national conference on machine learning, pages trophic interference in connectionist networks: The 71–79, 2013. sequential learning problem. volume 24 of Psychol- ogy of Learning and Motivation, pages 109 – 165. [VDOKK16] Aa¨ron Van Den Oord, Nal Kalchbrenner, and Academic Press, 1989. Koray Kavukcuoglu. Pixel recurrent neural net- works. In Proceedings of the 33rd International [MP69] Marvin Minsky and Seymour Papert. Perceptrons: Conference on International Conference on Ma- An Introduction to Computational Geometry. MIT chine Learning - Volume 48, ICML’16, pages 1747– Press, Cambridge, MA, USA, 1969. 1756. JMLR.org, 2016. [OWR+19] Pedro A. Ortega, Jane X. Wang, Mark Row- [VLB+17] Joel Veness, Tor Lattimore, Avishkar Bhoopc- land, Tim Genewein, Zeb Kurth-Nelson, Razvan hand, Agnieszka Grabska-Barwinska, Christopher Pascanu, Nicolas Heess, Joel Veness, Alexander Mattern, and Peter Toth. Online learning with gated Pritzel, Pablo Sprechmann, Siddhant M. Jayakumar, linear networks. arXiv preprint arXiv:1712.01897, Tom McGrath, Kevin Miller, Mohammad Ghesh- 2017. laghi Azar, Ian Osband, Neil C. Rabinowitz, Andra´s Gyo¨rgy, Silvia Chiappa, Simon Osin- [YCN+15] Jason Yosinski, Jeff Clune, Anh Nguyen, dero, Yee Whye Teh, Hado van Hasselt, Nando Thomas Fuchs, and Hod Lipson. Understanding de Freitas, Matthew Botvinick, and Shane Legg. neural networks through deep visualization. arXiv Meta-learning of sequential strategies. CoRR, preprint arXiv:1506.06579, 2015. abs/1905.03030, 2019. [Zin03] Martin Zinkevich. Online convex programming [RASC14] Ali Sharif Razavian, Hossein Azizpour, and generalized infinitesimal gradient ascent. In Josephine Sullivan, and Stefan Carlsson. Cnn Machine Learning, Proceedings of the Twentieth features off-the-shelf: An astounding baseline for International Conference (ICML 2003), August 21- recognition. 2014 IEEE Conference on Computer 24, 2003, Washington, DC, USA, pages 928–936, Vision and Pattern Recognition Workshops, Jun 2003. 2014. [ZPG17] Friedemann Zenke, Ben Poole, and Surya Gan- [RKSL17] Sylvestre-Alvise Rebuffi, Alexander guli. Continual learning through synaptic intelli- Kolesnikov, Georg Sperl, and Christoph H. gence. In Proceedings of the 34th International Lampert. icarl: Incremental classifier and rep- Conference on Machine Learning - Volume 70, resentation learning. 2017 IEEE Conference on ICML17, page 39873995. JMLR.org, 2017. Computer Vision and Pattern Recognition (CVPR), Jul 2017. [ZZ18] Quan-shi Zhang and Song-Chun Zhu. Visual inter- pretability for deep learning: a survey. Frontiers of [Rob95] Anthony V. Robins. Catastrophic forgetting, re- Information Technology & Electronic Engineering, hearsal and pseudorehearsal. Connect. Sci., 7:123– 19(1):27–39, 2018. 146, 1995. [SLC+18] Jonathan Schwarz, Jelena Luketina, Wo- jciech M. Czarnecki, Agnieszka Grabska- Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning, 2018.
Gated Linear Networks Additional Experiment Details Permuted-MNIST This section describes the implementation details and hyper- parameters for each experiment. GLN. We use one-vs-all GLNs composed of [100, 25, 1] neurons per layer and a context dimension of 6. The output of the network is determined by the last neuron. MLP and EWC. We use a ReLU network with [1000, 250, 10] neurons per layer. We use Adam [KB14] to optimize the cross-entropy loss using mini-batches of 20 data points. For EWC, we draw 100 samples for computing the Fisher matrix diagonals. We also optimize the constant that trades-off remembering and learning via grid-search, and denote it with λ in Table 1. Model log-learning rate dropout log λ GLN -4, -3, -2, -1 – – MLP -6, -5, -4, -3 Yes, No – EWC -6, -5, -4, -3 Yes, No 2, 3, 4 Table 1. Parameters explored during grid search. The best pa- rameters (shown in bold) are the ones that maximize the average F1-score over 10 random seeds. Logarithms are in base 10. MNIST saliency map experimental details We use one-vs-all GLNs composed of [50, 25, 1] neurons per layer and a context dimension of 4. The output of the network is determined by the last neuron. Constant learning rate of 10−2 is used to pass though the training data once. Learned weights for bias term are dropped from analysis. Computing Infrastructure All experiments were ran on single-GPU desktop PCs. Mod- els are implemented in JAX [BFH+18] making use of Haiku [HCNB20] and RLax [BHQK20].
