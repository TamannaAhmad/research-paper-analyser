Self-Supervised Sim-to-Real Adaptation for Visual Robotic Manipulation Rae Jeong, Yusuf Aytar, David Khosid, Yuxiang Zhou, Jackie Kay, Thomas Lampe, Konstantinos Bousmalis, Francesco Nori Abstract— Collecting and automatically obtaining reward signals from real robotic visual data for the purposes of training supervised learning algorithms can be quite challenging and time-consuming. Methods for utilizing unlabeled data can have a huge potential to further accelerate robotic learning. We consider here the problem of performing manipulation tasks from pixels. In such tasks, choosing an appropriate state representation is crucial for planning and control. This is even more relevant with real images where noise, occlusions and resolution affect the F1-score and reliability of state estimation. In this work, we learn a latent state representation implicitly with deep supervised learning in simulation, and then adapt it to the real domain using unlabeled real robot data. We propose to do so by optimizing sequence-based self- supervised objectives. These exploit the temporal nature of robot experience, and can be common in both the simulated and real domains, without assuming any alignment of underlying states in simulated and unlabeled real images. We propose Contrastive Forward Dynamics loss, which combines dynamics model learning with time-contrastive techniques. The learned state representation that results from our methods can be used to robustly solve a manipulation task in simulation and to successfully transfer the learned skill on a real system. We demonstrate the effectiveness of our approaches by training a vision-based supervised learning agent for cube stacking. Agents trained with our method, using only 5 hours of unlabeled real robot data for adaptation, shows a clear improvement over domain randomization, and standard visual domain adaptation techniques for sim-to-real transfer. I. INTRODUCTION Learning-based approaches, and specifically the ones that utilize the recent advances of deep learning, have shown strong generalization capacity and the ability to learn relevant Fig. 1: First step of our method trains a state-based and features for manipulation of real objects [1], [2], [3], [4], a vision-based agents in simulation. Using unlabeled real [5]. These features can be used to avoid explicit object pose data we then perform domain adaptation with a sequence- estimation [6] which is often inaccurate, even for known based self-supervised objective, that are computable on both objects, in the presence of occlusions and noise. Furthermore, simulated and real data. The simulated and real robot setups parameterization of the environment state with positions in we used are displayed at the bottom. R3 and rotations in SO(3) is not necessarily the best state representation for every task. Deep learning can provide task-relevant features and state representation directly from data. However, deep learning, Pixel-based agents trained in simulation do not gener- and especially deep supervised learning (RL), requires alize naively to the real world. However, recent sim-to- a significant amount of data, which is a critical challenge real transfer techniques have shown significant promise in for robotics [5]. For this reason, sim-to-real transfer is an reducing real-world sample complexity. Such techniques important area of research for vision-based robotic control either randomize the simulated environment in ways that help as simulations offer an abundance of labeled data. with generalization [7], [8], [9], use domain adaptation [10], or both [11]. Our work falls in the scope of unsupervised Authors are with DeepMind London, UK. {raejeong, yusufaytar, dkhosid, domain adaptation techniques, i.e. methods that are able to yuxiangzhou, kayj, thomaslampe, konstantinos, fnori}@google.com. Quali- utilize both labeled simulated and unlabeled real data. These tative results can be found in our supplementary video: https://youtu. be/pmLASU_MW_o have been successfully used both in computer vision [12] 9102 tcO 12 ]OR.sc[ 1v07490.0191:viXra
and in vision-based robot learning for manipulation [11] and that are robust under different transition dynamics. Unsu- locomotion [10]. pervised domain adaptation [12] has been successfully used The contribution of our work is two-fold: (a) we investi- for sim-to-real transfer in vision-based robotic grasping [11]. gate the use of sequence-based self-supervision as a way to Semi-supervised domain adaptation additionally utilizes any improve sim-to-real transfer; and (b) we develop contrastive labeled data that might be available, as was done by [11]. In forward dynamics (CFD), a self-supervised objective to many ways, zero-shot transfer, system identification, domain achieve that. We propose a two-step procedure (see Fig. adaptation–with or without labeled data in the real world–are 1) for such sequence-based self-supervised adaptation. In complementary groups of techniques. the first step, we use the simulated environment to learn c) Cube Stacking Task: Recent work on efficient multi- a policy that solves the task in simulation using synthetic task deep supervised learning [21] has shown the diffi- images and proprioception as observations. In the second culty of cube stacking task even in simulated environments step, we use synthetic and unlabeled real image sequences as the task requires several core abilities such as grasping, to adapt the state representation to the real domain. Besides lifting and precise placing. Sim-to-real method has also the task objective on the simulated images, this step also been applied for cube stacking task from vision where uses sequence-based self-supervision as a way to provide combination of domain randomization and imitation learning a common objective for representation learning that applies was used to perform zero-shot sim-to-real transfer of the in both simulation and reality without the need for paired cube stacking task [22]. However, the resulting policy only or aligned data. Our CFD objective additionally combines obtained a success rate of 35% over 20 trials in a limited dynamics model learning with time-contrastive techniques number of configurations reconfirming the difficulty of the to better utilize the structure of sequences in real robot data. cube stacking task. We demonstrate the effectiveness of our approach by d) Unsupervised Domain Adaptation: Unsupervised training a vision-based cube stacking RL agent. Our agent domain adaptation techniques are either feature-based or interacts with the real world with 20Hz closed-loop Cartesian pixel-based. Pixel-based adaptation is possible by changing velocity control from vision which makes our method appli- the observations to match those from the real environment cable to a large set of manipulation tasks. The cube stacking with image-based GANs [23]. Feature-based adaptation is task also emphasizes the generality of our approach for long done either by learning a transformation over fixed simulated horizon manipulation tasks. Most importantly, our method is and real feature representations, as done by [24] or by learn- able to make better use of the available unlabeled real world ing a domain-invariant feature extractor, also represented by data resulting in higher stacking performance, compared to a neural network [25], [26]. The latter has been shown to be domain randomization [13] and domain-adversarial neural more effective [26], and we employ a feature-level domain networks [14]. adversarial method [25] as a baseline. e) Sequence-based Self Supervision: Sequence-based II. RELATED WORK self-supervision is commonly applied for video represen- a) Manipulation: challenges and approaches: It is tation learning, particularly making use of local [27] and well acknowledged that both planning and state estimation global [28] temporal structures. Time-contrastive networks become challenging when performed in cluttered environ- (TCN) [29] utilize two temporally synchronous camera views ments [15]. During execution, continuously tracking the pose to learn view-independent high-level representations. By of manipulated objects becomes increasingly more difficult predicting temporal distance between frames, Aytar et al. in presence of occlusions, often caused by the gripper itself. [30] learn a representation that can handle small domain gaps Surveys reveal that pose estimation is still an essential (i.e. color changes and video artifacts) for the purpose of component in many approaches to grasping [1, fig. 3-5-7]; imitating YouTube gameplays in an Atari environment. To proposed approaches rely on some sort of supervision, either the best of our knowledge, sequence-based self-supervision in the form of model-based grasp quality measure [16], [17], for handling large visual domain gaps in sim-to-real transfer [18], or in the form of heuristics for grasp stability [1, fig. 18- for robotic learning have not been considered before. 19], or finally in the form of labelled data for learning [1, fig. 9]. III. OUR METHOD b) Sim-to-Real Transfer for Robotic Manipulation: In this section, we provide the detailed description of our Sim-to-real transfer learning aims to bridge the gaps between method for enabling sim-to-real transfer of visual robotic simulation and reality, which consist of differences in the manipulation. We propose a two stage training process. In dynamics and observation models such as image rendering. the first stage, state-based and vision-based agents are trained Sim-to-real transfer techniques can be grouped by the amount simultaneously in simulation with domain randomization. and kind of real world data they use. Techniques like domain We then collect unlabeled robot data by executing the randomization [9], [13] focus on zero-shot transfer. Others vision-based agent on the real robot. In the second stage, are able to utilize real data in order to adapt to the real we perform self-supervised domain adaptation by tuning the world via system identification or domain adaptation. Similar visual perception module with the help of sequence-based to system identification in classical control [19], recent self-supervised objectives optimized over simulation and real techniques like SimOpt [20] utilize real data to learn policies world data jointly.
Our method optimizes three main loss functions: (a) L RL is the supervised learning (RL) objective optimized by the state-based and vision-based agents in simulation, (b) L is the behavioral cloning loss utilized by the vision- BC based agent to speed up learning by imitating the state-based agent, and (c) L is the sequence-based self-supervised SS objective optimized on both simulation and real robot data. The purpose of L is to align the agent’s perception of real SS and simulated visuals by solving a common objective using a shared encoder. Our system is composed of four main neural networks: (a) an image representation encoder with parameters φ = {φ }L i i composed of L layers which embeds any visual observation o to a latent space as z = φ(o ), (b) a vision-based deep t t t Fig. 2: Left and right pixel observations in both real and policy network with parameters θ which combines the output domain randomized simulated environments. of the visual encoder with the proprioceptive observations and outputs an action, (c) a state-based policy network with parameters τ which takes the simulation state and outputs B. Second stage: Self-supervised sim-to-real adaptation an action, and (d) a self-supervised objective network with parameters ψ which takes the encoded visual observation z Although our vision-based agent can perform reasonably t (and action a if necessary) as input and directly computes well when transferred to the real robot, there is still sig- t the loss L . Fig. 1 presents a visual description of these nificant room for improvement, mostly due to the large SS components. In the remainder of this section, we discuss domain gap between simulation and the real robot. Our main the two stages of our method and present an objective for objective in this stage is to mitigate the negative effects of the sequence-based self-supervision. domain gap by utilizing the unlabeled robot data collected by our simulation-trained agent for domain adaptation. In A. First stage: Learning in simulation addition to well-explored domain adversarial training [25], In this stage we train a state-based agent and a vision- which we present as a strong baseline, we investigate the based agent with a shared experience replay. Our goal is to use of sequence-based self-supervised objectives for sim-to- speed up the learning process by leveraging the privileged real domain adaptation. information in simulation through the state-based agent, and Modality tuning [35], freezing the higher-level weights of distilling the learned skills into the vision-based agent using a trained network and adapting only the initial layers for a a shared replay buffer. Both of the agents are trained with an new modality (or domain), is a method shown to successfully off-policy supervised learning objective, L RL. We use a align multiple modalities (i.e. natural images, line drawings state of the art continuous control RL algorithm, Maximum and text descriptions), though it requires class labels in all a Posteriori Policy Optimization (MPO) [31], which uses modalities. In our context, it would require rewards for the an expectation-maximization-style policy optimization with real-world data which we do not have. Instead, we utilize a an approximate off-policy policy evaluation algorithm. As self-supervised objective while performing modality tuning shown in Fig. 1, the state-based agent has access to the (i.e. simulation-to-reality adaptation) which can be readily simulator state, which allows it to learn much faster than applied both in simulation and reality. However, there is no the vision-based agent that uses raw pixel observations. In guarantee that this alignment learned using a L objective SS essence, the state-based agent is an asymmetric behavior would indeed successfully transfer the vision-based policy policy, which provides diverse and relevant data for re- from simulation to the real world. In fact, different L SS inforcement learning of the vision-based agent. This idea objectives would result in different transfer performances. leverages the flexibility of off-policy RL, which has been Finding a suitable L objective for better transfer of the SS shown to improve sample complexity in a single-domain set- learned policy is of major importance as well. ting [32]. Additionally, we also utilize the behavioral cloning In the context of our neural network architecture, while (BC) objective [33] for the vision-based agent to imitate applying the modality tuning, we freeze the vision-based the state-based agent. L BC provides reliable training and agent’s policy network parameters θ and the encoder pa- further improves sample efficiency in the learning process, as rameters φ except for the first layer φ . This allows the 1 we show in Sect. V. We additionally employ DDPGfD [34] system to adapt its visual perception to the real world without which injects human demonstrations to the replay buffer and making major changes in the policy logic, which we expect asymmetric actor-critic for our stacking experiments. Our to be encoded in the higher layers of the neural network. final objective in the first stage can be written as follows: We also continue optimizing the L and L objectives RL BC along with L to ensure that as φ is adapting itself to SS 1 min L RL + L BC (1) solve the L , it also maintains good performance for the φ,θ,τ SS manipulation task. In other words, φ is forced to adapt itself 1
without compromising the performance of the vision-based agent. The final objective in the second stage is: min L + L + L (2) RL BC SS φ1,ψ Due to its wide adoption in the robotics settings, we employ the Time-Contrastive Networks (TCN) [29] objective for L in our self-supervised sim-to-real adaptation method, SS though any other sequence-based self-supervised objective can also be used here. In the next subsection we introduce an alternative loss for L which makes use of domain- SS Fig. 3: Rollouts of the multi-step future predictions in the specific properties of robotics, therefore potentially result in learned latent space. For instance, zˆ and zˆˆ are one and t+2 t+2 better transferable alignment. two step predictions of z , respectively. In our experiments, t+2 we use 5 step prediction for a trajectory length of 32. C. Contrastive Forward Dynamics Time-Contrastive Networks (TCN) [29], which we use as a baseline, and other sequence-based self-supervision methods In practice, while forming the negative pairs we pick [30], [36], [37], mainly exploit the temporal structure of all the other latent observations in the same mini-batch, the observations. However, with robot data we also have which also contains observations from the same sequence. physical dynamics of the real world probed by actions and To further enforce the prediction quality, we perform multi- perceived through observations. In this section we describe step future predictions by continuously applying the forward the contrastive forward dynamics (CFD) objective, which is dynamics model. These longer horizon predictions optimize able to utilize both observations and actions by learning a the same objective given in Eq. 3 where zˆ is replaced with i forward dynamics model in a latent space. Essentially we are any multi-step prediction of z . Fig. 3 illustrates how multi- i learning the latent transition dynamics of the environment step predictions are obtained using a single forward dynamics which has strong connections to the model-based optimal model. control approaches [38]. Therefore we can expect that the alignment achieved through our CFD objective potentially IV. SIMULATED AND REAL ENVIRONMENTS AND TASKS better transfers the learned policy from simulation to real The primary manipulation task we have used in this work world. We formally define the CFD objective below. is vision-based stacking of one cube on top of another. Assume we are given a dataset of sequences where each However, as this is a particularly hard task to solve [21] sequence s = {(o t, a t)}T t is of length T . o t denotes observa- from pixels from scratch with off-the-shelf RL algorithms, tions and a t denotes the actions at time t. Any observation we studied the ablation effects of different components of our o t is embedded into a latent space as z t = φ(o t) through the proposed RL framework on the easier problem of vision- encoder network φ. Given a transition (z t, a t, z t+1) in the based lifting instead. As lifting is an easier task, and a latent space, the forward dynamics model predicts the next required skill towards achieving stacking, we focused on the latent state as zˆ t+1 = f (z t, a t) where f is the prediction latter for the rest of our experimental analysis in simulation network. Instead of learning f by minimizing the prediction and for all our real world evaluations. error ||zˆ − z ||, which has a trivial solution achieved t+1 t+1 Fig. 1 shows our real robot setup, which is composed of by setting the latents to zero, we minimize a contrastive a 7-DoF Sawyer robotic arm, a basket and two cubes. The prediction loss. A contrastive loss [39], [40] takes pairs of agent receives the front left and right 64 × 64 RGB camera examples as input and predicts whether the two elements in images as observations, shown in Fig. 2. The two cameras are the pair are from the same class or not. It can also be im- positioned in a way that can help disambiguate 3D positions plemented as a multi-class classification objective comparing of the arm and the objects. In addition to these images, our one positive pair and multiple negative pairs [41], creating an observations also consist of the pose of the cameras, end- embedding space by pushing representations from the same effector position and angle, and the gripper finger angle. The “class” together and ones from different “classes” apart. In action space of the agent is 4D Cartesian velocity control of our context, (zˆ , z ) is our positive pair and any other non- i i the end effector, with an additional action for actuating the matching pairs (zˆ , z ) where k (cid:54)= i are the negative pairs. i k gripper. The real environment is modelled in simulation using With CFD, we solve such a multi-class classification problem the MuJoCo [42] simulator. Fig 1 also shows the simulated by minimizing the cross-entropy loss for any given latent version of our environment. Unless mentioned otherwise, observation z and its prediction zˆ as follows: i i all of our policies are trained in simulation with domain randomization and a shaped reward functions. (cid:40) (cid:41) The shaped reward function for lifting is a combination e−||zˆi−zi|| m φ,i fn − log e−||zˆi−zi|| + (cid:80) e−||zˆi−zk|| (3) of reaching, touching and lifting rewards. Let d gripper be k(cid:54)=i the Euclidean distance of a target object from the pinch
Training Method Task Success Domain Randomization 46.0 % End-to-End DANN 50.0 % SSDA with TCN 38.0 % DANN 50.0 % Two-Stage SSDA with TCN (Ours) 54.0 % SSDA with CFD (Ours) 62.0 % TABLE I: Sim-to-real transfer performance for vision-based cube stacking agent with unsupervised domain adaptation using DANN, self-supervised domain adaptation (SSDA) using TCN and CFD for the end-to-end and two-stage methods. Method Task Success SSDA without Task Objective 12.0 % SSDA with Task Objective (Ours) 62.0 % Fig. 4: Cube stacking performance in simulation for two- TABLE II: Cube stacking performance on the real system for stage self-supervised domain adaptation (SSDA) with CFD two-stage self-supervised domain adaptation (SSDA) with jointly optimized with and without the task objective. CFD optimized with and without the task objective. V. EXPERIMENTAL RESULTS AND DISCUSSION site of the end effector, and h t, h o be the target height and In this section, we discuss the details of our experiments, object height from the ground in meters. Our reach reward and attempt to answer the following questions: (a) Can is defined as r reach = 1(d gripper < 0.01), where 1 is the sequence-based self-supervision be used as a common auxil- indicator function. In practice we use reward shaping with iary objective for simulated and real data without degrading the Gaussian tolerance reward function as defined in the task performance in simulation? (b) Does doing so improve DeepMind Control Suite [43], with bounds [0, 0.01] and a final task performance in the real world? (c) How does using margin of 0.25. Our touch reward r touch = 1 contact is binary sequence-based self supervision for visual domain align- and provided by our simulator upon contact with the object. ment between simulation and reality compare with domain- Our lift reward is r lift = 1(|h t − h o| < 0.1 ∧ h o < 0.01) adversarial adaptation? (d) Is the use of actions in such a self- and the final shaped version we use during training: supervised loss important for bridging the sim-to-real domain r lift shaped = r reach + 1+rt 2ouch + r lift. As before, gap? (e) What is the performance difference of modality in practice the distance |h t − h o| is passed through tuning in our two-stage approach versus a one-stage end-to- the same tolerance function as above, with bounds end approach? and (f) What are the effects of the different [0, 0.1] and a margin of 15. For stacking we now components of our RL framework in solving manipulation have a top and a bottom target objects with positions tasks from scratch, i.e. without the shared replay buffer or x t, x b. If the cubes are in contact and on top of each behavior cloning, in simulation? other, the reward is 1. Otherwise, we have additional shaping to aid with training. More specifically, if A. Self-Supervised Sim-to-Real Adaptation h (x ) ≤ 0.025m we revert to a normalized lift reward o t We evaluated the following methods on our vision-based for the top object r = rlift shaped(xt) . Otherwise, stack 5 cube stacking task: domain randomization [44], unsupervised r stack = 2+rreach(xb)+1(ho(xt)> 5ho(xb)∨||xt−xb||2<0.07) , to domain adaptation with a domain adversarial (DANN) [14] account for bringing the cubes closer to each other. In loss, and self-supervised domain adaptation (SSDA) with practice we set r reach(x b) = 1 if it’s greater than 0.75. two sequence-based self-supervised objectives: the time- In the real world, the cubes are fitted with AR tags that are contrastive networks (TCN) [29] loss, and the contrastive only used for the purposes of fair and consistent evaluation forward dynamics (CFD) loss we proposed in Sect. III-C. We of our resulting policies: the 3D poses of the cubes are ablate two different training methods for domain adaptation, never available to an RL agent during training or testing. end-to-end and two-stage. The end-to-end training method At the beginning of every episode, the cubes are placed simply optimize Eq. 2 from Sect. III-B with respect to all in a random position by a hand-crafted controller. All real parameters, without the two-stage procedure described in world evaluations referred to in the rest of the section are on Sect. III-B. This means that all of the losses are jointly the stacking task and consist of 50 episodes. A real world optimized without freezing any part of the neural network. episode is considered a success if the green cube is on top Two-stage training procedure is described in Sect. III and of the yellow cube at any point throughout the episode. employs modality tuning [35]. Episodes are of length 200 with 20Hz control rate for both Table I shows the quantitative results from evaluating simulated and real environments. task success on the real robot. These experiments show
Fig. 5: Ablation of techniques used in conjunction with RL Fig. 6: Simulation performance on our vision-based stacking for cube lifting task in simulation. The plot shows the average task of our RL framework with and without behavior cloning return for the lifting task with and without shared replay (BC). Using BC results in faster training that maintains buffer and behavior cloning (BC). RL from state and RL stability with the addition of auxiliary adaptation objectives. from vision are trained only with the RL objective. method at the core of our framework, struggles with solving that DANN improves on top of the domain randomization this task, contrary to an MPO agent with access to the full baseline by a small margin. However, end-to-end adaptation state information. By sharing the replay buffer between the with the TCN loss results in degradation of performance. state-based agent and the vision-based agent, one can see that This is likely due to insufficient sharing of the encoder be- the vision-based agent is able to solve lifting in a reasonable tween the self-supervised objective using simulated data and amount of time. The addition of the behavior cloning (BC) real data. On the other hand, the two-stage self-supervised objective further improves the speed and stability of training. domain adaptation with TCN significantly improves over the Fig. 6 shows the even more profound effect our BC end-to-end variant and domain randomization baselines. This objective has on learning our vision-based cube stacking task. reconfirms that modality tuning used in the two-stage training Furthermore, one can also observe the stability of the method method results in significantly better sharing of the encoder. persists even when jointly training, end-to-end, with the TCN Finally, the two-stage self-supervised adaptation with our loss, or the DANN loss with real world data. CFD objective, which utilizes both the temporal structure VI. CONCLUSION of the observations and the actions, performs significantly better when compared to all other methods, yielding a 62 % In this work, we have presented our self-supervised do- task success. main adaptation method, which uses unlabeled real robot We also evaluated the importance of jointly optimizing data to improve sim-to-real transfer learning. Our method the RL and BC objectives in Eq. 2 for the two-stage self- is able to perform domain adaptation for sim-to-real trans- supervised domain adaptation. As one can see in Table II, fer learning of cube stacking from visual observations. In only optimizing L without the task objective significantly addition to our domain adaptation method, we developed SS reduces the performance. Fig. 4 further shows how the task contrastive forward dynamics (CFD), which combines dy- performance in simulation degrades when optimizing only namics model learning with time-contrastive techniques to the self-supervised objective. In essence, by only optimizing better utilize the structure available in unlabeled robot data. the self-supervised loss, the network catastrophically for- We demonstrate that using our CFD objective for adaptation gets [45] how to solve the manipulation task. yields a clear improvement over domain randomization, other self-supervised adaptation techniques and domain adversarial B. Ablations for different components of our RL framework methods. In order to assess the necessity and efficacy of the different Through our experiments, we discovered that optimizing components of our framework, described in Sect. III-A, only the first visual layers of the policy network in com- we provide ablation experimental results. Specifically we bination with jointly optimizing the supervised learning, examined the effects of the state-based agent that share a behavior cloning and self-supervised loss was necessary for replay buffer with the vision-based agent, and the addition of a successful application of self-supervised learning for sim- an auxiliary behavior cloning objective for the vision-based to-real transfer for robotic manipulation. Finally, the use agent to imitate the state-based agent. Fig. 5 shows these of sequence-based self-supervised loss by leveraging the effects on the cube lifting task. A vision-based agent trained dynamical structure in the robotic system ultimately resulted with MPO [31], the state-of-the-art continuous control RL in the best domain adaptation for our manipulation task.
REFERENCES [24] R. Caseiro, J. F. Henriques, P. Martins, and J. Batista, “Beyond the shortest path: Unsupervised Domain Adaptation by Sampling [1] J. Bohg, A. Morales, T. Asfour, and D. Kragic, “Data-driven grasp Subspaces Along the Spline Flow,” in CVPR, 2015. synthesis-a survey,” IEEE Transactions on Robotics, 2014. [25] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavio- [2] U. Viereck, A. t. Pas, K. Saenko, and R. Platt, “Learning a visuomotor lette, M. Marchand, and V. Lempitsky, “Domain-adversarial training of controller for real world robotic grasping using easily simulated depth neural networks,” The Journal of Machine Learning Research, vol. 17, images,” CoRL, 2017. no. 1, pp. 2096–2030, 2016. [3] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, [26] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan, and K. Goldberg, “Dex-Net 2.0: Deep learning to plan robust grasps “Domain separation networks,” in NIPS, 2016. with synthetic point clouds and analytic grasp metrics,” in RSS, 2017. [27] B. Fernando, H. Bilen, E. Gavves, and S. Gould, “Self-supervised [4] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen, “Learning hand- video representation learning with odd-one-out networks,” in Pro- eye coordination for robotic grasping with deep learning and large- ceedings of the IEEE conference on computer vision and pattern scale data collection,” CoRR, vol. abs/1603.02199, 2016. recognition, 2017, pp. 3636–3645. [5] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, [28] D. Wei, J. J. Lim, A. Zisserman, and W. T. Freeman, “Learning and D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine, using the arrow of time,” in Proceedings of the IEEE Conference on “Qt-opt: Scalable deep supervised learning for vision-based robotic Computer Vision and Pattern Recognition, 2018, pp. 8052–8060. manipulation,” CoRR, vol. abs/1806.10293, 2018. [29] P. Sermanet, C. Lynch, J. Hsu, and S. Levine, “Time-contrastive net- [6] B. Siciliano and O. Khatib, Springer Handbook of Robotics. Secau- works: Self-supervised learning from multi-view observation,” CoRR, cus, NJ, USA: Springer-Verlag New York, Inc., 2007. vol. abs/1704.06888, 2017. [7] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Sim-to- [30] Y. Aytar, T. Pfaff, D. Budden, T. Paine, Z. Wang, and N. de Freitas, real transfer of robotic control with dynamics randomization,” CoRR, “Playing hard exploration games by watching youtube,” in Advances vol. abs/1710.06537, 2017. in Neural Information Processing Systems, 2018, pp. 2930–2941. [8] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, [31] A. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess, J. Ibarz, S. Levine, R. Hadsell, and K. Bousmalis, “Sim-to-real via sim- and M. A. Riedmiller, “Maximum a posteriori policy optimisation,” to-sim: Data-efficient robotic grasping via randomized-to-canonical CoRR, vol. abs/1806.06920, 2018. adaptation networks,” CoRR, vol. abs/1812.07252, 2018. [32] D. Schwab, J. T. Springenberg, M. F. Martins, T. Lampe, M. Ne- [9] OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Jo´zefowicz, unert, A. Abdolmaleki, T. Hertweck, R. Hafner, F. Nori, and M. A. B. McGrew, J. W. Pachocki, J. Pachocki, A. Petron, M. Plappert, Riedmiller, “Simultaneously learning vision and feature-based control G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, policies for real-world ball-in-a-cup,” CoRR, vol. abs/1902.04706, L. Weng, and W. Zaremba, “Learning dexterous in-hand manipula- 2019. tion,” CoRR, vol. abs/1808.00177, 2018. [33] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel, [10] G. J. Stein and N. Roy, “Genesis-rt: Generating synthetic images “Overcoming exploration in supervised learning with demonstra- for training secondary real-world tasks,” in 2018 IEEE International tions,” CoRR, vol. abs/1709.10089, 2017. Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. [34] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, 7151–7158. N. Heess, T. Rotho¨rl, T. Lampe, and M. A. Riedmiller, “Leveraging [11] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakr- demonstrations for deep supervised learning on robotics problems ishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, and with sparse rewards,” CoRR, vol. abs/1707.08817, 2017. V. Vanhoucke, “Using simulation and domain adaptation to improve [35] Y. Aytar, L. Castrejon, C. Vondrick, H. Pirsiavash, and A. Torralba, efficiency of deep robotic grasping,” CoRR, vol. abs/1709.07857, 2017. “Cross-modal scene networks,” IEEE transactions on pattern analysis [12] G. Csurka, “Domain adaptation for visual applications: A comprehen- and machine intelligence, 2017. sive survey,” arxiv:1702.05374, 2017. [36] I. Misra, C. L. Zitnick, and M. Hebert, “Shuffle and learn: un- supervised learning using temporal order verification,” in European [13] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, Conference on Computer Vision. Springer, 2016, pp. 527–544. “Domain randomization for transferring deep neural networks from [37] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with simulation to the real world,” CoRR, vol. abs/1703.06907, 2017. contrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018. [14] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavio- [38] L. Grne and J. Pannek, Nonlinear Model Predictive Control: Theory lette, M. Marchand, and V. Lempitsky, “Domain-Adversarial Training and Algorithms. Springer Publishing Company, Incorporated, 2013. of Neural Networks,” arXiv e-prints, May 2015. [39] S. Chopra, R. Hadsell, Y. LeCun, et al., “Learning a similarity metric [15] A. Billard and D. Kragic, “Trends and challenges in robot manipula- discriminatively, with application to face verification,” in CVPR (1), tion,” Science, vol. 364, no. 6446, p. eaat8414, 2019. 2005, pp. 539–546. [16] V.-D. Nguyen, “Constructing Force-Closure Grasps,” IJRR, 1988. [40] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction [17] A. Rodriguez, M. T. Mason, and S. Ferry, “From caging to grasping,” by learning an invariant mapping,” in 2006 IEEE Computer Society IJRR, 2012. Conference on Computer Vision and Pattern Recognition (CVPR’06), [18] S. Makita and W. Wan, “A Survey of Robotic Caging and its vol. 2. IEEE, 2006, pp. 1735–1742. Applications,” Advanced Robotics, vol. 0, no. 0, pp. 1–15, 2017. [41] K. Sohn, “Improved deep metric learning with multi-class n-pair loss [19] S. Kolev and E. Todorov, “Physically consistent state estimation and objective,” in Advances in Neural Information Processing Systems, system identification for contacts,” in 2015 IEEE-RAS 15th Interna- 2016, pp. 1857–1865. tional Conference on Humanoid Robots (Humanoids), Nov 2015, pp. [42] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for 1036–1043. model-based control,” in Intelligent Robots and Systems (IROS), 2012 [20] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, IEEE/RSJ International Conference on. IEEE, 2012, pp. 5026–5033. N. D. Ratliff, and D. Fox, “Closing the sim-to-real loop: Adapting [43] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. de Las Casas, D. Bud- simulation randomization with real world experience,” CoRR, vol. den, A. Abdolmaleki, J. Merel, A. Lefrancq, T. P. Lillicrap, and M. A. abs/1810.05687, 2018. Riedmiller, “Deepmind control suite,” ArXiv, vol. abs/1801.00690, [21] M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, 2018. V. Mnih, N. Heess, and J. T. Springenberg, “Learning by playing [44] F. Sadeghi and S. Levine, “CAD2RL: Real single-image flight without solving sparse reward tasks from scratch,” in International Conference a single real image.” in RSS, 2017. on Machine Learning, 2018, pp. 4341–4350. [45] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, [22] Y. Zhu, Z. Wang, J. Merel, A. Rusu, T. Erez, S. Cabi, S. Tun- A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, yasuvunakool, J. Kramr, R. Hadsell, N. de Freitas, and N. Heess, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell, “Overcoming “Reinforcement and imitation learning for diverse visuomotor skills,” catastrophic forgetting in neural networks,” Proceedings of the Na- in Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsyl- tional Academy of Sciences of the United States of America, vol. 114 vania, June 2018. 13, pp. 3521–3526, 2016. [23] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, “Unsupervised pixel-level domain adaptation with generative adver- sarial networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 3722–3731.
