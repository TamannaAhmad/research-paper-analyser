Sparse Orthogonal Variational Inference for Gaussian Processes Jiaxin Shi Michalis K. Titsias Andriy Mnih Tsinghua University DeepMind DeepMind Abstract Training Prediction We introduce a new interpretation of sparse variational approximations for Gaussian pro- cesses using inducing points, which can lead to more scalable algorithms than previous methods. It is based on decomposing a Gaussian process as a sum of two indepen- dent processes: one spanned by a finite ba- sis of inducing points and the other captur- ing the remaining variation. We show that this formulation recovers existing approxima- tions and at the same time allows to obtain tighter lower bounds on the marginal likeli- hood and new stochastic variational inference Figure 1: The graphical model of SOLVE-GP. The algorithms. We demonstrate the efficiency of prior f ∼ GP(0, k) is decomposed into two indepen- these algorithms in several Gaussian process dent GPs (denoted by thick horizontal lines): f ∼ p (cid:107) (cid:107) models ranging from standard regression to and f ∼ p . The variables connected by thick lines ⊥ ⊥ multi-class classification using (deep) convo- form a multivariate Gaussian. X, y denote the training lutional Gaussian processes and report state- data. X∗ are the test inputs. f = f (X), f = f (X). (cid:107) (cid:107) ⊥ ⊥ of-the-art results on CIFAR-10 among purely u = f (Z) denote the inducing variables in standard (cid:107) GP-based models. SVGP methods. SOLVE-GP introduces another set of inducing variables v = f (O) to summarize p . ⊥ ⊥ ⊥ 1 INTRODUCTION they suffer from O(N 3) computation and O(N 2) stor- Gaussian processes (GP) (Rasmussen and Williams, age requirements given N training data points, which 2006) are nonparametric models for representing dis- has motivated a large body of research on sparse GP tributions over functions, which can be seen as a gener- methods (Csato and Opper, 2002; Lawrence et al., alization of multivariate Gaussian distributions to infi- 2002; Seeger et al., 2003; Quin˜onero-Candela and Ras- nite dimensions. The simplicity and elegance of these mussen, 2005a; Titsias, 2009; Hensman et al., 2013; models has led to their wide adoption in uncertainty Bui et al., 2017). GPs have also been unfavourably estimation for machine learning, including supervised compared to deep learning models for lacking repre- learning (Williams and Rasmussen, 1996; Williams sentation learning capabilities. and Barber, 1998), sequential decision making (Srini- Sparse variational GP (SVGP) methods (Titsias, 2009; vas et al., 2010), model-based planning (Deisenroth Hensman et al., 2013, 2015a) based on variational and Rasmussen, 2011), and unsupervised data analy- learning of inducing points have shown promise in ad- sis (Lawrence, 2005; Damianou et al., 2016). dressing these limitations. Such methods leave the Despite the successful application of these models, prior distribution of the GP model unchanged and instead enforce sparse structures in the posterior ap- Proceedings of the 23rdInternational Conference on Artifi- proximation though variational inference. This gives cial Intelligence and Statistics (AISTATS) 2020, Palermo, O(M 2N + M 3) computation and O(M N + M 2) stor- Italy. PMLR: Volume 108. Copyright 2020 by the au- age with M inducing points. Moreover, they allow us thor(s). to perform mini-batch training by sub-sampling data 2202 beF 22 ]LM.tats[ 4v69501.0191:viXra
Sparse Orthogonal Variational Inference for Gaussian Processes points. Successful application of SVGP allowed scal- sification, we also evaluated our method on a range able GP models trained on billions of data points (Sal- of regression datasets that range in size from tens of imbeni and Deisenroth, 2017). These advances in thousands to millions of data points. Our results show inference methods have also led to more flexibility that SOLVE-GP is often competitive with the more in model design. A recent convolutional GP model expensive SVGP counterpart that uses the same num- (van der Wilk et al., 2017) encodes translation invari- ber of inducing points, and outperforms SVGP when ance by summing over GPs that take image patches as given the same computational budget. inputs. The inducing points, which can be interpreted as image patches in this model, play a role similar to 2 BACKGROUND that of convolutional filters in neural networks. Their work showed that it is possible to implement repre- Here, we briefly review Gaussian processes and sparse sentation learning in GP models. Further extensions variational GP methods. A GP is an uncountable col- of such models into deep hierarchies (Blomqvist et al., lection of random variables indexed by a real-valued 2018; Dutordoir et al., 2019) significantly boosted the vector x taking values in X ⊂ Rd, of which any finite performance of GPs for natural images. subset has a multivariate Gaussian distribution. A GP As these works suggest, currently the biggest challenge is defined by a mean function m(x) = E[f (x)] and a in this area still lies in scalable inference. The compu- covariance function k(x, x(cid:48)) = Cov[f (x), f (x(cid:48))]: tational cost of the widely used SVGP methods scales f ∼ GP(m(x), k(x, x(cid:48))). cubically with the number of inducing points, mak- ing it difficult to improve the flexibility of posterior Let X = [x , x , . . . , x ](cid:62) ∈ RN×d be (the matrix con- approximations (Shi et al., 2019). For example, state- 1 2 N taining) the training data points and f = f (X) ∈ RN of-the-art models like deep convolutional GPs use only denote the corresponding function values. Similarly 384 inducing points for inference in each layer to get a we denote the test data points by X∗ and their func- manageable running time (Dutordoir et al., 2019). tion values by f ∗. Assuming a zero mean function, the We introduce a new framework, called SOLVE-GP, joint distribution over f , f ∗ is given by: which allows increasing the number of inducing points (cid:18) (cid:12) (cid:20) (cid:21)(cid:19) given a fixed computational budget. It is based on p(f , f ∗) := N f (cid:12) (cid:12) 0, K ff K f∗ , decomposing the GP prior as the sum of a low-rank f ∗ (cid:12) K ∗f K ∗∗ approximation using inducing points, and a full-rank where K is an N × N kernel matrix with its (i, j)th residual process. We observe that the standard SVGP ff entry as k(x , x ), and similarly [K ] = k(x , x∗), methods can be reinterpreted under such decompo- i j f∗ ij i j [K ] = k(x∗, x∗). In practice we often observe the sition. By introducing another set of inducing vari- ∗∗ ij i j training function values through some noisy measure- ables for the orthogonal complement, we can increase ments y, generated by the likelihood function p(y|f ). the number of inducing points at a much lower ad- For regression, the likelihood usually models indepen- ditional computational cost. With our method dou- dent Gaussian observation noise: y = f + (cid:15) , (cid:15) ∼ bling the number of inducing points leads to a 2-fold n n n n N (0, σ2). In this situation the exact posterior distri- increase in the cost of Cholesky decomposition, com- bution p(f ∗|y) can be computed in closed form: pared to the 8-fold increase for the original SVGP method. We show that SOLVE-GP is equivalent to f ∗|y ∼ N (K (K + σ2I)−1y, ∗f ff a structured covariance approximation for SVGP de- K − K (K + σ2I)−1K ). (1) fined over the union of the two sets of inducing points. ∗∗ ∗f ff f∗ Interestingly, under such interpretation our work can As seen from Eq. (1), exact prediction involves the in- be seen as a generalization of the recently proposed verse of matrix K + σ2I, which requires O(N 3) com- ff decoupled-inducing-points method (Salimbeni et al., putation. For large datasets, we need to avoid the 2018). As the decoupled method often comes with cubic complexity by resorting to approximations. a complex dual formation, our framework provides a simpler derivation and more intuitive understanding Inducing points have played a central role in previ- for it. ous works on scalable GP inference. The general idea is to summarize f with a small number of variables We conducted experiments on convolutional GPs and u = f (Z), where Z = [z , . . . , z ](cid:62) ∈ RM×d is a 1 M their deep variants. To the best of our knowledge, we set of parameters, called inducing points, in the input are the first to train a purely GP-based model with- space. The augmented joint distribution over u, f , f ∗ out any neural network components to achieve over is p(f , f ∗|u)p(u), where p(u) = N (0, K ) and K uu uu 80% test F1-score on CIFAR-10. No data augmen- denotes the kernel matrix of inducing points with the tation was used to obtain these results. Besides clas- (i, j)th entry corresponding to k(z , z ). There is a i j
Jiaxin Shi, Michalis K. Titsias, Andriy Mnih long history of developing sparse approximations for pend on u.1 Therefore, samples from p(f |u) can be GPs by making different independence assumptions reparameterized as for the conditional distribution p(f , f ∗|u) to reduce f ∼ p (f ) := N (0, K − Q ), the computational cost (Quin˜onero-Candela and Ras- ⊥ ⊥ ⊥ ff ff mussen, 2005b). However, these methods made mod- f = f + K K−1u. (4) ⊥ fu uu ifications to the GP prior and tended to suffer from The reason for denoting the zero-mean component as degeneracy and overfitting problems. f shall become clear later. Now we can reparameter- ⊥ Sparse variational GP methods (SVGP), first pro- ize the augmented prior distribution p(f , u) as posed in Titsias (2009) and later extended for mini- batch training and non-conjugate likelihoods (Hens- u ∼ p(u), f ⊥ ∼ p ⊥(f ⊥), f = K fuK− uu1u + f ⊥, (5) man et al., 2013, 2015a), provide an elegant solution and the joint distribution of the GP model becomes to these problems. By reformulating the posterior in- ference problem as variational inference and restrict- p(y, u, f ) = p(y|f + K K−1u)p(u)p (f ). (6) ing the variational distribution to be q(f , f ∗, u) := ⊥ ⊥ fu uu ⊥ ⊥ q(u)p(f , f ∗|u), the variational lower bound for mini- Posterior inference for f in the original model then mizing KL [q(f , f ∗, u)(cid:107)p(f , f ∗, u|y)] simplifies to: turns into inference for u and f . If we approximate ⊥ the above GP model by considering a factorised ap- (cid:88)N E [log p(y |f )] − KL [q(u)(cid:107)p(u)] . (2) proximation q(u)p ⊥(f ⊥), where q(u) is a variational q(u)p(fn|u) n n distribution and p ⊥(f ⊥) is the prior distribution of f ⊥ n=1 that appears also in Eq. (6), we arrive at the stan- dard SVGP method. To see this, note that minimizing For GP regression the bound has a collapsed form ob- KL [q(u)p (f )(cid:107)p(u, f |y)] is equivalent to maximiz- tained by solving for the optimal q(u) and plugging it ⊥ ⊥ ⊥ ing the variational lower bound into (2) (Titsias, 2009): E log p(y|f + K K−1u) − KL [q(u)(cid:107)p(u)] , log N (y|0, Q + σ2I) − 1 tr (K − Q ) , (3) q(u)p⊥(f⊥) ⊥ fu uu ff 2σ2 ff ff which is the SVGP objective (Eq. (2)) using the repa- rameterization in Eq. (4). where Q = K K−1K . Computing this objec- ff fu uu uf tive requires O(M 2N + M 3) operations, in contrast Under this interpretation of the standard SVGP to the O(N 3) complexity of exact inference. The in- method, it becomes clear that we can modify the form ducing points Z can be learned as variational param- of the variational distribution q(u)p (f ) to improve ⊥ ⊥ eters by maximizing the lower bound. More generally, the F1-score of the posterior approximation. There if we do not collapse q(u) and let q(u) = N (m , S ), are two natural options: (i) keep p (f ) as part of the u u ⊥ ⊥ where m , S are trainable parameters, we can use the approximation and alter q(u) so that it will have some u u uncollapsed bound for mini-batch training and non- dependence on f , and (ii) keep q(u) independent from ⊥ Gaussian likelihoods (Hensman et al., 2013, 2015a). f , and replace p (f ) with a more structured varia- ⊥ ⊥ ⊥ tional distribution q(f ). While both options lead to ⊥ new bounds and more accurate approximations than 3 SOLVE-GP the standard method, we will defer the discussion of (i) to appendix A and focus on (ii) because it is amenable Despite the success of SVGP methods, their O(M 3) to large-scale training, as we will show next. complexity makes it difficult for the flexibility of posterior approximation to grow with the dataset 3.2 Orthogonal Decomposition size. We present a new framework called Sparse OrthogonaL Variational infErence for Gaussian Pro- As suggested in section 3.1, we consider improving the cesses (SOLVE-GP), which allows the use of an addi- variational distribution for f . However, the complex- ⊥ tional set of inducing points at a lower computational ity of inferring f is the same as for f and thus cubic. ⊥ cost than the standard SVGP methods. Resolving the problem requires a better understanding of the reparameterization we used in section 3.1. 3.1 Reinterpreting SVGP The key observation here is that the reparameteri- We start by reinterpreting SVGP methods using a sim- zation in Eq. (5) corresponds to an orthogonal de- ple reparameterization, which will then lead us to pos- composition in the function space. For simplicity, sible ways of improving the approximation. First we 1Note that kernel matrices like K depend on Z in- uu notice that the covariance of the conditional distribu- stead of u; the subscript only indicates that this is the tion p(f |u) = N (K K−1u, K − Q ) does not de- covariance matrix of u. fu uu ff ff
Sparse Orthogonal Variational Inference for Gaussian Processes we first derive such decomposition in the Reproduc- q(u)p (v )p (f |v ). To obtain better approxima- ⊥ ⊥ ⊥ ⊥ ⊥ ing Kernel Hilbert Space (RKHS) induced by k, and tions we can replace the prior factor p (v ) with a ⊥ ⊥ then generalize the result to the GP sample space. tunable variational factor q(v ) := N (m , S ): ⊥ v v The RKHS with kernel k is the closure of the space {(cid:80)(cid:96) c k(x(cid:48) , ·), c ∈ R, (cid:96) ∈ N+, x(cid:48) ∈ X }, with the q(u, f ⊥, v ⊥) = q(u)q(v ⊥)p ⊥(f ⊥|v ⊥). i=1 i i i i inner product defined as (cid:104)f, k(x, ·)(cid:105) = f (x), ∀f ∈ H This gives the SOLVE-GP variational lower bound: H. Let V denote the linear span of the kernel ba- sis functions indexed by the inducing points: V := E (cid:2) log p(y|f + K K−1u)(cid:3) {(cid:80)M α k(z , ·), α = [α , . . . , α ](cid:62) ∈ RM }. For any q(u)q⊥(f⊥) ⊥ fu uu j=1 j j 1 M − KL [q(u)(cid:107)p(u)] − KL [q(v )(cid:107)p (v )] , (8) function f ∈ H, we can decompose it (Cheng and ⊥ ⊥ ⊥ Boots, 2016) as (cid:82) where q (·) := p (·|v )q(v )dv is the varia- ⊥ ⊥ ⊥ ⊥ ⊥ tional predictive distribution for p . Simple com- f = f + f , f ∈ V and f ⊥ V, ⊥ (cid:107) ⊥ (cid:107) ⊥ putations show that q (f ) = N (C C−1m , S ), ⊥ ⊥ fv vv v f⊥ Assuming f (cid:107) = (cid:80)M j=1 α j(cid:48) k(z j, ·), then we can solve w Chere :=S f⊥ K= C −ff Q+ C if svC th− v ev1( cS ov va− riC anv cv e)C m− v av1 tC rixvf . ofHe pre for the coefficients (details in appendix B): α(cid:48) = ff ff ff ⊥ on the training inputs and similarly for the other k(Z, Z)−1f (Z), where k(Z, Z) denotes the kernel ma- matrices. Because the likelihood factorizes given trix of Z. Therefore, f (i.e., f + K K−1u), the first term of Eq. (8) ⊥ fu uu f (cid:107)(x) = k(x, Z)k(Z, Z)−1f (Z), f ⊥ = f − f (cid:107). (7) simplifies to (cid:80)N n=1 E q(u)q(f⊥(xn))[log p(y n|f ⊥(x n) + k(x , Z)K−1u)]. Therefore, we only need to compute n uu Here k(x, Z) := [k(z , x), . . . , k(z , x)]. Although 1 M marginals of q (f ) at individual data points. In the ⊥ ⊥ Eq. (7) is derived by assuming f ∈ H, it motivates general setting, the SOLVE-GP lower bound can be us to study the same decomposition for f ∼ GP(0, k). maximized in O(N M¯ 2 + M¯ 3) time per gradient up- Then f (cid:107) becomes k(·, Z)K− uu1u. Interestingly, we can date, where M¯ = max(M, M 2). In mini-batch training verify that this is a sample from a GP with a zero mean N is replaced by the batch size. The predictive density function and covariance function Cov[f (x), f (x(cid:48))] = (cid:107) (cid:107) at test data points can be found in appendix D. k(x, Z)K−1k(Z, x(cid:48)). Similarly we can show that f is uu ⊥ To intuitively understand the improvement over the a sample from another GP and we denote these two standard SVGP methods, we derive a collapsed bound independent GPs as p and p (Hensman et al., 2017): (cid:107) ⊥ for GP regression using (8) and compare it to the Tit- f ∼ p ≡ GP(0, k(x, Z)K−1k(Z, x(cid:48))), sias (2009) bound. Plugging in the optimal q(u), and (cid:107) (cid:107) uu simplifying (see appendix C), gives the bound f ∼ p ≡ GP(0, k(x, x(cid:48)) − k(x, Z)K−1k(Z, x(cid:48))). ⊥ ⊥ uu 1 Marginalizing out the GPs at the training points X, it log N (y|C C−1m , Q + σ2I) − tr(S ) fv vv v ff 2σ2 f⊥ is easy to show that − KL [N (m , S )(cid:107)N (0, C )] . (9) v v vv f = f (X) = K K−1u ∼ N (0, K K−1K ), (cid:107) (cid:107) fu uu fu uu uf With an appropriate choice of q(v ) this bound can be ⊥ f ⊥ = f ⊥(X) ∼ N (0, K ff − K fuK− uu1K uf ). tighter than the Titsias (2009) bound. For example, notice that when q(v ) is equal to the prior p (v ), ⊥ ⊥ ⊥ This is exactly the decomposition we used in sec- i.e., m = 0 and S = C , the bound in (9) reduces v v vv tion 3.1, and the meaning of f becomes clear. ⊥ to the one in (3). Another interesting special case arises when the variational distribution has the same 3.3 SOLVE-GP Lower Bound covariance matrix as the prior (i.e., S = C ), while v vv the mean m is learnable. Then the bound becomes v The decomposition described in the previous section gives new insights for improving the variational distri- log N (y|C C−1m , Q + σ2I) bution for f . Specifically, we can introduce a second fv vv v ff ⊥ 1 1 set of inducing variables v ⊥ := f ⊥(O) to approximate − 2σ2 tr (K ff − Q ff ) − 2 m(cid:62) v C− vv1m v. (10) p , as illustrated in Fig. 1. We call this second set ⊥ O = [o 1, . . . , o M2](cid:62) ∈ RM2×d the orthogonal inducing Here we see that the second set of inducing variables points. The joint model distribution is then v mostly determines the mean prediction over y, ⊥ which is zero in the Titsias (2009) bound (Eq. (3)). p(y|f + K K−1u)p(u)p (f |v )p (v ). ⊥ fu uu ⊥ ⊥ ⊥ ⊥ ⊥ Our method introduces another set of inducing points First notice that the standard SVGP methods to improve the variational approximation. One natu- correspond to using the variational distribution ral question to ask is, how does this compare to the
Jiaxin Shi, Michalis K. Titsias, Andriy Mnih standard SVGP algorithm with the inducing points Inter-domain and Convolutional GPs. Simi- chosen to be union of the two sets? We answer it as lar to SVGP methods, SOLVE-GP can deal with follows: 1) Given the same number of inducing points, inter-domain inducing points (L´azaro-Gredilla and SOLVE-GP is more computationally efficient than the Figueiras-Vidal, 2009) which lie in a different do- standard SVGP method; 2) SOLVE-GP can be inter- main from the input space. The inducing variables preted as using a structured covariance in the varia- u, which we used to represent outputs of the GP at tional approximation for SVGP. the inducing points, are now defined as u = g(Z) := [g(z ), . . . , g(z )](cid:62), where g is a different function 1 M Computational Benefits. For a quick comparison, from f that takes inputs in the domain of inducing we analyze the cost of the Cholesky decomposition points. In convolutional GPs (van der Wilk et al., in both methods. We assume the time complexity 2017), the input domain is the space of images, while of decomposing an M × M matrix is cM 3, where c the inducing points are in the space of image patches. is constant w.r.t. M . For SOLVE-GP, to compute The convolutional GP function is defined as f (x) = the inverse and the determinant of K and C , (cid:80) w g (cid:0) x[p](cid:1) , where g ∼ GP(0, k ), x[p] is the pth uu vv p p g we need the Cholesky factors of them, which cost patch in x, and w = [w , . . . , w ](cid:62) are the assigned 1 P c(M 3 + M 23). For SVGP with M inducing points, we weights for different patches. In SOLVE-GP, we can need the Cholesky factor of K uu, which costs cM 3. choose either Z, O, or both to be inter-domain as long Adding another M inducing points in SVGP leads to as we can compute the covariance between u, v and an 8-fold increase (i.e., from cM 3 to 8cM 3) in the cost f . For convolutional GPs, we let Z and O both be of the Cholesky decomposition, compared to the 2-fold collections of image patches. Examples of the covari- increase if we switch to SOLVE-GP with M 2 = M or- ance matrices we need for this model include K vf and thogonal inducing points. A more rigorous analysis is K (used for C ). They can be computed as vu vv given in appendix D, where we enumerate all the cubic- cost operations needed when we compute the bound. [K ] = Cov[g(o ), f (x )] = (cid:88) w k (o , x[p]), vf ij i j p g i j p Structured Covariance. We can express our vari- [K ] = Cov[g(o ), g(z )] = k (o , z ). vu ij i j g i j ational approximation w.r.t. the original GP. Let v = f (O) denote the function outputs at the orthogonal in- Deep GPs. We show that we can integrate SOLVE- ducing points. We then have the following relationship GP with popular doubly stochastic variational infer- between u, v and u, v : ⊥ ence algorithms for deep GPs (Salimbeni and Deisen- (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) u I 0 u roth, 2017). The joint distribution of a deep GP model = . v K vuK− uu1 I v ⊥ with inducing variables in all layers is Therefore, the joint variational distribution over u L and v that corresponds to the factorized q(u)q(v ⊥) p(y, f 1:L, u1:L) = p(y|f L) (cid:89) (cid:2) p(f (cid:96)|u(cid:96), f (cid:96)−1)p(u(cid:96))(cid:3) , is also Gaussian. By change-of-variable we can ex- (cid:96)=1 press it as q(u, v) = N (m , S ), where m = u,v u,v u,v (cid:2) m , m + K K−1m (cid:3)(cid:62) and where we define f 0 = X and f (cid:96) is the output of the u v vu uu u (cid:96)th-layer GP. The doubly stochastic algorithm applies (cid:20) S S K−1K (cid:21) SVGP methods to each layer conditioned on sam- S = u u uu uv . u,v K K−1S S + K K−1S K−1K ples from the variational distribution in the previous vu uu u v vu uu u uu uv layer. The variational distribution over u1:L, f 1:L is From S u,v we can see that our approach is differ- q(f 1:L, u1:L) = (cid:81)L (cid:2) p(f (cid:96)|u(cid:96), f (cid:96)−1)q(u(cid:96))(cid:3) . This gives (cid:96)=1 ent from making the mean-field assumption q(u, v) = a similar objective as in the single layer case (Eq. (2)): q(u)q(v), instead it captures the covariance between E (cid:2) log p(y|f L)(cid:3) − (cid:80)L KL (cid:2) q(u(cid:96))(cid:107)p(u(cid:96))(cid:3) , where u, v through a structured parameterization. q(fL) (cid:96)=1 q(f L) = (cid:82) (cid:81)L (cid:2) p(f (cid:96)|u(cid:96), f (cid:96)−1)q(u(cid:96))du(cid:96)(cid:3) df 1:L−1. Ex- (cid:96)=1 tending this using SOLVE-GP is straightforward: we 4 EXTENSIONS simply introduce orthogonal inducing variables v1:L ⊥ for all layers, which yields the lower bound: One direct extension of SOLVE-GP involves using more than two sets of inducing points by repeatedly E [log p(y|f L + KL (KL )−1uL)]− q(uL,fL) ⊥ fu uu applying the decomposition. However, this adds more ⊥ L complexity to the implementation. Below we show (cid:88) (cid:8) KL[q(u(cid:96))(cid:107)p(u(cid:96))] + KL[q(v(cid:96) )(cid:107)p (v(cid:96) )](cid:9) . (11) ⊥ ⊥ ⊥ that the SOLVE-GP framework can be easily extended (cid:96)=1 to different GP models where the standard SVGP method applies. The expression for q(uL, f L) is given in appendix E. ⊥
Sparse Orthogonal Variational Inference for Gaussian Processes 5 RELATED WORK 6 EXPERIMENTS Many approximate algorithms have been proposed to Since ODVGP is a special case of SOLVE-GP, we use overcome the computational limitations of GPs. The M, M to refer to |β| and |γ| in their algorithm, re- 2 simplest of these are based on subsampling, such as spectively. the subset-of-data training (Rasmussen and Williams, 2006) and the Nystr¨om approximation (Williams and 6.1 1D Regression Seeger, 2001). Better approximations can be con- structed by learning a set of inducing points to sum- We begin by illustrating our method on Snelson’s 1D marize the dataset. As mentioned in section 2, these regression problem (Snelson and Ghahramani, 2006) works can be divided into approximations to the with 100 training points and mini-batch size 20. We GP prior (SoR, DTC, FITC, etc.; Quin˜onero-Candela compare the following methods: SVGP with 5 and 10 and Rasmussen, 2005b), and sparse variational meth- inducing points, ODVGP (M = 5, M = 100), and 2 ods (Titsias, 2009; Hensman et al., 2013, 2015a). SOLVE-GP (M = 5, M = 5). 2 Recently there have been many attempts to reduce The results are plotted in Fig. 2. First we can see the computational cost of using a large set of inducing that 5 inducing points are insufficient to summarize points. A notable line of work (Wilson and Nickisch, the training set: SVGP (M = 5) cannot fit data well 2015; Evans and Nair, 2018; Gardner et al., 2018) in- and underestimates the variance in regions beyond the volves imposing grid structures on the locations of Z to training data. Increasing M to 10 fixes the issues, but perform fast structure-exploiting computations. How- requires 8x more computation for the Cholesky decom- ever, to get such benefits Z need to be fixed due to the position than using 5 inducing points2. The decoupled structure constraints, which often suffers from curse of formulation provides a cheaper alternative and we have dimensionality in the input space. tried ODVGP (M = 5, M = 100), which has 100 ad- 2 ditional inducing points for modeling the mean func- Another direction for allowing the use of more induc- tion. Comparing Fig. 2a and Fig. 2b, we can see that ing points is the decoupled method (Cheng and Boots, this results in a much better fit for the mean function. 2017), where two different sets of inducing points are However, the model still overestimates the predictive used for modeling the mean and the covariance func- variance. As ODVGP is a special case of the SOLVE- tion. This gives linear complexity in the number of GP framework, we can improve on it in terms of co- mean inducing points which allows using many more variance modeling. As seen in Fig. 2c, adding 5 or- of them. Despite the increasing interest in decoupled thogonal inducing points can closely approximate the inducing points (Havasi et al., 2018; Salimbeni et al., results of SVGP (M = 10), with only a 2-fold increase 2018), the method has not been well understood due in the cost of the Cholesky decomposition relative to to its complexity. We found that SOLVE-GP is closely SVGP (M = 5). related to a recent development of decoupled methods: the orthogonally decoupled variational GP (ODVGP, 6.2 Convolutional GP Models Salimbeni et al., 2018), as explained next. One class of applications that benefit from the SOLVE- Connection with Decoupled Inducing Points. GP framework is the training of large, hierarchical GP If we set the β and γ inducing points in ODVGP (Sal- models where the true posterior distribution is diffi- imbeni et al., 2018) to be Z and O, their approach cult to approximate with a small number of inducing becomes equivalent to using the variational distribu- points. Convolutional GPs (van der Wilk et al., 2017) tion q(cid:48)(u, v) = N (m(cid:48) , S(cid:48) ), where u,v u,v and their deep variants (Blomqvist et al., 2018; Du- (cid:20) (cid:21) tordoir et al., 2019) are such models. There induc- m m(cid:48) u,v = m + K u K−1m , S(cid:48) u,v = ing points are feature detectors just like CNN filters, v vu uu u which play a critical role in predictive performance. (cid:20) S S K−1K (cid:21) u u uu uv . As explained in section 4, it is straightforward to ap- K vuK− uu1S u K vv +K vuK− uu1(S u −K uu)K− uu1K uv ply SOLVE-GP to these models. By comparing S to S(cid:48) , we can see that we gener- u,v u,v Convolutional GPs. We train convolutional GPs alize their method by introducing S , which replaces v the original residual K − K K−1K (or C ), so on the CIFAR-10 dataset, using GPs with TICK ker- vv vu uu uv vv nels (Dutordoir et al., 2019) to define the patch re- that we allow more flexible covariance modeling while sponse functions. Table 1 shows the results for SVGP still keeping the block structure. Thus ODVGP is a special case of SOLVE-GP where q(v ⊥) is restricted 2In practice the cost is negligible in this toy problem to have the same covariance C as the prior. but we are analyzing the theoretical complexity. vv
Jiaxin Shi, Michalis K. Titsias, Andriy Mnih (a) SVGP, 5 (b) ODVGP, 5 + 100 (c) SOLVE-GP, 5 + 5 (d) SVGP, 10 Figure 2: Posterior processes on the Snelson dataset, where shaded bands correspond to intervals of ±3 standard deviations. The learned inducing locations are shown at the bottom of each figure, where + correspond to Z; blue and dark triangles correspond to O in ODVGP and SOLVE-GP, respectively. 0.069 0.068 0.067 0.066 0.065 0.064 0.063 0.062 0.061 0 20 40 60 80 100 Epoch ESMR tseT 1.36 SOLVE-GP, 1024+1024 1.34 SVGP, 1024 1.32 SVGP, 2048 1.30 ODVGP, 1024+8192 1.28 1.26 1.24 1.22 1.20 0 20 40 60 80 100 Epoch LL tseT 0.070 0.065 0.060 0 20 40 60 80 100 Epoch (a) w/o whitening ESMR tseT 1.45 SOLVE-GP, 1024+1024 1.40 SVGP, 1024 1.35 SVGP, 2048 ODVGP, 1024+8192 1.30 1.25 1.20 1.15 1.10 0 20 40 60 80 100 Epoch LL tseT (b) w/ whitening (except ODVGP) Figure 3: Test RMSE and predictive log-likelihoods during training on HouseElectric. with 1K and 2K inducing points, SOLVE-GP (M = Table 1: Convolutional GPs for CIFAR-10 classifica- 1K, M = 1K), and SVGP (M = 1.6K) that has a tion. Previous SOTA is 64.6% by SVGP with 1K in- 2 similar running time on GPU as SOLVE-GP. Clearly ducing points (van der Wilk et al., 2017). SOLVE-GP outperforms SVGP (M = 1K). It also outperforms SVGP (M = 1.6K), which has the same M(+M 2) Test Acc Test LL Time running time, and performs on par with the more ex- 1K 66.07% -1.59 0.241 s/iter SVGP pensive SVGP (M = 2K), which is very encourag- 1.6K 67.18% -1.54 0.380 s/iter ing. This suggests that the structured covariance ap- SOLVE-GP 1K + 1K 68.19% -1.51 0.370 s/iter proximation is fairly accurate even for this large, non- SVGP 2K 68.06% -1.48 0.474 s/iter conjugate model. Deep Convolutional GPs. We further extend (2019); Arora et al. (2019). Note that all the results SOLVE-GP to deep convolutional GPs using the tech- are obtained without data augmentation. niques described in section 4. We experiment with 2- layer and 3-layer models that have 1K inducing points 6.3 Regression Benchmarks in the output layer and 384 inducing points in other layers. The results are summarized in Table 3. These Besides classification experiments, we evaluate our models are already quite slow to train on a single GPU, method on 10 regression datasets, with size rang- as indicated by the time per iteration. SOLVE-GP al- ing from tens of thousands to millions. The set- lows to double the number of inducing points in each tings are followed from Wang et al. (2019) and de- layer with only a 2-fold increase in computation. This scribed in detail in appendix G. We implemented gives superior performance on both F1-score and test SVGP with M = 1024&2048 inducing points, ODVGP predictive likelihoods. The double-size SVGP takes a and SOLVE-GP (M = 1024, M = 1024), as well as week to run and is only for comparison purpose. 2 SVGP with M = 1536 inducing points, which has As shown above, on both single layer and deep convo- roughly the same training time per iteration on GPU lutional GPs, we improve the state-of-the-art results of as the SOLVE-GP objective. An attractive property of CIFAR-10 classification by 3-4 percentage points. This ODVGP is that by restricting the covariance of q(v ) ⊥ leads to more than 80% F1-score on CIFAR-10 with a to be the same as the prior covariance C , it can use vv purely GP-based model, without any neural network far larger M , because the complexity is linear with M 2 2 components, closing the gap between GP/kernel re- by sub-sampling the columns of K for each gradient vv gression and CNN baselines presented in Novak et al. update. Thus for a fair comparison, we also include
Sparse Orthogonal Variational Inference for Gaussian Processes Table 2: Test log-likelihood values for the regression datasets. The numbers in parentheses are standard errors. Best mean values are highlighted, and asterisks indicate statistical significance. Kin40k Protein KeggDirected KEGGU 3dRoad Song Buzz HouseElectric N 25,600 29,267 31,248 40,708 278,319 329,820 373,280 1,311,539 d 8 9 20 27 3 90 77 9 1024 0.094(0.003) -0.963(0.006) 0.967(0.005) 0.678(0.004) -0.698(0.002) -1.193(0.001) -0.079(0.002) 1.304(0.002) SVGP 1536 0.129(0.003) -0.949(0.005) 0.944(0.006) 0.673(0.004) -0.674(0.003) -1.193(0.001) -0.079(0.002) 1.304(0.003) 1024 + 1024 0.137(0.003) -0.956(0.005) -0.199(0.067) 0.105(0.033) -0.664(0.003) -1.193(0.001) -0.078(0.001) 1.317(0.002) ODVGP 1024 + 8096 0.144(0.002) -0.946(0.005) -0.136(0.063) 0.109(0.033) -0.657(0.003) -1.193(0.001) -0.079(0.001) 1.319(0.004) SOLVE-GP 1024 + 1024 *0.187(0.002) -0.943(0.005) 0.973(0.003) 0.680(0.003) -0.659(0.002) -1.192(0.001) *-0.071(0.001) *1.333(0.003) SVGP 2048 0.137(0.003) -0.940(0.005) 0.907(0.003) 0.665(0.004) -0.669(0.002) -1.192(0.001) -0.079(0.002) 1.304(0.003) Table 3: Deep convolutional GPs for CIFAR-10 classi- ible covariance modeling seems to be more important, fication. Previous SOTA is 76.17% by a 3-layer model as SOLVE-GP outperforms ODVGP (M = 8096) on 2 with 384 inducing points in all layers (Dutordoir et al., all datasets except for 3dRoad. 2019). In Fig. 3a we plot the evolution of test RMSE and test log-likelihoods during training on HouseElectric. (a) 2-layer model Interestingly, ODVGP (M = 8096) performs on par 2 SVGP SOLVE-GP SVGP with SOLVE-GP early in training before falling be- M(+M ) 384, 1K 384 + 384, 1K + 1K 768, 2K hind it substantially. The beginning stage is likely 2 Test Acc 76.35% 77.80% 77.46% where the additional inducing points give good pre- Test LL -1.04 -0.98 -0.98 dictions but are not in the best configuration for max- Time 0.392 s/iter 0.657 s/iter 1.104 s/iter imizing the training lower bounds. This phenomenon is also observed on Protein, Elevators, and Kin40k. We believe such mismatch between the training lower (b) 3-layer model bound and predictive performance is caused by fix- SVGP SOLVE-GP SVGP ing the covariance matrix of q(v ⊥) to the prior co- 384 + 384, 384 + 384, variance. SVGP (M = 2048) does not improve over M(+M ) 384, 384, 1K 768, 768, 2K 2 1K + 1K SVGP (M = 1024) and is outperformed by SOLVE- Test Acc 78.76% 80.30% 80.33% GP. Suggested above, this might be due to the diffi- Test LL -0.88 -0.79 -0.82 culty of optimising large covariance matrices. To ver- Time 0.418 s/iter 0.752 s/iter 1.246 s/iter ify this, we tried the “whitening” trick (Murray and Adams, 2010; Hensman et al., 2015b), described in appendix F, which is often used to make optimiza- tion easier by reducing the correlation in the posterior ODVGP (M = 8096), where in each iteration 1024 2 distributions. As shown in Fig. 3b, the performance of columns of K are sampled to estimate the gradient. vv SVGP (M = 2048) and SOLVE-GP becomes similar Other experimental details are given in appendix G. with whitening. We did not use whitening in ODVGP We report the predictive log-likelihoods on test data because it has a slightly different parameterization to in Table 2. For space reasons, we provide the re- allow sub-sampling K . vv sults on two small datasets (Elevators, Bike) in ap- pendix H. We can see that performance of SOLVE-GP 7 CONCLUSION is competitive with SVGP (M = 2048) that involves 4x more expensive Cholesky decomposition. Perhaps surprisingly, despite using a less flexible covariance in We proposed SOLVE-GP, a new variational inference the variational distribution, SOLVE-GP often outper- framework for GPs using inducing points, that unifies forms SVGP (M = 2048). We believe this is due to the and generalizes previous sparse variational methods. optimization difficulties introduced by the 2048 × 2048 This increases the number of inducing points we can covariance matrix and will test hypothesis on the use for a fixed computational budget, which allows to HouseElectric dataset below. On most datasets, using improve performance of large, hierarchical GP mod- a large number of additional inducing points for mod- els at a manageable computational cost. Future work eling the mean function did improve the performance, includes experiments on challenging datasets like Im- as shown by the comparison between ODVGP (M = ageNet and investigating other ways to improve the 2 1024) and ODVGP (M = 8096). However, more flex- variational distribution, as mentioned in section 3.1. 2
Jiaxin Shi, Michalis K. Titsias, Andriy Mnih Acknowledgements Marton Havasi, Jos´e Miguel Hern´andez-Lobato, and Juan Jos´e Murillo-Fuentes. Deep Gaussian processes We thank Alex Matthews and Yutian Chen for helpful with decoupled inducing inputs. arXiv preprint suggestions on improving the paper. arXiv:1801.02939, 2018. James Hensman, Nicolo Fusi, and Neil D Lawrence. References Gaussian processes for big data. arXiv preprint Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, arXiv:1309.6835, 2013. Ruslan Salakhutdinov, and Ruosong Wang. On ex- James Hensman, Alexander Matthews, and Zoubin act computation with an infinitely wide neural net. Ghahramani. Scalable variational Gaussian process arXiv preprint arXiv:1904.11955, 2019. classification. In Artificial Intelligence and Statis- Kenneth Blomqvist, Samuel Kaski, and Markus tics, pages 351–360, 2015a. Heinonen. Deep convolutional Gaussian processes. James Hensman, Alexander G Matthews, Maurizio arXiv preprint arXiv:1810.03052, 2018. Filippone, and Zoubin Ghahramani. MCMC for Thang D. Bui, Josiah Yan, and Richard E. Turner. variationally sparse Gaussian processes. In Advances A unifying framework for Gaussian process pseudo- in Neural Information Processing Systems, pages point approximations using power expectation prop- 1648–1656, 2015b. agation. Journal of Machine Learning Research, 18 James Hensman, Nicolas Durrande, and Arno Solin. (104):1–72, 2017. Variational Fourier features for Gaussian processes. Ching-An Cheng and Byron Boots. Incremental vari- Journal of Machine Learning Research, 18(151):1– ational sparse Gaussian process regression. In Ad- 151, 2017. vances in Neural Information Processing Systems, Daniel Hern´andez-Lobato, Jos´e M Hern´andez-Lobato, pages 4410–4418, 2016. and Pierre Dupont. Robust multi-class Gaussian Ching-An Cheng and Byron Boots. Variational infer- process classification. In Advances in Neural Infor- ence for Gaussian process models with linear com- mation Processing Systems, pages 280–288, 2011. plexity. In Advances in Neural Information Process- Diederik P Kingma and Max Welling. Auto-encoding ing Systems, pages 5184–5194, 2017. variational bayes. arXiv preprint arXiv:1312.6114, L. Csato and M. Opper. Sparse online Gaussian pro- 2013. cesses. Neural Computation, 14:641–668, 2002. N. D. Lawrence, M. Seeger, and R. Herbrich. Fast Andreas C. Damianou, Michalis K. Titsias, and Neil D. sparse Gaussian process methods: the informative Lawrence. Variational inference for latent variables vector machine. In Advances in Neural Information and uncertain inputs in Gaussian processes. Journal Processing Systems. MIT Press, 2002. of Machine Learning Research, 17(42):1–62, 2016. Neil Lawrence. Probabilistic non-linear principal com- Marc Deisenroth and Carl E Rasmussen. PILCO: A ponent analysis with Gaussian process latent vari- model-based and data-efficient approach to policy able models. Journal of Machine Learning Research, search. In International Conference on Machine 6(Nov):1783–1816, 2005. Learning, pages 465–472, 2011. Miguel L´azaro-Gredilla and Anibal Figueiras-Vidal. Vincent Dutordoir, Mark van der Wilk, Artem Arte- Inter-domain Gaussian processes for sparse infer- mev, Marcin Tomczak, and James Hensman. Trans- ence using inducing features. In Advances in Neural lation insensitivity for deep convolutional Gaussian Information Processing Systems, pages 1087–1095, processes. arXiv preprint arXiv:1902.05888, 2019. 2009. Trefor Evans and Prasanth Nair. Scalable Gaussian Iain Murray and Ryan P Adams. Slice sampling co- processes with grid-structured eigenfunctions (GP- variance hyperparameters of latent Gaussian mod- GRIEF). In International Conference on Machine els. In Advances in Neural Information Processing Learning, pages 1416–1425, 2018. Systems, pages 1732–1740, 2010. Jacob Gardner, Geoff Pleiss, Ruihan Wu, Kilian Wein- Roman Novak, Lechao Xiao, Yasaman Bahri, Jae- berger, and Andrew Wilson. Product kernel interpo- hoon Lee, Greg Yang, Daniel A. Abolafia, Jeffrey lation for scalable Gaussian processes. In Artificial Pennington, and Jascha Sohl-dickstein. Bayesian Intelligence and Statistics, pages 1407–1416, 2018. deep convolutional networks with many channels are Gaussian processes. In International Conference on Xavier Glorot and Yoshua Bengio. Understanding the Learning Representations, 2019. difficulty of training deep feedforward neural net- works. In Artificial Intelligence and Statistics, pages J. Quin˜onero-Candela and C. E. Rasmussen. A uni- 249–256, 2010. fying view of sparse approximate Gaussian process
Sparse Orthogonal Variational Inference for Gaussian Processes regression. Journal of Machine Learning Research, Ke Alexander Wang, Geoff Pleiss, Jacob R Gard- 6:1939–1959, 2005a. ner, Stephen Tyree, Kilian Q Weinberger, and Andrew Gordon Wilson. Exact Gaussian pro- Joaquin Quin˜onero-Candela and Carl Edward Ras- cesses on a million data points. arXiv preprint mussen. A unifying view of sparse approximate arXiv:1903.08114, 2019. Gaussian process regression. Journal of Machine Learning Research, 6(Dec):1939–1959, 2005b. Christopher KI Williams and David Barber. Bayesian classification with Gaussian processes. IEEE Trans- Carl Edward Rasmussen and Christopher KI Williams. actions on Pattern Analysis and Machine Intelli- Gaussian Processes for Machine Learning. MIT gence, 20(12):1342–1351, 1998. Press, 2006. Christopher KI Williams and Carl Edward Rasmussen. Danilo Jimenez Rezende, Shakir Mohamed, and Daan Gaussian processes for regression. In Advances in Wierstra. Stochastic meta-optimization synthesis and approx- Neural Information Processing Systems, pages 514– imate inference in deep generative models. In In- 520, 1996. ternational Conference on Machine Learning, pages 1278–1286, 2014. Christopher KI Williams and Matthias Seeger. Using the Nystr¨om method to speed up kernel machines. Hugh Salimbeni and Marc Deisenroth. Doubly In Advances in Neural Information Processing Sys- stochastic variational inference for deep Gaussian tems, pages 682–688, 2001. processes. In Advances in Neural Information Pro- Andrew Wilson and Hannes Nickisch. Kernel inter- cessing Systems, pages 4588–4599, 2017. polation for scalable structured Gaussian processes Hugh Salimbeni, Ching-An Cheng, Byron Boots, and (KISS-GP). In International Conference on Ma- Marc Deisenroth. Orthogonally decoupled varia- chine Learning, pages 1775–1784, 2015. tional Gaussian processes. In Advances in Neural Information Processing Systems, pages 8711–8720, 2018. M. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast forward selection to speed up sparse Gaussian process regression. In Ninth International Workshop on Artificial Intelligence. MIT Press, 2003. Jiaxin Shi, Mohammad Emtiyaz Khan, and Jun Zhu. Scalable training of inference networks for Gaussian- process models. In International Conference on Ma- chine Learning, pages 5758–5768, 2019. Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Ad- vances in Neural Information Processing Systems, pages 1257–1264, 2006. Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimiza- tion in the bandit setting: no regret and experimen- tal design. In International Conference on Machine Learning, pages 1015–1022, 2010. Michalis Titsias. Variational learning of inducing vari- ables in sparse Gaussian processes. In Artificial In- telligence and Statistics, pages 567–574, 2009. Michalis Titsias and Miguel L´azaro-Gredilla. Dou- bly stochastic variational bayes for non-conjugate inference. In International Conference on Machine Learning, pages 1971–1979, 2014. Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional Gaussian processes. In Advances in Neural Information Processing Sys- tems, pages 2849–2858, 2017.
Jiaxin Shi, Michalis K. Titsias, Andriy Mnih A Tighter Sparse Variational Bounds for GP Regression As mentioned in section 3.1, another way to improve the variational distribution q(u)p (f ) in SVGP is to make ⊥ ⊥ u and f dependent. The best possible approximation of this type is obtained by the setting q(u) to the optimal ⊥ exact posterior conditional q∗(u) = p(u|f , y). The corresponding collapsed bound for GP regression can be ⊥ derived by analytically marginalising out u from the joint model in Eq. (6), (cid:90) p(y|f ) = p(y|f + K K−1u)p(u) du ⊥ ⊥ fu uu = N (y|f , Q + σ2I), (12) ⊥ ff and then forcing the approximation p (f ): ⊥ ⊥ E log N (y|f , Q + σ2I). (13) p⊥(f⊥) ⊥ ff This bound has a closed-form as log N (y|0, Q + σ2I) − 1 tr (cid:2) (Q + σ2I)−1(K − Q )(cid:3) , (14) ff 2 ff ff ff Applying the matrix inversion lemma to (Q +σ2I)−1, we have an equivalent form that can be directly compared ff with Eq. (3): log N (y|0, Q + σ2I) − 1 tr(K − Q ) + 1 tr (cid:2) K (K + σ−2K K )−1K (K − Q )(cid:3) , (15) ff 2σ2 ff ff 2σ4 fu uu uf fu uf ff ff (cid:124) (cid:123)(cid:122) (cid:125) =Eq. (3) where the first two terms recover Eq. (3), suggesting this is a tighter bound than the Titsias (2009) bound. This bound is not amenable to large-scale datasets because of O(N 2) storage and O(M N 2) computation time (dominated by the matrix multiplication K K ) requirements. However, it is still of theoretical interest and uf ff can be applied to medium-sized regression datasets, just like the SGPR algorithm using the Titsias (2009) bound. B Details of Orthogonal Decomposition In section 3.2 we described the following orthogonal decomposition for f ∈ H, where H is the RKHS induced by kernel k: f = f + f , f ∈ V and f ⊥ V. (16) (cid:107) ⊥ (cid:107) ⊥ Here V is the subspace spanned by the inducing basis: V = {(cid:80)M α k(z , ·), α = [α , . . . , α ](cid:62) ∈ RM }. Since j=1 j j 1 M f ∈ V , we let f = (cid:80)M α(cid:48) k(z , ·). According to the properties of orthogonal projection, we have (cid:107) (cid:107) j=1 j j (cid:104)f, g(cid:105) = (cid:104)f , g(cid:105) , ∀g ∈ V, (17) H (cid:107) H where (cid:104)(cid:105) is the RKHS inner product that satisfies the reproducing property: (cid:104)f, k(x, ·)(cid:105) = f (x). Similarly let H H g = (cid:80)M β k(z , ·). Then (cid:104)f, g(cid:105) = (cid:80)M β f (z ), (cid:104)f , g(cid:105) = (cid:80)M (cid:80)M α(cid:48)β k(z , z ). Plugging into Eq. (17) j=1 j j H j=1 j j (cid:107) H i=1 j=1 i j i j and rearranging the terms, we have β(cid:62)(f (Z) − k(Z, Z)α(cid:48)) = 0, ∀β ∈ RM , (18) where f (Z) = [f (z ), . . . , f (z )](cid:62), k(Z, Z) is a matrix with the ij-th term as k(z , z ), and α(cid:48) = [α(cid:48) , . . . , α(cid:48) ](cid:62). 1 M i j 1 M Therefore, α(cid:48) = k(Z, Z)−1f (Z), (19) and it follows that f (x) = k(x, Z)k(Z, Z)−1f (Z), where k(x, Z) = [k(z , x), . . . , k(z , x)]. The above analysis (cid:107) 1 M arrives at the decomposition: f = k(·, Z)k(Z, Z)−1f (Z), f = f − f . (20) (cid:107) ⊥ (cid:107) Although the derivation from Eq. (16) to (19) relies on the fact f ∈ H, such that the inner product is well-defined, the decomposition in Eq. (20) is valid for any function f on X . This motivates us to study it for f ∼ GP(0, k). Substituting u for f (Z) and K for k(Z, Z), we have for f ∼ GP(0, k): uu f = k(·, Z)K−1u ∼ p ≡ GP(0, k(x, Z)K−1k(Z, x(cid:48))), (21) (cid:107) uu (cid:107) uu f ∼ p ≡ GP(0, k(x, x(cid:48)) − k(x, Z)K−1k(Z, x(cid:48))). (22) ⊥ ⊥ uu
Sparse Orthogonal Variational Inference for Gaussian Processes C The Collapsed SOLVE-GP Lower Bound We derive the collapsed SOLVE-GP lower bound in Eq. (9) by seeking the optimal q(u) that is independent of f . First we rearrange the terms in the uncollapsed SOLVE-GP bound (Eq. (8)) as ⊥ E (cid:8)E (cid:2) log N (cid:0) y|f + K K−1u, σ2I(cid:1)(cid:3)(cid:9) − KL [q(u)(cid:107)p(u)] − KL [q(v )(cid:107)p (v )] . (23) q(u) q⊥(f⊥) ⊥ fu uu ⊥ ⊥ ⊥ where q (f ) = N (m , S ), and m = C C−1m , S = C + C C−1(S − C )C−1C . In the first ⊥ ⊥ f⊥ f⊥ f⊥ fv vv v f⊥ ff fv vv v vv vv vf term we can simplify the expectation over f as: ⊥ E log N (cid:0) y|f + K K−1u, σ2I(cid:1) q⊥(f⊥) ⊥ fu uu (cid:20) (cid:21) N N 1 = E − log 2π − log σ2 − (y − f − K K−1u)(cid:62)(y − f − K K−1u) q⊥(f⊥) 2 2 2σ2 ⊥ fu uu ⊥ fu uu (cid:20) (cid:21) N N 1 = − log 2π − log σ2 − (y − m − K K−1u)(cid:62)(y − m − K K−1u) 2 2 2σ2 f⊥ fu uu f⊥ fu uu (cid:20) (cid:21) 1 − E (f − m )(cid:62)(f − m ) q⊥(f⊥) 2σ2 ⊥ f⊥ ⊥ f⊥ 1 = log N (y|K K−1u + m , σ2I) − tr(S ). (24) fu uu f⊥ 2σ2 f⊥ Plugging into Eq. (23) and rearranging the terms, we have E (cid:2) log N (y|K K−1u + m , σ2I)(cid:3) − KL [q(u)(cid:107)p(u)] − 1 tr(S ) − KL [q(v )(cid:107)p (v )] . (25) q(u) fu uu f⊥ 2σ2 f⊥ ⊥ ⊥ ⊥ (cid:124) (cid:123)(cid:122) (cid:125) ≤log (cid:82) N (y|KfuK− uu1u+mf⊥,σ2I)p(u) du Clearly the leading two terms form a variational lower bound of the joint distribution N (y|K K−1u + fu uu m , σ2I)p(u). The optimal q(u) will turn it into the log marginal likelihood: f⊥ (cid:90) log N (y|K K−1u + m , σ2I)p(u) du = log N (y|m , Q + σ2I). (26) fu uu f⊥ f⊥ ff Plugging this back, we have the collapsed SOLVE-GP bound in Eq. (9): 1 log N (y|C C−1m , Q + σ2I) − tr(S ) − KL [N (m , S )(cid:107)N (0, C )] , (27) fv vv v ff 2σ2 f⊥ v v vv Moreover, we could find the optimal q∗(v) = N (m∗ , S∗ ) by setting the derivatives w.r.t. m and S to be zeros: v v v v m∗ = C [C + C A−1C ]−1C A−1y, (28) v vv vv vf fv vf S∗ = C [C + σ−2C C ]−1C , (29) v vv vv vf fv vv where A = Q + σ2I. Then the collapsed bound with the optimal q(v ) is ff ⊥ 1 log N (y|C C−1m∗ , A) − tr[C − B(B + σ2I)−1B] − KL [N (m∗ , S∗ )(cid:107)N (0, C )] , (30) fv vv v 2σ2 ff v v vv where B = C C−1C . fv vv vf D Computational Details D.1 Training To compute the lower bound in Eq. (8), we write it as N (cid:88) E [log p(y |f (x ))] − KL [q(u)(cid:107)p(u)] − KL [q(v )(cid:107)p (v )], (31) q(f(xn);Θ) n n ⊥ ⊥ ⊥ n=1
Jiaxin Shi, Michalis K. Titsias, Andriy Mnih Algorithm 1 The SOLVE-GP lower bound via Cholesky decomposition. We parameterize the variational covariance matrices with their Cholesky factors S = L L(cid:62), S = L L(cid:62). A = L0 \ K denotes the solution of u u u v v v u uv L0 A = K . (cid:12) denotes elementwise multiplication. The differences from SVGP are shown in blue. u uv Input: X (training inputs), y (targets), Z, O (inducing points), m , L , m , L (variational parameters) u u v v 1: K = k(Z, Z), K = k(O, O) uu vv 2: L0 = Cholesky(K ), K = k(Z, O), A := L0 \ K , C = K − A(cid:62)A, L0 = Cholesky(C ) u uu uv u uv vv vv v vv 3: K = k(Z, X), K = k(O, X) uf vf 4: B := L0 \ K , C = K − A(cid:62)B, D := L0 \ C u uf vf vf v vf 5: E := L0 \ B, F := L(cid:62)E, G := L0 \ D, H := L(cid:62)G u u v v 6: µ(X) = E(cid:62)m + G(cid:62)m u v 7: σ2(X) = diag(K ) + (F (cid:12) F)(cid:62)1 − (B (cid:12) B)(cid:62)1 + (H (cid:12) H)(cid:62)1 − (D (cid:12) D)(cid:62)1 ff 8: Compute LLD = (cid:80)N E log p(y |f (x )) in closed form or using quadrature/Monte Carlo. n=1 N (µ(xn),σ2(xn)) n n 9: function Compute KL(m, L, L0) 10: P = L0 \ L, a = L0 \ m 11: return log(diag(L0))(cid:62)1 − log(diag(L))(cid:62)1 + 1/2((P (cid:12) P)(cid:62)1 + a(cid:62)a − M ) 12: end function 13: KL = Compute KL(m , L , L0 ), KL = Compute KL(m , L , L0 ) u u u u v v v v 14: return LLD − KL − KL u v where Θ := {m , S , m , S , Z, O} and q(f (x ); Θ) defines the marginal distribution of f = f + K K−1u for u u v v n ⊥ fu uu the n-th data point given u ∼ q(u) and f ∼ q (f ). We can write q(f (x ); Θ) as ⊥ ⊥ ⊥ n q(f (x ); Θ) = N (µ(x ), σ2(x )), (32) n n n where µ(x ) = k(x , Z)K−1m + c(x , O)C−1m , (33) n n uu u n vv v σ2(x ) = k(x , Z)K−1S K−1k(Z, x ) + c(x , x ) + c(x , O)C−1(S − C )C−1c(O, x ). (34) n n uu u uu n n n n vv v vv vv n Here c(x, x(cid:48)) := k(x, x(cid:48)) − k(x, Z)K−1k(Z, x) denotes the covariance function of p . The univariate expec- uu ⊥ tation of log p(y |f (x )) under q(f (x ); Θ) can be computed in closed form (e.g., for Gaussian likelihoods) n n n or using quadrature (Hensman et al., 2015b). It can also be estimated by Monte Carlo with the reparame- terization trick (Kingma and Welling, 2013; Titsias and L´azaro-Gredilla, 2014; Rezende et al., 2014) to prop- agate gradients. For large datasets, an unbiased estimate of the sum can be used for mini-batch training: N (cid:80) E [log p(y|f (x))], where B denotes a small batch of data points. |B| (x,y)∈B q(f(x);Θ) Besides the log-likelihood term, we need to compute the two KL divergence terms: KL [q(u)(cid:107)p(u)] = 1 (cid:2) log det K − log det S − M + tr(K−1S ) + m(cid:62)K−1m (cid:3) , (35) 2 uu u uu u u uu u KL [q(v )(cid:107)p (v )] = 1 (cid:2) log det C − log det S − M + tr(C−1S ) + m(cid:62)C−1m (cid:3) . (36) ⊥ ⊥ ⊥ 2 vv v vv v v vv v We note that if the blue parts in Eqs. (31) to (34) are removed, then we recover the SVGP lower bound in Eq. (2). An implementation of the above computations using the Cholesky decomposition is shown in algorithm 1. D.2 Prediction We can predict the function value at a test point x∗ with the approximate posterior by substituting x∗ for x n in Eq. (32). For multiple test points X∗, we denote the joint predictive density by N (f ∗|µ∗, Σ∗), where the predicted mean and covariance are µ∗ = K K−1m + C C−1m , (37) ∗u uu u ∗v vv v Σ∗ = K K−1S K−1K + C − C C−1(C − S )C−1C . (38) ∗u uu u uu u∗ ∗∗ ∗v vv vv v vv v∗
Sparse Orthogonal Variational Inference for Gaussian Processes Table 4: Cubic-cost operations in SOLVE-GP and SVGP, following the implementation in algorithm 1. SVGP SOLVE-GP O(NM2) × 1 O(NM2) × 1 Matrix multiplication O(NM2) × 1 2 O(NMM ) × 1 2 O(MM2) × 1 2 O(M3) × 1 Cholesky O(M3) × 1 O(M3) × 1 2 O(M3) × 1 O(NM2) × 2 Solving triangular O(M3) × 1 O(M3) × 1 matrix equations O(NM2) × 2 2 O(NM2) × 2 2 O(M M2) × 1 2 Matmul Cholesky Trisolve Matmul Cholesky Trisolve 30 10 24 8 8 24 8 18 6 12 4 3 8 8 7 2 6 4 2 3 1 1 1 2 1 1 0 0 0 0 SVGP (M) SVGP (2M) SOLVE-GP (M+M) SVGP (M) SVGP (2M) SOLVE-GP (M+M) (a) N ≈ M (b) M (cid:29) N Figure 4: Comparison of computational cost for SVGP and SOLVE-GP. For each method and each type of cubic-cost operation, we plot the factor of increase in cost compared to a single operation on M × M matrices. D.3 Computational Complexity As mentioned in section 3.3, the time complexity of SOLVE-GP is O(N M¯ 2 + M¯ 3) per gradient update, where M¯ = max(M, M ) and N is the batch size. Here we provide a more fine-grained analysis by counting cubic- 2 cost operations and compare to the standard SVGP method. We underlined all the cubic-cost operations in algorithm 1, including matrix multiplication, Cholesky decomposition, and solving triangular matrix equations. We count them for SVGP and SOLVE-GP. The results are summarized in Table 4. For comparison purposes, we study two cases of mini-batch training: (i) N ≈ M and (ii) M (cid:29) N . We consider SOLVE-GP with M = M , which has 2M inducing points in total, and then compare to SVGP with M and 2M 2 inducing points. For each method and each type of operation, we plot the factor of increase in cost compared to a single operation on M × M matrices. For instance, when N ≈ M (Fig. 4a), SVGP with M inducing points requires solving three triangular matrix equations for M × M matrices. Doubling the number of inducing points in SVGP increases the cost by a factor of 8, plotted as 24 for SVGP (2M ). In contrast, in SOLVE-GP with M orthogonal inducing points we only need to solve 7 triangular matrix equations for M × M matrices. The comparison under the case of M (cid:29) N is shown in Fig. 4b. In this case SOLVE-GP additionally introduces one O(M 3) matrix multiplication operation, but overall the algorithm is still much faster than SVGP (2M ) given the speed-up in Cholesky decomposition and solving matrix equations.
Jiaxin Shi, Michalis K. Titsias, Andriy Mnih E Details of Eq. (11) The variational distribution in Eq. (11) is defined as: (cid:90) L L−1 q(uL, f L) = (cid:89) (cid:2) p (f (cid:96) |v(cid:96) , f (cid:96)−1, u(cid:96)−1)q(v(cid:96) )q(u(cid:96))du(cid:96)dv(cid:96) (cid:3) (cid:89) df (cid:96) . (39) ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ (cid:96)=1 (cid:96)=1 F Whitening Similar to the practice in SVGP methods, we can apply the “whitening” trick (Murray and Adams, 2010; Hensman et al., 2015b) to SOLVE-GP. The goal is to improve the optimization of variational approximations by reducing correlation in the posterior distributions. Specifically, we could “whiten” u by using u(cid:48) = K−1/2u, uu where K−1/2 denotes the Cholesky factor of the prior covariance K . Then posterior inference for u turns uu uu into inference for u(cid:48), which has an isotropic Gaussian prior N (0, I). Then we parameterize the variational distribution w.r.t. u(cid:48): q(u(cid:48)) = N (m , S ). Whitening q(v ) is similar to whitening q(u), i.e., we parameterize u u ⊥ the variational distribution w.r.t. v(cid:48) = C−1/2v and set q(v(cid:48) ) = N (m , S ). The algorithm can be derived by ⊥ vv ⊥ ⊥ v v replacing m , m with L0 m , L0 m , and S , S with L0 S L0 (cid:62) , L0 S L0 (cid:62) in algorithm 1 and removing the u v u u v v u v u u u v v v canceled terms. G Experiment Details For all experiments, we use kernels with a shared lengthscale across dimensions. All model hyperparameters, including kernel parameters, patch weights in convolutional GP models, and observation variances in regres- sion experiments, are optimized jointly with variational parameters using ADAM. The variational distributions q(u) and q(v ) are by default initialized to the prior distributions. Unless stated otherwise, no “whitening” ⊥ trick (Murray and Adams, 2010; Hensman et al., 2015b) is used for SVGP or SOLVE-GP. G.1 1D Regression We randomly sample 100 training data points from Snelson’s dataset (Snelson and Ghahramani, 2006) as the training data. All models use Gaussian RBF kernels and are trained for 10K iterations with learning rate 0.01 and mini-batch size 20. The GP kernel is initialized with lengthscale 1 and variance 1. The Gaussian likelihood is initialized with variance 0.1. G.2 Convolutional GP Models All models are trained for 300K iteration with learning rate 0.003 and batch size 64. The learning rate is annealed by 0.25 every 50K iterations to ensure convergence. We use a zero mean function and the robust multi-class classification likelihood (Hern´andez-Lobato et al., 2011). The Gaussian RBF kernels for the patch response GPs in all layers are initialized with lengthscale 5 and variance 5. We used the TICK kernel (Dutordoir et al., 2019) for the output layer GP, for which we use a Mat´ern32 kernel between patch locations with lengthscale initialized to 3. We initialize the inducing patch locations to random values in [0, H] × [0, W ], where [H, W ] is the shape of the output feature map in patch extraction. Convolutional GPs We set patch size to 5 × 5 and stride to 1. We use the whitening trick in all single-layer experiments for u (and v ) since we find it consistently improves the performance. Inducing points are initialized ⊥ by cluster centers which are generated from running K-means on M × 100 (for SVGP) or (M + M ) × 100 (for 2 SOLVE-GP) image patches. The image patches are randomly sampled from 1K images randomly selected from the dataset. Deep Convolutional GPs The detailed model configurations are summarized in Table 5. No whitening trick is used for multi-layer experiments because we find it hurts performance. Inducing points in the input layer are initialized in the same way as in the single-layer model. In Blomqvist et al. (2018); Dutordoir et al. (2019), three-layer models were initialized with the trained values of a two-layer model to avoid getting stuck in bad
Sparse Orthogonal Variational Inference for Gaussian Processes local minima. Here we design an initialization scheme that allows training deeper models without the need of pretraining. We initialize the inducing points in deeper layers by running K-means on M × 100 (for SVGP) or (M + M ) × 100 (for SOLVE-GP) image patches which are randomly sampled from the projections of 1K 2 images to these layers. The projections are done by using a convolution operation with random filters generated using Glorot uniform (Glorot and Bengio, 2010). We also note that when implementing the forward sampling for approximating the log-likelihood term, we follow the previous practice (Dutordoir et al., 2019) to ignore the correlations between outputs of different patches to get faster sampling, which works well in practice. While it is also possible to take into account the correlation when sampling as this only increases the computation cost by a constant factor, doing this might require multi-GPU training due to the additional memory requirements. Table 5: Model configurations of deep convolutional GPs. 2-layer 3-layer Layer 0 patch size 5 × 5, stride 1, out channel 10, patch size 5 × 5, stride 1, out channel 10 Layer 1 patch size 4 × 4, stride 2 patch size 4 × 4, stride 2, out channel 10 Layer 2 - patch size 5 × 5, stride 1 G.3 Regression Benchmarks The experiment settings are followed from Wang et al. (2019), where we used GPs with Mat´ern32 kernels and 80% / 20% training / test splits. A 20% subset of the training set is used for validation. We repeat each experiment 5 times with random splits and report the mean and standard error of the performance metrics. For all datasets we train for 100 epochs with learning rate 0.01 and mini-batch size 1024. H Additional Results H.1 Regression Benchmarks Due to space limitations in the main text, we include the Root precision (RMSE) on test data in Table 6. The results on Elevators and Bike are shown in Table 7. Table 6: Test RMSE values of regression datasets. The numbers in parentheses are standard errors. Best mean values are highlighted, and asterisks indicate statistical significance. Kin40k Protein KeggDirected KEGGU 3dRoad Song Buzz HouseElectric N 25,600 29,267 31,248 40,708 278,319 329,820 373,280 1,311,539 d 8 9 20 27 3 90 77 9 1024 0.193(0.001) 0.630(0.004) 0.098(0.003) 0.123(0.001) 0.482(0.001) 0.797(0.001) 0.263(0.001) 0.063(0.000) SVGP 1536 0.182(0.001) 0.621(0.004) 0.098(0.002) 0.123(0.001) 0.470(0.001) 0.797(0.001) 0.263(0.001) 0.063(0.000) 1024 + 1024 0.183(0.001) 0.625(0.004) 0.176(0.012) 0.156(0.004) 0.467(0.001) 0.797(0.001) 0.263(0.001) 0.062(0.000) ODVGP 1024 + 8096 0.180(0.001) 0.618(0.004) 0.157(0.009) 0.157(0.004) 0.462(0.002) 0.797(0.001) 0.263(0.001) 0.062(0.000) SOLVE-GP 1024 + 1024 *0.172(0.001) 0.618(0.004) 0.095(0.002) 0.123(0.001) 0.464(0.001) 0.796(0.001) 0.261(0.001) *0.061(0.000) SVGP 2048 0.177(0.001) 0.615(0.004) 0.100(0.003) 0.124(0.001) 0.467(0.001) 0.796(0.001) 0.263(0.000) 0.063(0.000) H.2 Convolutional GP Models We include here the full tables for CIFAR-10 classification, where we also report the accuracies and predictive log-likelihoods on the training data. Table 8 contains the results by convolutional GPs. Table 9 and Table 10 include the results of 2/3-layer deep convolutional GPs.
Jiaxin Shi, Michalis K. Titsias, Andriy Mnih Table 7: Regression results on Elevators and Bike. Best mean values are highlighted. Elevators Bike N = 10, 623, d = 18 N = 11, 122, d = 17 Test LL RMSE Test LL RMSE 1024 -0.516(0.006) 0.398(0.004) -0.218(0.006) 0.283(0.003) SVGP 1536 -0.511(0.007) 0.396(0.004) -0.203(0.006) 0.279(0.003) 1024 + 1024 -0.518(0.006) 0.397(0.004) -0.191(0.006) 0.272(0.003) ODVGP 1024 + 8096 -0.523(0.006) 0.399(0.004) -0.186(0.006) 0.270(0.003) SOLVE-GP 1024 + 1024 -0.509(0.007) 0.395(0.004) -0.189(0.006) 0.272(0.003) SVGP 2048 -0.507(0.007) 0.395(0.004) -0.193(0.006) 0.276(0.003) Table 8: Convolutional GPs for CIFAR-10 classification. Train Acc Train LL Test Acc Test LL Time 1000 77.81% -1.36 66.07% -1.59 0.241 s/iter SVGP 1600 78.44% -1.26 67.18% -1.54 0.380 s/iter SOLVE-GP 1000 + 1000 79.32% -1.20 68.19% -1.51 0.370 s/iter SVGP 2000 79.46% -1.22 68.06% -1.48 0.474 s/iter Table 9: 2-layer deep convolutional GPs for CIFAR-10 classification. Inducing Points Train Acc Train LL Test Acc Test LL Time SVGP 384, 1K 84.86% -0.82 76.35% -1.04 0.392 s/iter SOLVE-GP 384 + 384, 1K + 1K 87.59% -0.72 77.80% -0.98 0.657 s/iter SVGP 768, 2K 87.25% -0.74 77.46% -0.98 1.104 s/iter Table 10: 3-layer deep convolutional GPs for CIFAR-10 classification. Inducing Points Train Acc Train LL Test Acc Test LL Time SVGP 384, 384, 1K 87.70% -0.67 78.76% -0.88 0.418 s/iter SOLVE-GP (384 + 384) × 2, 1K + 1K 89.88% -0.57 80.30% -0.79 0.752 s/iter SVGP 768, 768, 2K 90.01% -0.58 80.33% -0.82 1.246 s/iter
