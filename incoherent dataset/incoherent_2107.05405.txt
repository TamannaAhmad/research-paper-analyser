Learning Expected Emphatic Traces for Deep RL Ray Jiang Shangtong Zhang DeepMind University of Oxford London, UK Oxford, UK rayjiang@google.com shangtong.zhang@cs.ox.ac.uk Veronica Chelu Adam White Hado van Hasselt McGill University DeepMind DeepMind Montreal, QC, Canada Edmonton, Canada London, UK veronica.chelu@mail.mcgill.ca adamwhite@google.com hado@google.com Abstract Off-policy sampling and experience replay are key for improving sample efficiency and scaling model-free temporal difference learning methods. When combined with function approximation, such as neural networks, this combination is known as the deadly triad and is potentially unstable. Recently, it has been shown that stability and good performance at scale can be achieved by combining emphatic weightings and multi-step updates. This approach, however, is generally limited to sampling complete trajectories in order, to compute the required emphatic weighting. In this paper we investigate how to combine emphatic weightings with non-sequential, off-line data sampled from a replay buffer. We develop a multi-step emphatic weighting that can be combined with replay, and a time-reversed n-step TD learning algorithm to learn the required emphatic weighting. We show that these state weightings reduce variance compared with prior approaches, while providing convergence guarantees. We tested the approach at scale on Atari 2600 video games, and observed that the new X-ETD(n) agent improved over baseline agents, highlighting both the scalability and broad applicability of our approach. Many deep supervised learning systems are not sample efficient. A simple and effective way to improve sample efficiency is to make better use of prior experience via replay (Lin, 1992; Mnih et al., 2015; Schaul et al., 2016; Hessel et al., 2018). Previous work demonstrated, somewhat surprisingly, that increasing the amount of replay in a model-free learning system can surpass the sample efficiency and final performance of model-based agents which utilize significantly more computation (van Hasselt et al., 2019). While improving on sample efficiency, using experience replay also introduces more potential for instability. Most approaches update from mini-batches of previous experience corresponding to older policies, and are therefore off-policy (Mnih et al., 2015; Hessel et al., 2018). Unfortunately combining bootstrapping via temporal-difference updates, function approximation and off-policy learning—known as the deadly triad (Sutton and Barto, 2018)—can destabilize learning resulting in “soft divergence”, slower learning, and reduced sample efficiency even if the parameters do not fully diverge (van Hasselt et al., 2018). Additionally, learning methods based on off-policy importance sampling (IS) corrections can result in high variance and poor performance during learning. This can be improved in practice by bootstrapping more, for instance by cleverly clipping the IS ratios as in the V-trace algorithm (Espeholt et al., 2018) or ABTD (Mahmood et al., 2017), though bootstrapping too much can exacerbate issues related to the deadly triad. In order to prevent divergence, we can try to correct the mismatch between the state distribution in the replay buffer and the current policy. The emphatic TD(λ) algorithm (ETD(λ); Sutton et al., 2016) Preprint. Under review. 1202 luJ 21 ]GL.sc[ 1v50450.7012:viXra
reweights the TD(λ) updates with an “emphatic” state weighting based on a “followon” trace that, intuitively, keeps track of how important each state is in the learning process. For instance, states that are heavily used to update other state values, via bootstrapping, will receive more emphasis, which ensures their values are sufficiently accurate even if they are updated infrequently. This prevents divergent learning dynamics. ETD(λ) uses eligibility traces and has not yet been combined with neural network function ap- proximation or replay. However, the idea of emphatic weighting is not restricted to trace-based (“backward-view”) algorithms and can be extended to other settings. For instance, n-step Emphatic TD (NETD) (Jiang et al., 2021) is a recent algorithm that combines emphatic weighting with n-step forward-view updates as well as V-trace learning targets. For consistency with the canonical name TD(n), for n-step TD learning, we call this algorithm ETD(n) in this paper. This was shown to outperform V-trace at scale in Atari and diagnostic MDP experiments (Jiang et al., 2021). The emphatic weightings used in ETD(n) are sequentially accumulated over time, in the form of trajectory-dependent traces, and can thus only be computed from online sequential trajectories, or full episodes of offline trajectories. In this paper we investigate how to combine emphatic weightings with non-sequential, off-line data sampled from a replay buffer. The idea is to estimate expected emphatic weightings as a function of state (Zhang et al., 2020b; van Hasselt et al., 2020), allowing us to appropriately weight the learning updates even if the inputs are sampled out of order. This reduces well-known variance issues with emphatic weightings (Ghiassian et al., 2018; Imani et al., 2018; Zhang et al., 2020b). We show in Sec. 3 that well-estimated emphatic weights reduce the potentially high variance of ETD(n) and achieve convergence with an upper bound on the bias from the ground-truth value function. Our contributions include 1) an off-policy time-reversed TD learning algorithm to learn the expected n-step emphatic trace using non-sequential data; 2) a discussion of potential stabilization techniques; 3) an analysis of theoretical properties of variance, stability and convergence for the resulting algorithm X-ETD(n); 4) an investigation of practical benefits of the approach when used at scale: we observed improved performance on Atari when using X-ETD(n) with replay. 1 Background We denote random variables with uppercase (e.g., S) and the obtained values with lowercase letters (e.g., S = s). Multi-dimensional functions or vectors are bolded (e.g., b), as are matrices (e.g. A). For all state-dependent functions, we also allow time-dependent shorthands (e.g., γ = γ(S )). t t 1.1 supervised learning problem setup We consider the usual RL setting in which an agent interacts with an environment, modelled as an infinite horizon Markov Decision Process (MDP) (S, A, P, r), with a finite state space S, a finite action space A, a state-transition distribution P : S ×A → P(S) (with P(S) the set of probability distributions on S and P (s(cid:48)|s, a) the probability of transitioning to state s(cid:48) from s by choosing action a), and a reward function r : S × A → R. A policy π : S → P(A) maps states to distributions over actions; π(a|s) denotes the probability of choosing action a in state s and π(s) denotes the probability distribution of actions in state s. Let S , A , R denote the random variables of state, t t t action and reward at time t, respectively. The goal of policy evaluation is to estimate the value function v , defined as the expectation of the π discounted return under policy π: . G = R + Σ∞ γ R = R + γ G , t t+1 i=t+1 i i+1 t+1 t+1 t+1 . v (s) = E[G | S = s, A ∼ π(S ), S ∼ P (S , A ) for all k ≥ t] , π t t k k k+1 k k where γ : S → [0, 1] is a discount factor. We consider function approximation and use v as our w estimate of v , where w are parameters of v to be updated. π w In the case of off-policy policy evaluation, though our goal is to estimate v , the actions for interacting π with the MDP are sampled according to a different policy µ. We refer to π and µ as target and behavior policies respectively and make the following assumption for the behavior policy µ: Assumption 1. (Ergodicity) The Markov chain induced by µ is ergodic. 2
Assumption 2. (Coverage) π(a|s) > 0 =⇒ µ(a|s) > 0 holds for any (s, a). Under Assumption 1, we use d to denote the ergodic distribution of the chain induced by µ. In this µ paper, we consider two off-policy learning settings: the sequential setting and the i.i.d. setting. In the sequential setting, the algorithm is presented with an infinite sequence as induced by the interaction (S , A , R , S , A , R , . . . ), 0 0 1 1 1 2 . where A ∼ µ(S ), R = r(S , A ), S ∼ P (S , A ). The idea is then that we update the value t t t+1 t t t+1 t t and/or policy at each of these states S , using data following the state (e.g., the sampled return). t Updates at state S always happen before updates at states S , for k > 0. t t+k In the i.i.d. setting, the algorithm is presented with an infinite number of finite sequences of length n {(Sk, Ak, Rk, Sk, Ak, Rk, . . . , Sk)} , 0 0 1 1 1 2 n k=1,2,... where the starting state of a sequence is sampled i.i.d., such that Sk ∼ d , and then the generating 0 µ . process for the subsequent steps is the same as before: Ak ∼ µ(Sk), Rk = r(Sk, Ak), Sk ∼ t t t+1 t t t+1 P (Sk, Ak). The idea is then that we update the value and/or policy of the first state in each sequence, t t Sk, using the rest of that sequence, e.g., by constructing a bootstrapped n-step return. 0 The sequential setting corresponds to the canonical agent-environment interaction (Sutton and Barto, 2018). Sequential algorithms are often data inefficient, since typically each state S is updated only t once and then discarded (e.g., Watkins and Dayan, 2004). One way to improve data efficiency is to store the sequential data in a replay buffer (Lin, 1992) and reuse these for further updates. If µ is stationary and these tuples are uniformly sampled from a large-enough buffer, their distribution is almost the same as d . Hence uniform replay is akin to the i.i.d. setting. If we sample from the replay µ buffer with different priorities, e.g., Sk is sampled from some other distribution d , the updates to Sk 0 p 0 can be reweighted with importance-sampling ratios d (Sk)/d (Sk) to retain the similarity to the i.i.d. µ 0 p 0 setting. Therefore, for simplicity and clarity, we present our theoretical results in the i.i.d. setting.1 1.2 Policy Evaluation We use the sequential setting and linear function approximation to demonstrate three RL algorithms for off-policy policy evaluation. We denote the features of state S by φ(S ) or φ . t t t Off-policy TD(n) Off-policy TD(n) updates w iteratively as (cid:16) (cid:17) w = w + α (cid:80)t+n−1 (cid:81)k−1 ρ γ ρ δ (w )φ , t+1 t k=t i=t i i+1 k k t t . where ρ = π(At|St) is an importance sampling (IS) ratio and δ (w ) is the TD error: t µ(At|St) k t δ (w ) = R + γ v (S ) − v (S ) . k t k+1 k+1 wt k+1 wt k ETD(n) Off-policy TD(n) can possibly diverge with function approximation. Emphatic weightings are an approach to address this issue (Sutton et al., 2016). In particular, ETD(n) considers the following “followon trace” to stabilize the off-policy TD(n) updates (Jiang et al., 2021): (cid:16) (cid:17) F = (cid:81)t−1 ρ γ F + 1, with F = F = · · · = F = 1, (1) t i=t−n i i+1 t−n 0 1 n−1 thus updating w iteratively as t (cid:16) (cid:17) w = w + αF (cid:80)t+n−1 (cid:81)k−1 ρ γ ρ δ (w )φ . (2) t+1 t t k=t i=t i i+1 k k t t Jiang et al. (2021) proved that this ETD(n) update is stable. In this paper, we consider stability in the sense of Sutton et al. (2016), i.e., a stochastic algorithm computing {w } according to . . t w = w + α (b − A w ) is said to be stable if A = lim E[A ] is positive definite (p.d.)2. t+1 t t t t t t→∞ t 1In practice, computing d (Sk)/d (Sk) exactly is usually impossible. One can, however, approximate it µ 0 p 0 with 1/(d (Sk)N) with N being the size of the replay buffer. We refer the reader to (Schaul et al., 2016) for p 0 more details about this approximation. 2See (7) for specific forms of vector b , matrix A in our case. t t 3
Notice F in (1) is a trace, defined on a sequence of transitions ranging, via recursion on previous t values F , into the indefinite past. When we do not have access to this sequence to compute the right t−n weighting for a given state, for instance because we sampled this state uniformly from a replay buffer, we need to consider an alternative way to correctly weight the update. This incompatibility between the i.i.d. setting and ETD(n) is the main problem we address in the paper. To differentiate from our proposed emphatic weighting, from here on we refer to the ETD(n) trace as the Monte Carlo trace since it is a Monte Carlo return in reversed time with “reward” signal of 1 every n steps. In addition, we use the term emphasis as a shorthand for “emphatic trace”. 2 Proposal: Learn the Expected Emphasis In order to apply emphatic traces to non-sequential i.i.d. data, we propose, akin to Zhang et al. (2020b), to directly learn a prediction model that estimates the limiting expected emphatic trace, i.e., we train a function f parameterized by θ such that f (s) approximates lim E [F | S = s].3 θ θ t→∞ µ k k We use the learned emphasis f in place of the Monte Carlo ETD(n) trace F , to re-weight the θk k n-step TD update in (2). We refer to the resulting value learning algorithm eXpected Emphatic TD(n), X-ETD(n). Thanks to the trace prediction model f , a sequential trajectory is no longer necessary for θ computing the emphatic weighting X-ETD(n). We know that using a learned expected emphasis can introduce approximation errors. Thus Section 3 contains a theoretical analysis, which shows that as long as the function approximation error is not too large, stability, convergence, and a reduction in variance, are all guaranteed. We dedicate this section to the describing how to learn f . θ The n-step emphatic traces in (1) are designed to emphasize n-step TD updates. Consequentially, the trace recursion in (1) follows the same blueprint as TD(n), but in the reverse direction of time. Hence a natural choice of learning algorithm for the expected emphasis is time-reversed TD learning that learns the “reward” 1 every nth step using off-policy TD(n) with the time index reversed. Considering the i.i.d. setting, we update θ using a “semi-gradient” (Sutton and Barto, 2018) TD update: k θ = θ + αθ (cid:2)(cid:0)(cid:81)n γkρk (cid:1) f (Sk) + 1 − f (Sk)(cid:3) ∇ f (Sk), (3) k+1 k k t=1 t t−1 θk 0 θk n θk θk n where αθ is a possibly time-dependent step size, ρk =. π(Ak t |S tk) , and γk =. γ(Sk) . This update k t µ(Ak|Sk) t t i t corresponds to the semi-gradient of the following loss, LF =. (cid:2)(cid:0)(cid:81)n γkρk (cid:1) f (Sk) + 1 − f (Sk)(cid:3)2 . (4) k t=1 t t−1 θk 0 θk n For the case of linear value function approximation, the update in (3) may not be stable because its update matrix, ΦT (I − (ΓPT )n)D Φ, is not guaranteed to be positive definite (details in the appendix). Here P is the tranπ sition mµ atrix such that P (s, s(cid:48)) =. (cid:80) π(a|s)P (s(cid:48)|s, a), Γ is a π . π a diagonal matrix such that Γ(s, s) = γ(s), D is a diagonal matrix whose diagonal entry is d , and µ µ Φ is the feature matrix whose s-th row is φ(s)(cid:62). Thus we propose two stabilization techniques for the time-reversed TD learning updates. IS Clipping One straightforward way is to clip the IS ratios in (3) just like in V-trace (Espeholt et al., 2018), i.e., we update θ iteratively as θ = θ + αθ (cid:2)(cid:0)(cid:81)n γk min(ρk , ρ¯)(cid:1) f (Sk) + 1 − f (Sk)(cid:3) ∇ f (Sk), (5) k+1 k k t=1 t t−1 θk 0 θk n θk θk n for some ρ¯ > 0; typically ρ¯ = 1. Define the substochastic matrix P such that for any state s, s(cid:48), ρ¯ P (s, s(cid:48)) =. (cid:80) µ(a|s) min(ρ(a|s), ρ¯)p(s(cid:48)|s, a)γ(s(cid:48)). ρ¯ a Then the update matrix of (5) is ΦT (I − (PT )n)D Φ (see details in the appendix). ρ¯ µ We prove that when estimating the expected emphasis using linear function approximation, there exist conditions under which we can guarantee stability at the cost of incurring additional bias. 3For the ease of presentation, we assume the existence of such a limit following Sutton et al. (2016); Jiang et al. (2021). The existence can be proved in a similar way to Lemma 1 of Zhang et al. (2019), which we leave for future work. 4
Proposition 1. There exists a constant τ > 0 such that the update in (5) is stable whenever ρ¯ < τ . See its proof in the appendix. One such constant is τ = max 1/γ(s) where the maximum of s discounts γ(s) is over states. Notice that while achieving stability, clipping at 1/γ also restricts variance of learning to a finite amount since the Monte Carlo ETD(n) trace is bounded. In practice, we tune ρ¯ to optimize a bias-stability trade-off. Auxiliary Monte-Carlo loss In most learning settings (e.g., Mnih et al. (2015)), both sequential samples and i.i.d. samples are available. To take advantage of this fact, we can stabilize the emphasis learning by partially regressing on the Monte Carlo emphatic trace. We can thus learn the parameters θ by TD-learning using samples from the replay buffer and by Monte Carlo learning using online experience: θ = θ + αθ (cid:2)(cid:0)(cid:81)n γkρk (cid:1) f (Sk) + 1 − f (Sk)(cid:3) ∇ f (Sk) (6) k+1 k k t=1 t t−1 θk 0 θk n θk θk n + αθ β (F − f (S )) ∇ f (S ). k k θk k θk θk k This update corresponds to the joint loss function: LF,MC =. (cid:2)(cid:0)(cid:81)n γkρk (cid:1) f (Sk) + 1 − f (Sk)(cid:3)2 + β(f (S ) − F )2, k t=1 t t−1 θk 0 θk n θk k k where β is a hyper-parameter for balancing the two losses. When f uses linear function approxima- θ tion, we prove the following guarantee on its stability (proof in the appendix). Proposition 2. There exists a constant ξ such that the update in (6) is stable whenever β > ξ. The time-reversed TD update can be unstable, whereas the Monte Carlo update target F can have k large variance (Sutton et al., 2016; Jiang et al., 2021). By choosing β, we optimize a variance-stability trade off. 3 Expected Emphatic TD learning To prevent deadly triads, we use the learned expected emphasis f to re-weight the learning updates θ of TD(n). In this section, we analyze the resulting algorithm, X-ETD(n). For simplicity, let the trace model f be parameterized by a fixed parameter θ.4 In this section, we analyze X-ETD(n) in the θ sequential setting for the ease of presentation. A similar analysis would apply to X-ETD(n) in the i.i.d. setting. Then X-ETD(n) updates w iteratively as w = w + αwf (S )∆w, (7) t+1 t t θ t t where ∆w =. (cid:80)t+n−1 (cid:16) (cid:81)k−1 γ ρ (cid:17) ρ (R + γ w(cid:62)φ(S ) − w(cid:62)φ(S ))φ(S ). t k=t i=t i+1 i k k+1 k+1 t k+1 t t t Equivalently, we can write (7) as w = w + αw(b − A w ), where t+1 t t t t t A =. f (S )φ(S ) (cid:80)t+n−1 (cid:16) (cid:81)k−1 γ ρ (cid:17) ρ [φ(S ) − γ φ(S )](cid:62) t θ t t k=t i=t i+1 i k k k+1 k+1 b =. f (S ) (cid:80)t+n−1 (cid:16) (cid:81)k−1 γ ρ (cid:17) ρ R φ(S ) t θ t k=t i=t i+1 i k k+1 t As we use f (S ) to reweight the update, it is convenient to define a diagonal matrix Dθ with diagonal θ t . µ entries [Dθ ] = d (s)f (s) for any state s. In X-ETD(n), we approximate lim E [F |S = µ ss µ θ t→∞ µ t t . s] with f (s). In this, we also define a ground-truth diagonal matrix Df such that [Df ] = θ . µ µ ss d (s) lim E [F |S = s], and their difference, D(cid:15) = Dθ − Df , is the (d -weighted) function µ t→∞ µ t t µ µ µ µ approximation error matrix of the emphasis approximation. It can be computed that . . A = lim E[A ] = Φ(cid:62)Dθ (I − (P Γ)n)Φ, b = lim E[b ] = Φ(cid:62)Dθ rn, t→∞ t µ π t→∞ t µ π where rn =. (cid:80)n−1(P Γ)ir is the n-step reward vector with r (s) =. (cid:80) π(a|s)r(s, a). π i=0 π π π a 4This is not a special setting since the expected emphasis learning process is independent from learning parameters w. 5
3.1 Variance Learning to estimate the emphatic trace not only makes value-learning compatible with offline learning methods that make use of replay buffers, but is also instrumental in reducing the variance of the value-learning updates. The incremental update of ETD(n) in (2) can be rewritten as F ∆w. t t The following proposition shows that when the trace approximation error is small enough, variance in learning can indeed be reduced by replacing the Monte Carlo trace F with the learned trace f (S ). t θ t . Proposition 3. (Reduced variance) Let (cid:15) = |f (s) − f (s)| be the trace approximation error at a s θ state s. For any s, there exists a time t¯ > 0, such that for all t > t¯, (cid:15) ((cid:15) + 2f (s)) < V(F |S = s) =⇒ V(f (S )∆w|S = s) ≤ V(F ∆w|S = s). s s t t θ t t t t t t The inequality is strict if V(∆w|S = s) > 0. t t (Proof in the appendix.) In some cases V(F |S = s) can be infinite (Sutton et al., 2016); then the t t condition in Proposition 3 holds trivially. This also underpins the importance of variance reduction. 3.2 Convergence Next, under the following assumption about the learning rate, we show the convergence of (7). Assumption 3. (Learning rates) The learning rates {αw}∞ are nonnegative, deterministic, and t t=0 satisfy (cid:80) αw = ∞, (cid:80) (αw)2 < ∞. t t t t Theorem 1. (Convergence of X-ETD(n)) Under Assumptions 1-3, for the iterates {w } generated t by (7), there exists a constant η > 0 such that (cid:13) (cid:13) (cid:13)D(cid:15) µ(cid:13) < η =⇒ lim t→∞ w t = A−1b a.s.. The proof of this theorem is in the appendix, along with a stability guarantee for the X-ETD(n) updates. Theorem 1 shows that under some mild conditions, assuming the function approximation . error is not too large, X-ETD(n) converges to w = A−1b.. ∞ We now study the performance of w , i.e., the distance between the value prediction by w and the ∞ ∞ true value function v . π Proposition 4. (Suboptimality of the fixed point) Under Assumptions 1 & 4, there exists positive constants c , c , and c such that 1 2 3 (cid:13) (cid:13)D(cid:15) µ(cid:13) (cid:13) ≤ c 1 =⇒ (cid:107)Φw ∞ − v π(cid:107) ≤ c 2(cid:13) (cid:13)D(cid:15) µ(cid:13) (cid:13) + c 3(cid:13) (cid:13) (cid:13)Π Df µv π − v π(cid:13) (cid:13) (cid:13) Df µ, (cid:13) (cid:13) where (cid:13)Π v − v (cid:13) is the value estimation error of the unbiased fixed point using the Monte (cid:13) Df µ π π(cid:13) Df µ Carlo emphasis. We prove this proposition in the appendix. Figure 1: RMSE in the value estimates and RMSE in expected trace approximation over time in a modified version of Baird’s counterexample (Baird, 1995). We report the performance of each algorithm using the best performing hyper-parameters (according to RMSE of the value function) from an extensive sweep (described in text). Shaded regions indicate two standard deviations of the mean performance computed from 100 independent runs. 6
3.3 Illustration on Baird’s counterexample We illustrate the theoretical results in this section on a small MDP modified from Baird’s coun- terexample (Baird, 1995). The MDP has seven states with linear features. The over-parametrizing features are designed to cause instability even though the true values can be represented. See Sutton and Barto (2018) for an extensive discussion and analysis of Baird’s counterexample. As in Zhang et al. (2020b) we modify the MDP (shown in Fig. 5(a) in the appendix). The canonical version of Baird’s MDP causes all but gradient TD and residual gradient methods (Sutton et al., 2009) to diverge. Here, we are more interested in a challenging MDP and are not focused on extreme divergence cases. We use γ = 0.95 as discount and a target policy π(solid|·) = 0.3. We tested all combinations of αw ∈ {2i : i = −6, . . . , −14} and αθ = αwβ, with β ∈ {0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0}. Figure 1 summarizes the results with additional results in the appendix. X-ETD(n) was more stable in this small but challenging MDP. ETD(n) exhibited high variance and instability. X-ETD(n) had very low variance, echoing the conclusion of Prop. 3 that X-ETD(n) has lower variance when its emphasis errors are small. X-ETD(n) also converged faster to the true fixed point, illustrating Theorem 1 and, moreover, achieving the optimal fixed point, far better than the worst case upper bound of Prop. 4. Note, even when choosing αw and β of X-ETD(n) to minimize the RMSE in the value function, the emphasis approximation error exhibits steady improvement. 4 Experiments Back to the problem that motivated this paper, our goal is to stabilize learning at scale when using experience replay. Inspired by the performance achieved by Surreal (Jiang et al., 2021) and previously by StacX (Zahavy et al., 2020), both extensions of IMPALA (Espeholt et al., 2018), we adopt the same off-policy setting of learning auxiliary tasks to test X-ETD(n), with the additional use of experience replay. It has become conventional to clip IS ratios to reduce variance and improve learning results (Espeholt et al., 2018; Zahavy et al., 2020; Hessel et al., 2021a). We similarly adapt X-ETD(n) to the control setting by clipping IS ratios at 1 in both policy evaluation, as described in Clipping of Sec. 2, and applying the learned emphatic weighting to the corresponding policy gradients. The setting is similar to Jiang et al. (2021); further details are in the appendix. Data We evaluate X-ETD(n) on a widely used deep RL benchmark, Atari games from the Arcade Learning Environment (Bellemare et al., 2013)5. The input observations are in RGB format without downsampling or gray scaling. We use an action repeat of 4, with max pooling over the last two frames and the life termination signal. This is the same data format as that used in Surreal (Jiang et al., 2021) and StacX (Zahavy et al., 2020). In addition, we randomly sample half of the training data from an experience replay buffer which contains the most recent 10,000 sequences of length 20. In order to compare with previous works, we use the conventional 200M online frames training scheme, with an evaluation phase at 200M-250M learning frames. Baseline Agent Surreal is an IMPALA-based agent that learns two auxiliary tasks with different discounts γ1, γ2 simultaneously while learning the main task (Fig. 2, in gray). The auxiliary tasks are learned off-policy since the agent generates behaviours only from the main policy output. Prior to applying X-ETD(n), we swept extensively on its hyper-parameters to produce the best baseline we could find with 50% replay data (see the appendix for further details and hyper-parameters). X-ETD(n) Agents We investigate whether X-ETD(n) updates can improve off-policy learning of the auxiliary tasks. For each of the two auxiliary tasks, we implement an additional Multilayer Perceptron (MLP) that predicts the expected emphatic trace using the time-reversed TD learning loss Lf in (4) (see Fig. 2, in blue). The prediction outputs f i are then used to re-weight both the V-trace value and policy updates for the auxiliary task i = 1, 2, similar Jiang et al. (2021). In order to isolate the effect of using X-ETD(n) from any changes to internal representations as a result of the additional trace learning losses, we prevent the gradients from back-propagating to the core of the agent. We denote learned emphasis with auxiliary Monte Carlo loss as X-ETD(n)-MC, described in Sec. 2. We 5Licensed under GNU General Public License v2.0. 7
Losses L(w; γ) L(w; γi) +wLf(θi; γi),i = 1,2 π, v π1, v1 f1 π2, v2 f2 MLP MLP MLP MLP MLP LSTM auxiliary heads Deep Residual Block stop gradient backprop observation Figure 2: Block diagram of X-ETD(n). Agent has one main head, two auxiliary heads and two trace heads with stop gradients. The subset of architecture in gray denotes the baseline agent Surreal, and those highlighted in blue are used in trace learning. We use the IMPALA loss on each head with different discounts γ, γ1, γ2. The behavior policy is fixed to be π. The predicted traces f 1, f 2 are learned with time-reversed TD losses Lf (θi; γi) weighted by w for the auxiliary heads i = 1, 2. implement all agents in a distributed system based on JAX libraries (Hennigan et al., 2020; Budden et al., 2020; Hessel et al., 2020)6 using a TPU Pod infrastructure called Sebulba (Hessel et al., 2021b). Evaluation Running many seeds on all 57 Atari games is expensive. However a single metric with few seeds can be noisy, or hard to properly interpret. Hence we adopt a four-faceted evaluation strategy. We report mean and median human normalized training curves with standard deviations across 3 seeds (Fig. 4), accompanied by a bar plot of per-game improvements in normalized scores averaged across 3 seeds and the evaluation window (Fig. 3). In addition, to test rigorously whether X-ETD(n) improved performance, we apply a one-sided Sign Test (Arbuthnot, 1712) on independent pairs of agent scores on 57 (games) x 3 (seeds) to compute its p-value, where scores are averaged across the evaluation window and the baseline and test agent seeds are paired randomly. To guarantee random pairing, we uniformly sample and pair the seeds 10,000 times and take the average number of games on which the test agent is better. The p-value is the probability of observing the stated results under the null hypothesis that the algorithm performs equally. Results might be thought of as statistically significant when p < 0.05. Statistics X-ETD(n) X-ETD(n)-MC baseline: Surreal Median human normalized score 503 537 525 Mean human normalized score 2122 2090 1879 # games > baseline 97 (out of 171) 97 (out of 171) N/A p-value from Sign Test 0.046 0.046 N/A Table 1: Performance statistics for Surreal and learned emphases applied to Surreal on 57 Atari games. Scores are human normalized, averaged across 3 seeds and across the evaluation phase (200M-250M). Results Table 1, shows that the baseline Surreal agent with 50% replay achieved a mean score of 1879, a median score of 525; X-ETD(n) using the same amount of replay data achieved a mean score of 2122, a median score of 503; while X-ETD(n)-MC obtained mean 2090 and median 537. The median scores are human-normalized and averaged across 3 seeds, then averaged over 200-250M frames evaluation window. On a per-game level, both X-ETD(n) and X-ETD(n)-MC outperformed Surreal on 97 out of 57x3=171 games with a p-value of 0.046. Fig. 3 shows per-game improvements over Surreal. The improvements are stable across two emphatic variants on the majority of games. In the games where learned emphasis hurt performance, we observed two scenarios: 1) the predicted emphasis collapsed to 1, e.g. assault, road_runner, 2) the predicted emphasis runs wild to huge values, e.g. yars_revenge, crazy_climber, centipede. The huge negative predictions are especially detrimental as the gradient directions are flipped. In Fig. 4 shows mean and median training curves, with standard deviations across 3 seeds for each learning frame and then smoothed via a standard 1-D Gaussian filter (σ = 10) for clarity. Adding auxiliary Monte Carlo loss is a double-edged sword, while improving stability of the time-reversed 6Licensed under Apache License 2.0. 8
tluassa rennur_daor xineohp egnever_sray rebmilc_yzarc edepitnec row_fo_draziw rennug_rats xiretsa llabnip_oediv gniiks rehpog sitnalta diarrevir radima knud_elbuod gnilwob ybred_gnihsif kcatta_nomed siralos etibtsorf llaftip egnever_amuzetnom yaweerf orudne erutnev eye_etavirp gnop gnixob sinnet tsieh_knab neila emag_siht_eman krezreb rativarg llurk dnuorrus sdioretsa mahknatut trebq oreh retsam_uf_gnuk tuokaerb rednefed tolip_emit redir_maeb sredavni_ecaps namcap_sm noxxaz yekcoh_eci enoz_elttab knatobor ooragnak tseuqaes dnammoc_reppohc nwod_n_pu dnobsemaj 104 103 102 101 100 0 100 101 102 103 )dezilamron namuh( laerruS revo tnemevorpmI X-ETD(n) X-ETD(n)-MC SSuurrrreeaall Figure 3: Improvement in individual human normalized game scores compared to Surreal: X- ETD(n) in orange and X-ETD(n)-MC in blue. Both improve over baseline Surreal in median human normalized scores on 57 Atari games, and scores improved in more games than in which they deteriorated. Results averaged between 200M - 250M frames (evaluation phase) across 3 seeds. 500 450 400 350 300 250 50 100 150 200 Learning frames (Millions) serocs dezilamron namuh naideM Surreal 2300 X-ETD(n) X-ETD(n)-MC 2200 ETD(n)(2021) 2100 2000 1900 StacX(2020) 1800 1700 Meta-gradient (2018) 1600 UNREAL(2016) 1500 150 160 170 180 190 200 Learning frames (Millions) (a) Median human normalized scores serocs dezilamron namuh naeM Surreal X-ETD(n) X-ETD(n)-MC ETD(n)(2021) (b) Mean human normalized scores Figure 4: Training curves of (a) median and (b) mean human normalized scores on 57 Atari games, with standard deviations (shaded area) across 3 seeds. TD learning, it also brings in higher variance from the Monte Carlo emphasis. For this reason X-ETD(n)-MC exhibited more variance than X-ETD(n) during training, however, it also lead to more stability in score improvement across games, especially mitigating losses where learned emphasis failed to help (see Fig. 3). What X-ETD(n) approximates is essentially similar to the density ratio between the state distributions of the target and behavior policies in that they both share a backward Bellman equation (Liu et al., 2018; Zhang et al., 2020b). Learning density ratios is an active research area but past works usually only tested on benchmarks with low-dimensional observations (e.g. MuJoCo (Todorov et al., 2012), (Liu et al., 2018; Nachum et al., 2019; Zhang et al., 2020a; Uehara et al., 2020; Yang et al., 2020)). In this work, we demonstrate that performance improvement is entirely possible when applying learned emphasis to challenging Atari games using high-dimensional image observations. 9
5 Related Work The idea of learning expected emphatic traces as a function of state has been explored before on the canonical followon trace for backward view TD(λ), to improve trackability of the critic in off-policy actor-critic algorithms (Zhang et al., 2020b). However in this work we focus on the n-step trace from Jiang et al. (2021) in the forward view, to improve data efficiency in deep RL. Our proposed stabilization techniques, to facilitate at-scale learning, differ from Zhang et al. (2020b). Though Zhang et al. (2020b) also use a learned trace to reweight 1-step off-policy TD in the GEM-ETD algorithm, theoretical analyses were not provided. In contrast, we provide a thorough theoretical analysis for X-ETD(n). Finally, we demonstrate the effectiveness of our methods in challenging Atari domains, while Zhang et al. (2020b) experiment with only small diagnostic environments. The idea of bootstrapping in the reverse direction has also been explored by Wang et al. (2007, 2008); Hallak and Mannor (2017); Gelada and Bellemare (2019) in learning density ratios and by Zhang et al. (2020c) in learning reverse general value functions to represent retrospective knowledge. Besides learning a scalar followon trace, van Hasselt et al. (2020) learn a vector eligibility trace (Sutton, 1988), which, together with Satija et al. (2020), inspired our use of an auxiliary Monte Carlo loss. 6 Conclusion In this paper, we propose a simple time-reversed TD learning algorithm for learning expected emphases that is applicable to non-sequential i.i.d. data. We proved that under certain conditions the resulting algorithm X-ETD(n) has low variance, is stable and convergence to a reasonable fixed point. Furthermore, it improved off-policy learning results upon well-established baselines on Atari 2600 games, demonstrating its generality and wide applicability. In future works, we would like to study X-ETD(n) in more diverse off-policy learning settings using different data sources. 10
References Arbuthnot, J. (1712). II. An argument for divine providence, taken from the constant regularity observ’d in the births of both sexes. By Dr. John Arbuthnott, Physitian in Ordinary to Her Majesty, and Fellow of the College of Physitians and the Royal Society. Philosophical Transactions of the Royal Society of London, 27(328):186–190. Baird, L. (1995). Residual algorithms: supervised learning with function approximation. Pro- ceedings of the Twelfth International Conference on Machine Learning, pages 30–37. Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. (2013). The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279. Bertsekas, D. P. and Tsitsiklis, J. N. (1995). Neuro-dynamic programming: an overview. In Proceedings of 1995 34th IEEE conference on decision and control, volume 1, pages 560–564. IEEE. Budden, D., Hessel, M., Quan, J., Kapturowski, S., Baumli, K., Bhupatiraju, S., Guy, A., and King, M. (2020). RLax: supervised learning in JAX. Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K. (2018). IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. CoRR. Gelada, C. and Bellemare, M. G. (2019). Off-policy deep supervised learning by bootstrapping the covariate shift. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3647–3655. Ghiassian, S., Patterson, A., White, M., Sutton, R. S., and White, A. (2018). Online off-policy prediction. CoRR, abs/1811.02597. Golub, G. H. and Van Loan, C. F. (2013). Matrix computations, volume 3. JHU press. Hallak, A. and Mannor, S. (2017). Consistent on-line off-policy evaluation. In International Conference on Machine Learning, pages 1372–1383. PMLR. Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. (2020). Haiku: Sonnet for JAX. Hessel, M., Budden, D., Viola, F., Rosca, M., Sezener, E., and Hennigan, T. (2020). Optax: composable gradient transformation and optimisation, in JAX! Hessel, M., Danihelka, I., Viola, F., Guez, A., Schmitt, S., Sifre, L., Weber, T., Silver, D., and van Hasselt, H. (2021a). Muesli: Combining improvements in policy optimization. In International Conference on Machine Learning. PMLR. Hessel, M., Kroiss, M., Clark, A., Kemaev, I., Quan, J., Keck, T., Viola, F., and van Hasselt, H. (2021b). Podracer architectures for scalable supervised learning. Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2018). Rainbow: Combining improvements in deep supervised learning. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). Hessel, M., Soyer, H., Espeholt, L., Czarnecki, W., Schmitt, S., and van Hasselt, H. (2019). Multi- task deep supervised learning with popart. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):3796–3803. Imani, E., Graves, E., and White, M. (2018). An off-policy policy gradient theorem using emphatic weightings. Proceedings of the 32nd International Conference on Neural Information Processing Systems (NeurIPS 2018). Jiang, R., Zahavy, T., White, A., Xu, Z., Hessel, M., Blundell, C., and van Hasselt, H. (2021). Emphatic algorithms for deep supervised learning. In International Conference on Machine Learning. PMLR. 11
Levin, D. A. and Peres, Y. (2017). Markov chains and mixing times, volume 107. American Mathematical Soc. Lin, L.-J. (1992). Self-improving reactive agents based on supervised learning, planning and teaching. Machine learning, 8(3-4):293–321. Liu, Q., Li, L., Tang, Z., and Zhou, D. (2018). Breaking the curse of horizon: Infinite-horizon off-policy estimation. arXiv preprint arXiv:1810.12429. Mahmood, A. R., Yu, H., and Sutton, R. S. (2017). Multi-step off-policy learning without importance sampling ratios. arXiv preprint arXiv:1702.03006. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015). Human-level control through deep supervised learning. Nature. Nachum, O., Chow, Y., Dai, B., and Li, L. (2019). Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. arXiv preprint arXiv:1906.04733. Satija, H., Amortila, P., and Pineau, J. (2020). Constrained markov decision processes via backward value functions. In International Conference on Machine Learning, pages 8502–8511. PMLR. Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016). Prioritized experience replay. In Bengio, Y. and LeCun, Y., editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine learning, 3(1):9–44. Sutton, R. S. and Barto, A. G. (2018). supervised learning: An Introduction. The MIT Press, Cambridge, MA. Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvári, C., and Wiewiora, E. (2009). Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 993–1000. Sutton, R. S., Mahmood, A. R., and White, M. (2016). An emphatic approach to the problem of off-policy temporal-difference learning. The Journal of Machine Learning Research, 17(1):2603– 2631. Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE. Uehara, M., Huang, J., and Jiang, N. (2020). Minimax weight and q-function learning for off-policy evaluation. In International Conference on Machine Learning, pages 9659–9668. PMLR. van Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J. (2018). Deep supervised learning and the deadly triad. CoRR, abs/1812.02648. van Hasselt, H., Hessel, M., and Aslanides, J. (2019). When to use parametric models in supervised learning? In Advances in Neural Information Processing Systems 36, NeurIPS. van Hasselt, H., Madjiheurem, S., Hessel, M., Silver, D., Barreto, A., and Borsa, D. (2020). Expected eligibility traces. arXiv preprint arXiv:2007.01839. Varga, R. S. (1962). Iterative analysis. Springer. Wang, T., Bowling, M., and Schuurmans, D. (2007). Dual representations for dynamic programming and supervised learning. In 2007 IEEE International Symposium on Approximate Dynamic Programming and supervised learning, pages 44–51. IEEE. 12
Wang, T., Bowling, M., Schuurmans, D., and Lizotte, D. J. (2008). Stable dual dynamic programming. In Advances in neural information processing systems, pages 1569–1576. Watkins, C. J. C. H. and Dayan, P. (2004). Q-learning. Machine Learning, 8:279–292. White, M. (2017). Unifying task specification in supervised learning. In International Conference on Machine Learning, pages 3742–3750. PMLR. Yang, M., Nachum, O., Dai, B., Li, L., and Schuurmans, D. (2020). Off-policy evaluation via the regularized lagrangian. arXiv preprint arXiv:2007.03438. Zahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van Hasselt, H., Silver, D., and Singh, S. (2020). A self-tuning actor-critic algorithm. 34th Conference on Neural Information Processing Systems (NeurIPS 2020). Zhang, S., Boehmer, W., and Whiteson, S. (2019). Generalized off-policy actor-critic. In Advances in Neural Information Processing Systems, volume 32. Zhang, S., Liu, B., and Whiteson, S. (2020a). Gradientdice: Rethinking generalized offline estimation of stationary values. In International Conference on Machine Learning, pages 11194–11203. PMLR. Zhang, S., Liu, B., Yao, H., and Whiteson, S. (2020b). Provably convergent two-timescale off-policy actor-critic with function approximation. In International Conference on Machine Learning, pages 11204–11213. PMLR. Zhang, S., Veeriah, V., and Whiteson, S. (2020c). Learning retrospective knowledge with reverse supervised learning. In Advances in Neural Information Processing Systems, volume 33. 13
Supplementary Material A Time-reversed TD Instability The asymptotic update matrix of (3) is  (cid:34) (cid:32) n (cid:33) (cid:35)(cid:62) . (cid:89) A = lim E[A k] = lim E φ(S nk) φ(S nk) − γ tkρk t−1 φ(S 0k)  k→∞ k→∞ t=1  (cid:34) (cid:32) n (cid:33) (cid:35)(cid:62)  (cid:88) (cid:89) = d µ(s)E φ(S nk φ(S nk) − γ tkρk t−1 φ(S 0k) | S nk = s s t=1 = Φ(cid:62)D (I − D−1(ΓPT )nD )Φ, µ µ π µ = ΦT (I − (ΓPT )n)D Φ. π µ Notice that A is not necessarily p.d.. It is the matrix transpose of the steady state n-steps TD update matrix Φ(cid:62)D (I − (P Γ)n)Φ. Thus the time-reversed TD is not always stable. µ π We now state a Lemma rephrased from Sutton et al. (2016), which will be repeatedly used in this paper. Lemma 1. (Sutton et al., 2016) Let X be a matrix with full column rank, D be a diagonal matrix with strictly positive diagonal entries, P be a substochastic matrix, the row vector 1(cid:62)D(I − P) be elementwise strictly positive, then X(cid:62)D(I − P)X is p.d.. Proof. We first show that . Y = D(I − P) + (D(I − P))(cid:62) is p.d.. Since Y is symmetric, Corollary in page 23 of Varga (1962) states that Y is p.d. if Y is strcitly diagonally dominant, i.e., if for any i, (cid:88) |Y(i, i)| > |Y(i, j)|. (8) j(cid:54)=i Note that the diagonal entries of Y are nonnegative and the off-diagnoal entries of Y are nonpositive. Consequently, (8) is equivalent to (Y1)(i) > 0. We have Y1 = D(I − P)1 + (1(cid:62)D(I − P))(cid:62). Since P is a substochastic matrix, ((I − P)1)(i) ≥ 0 holds for any i, it is then easy to see Y1(i) > 0. Consequently, Y is p.d.. By Sutton (1988), D(I − P) is p.d. as well, implying that XD(I − P)X is p.d.. Proof of Proposition 1 Proof. It suffices, according to Lemma 1, to show that the row vector . k(cid:62) = d(cid:62) − d(cid:62)(P )n µ µ ρ¯ is strictly element-wise positive. Let . u = max min(ρ(a|s), ρ¯), ρ¯ s,a . γ = max γ(s). s 14
we have (cid:88) P (s, s(cid:48)) ≤ µ(a|s)p(s(cid:48)|s, a)u γ(s(cid:48)) = P (s, s(cid:48))u γ(s(cid:48)) ≤ P (s, s(cid:48))u γ, ρ¯ ρ¯ µ ρ¯ µ ρ¯ a (cid:88) (cid:88) =⇒ P (s, j)P (j, s(cid:48)) ≤ P (s, j)P (j, s(cid:48))u2γ2 ρ¯ ρ¯ µ µ ρ¯ j j =⇒ P2(s, s(cid:48)) ≤ P2 (s, s(cid:48))u2γ2 ρ¯ µ ρ¯ =⇒ Pn(s, s(cid:48)) ≤ Pn(s, s(cid:48))unγn ρ¯ µ ρ¯ So (cid:88) k(s) = d (s) − d (s¯)Pn(s¯, s) µ µ ρ¯ s¯ (cid:88) ≥ d (s) − d (s¯)Pn(s¯, s)unγn µ µ µ ρ¯ s¯ = d (s)(1 − unγn) µ ρ¯ Thus 1 u < ρ¯ γ is a sufficient condition for the key matrix to be p.d., which completes the proof. Proof of Proposition 2 Proof. The asymptotic update matrix of (6) is . A = Φ(cid:62)((I + β) − (ΓP(cid:62))n)D Φ. π µ For this matrix to be p.d., it suffices, according to Lemma 1, to have the row vector 1(cid:62)D ((1 + β)I − (P Γ)n) µ π to be strictly element-wise positive. Clearly, one sufficient condition is that (cid:0) d(cid:62)(P Γ)n(cid:1) (s) 1 + β > max µ π , s d µ(s) which completes the proof. B X-ETD(n) Proof of Proposition 3 Proof. It is easy to see V(f (S )∆w|S = s) = f 2(s)V(∆w|S = s). θ t t t θ t t By the rule of the variance of the product of (conditionally) independent random variables, we have V(F ∆w|S = s) t t t =V(F |S = s)V(∆w|S = s) + V(F |S = s)E2[∆w|S = s] + V(∆w|S = s)E2[F |S = s] t t t t t t t t t t t t ≥V(F |S = s)V(∆w|S = s) + V(∆w|S = s)E2[F |S = s]. t t t t t t t t Then it is easy to see that one sufficient condition for V(f (S )∆w|S = s) ≤ V(F ∆w|S = s) θ t t t t t t to hold is that f 2(s) ≤ V(F |S = s) + E2[F |S = s]. θ t t t t 15
For any state s, simple algebraic manipulation shows that f 2(s) − f 2(s) ≤ (cid:15) ((cid:15) + 2f (s)), θ s s i.e., for any s and t, (cid:15) ((cid:15) + 2f (s)) < V(F |S = s) s s t t =⇒ f 2(s) − f 2(s) < V(F |S = s). θ t t Or equivalently, (cid:15) ((cid:15) + 2f (s)) < V(F |S = s) s s t t =⇒ f 2(s) = V(F |S = s) + f 2(s) − τ θ t t for some τ > 0. Since lim E2[F |S = s] = f 2(s), t t t→∞ for any s, there exists a t¯such that for all t > t¯, |E2[F |S = s] − f 2(s)| < τ. t t Consequently, for any t > t¯, (cid:15) ((cid:15) + 2f (s)) < V(F |S = s) s s t t =⇒ f 2(s) < V(F |S = s) + E2[F |S = s], θ t t t t which completes the proof. B.1 Stability After making the following assumption about the features, we show that the update of X-ETD(n) (7) is stable as long as the function approximation error is not too large. Assumption 4. (Features) The feature matrix Φ has full column rank. Lemma 2. (Stability) Under Assumptions 1 & 4, there exists a constant η > 0 such that 0 (cid:13) (cid:13)D(cid:15) µ(cid:13) (cid:13) < η 0 =⇒ A is p.d. Proof. As shown by Jiang et al. (2021), Df (I − (P Γ)n) is p.d., i.e., for any y, µ π . g(y) = y(cid:62)Df (I − (P Γ)n)y > 0. µ π . Since g(y) is a continuous function, it obtains its minimum value in the compact set Y = {y : (cid:107)y(cid:107) = 1}, say, e.g., η, i.e., g(y) ≥ η > 0 holds for any y ∈ Y. In particular, for any y ∈ RK, y g( ) ≥ η (cid:107)y(cid:107) i.e., y(cid:62)Df (I − (P Γ)n)y ≥ η(cid:107)y(cid:107)2. µ π Let . η η = , 0 (cid:107)I − (P Γ)n(cid:107) π we have for any y, y(cid:62)Dθ (I − (P Γ)n)y µ π =y(cid:62)Df (I − (P Γ)n)y + y(cid:62)Dε (I − (P Γ)n)y µ π µ π ≥η(cid:107)y(cid:107)2 + y(cid:62)Dε (I − (P Γ)n)y µ π ≥η(cid:107)y(cid:107)2 − |y(cid:62)Dε (I − (P Γ)n)y| µ π ≥η(cid:107)y(cid:107)2 − (cid:107)y(cid:107)2(cid:13) (cid:13)Dε µ(cid:13) (cid:13)(cid:107)I − (P πΓ)n(cid:107) =(η 0 − (cid:13) (cid:13)Dε µ(cid:13) (cid:13))(cid:107)I − (P πΓ)n(cid:107)(cid:107)y(cid:107)2, i.e., when (cid:13) (cid:13)D(cid:15) µ(cid:13) (cid:13) < η 0 holds, Df θ (I − (P πΓ)n) is p.d., which, together with Assumption 4, immedi- ately implies that A is p.d.. 16
Following the definition of stability in Sutton et al. (2016), A being p.d. gives stable steady state updates. Proof of Theorem 1 Proof. We first consider Eq. (7) in the sequential setting. . Let Z = (S , A , . . . , S , A , S ). Assumption 1 implies that the Markov chain {Z } is t t t t+n t+n t+n+1 t ergodic. We use d to denote its ergodic distribution. For z = (s , a , . . . , s , a , s ), we define z 1 1 n n n+1 matrix-valued and vector-valued functions n (cid:32)k−1 (cid:33) A(z) =. f (s )φ(s ) (cid:88) (cid:89) γ(s ) π(a i|s i) π(a k|s k) (φ(s ) − γ(s )φ(s ))(cid:62), θ 1 1 i+1 µ(a |s ) µ(a |s ) k k+1 k+1 i i k k k=1 i=1 n (cid:32)k−1 (cid:33) b(z) =. f (s ) (cid:88) (cid:89) γ(s ) π(a i|s i) π(a k|s k) r(s , a )φ(s ), θ 1 i+1 µ(a |s ) µ(a |s ) k k k i i k k k=1 i=1 which allows us to rewrite (7) as w = w + αw(b(Z ) − A(Z )w ). t+1 t t t t t For stochastic approximation algorithms like this, Proposition 4.8 in Bertsekas and Tsitsiklis (1995) asserts that {w } convergs almost surely if the following five conditions are satisfied: t (a) The learning rates {αw} are nonnegative, deterministic, and satisfy (cid:80) αw = ∞, (cid:80) (αw)2 < ∞ t t t t t (b) {Z } is ergodic t (c) E [A(z)] is positive definite dz (d) max (cid:107)A(z)(cid:107) < ∞, max (cid:107)b(z)(cid:107) < ∞ z z (e) There exist scalars c and τ with τ ∈ [0, 1) such that 0 0 (cid:107)E[A(Z )] − E [A(z)](cid:107) ≤ cτ t, (cid:107)E[b(Z )] − E [b(z)](cid:107) ≤ cτ t. t dz 0 t dz 0 In our case, (a) is satisfied by Assumption 3. (b) follows from Assumption 1. Since E [A(z)] = A, E [b(z)] = b, (c) follows from Lemma 2. (d) is obvious since we consider a finte stad tez action MDP. dz To verify (e), consider (cid:88) E[A(Z )] = Pr(Z = z)A(z). t t z So (cid:13) (cid:13) (cid:13)(cid:88) (cid:13) (cid:107)E[A(Z )] − E [A(z)](cid:107) = (cid:13) (Pr(Z = z) − d (z)) A(z)(cid:13) t dz (cid:13) t z (cid:13) (cid:13) (cid:13) z (cid:88) ≤ | Pr(Z = z) − d (z)|(cid:107)A(z)(cid:107) t z z (cid:32) (cid:33) (cid:88) ≤ | Pr(Z = z) − d (z)| max (cid:107)A(z)(cid:107) t z z z According to Theorem 4.9 in Levin and Peres (2017), under Assumption 1, there exists scalar c and 0 τ with τ ∈ [0, 1) such that 0 0 (cid:88) | Pr(Z = z) − d (z)| ≤ c τ t. t z 0 0 z Consequently, the first condition in (e) is verified; the second condition in (e) can be verified in the same way. With all the five conditions satisfied, Proposition 4.8 in Bertsekas and Tsitsiklis (1995) asserts that lim w = A−1b a.s.. t t→∞ The ergodic distribution d and probability of Z = z remain the same in the i.i.d. setting as long as z t S ∼ d . Therefore we reach the same conclusion in the i.i.d. setting. t µ 17
Proof of Proposition 4 Proof. For the sake of readability, in this proof, we define . L = I − (P Γ)n π as shorthand. We first bound the distance between w and the unbiased fixed point ∞ . w = (Φ(cid:62)Df LΦ)−1Φ(cid:62)Df rn. ∗ µ µ π We have (cid:107)w − w (cid:107) (9) ∞ ∗ ≤(cid:13) (cid:13)(cid:0) (Φ(cid:62)Dθ LΦ)−1 − (Φ(cid:62)Df LΦ)−1(cid:1) Φ(cid:62)Dθ rn(cid:13) (cid:13) + (cid:13) (cid:13)(Φ(cid:62)Df LΦ)−1Φ(cid:62)(Dθ − Df )rn(cid:13) (cid:13) µ µ µ π µ µ µ π ≤(cid:13) (cid:13)(Φ(cid:62)Dθ LΦ)−1(cid:13) (cid:13)(cid:13) (cid:13)(Φ(cid:62)Df LΦ)−1(cid:13) (cid:13)(cid:13) (cid:13)Φ(cid:62)D(cid:15) LΦ(cid:13) (cid:13)(cid:13) (cid:13)Φ(cid:62)(Df + D(cid:15) )rn(cid:13) (cid:13) + (cid:13) (cid:13)(Φ(cid:62)Df LΦ)−1Φ(cid:62)D(cid:15) rn(cid:13) (cid:13) µ µ µ µ µ π µ µ π (cid:13) (cid:13) (cid:13) (cid:13)(cid:13) (cid:13) (Using (cid:13)X−1 − Y−1(cid:13) ≤ (cid:13)X−1(cid:13)(cid:13)Y−1(cid:13)(cid:107)X − Y(cid:107)) We now bound (Φ(cid:62)Dθ LΦ)−1, According to Corollary 8.6.2 of Golub and Van Loan (2013), for any µ two matrices X and Y, |σ (X + Y) − σ (X)| ≤ (cid:107)Y(cid:107), min min where σ (·) indicates the smallest singular value. If X is nonsingular and we select some c ∈ min 0 (0, σ (X)), we get min (cid:107)Y(cid:107) ≤ σ (X) − c min 0 =⇒ (cid:13) (cid:13)(X + Y)−1(cid:13) (cid:13) = 1 ≤ 1 ≤ c−1. σ (X + Y) σ (X) − (cid:107)Y(cid:107) 0 min min In our case, we consider Φ(cid:62)Df LΦ as X and Φ(cid:62)D(cid:15) LΦ as Y, we get µ µ (cid:13) (cid:13)Φ(cid:62)D(cid:15) µLΦ(cid:13) (cid:13) ≤ σ min(Φ(cid:62)Df µLΦ) − c 0 (10) =⇒ (cid:13) (cid:13)(Φ(cid:62)Dθ LΦ)−1(cid:13) (cid:13) ≤ c−1. µ 0 Here the nonsingularity of Φ(cid:62)Df LΦ is proved in Jiang et al. (2021). Combining the spectral radius µ bounds of Φ(cid:62)Df LΦ with (9) and (10), it is easy to see that there exists a constant c > 0 such that µ 1 (cid:107)w ∞ − w ∗(cid:107) ≤ c 1(cid:13) (cid:13)D(cid:15) µ(cid:13) (cid:13). Consequently, (cid:107)Φw ∞ − v π(cid:107) ≤ (cid:107)Φw ∞ − Φw ∗(cid:107) + (cid:107)Φw ∗ − v π(cid:107) ≤ (cid:107)Φ(cid:107)c 1(cid:13) (cid:13)D(cid:15) µ(cid:13) (cid:13) + (cid:107)Φw ∗ − v π(cid:107) According to Lemma 1 and Theorem 1 in White (2017), there exists a constant c > 0 such that 2 (cid:13) (cid:13) (cid:107)Φw − v (cid:107) ≤ c (cid:13)Π v − v (cid:13) . ∗ π Df µ 2(cid:13) Df µ π π(cid:13) Df µ Using the equivalence between norms, we have for some constant c > 0, 3 (cid:107)Φw − v (cid:107) ≤ c (cid:107)Φw − v (cid:107) , ∗ π 3 ∗ π Df µ which completes the proof. In particular, one possible η is 1 σ (Φ(cid:62)Df LΦ) − c min µ 0 . (cid:107)Φ(cid:62)(cid:107)(cid:107)LΦ(cid:107) 18
C IMPALA V-trace Since the product of IS ratios in off-policy TD(n) can lead to high variances, V-trace (Espeholt et al., 2018) clips the IS ratios to reduce variance. V-trace updates w iteratively as t+n−1 (cid:32)k−1 (cid:33) . (cid:88) (cid:89) w = w + α c¯ γ ρ¯ δ (w )φ , t+1 t i i+1 k k t t k=t i=t . . where ρ¯ = min(ρ¯, ρ ), c¯ = min(c¯, ρ ). In practice, the clipping thresholds c¯ and ρ¯ are often equal, t t t t so that c¯ ≡ ρ¯ . Clipping due to c¯ is equivalent to bootstrapping more, as discussed by Mahmood t t et al. (2017). It is straightforward to apply both off-policy n-step TD and V-trace in the i.i.d. setting. Control The V-trace update is most often used in actor-critic systems, such as Impala (Espeholt et al., 2018). Consider the current policy π parametrized by an additional set of policy parameters ϑ. ϑ Following the derivation of policy gradient in Espeholt et al. (2018), we update the actor, parameters ϑ in the following direction: ρ¯ (R + γ v − v (S ))∇ log π (A |S ) , t t+1 t+1 t+1 w t ϑ ϑ t t where v is the V-trace target, t+1 t+n (cid:32) k−1 (cid:33) . (cid:88) (cid:89) v = v (S ) + c¯ γ ρ¯ δ (w), t+1 w t+1 i i+1 k k k=t+1 i=t+1 and the critic v is learned using the aforementioned policy evaluation V-trace update. This learning w algorithm has been very successful (e.g., Espeholt et al., 2018; Hessel et al., 2019) in mild off-policy learning settings. Jiang et al. (2021) extends ETD(n) trace to the control setting through re-weighting the policy gradient in the learning updates as well as the value gradient by the ETD(n) trace. D Hyperparameters Architectures. Table 2: Network architecture Parameter convolutions in block (2, 2, 2, 2) channels (64, 128, 128, 64) kernel sizes (3, 3, 3, 3) kernel strides (1, 1, 1, 1) pool sizes (3, 3, 3, 3) pool strides (2, 2, 2, 2) frame stacking 4 head hiddens 512 activation Relu trace hiddens 256 Our DNN architecture is composed of a shared torso, which then splits to different heads. We have a head for the policy, a head for the value function and a head for the emphatic trace (multiplied by the number of auxiliary tasks). The value and policy heads are two-layered MLPs with 512 hidden units, where the output dimension corresponds to 1 for the value function head. For the policy head, we have |A| outputs that correspond to softmax logits. The trace head is a two-layered MLP with 256 hidden units, and the output dimension 1. We use ReLU activations on the outputs of all the layers besides the last layer. For the policy head, we apply a softmax layer and use the entropy of this softmax distribution as a regularizer. The torso of the network is composed from residual blocks. In each block there is a convolution layer, with stride, kernel size, channels specified in Table 2, with an optional pooling layer following 19
it. The convolution layer is followed by n - layers of convolutions (specified by blocks), with a skip contention. The output of these layers is of the same size of the input so they can be summed. The block convolutions have kernel size 3, stride 1. Hyperparameters. Table 3 lists all the hyperparameters used by our agent. Most of the hyperparam- eters follow the reported parameters from the IMPALA paper. For completeness, we list all of the exact values that we used below. Table 3: Hyperparameters table Parameter Value total environment steps 200e6 optimizer RMSPROP start learning rate 2 · 10−4 end learning rate 0 trace weight w 1 decay 0.99 eps 0.1 importance sampling clip 1 gradient norm clip 1 trajectory n 10 online batch size 6 replay batch size 6 replay buffer size 104 sampling priority uniform discount γ (main) σ(4.6) ≈ .99 discount γ1 (1st auxiliary) σ(4.4) ≈ .988 discount γ2 (2nd auxiliary) σ(4.2) ≈ .985 20
E Additional Results top level 2θ 2θ 2θ 2θ 2θ 2θ 1 2 3 4 5 6 + + + + + + θ θ θ θ θ θ 8 8 8 8 8 8 θ + 2θ 7 8 (a) MDP (b) Comparison Figure 5: MDP illustration. (a) Modified Baird’s counterexample. Red lines indicate the target policy π(solid|·) = 0.3, π(dashed|·) = 0.7. Blue lines indicate the behavior policy µ(solid|·) = 6/7, µ(dashed|·) = 1/7. When action is “dashed”, the agent goes to a random state on the top level. When action is “solid”, the agent goes to the bottom state. (b) RMSE in the value estimates and RMSE in expected trace approximation over time in a modified version of Baird’s counterexample. We report the performance of each algorithm using the best performing hyperparameters (according to RMSE of the value function) from an extensive sweep (described in text). Shaded regions indicate two standard deviations of the mean performance computed from 100 independent runs. Figure 6: RMSE in the value estimates over time in a modified version of Baird’s counterexample. We plot each run individually to better characterize the performance of each algorithm. 21
