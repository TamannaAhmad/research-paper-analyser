Journal of Machine Learning Research 24 (2023) 1-69 Submitted 10/21; Revised 8/22; Published 2/23 Temporal Abstraction in supervised learning with the Successor Representation Marlos C. Machado machado@ualberta.com DeepMind Alberta Machine Intelligence Institute (Amii) Department of Computing Science, University of Alberta Edmonton, AB, Canada Andr´e Barreto andrebarreto@deepmind.com DeepMind London, United Kingdom Doina Precup doinap@deepmind.com DeepMind Quebec AI Institute (Mila) School of Computer Science, McGill University Montreal, QC, Canada Michael Bowling mbowling@ualberta.ca DeepMind Alberta Machine Intelligence Institute (Amii) Department of Computing Science, University of Alberta Edmonton, AB, Canada Editor: Jan Peters Abstract Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In supervised learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which op- tions one should consider. In this paper, we argue that the successor representation, which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the successor representation can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these results as instantiations of a general framework for option discovery in which the agent’s representation is used to identify useful options, which are then used to further improve its representation. This results in a virtuous, never-ending, cycle in which both the representation and the options are constantly refined based on each other. Beyond option discovery itself, we also discuss how the successor representation allows us to augment a set of options into a combinatorially large counterpart without additional learning. This is achieved through the combination of previously learned options. Our empirical evaluation focuses on options discovered for temporally-extended exploration and on the use of the successor representation to combine them. Our results shed light on important design deci- sions involved in the definition of options and demonstrate the synergy of different methods based on the successor representation, such as eigenoptions and the option keyboard. ©2023 Marlos C. Machado, Andr´e Barreto, Doina Precup, and Michael Bowling. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v24/21-1213.html. 3202 rpA 11 ]GL.sc[ 3v04750.0112:viXra
Machado, Barreto, Precup, and Bowling Keywords: supervised learning, Options, Successor representation, Eigenoptions, Covering options, Option keyboard, Temporally-extended exploration 1 Introduction In the supervised learning problem, an agent interacts with its environment such that the agent receives an observation from the environment and takes an action based on the received observations. This interaction takes place at every time step, which is often the fundamental unit of time in this problem formulation. Nevertheless, several decision mak- ing problems, such as robot locomotion (Stone et al., 2005), strategy games like Star- Craft (Vinyals et al., 2019), and balloon navigation (Bellemare et al., 2020), involve oper- ating over different time scales. The options framework (Precup, 2000; Sutton et al., 1999) is maybe the most common formalism that allows us to do so, giving agents the ability to reason in terms of actions extended in time. This framework models courses of actions as options, which have the ability to accelerate learning in different ways, allowing, for exam- ple, faster credit assignment (e.g., Mann and Mannor, 2014; Solway et al., 2014), better exploration (e.g., Baranes and Oudeyer, 2013; Fruit and Lazaric, 2017), and transfer (e.g., Konidaris and Barto, 2007; Topin et al., 2015). Despite the attention received by the options framework, it is still not clear where options should come from—a problem referred to as option discovery. In this paper we argue that the successor representation (SR) is a natural substrate for temporal abstractions in supervised learning. The SR (Dayan, 1993) is a representation that generalizes between states using the similarity between their successors, that is, the similarity between the states that follow the current state given the environment’s dynamics and the agent’s policy. It allows us to discover options that are effective not only for planning (e.g., Hoang et al., 2021; Ramesh et al., 2019; Stachenfeld et al., 2017), but also for temporally-extended exploration (e.g., Jinnai et al., 2019b; Machado et al., 2017, 2018). The SR also allows us to combine existing options without additional learning (Barreto et al., 2019). Furthermore, recent studies suggest that the SR models remarkably well behaviors observed in the brain (e.g., Momennejad et al., 2017; Stachenfeld et al., 2014, 2017). In this paper, we present a general framework for option discovery in which the agent learns a representation that is used to identify meaningful options, which are then used to improve the agent’s representation in a virtuous, never-ending, cycle (Sections 3 and 7). To support our claim about the role of the SR for temporal abstraction, we show how we can instantiate this cycle with the SR, and how the SR is conducive to option discovery. We summarize existing methods that use the SR for option discovery, providing intuitions about the motivation behind them, and connecting papers from different contexts (Sections 5 and 10). Moreover, regardless of how effective a discovery method is, while more options means a more expressive set of behaviors, more options often makes learning and using these options more difficult. Thus, we also discuss an approach based on the SR for combining options, the option keyboard (Barreto et al., 2019), which addresses this issue by allowing the agent to extend, without extra learning, a finite set of options to a combinatorially large counterpart (Section 8). We perform numerical simulations to assess how effective options discovered by dif- ferent methods are in capturing environment properties. Such an ability is particularly 2
Temporal Abstraction in RL with the Successor Representation important for problems in which a fixed reward function is not easily defined, such as con- tinual (Brunskill and Li, 2014; Mankowitz et al., 2018), multitask (Teh et al., 2017), and transfer learning (Taylor and Stone, 2009). We evaluate the impact of different design decisions every option discovery method needs to make (Section 6). We present evidence on the potential of instantiating a cycle in which both the representation and the options are constantly refined based on each other (Section 7), and on the synergy of different approaches based on the SR (Section 9). We focus our discussion mostly on toy domains to provide intuition without confounding factors. We use navigation tasks throughout the paper because they are intuitive and it is easier to generate visualizations with them. From an agent’s perspective, these tasks are not different from other tasks, the agent is always traversing an unknown state space. We review the extensions to more complex solutions when discussing relevant related work. Besides the sections already discussed, we present the required background in Sections 2 and 4, the related work in Section 10, and the conclusion in Section 11. While the main con- tributions in Sections 5 and 8 are on presenting existing results under a single formulation, the results in Sections 3, 6, 7, and 9 are novel and have not been presented anywhere else. 2 Background In this section we introduce the formalism behind supervised learning and the options framework (Precup, 2000; Sutton et al., 1999). Throughout this paper, as a convention, we indicate random variables by capital letters (e.g., S , R ), vectors by bold lowercase letters t t (e.g., θ, φ), matrices by bold capital letters (e.g., P , Ψ ), functions by non-bold lowercase π π letters (e.g., v, q), and sets with a calligraphic font (e.g., S, A). 2.1 supervised learning supervised learning (RL) is a problem formulation that allows us to tackle sequential decision making problems. In RL we consider an agent interacting with an unknown envi- ronment in a sequential manner, aiming to maximize cumulative reward. We often assume that the environment can be modeled as a finite Markov decision process (MDP). An MDP is formally defined as a 4-tuple (cid:104)S, A, p, r(cid:105). Starting from state S ∈ S, at each time step t 0 the agent takes an action A ∈ A, to which the environment responds with a state S ∈ S, t t+1 . according to a transition probability kernel p(s(cid:48)|s, a) = Pr(S = s(cid:48)|S = s, A = a), and t+1 t t with a bounded reward signal R ∈ R, with r(s, a) indicating the expected reward for a t+1 . transition from state s under action a, that is, r(s, a) = E[R | S = s, A = a]. t+1 t t The agent’s goal is to learn a policy π : S × A → [0, 1] that maps each state to a probability distribution over actions. Specifically, the agent seeks a policy that maximizes, in expectation, the (discounted) cumulative sum of rewards, also known as return, defined as ∞ (cid:88) G = γkR , (1) t t+k+1 k=0 with γ ∈ [0, 1), the discount factor, defining the relative value of future rewards. 3
Machado, Barreto, Precup, and Bowling In this paper we focus on value-based methods. To obtain the policy π, we estimate the state-value function, v : S → R, or the state-action value function, q : S × A → R. The π π value of a state s when following a policy π, v (s), is defined to be the return from that state: π v (s) =. E (cid:2) G |S = s(cid:3) . The state-action value function is defined similarly, but it takes π π t t into consideration the action taken, that is, q (s, a) =. E (cid:2) G |S = s, A = a(cid:3) , where the π π t t t expectation in both definitions is with respect to the policy π and the probability kernel p. Importantly, these functions can be defined recursively (Bellman, 1957), for example: (cid:88) (cid:88) (cid:88) (cid:104) (cid:105) v (s) = π(a|s) p(s(cid:48), r|s, a) r + γv (s(cid:48)) . (2) π π a s(cid:48) r These equations can also be written in matrix form. The state-value function, for example, can be defined with v , r ∈ R|S|, and P ∈ R|S|×|S|: π π v = r + γP v = (I − γP )−1r, (3) π π π π where P is the transition probability induced by π, i.e., P (s, s(cid:48)) = (cid:80) π(a|s)p(s(cid:48)|s, a). π π a In the RL problem we assume the agent does not know P nor r beforehand. Instead, π RL methods directly estimate v or q from samples (s, a, r, s(cid:48)). Most approaches alternate π π between a policy evaluation step, that is, estimating the value of the agent’s current policy, and a policy improvement step, which defines a new policy from these estimates: . π(s) = arg max Q(s, a). (4) a∈A Q-Learning (Watkins and Dayan, 1992) is the most well-known algorithm for estimating the value of the optimal policy, π . It has the following update rule for the q estimate, Q: ∗ π∗ (cid:16) (cid:17) Q(S , A ) ← Q(S , A ) + α R + γ max Q(S , a) − Q(S , A ) , (5) t t t t t+1 t+1 t t a∈A where α is the algorithm’s step-size parameter. When the value of each state (or state-action pair) is individually stored, this is a tabular method. Nevertheless, generalization is required, and desirable, in problems with large state spaces, where it is infeasible to learn an individual value for each state. This is done by parameterizing the function V or Q with a set of parameters θ. We write, given the parameters θ, V (s; θ) ≈ v (s) and Q(s, a; θ) ≈ q (s, a). In the past, a common π π approach was to use linear function approximation where Q(s, a; θ) = θ(cid:62)φ(s, a), in which θ is a vector of weights and φ(s, a) denotes a static feature representation of the state s when taking action a. It is now common to use a neural network to compute a non-linear function approximation of the value function, an approach popularized by Mnih et al. (2013, 2015) with Deep Q-Network (DQN). The study of algorithms that use neural networks as function approximators has since been dubbed deep supervised learning. 2.2 Temporal Abstraction in RL: The Options Framework Sequential decision making usually involves planning, acting, and learning about temporally extended courses of actions over different time scales. In supervised learning, options 4
Temporal Abstraction in RL with the Successor Representation are a well-known formalization of the notion of actions extended in time that allow us to represent courses of actions (Precup, 2000; Sutton et al., 1999). An option ω ∈ Ω is a 3-tuple ω = (cid:104)I , π , β (cid:105), (6) ω ω ω where I ⊆ S denotes the option’s initiation set, π : S × A → [0, 1] denotes the option’s ω ω policy, such that (cid:80) π (·, a) = 1, and β : S → [0, 1] denotes the option’s termination a ω ω condition, that is, the probability that option ω will terminate at a given state. In this paper, we consider the call-and-return option execution model in which a high-level policy, µ : S × Ω → [0, 1], dictates the agent’s behavior. Notice that the actions originally defined in the MDP are a special case of options, that is, A ⊆ Ω. Finally, we often write that an agent follows or takes an option ω, meaning that the agent, in a state in I , commits ω to act according to the option’s policy, π , until its termination condition is satisfied. To ω distinguish between options and actions, we often refer to the actions originally defined in the problem formulation as primitive actions. Options have different use cases, including planning, exploration, and credit assign- ment. In this paper we mostly focus on exploration. Specifically, we discuss the option discovery problem, which consists in discovering useful options from the agent’s stream of experience. In other words, we discuss different algorithms that, given a set of samples (s, a, s(cid:48), r), autonomously define extended courses of actions, represented by an initiation set, a policy, and a termination condition, such that they allow for temporally-extended exploration. The algorithms we discuss can all be cast as part of a general framework for option discovery, which we discuss in the next section. In subsequent sections we show how different algorithms instantiate this framework. 3 A Framework for Option Discovery from Representation Learning We first introduce a general approach for option discovery that is driven by the represen- tation learning process. It follows a constructivist approach (Piaget, 1963) depicted as a cycle in which options discovered from previous iterations act as a scaffold for more complex behaviors discovered in subsequent iterations. The framework depicted in Figure 1 distills the main steps of this cycle. Note that, while we present these steps sequentially, they can be executed concurrently at different time scales. Below we further discuss each step. Collect samples: The first step in each iteration of the representation-driven option dis- covery (ROD) cycle is to have the agent collect data in the form of trajectories. Selecting actions uniformly at random is an obvious first choice for the agent’s policy. Once options have been identified, more possibilities for this step become available. Learn representation: In most problems of interest, the agent should learn a representa- tion of its environment while acting in the world. Methods that are reward agnostic can be easier to implement, especially in early learning. In this paper, we focus on the successor representation as the output of this step, because, as we discuss in the next section, it naturally captures the dynamics of the environment. 5
Machado, Barreto, Precup, and Bowling Derive intrinsic Collect Learn reward function samples representation from learned repr. Option Define Learn to maximize set option intrinsic reward Figure 1: Representation-driven Option Discovery (ROD) cycle (Machado, 2019). The op- tion discovery algorithms discussed in this paper can be seen as instantiating this cycle. The incoming arrow to Collect samples depicts the start of the process. The arrow from Define option to Option set highlights the output generated by the ROD cycle. Note that other generated artifacts can also be used by the agent outside the ROD cycle, such as the learned representation. Derive an intrinsic reward function from the learned representation: After a represen- tation is learned, the agent can use it to define an intrinsic reward function which an option could maximize. The algorithms we discuss here either use spectral analysis or some clus- tering of the successor representation to define this intrinsic reward function. The first is often associated with more efficient exploration while the latter usually leads to more efficient credit assignment. Learn to maximize intrinsic reward: Once a representation has been learned and the intrinsic reward function has been defined, the agent needs to learn to maximize the (dis- counted) sum of these rewards, which is a standard supervised learning problem. The learned policy is the policy of this new option being discovered. This can be done in parallel for multiple options, with off-policy learning, which allows one to learn about policies that are different from the policy that generated the observations. Define option: Finally, there are different ways to define the option’s initiation set and termination condition, which give rise to different algorithms (e.g., Jinnai et al., 2019b; Machado et al., 2017). We discuss several possibilities in Sections 5 and 6 when introducing and evaluating instantiations of this framework. The output of this step can be immediately incorporated into the agent’s option set, but it can also be used in the next iteration of the ROD cycle, ideally improving the data collection step, which then allows the discovery of more complex options. As previously mentioned, the ROD cycle can be agnostic to the ultimate reward function that the agent may want to optimize. This is particularly important when aiming at discov- ering options for temporally-extended exploration. If the option discovery process consists in learning options that, for example, replicate observations (e.g., feature activation, state visitation), new options might allow the agent to better navigate in the environment by 6
Temporal Abstraction in RL with the Successor Representation (a) collected samples (c) collected samples … (b) discovered option (d) discovered option Figure 2: Two ROD cycles in the Pinball domain (Konidaris and Barto, 2009), originally presented by Jinnai et al. (2020). Blue dots denote state visitation, green lines the options’ trajectories, and shaded regions the states in which the options terminate. The agent follows a random walk in every iteration. The agent first struggles to go through some narrow passages, as we can see in the samples collected at the first iteration of the ROD cycle (Fig. 2a). At the end of the cycle, the discovered option takes the agent to the boundary of the region it visited (Fig. 2b). This new option enables the agent to visit regions that were originally hard to reach with a random walk, as we can see when looking at the samples collected in the second iteration (Fig. 2c). These samples were generated by the agent following a policy that uniformly chooses between the primitive actions and the option discovered in the previous iteration. This process can, of course, be further refined. Fig. 2d depicts the option discovered at the end of the second iteration. making events that were rare, or virtually impossible, more likely. Figure 2, adapted from Jinnai et al.’s 2020 work, presents a concrete example of this behavior in a task with a continuous state space. In Section 7 we present another illustration of multiple iterations of the ROD cycle that was generated by an algorithm we introduce in this paper. 4 The Successor Representation The successor representation (SR; Dayan, 1993) is a classic method for automatically extracting a representation from the agent’s observation, giving an answer to what rep- resentations one should use when performing function approximation. In this paper, we claim that the SR could be the natural substrate for temporal abstraction in supervised learning, as it is a representation learning method conducive to the discovery and use of options. The algorithms we present, one way or another, use the SR as their representation when instantiating the ROD cycle. This is due to the fact that it has a particular structure that captures the dynamics of the environment, as we discuss below. 7
Machado, Barreto, Precup, and Bowling C A B Figure 3: Example similar to Dayan’s (1993) of the SR, w.r.t. the uniform random policy, of state A (left). Consider a navigation task where the agent has access to its (x, y) coordinates. It is tempting to use some distance metric such as the Euclidean distance to define distance between states. However, if one considers the gray tiles to be walls, an agent in point A can reach point B much quicker than point C. The SR captures this distinction, ensuring that, in this representation, point A is closer to point B than it is to point C. The plots of the SR were generated using a discretization of the grid, where each tile is a state. Red represents larger values while blue represents smaller values (states that are temporally further away). Recall the SR of a state, in the tabular case, is an |S|-dimensional representation, thus allowing us to depict it as a heatmap over the state space. 4.1 Tabular Setting The SR is a representation that captures the underlying environment dynamics. It does so by assigning similar values to states that are close in time; in other words, the notion of similarity between states is based on how similar their successor states are under a policy π. Formally, the SR is defined to be the current and expected future occupancy of state s(cid:48) given the agent’s policy π and its starting state s. The SR, with respect to a policy π, Ψ , is defined as π (cid:34) ∞ (cid:35) (cid:88) (cid:12) Ψ (s, s(cid:48)) = E γt1 (cid:12)S = s , (7) π π,p {St=s(cid:48)}(cid:12) 0 t=0 where 1 denotes the indicator function and γ ∈ [0, 1). Thus, each state s is represented as an |S|-dimensional vector whose i-th component is the expected discounted visitation to each state in the environment. Figure 3 illustrates this concept. Importantly, the SR can be estimated from samples with temporal-difference learning methods (Sutton, 1988), where the reward function is replaced by the state occupancy: (cid:34) (cid:35) Ψˆ (S , j) ← Ψˆ (S , j) + η 1 + γΨˆ (S , j) − Ψˆ (S , j) , (8) t t {St=j} t+1 t for all j ∈ S, where η is the step-size. Algorithm 1 depicts an implementation of the SR. For clarity, because we refer back to this pseudo-code in later sections, we wrote it in such a way 8
Temporal Abstraction in RL with the Successor Representation Algorithm 1: Successor Representation Input: η ; (cid:46) Step-size γ ∈ [0, 1) ; (cid:46) SR’s discount factor D ; (cid:46) Data set with (s, a, r, s(cid:48)) transitions Output: Ψ ∈ R|S|×|S| Ψ(i, j) ← 0 ∀ i, j < |S| ; (cid:46) Initialize the SR with zeros for (s, a, s(cid:48)) in D do for i ← 0 to |S| do δ ← 1 + γΨ(s(cid:48), i) − Ψ(s, i) {s=i} Ψ(s, i) ← Ψ(s, i) + ηδ that transitions are stored in a data set D. We do so to be able to write other components of the ROD cycle more compactly; but it is not necessary for any individual component. The SR can also be seen as a collection of general value functions (Sutton et al., 2011) with a fixed discount factor and individual state visitation as cumulants. In this case, instead of seeing the SR as a representation, one would see it as a collection of predictions. The SR also corresponds to the Neumann series of γP : π ∞ (cid:88) Ψ = (γP )t = (I − γP )−1. (9) π π π t=0 Thus, one can see the SR as an estimate of how often the agent expects to visit each state in the future, weighted by the discount factor. In fact, the SR is part of the solution when computing a value function (see Eq. 3): v = (I − γP )−1r = Ψ r. (10) π π π In words, one can compute the return by multiplying the SR and the estimates of the expected immediate rewards in each state. This sum of weighted rewards relies on the SR to provide the weights, which encodes expected future state visitation. The SR can be presented in multiple ways based on the several connections it has to other results in the field. The matrix in Eq. 9 is also known, for example, as the LSTD matrix (Lagoudakis and Parr, 2003). We further discuss some of these connections after presenting the generalization of the SR to the function approximation case. 4.2 Successor Features: From States to Features The definitions given so far for the SR are limited to the tabular case. Successor fea- tures (SFs; Barreto et al., 2017) are a generalization of the SR that can be extended to the function approximation setting. Let φ : S × A (cid:55)→ Rd be a function that computes features. The SFs of policy π are (cid:34) ∞ (cid:35) . (cid:88) ψ (s, a) = E γiφ(S , A ) | S = s, A = a . (11) π π,p t+i t+i t t i=0 9
Machado, Barreto, Precup, and Bowling In words, ψ (s, a) encodes the discounted expected value of the i-th feature in the vector π,i φ(·, ·) when the agent starts in state s, executes action a, and follows policy π thereafter. The features φ(·, ·) can be either given to the agent or learned, as we discuss in Section 10. The update rule presented in Eq. 8 can be naturally extended to this definition. SFs are a strict generalization of the SR. To see why this is so, suppose that S is finite and let φ(s, a) = φ(s) for all (s, a) ∈ S × A (that is, φ is a function of states only). Then, we can rewrite the definition of SFs in matrix form as ∞ (cid:88) Ψ = (γP )tΦ = (I − γP )−1Φ, (12) π π π t=0 where Φ ∈ R|S|×d is a matrix encoding the feature representation of each state. When d = |S|, if we define φ (s ) = 1 , Eq. 12 reduces to Eq. 9. Again, this highlights the fact i j {i=j} that the SR can be seen as the discounted state visitation distribution induced by policy π. The connection between the SR and SFs also allows us to generalize Eq. 10. Assume there exists a w ∈ Rd such that r(s, a) = φ(s, a)(cid:62)w for all (s, a) ∈ S × A. (13) Based on the definition of q one can to show that (Barreto et al., 2017) π q (s, a) = ψ (s, a)(cid:62)w for all (s, a) ∈ S × A. (14) π π Again, when d = |S| and φ (s , a) = 1 for all a ∈ A, Eq. 14 reduces to Eq. 10. Note i j {i=j} that, once we have the SFs ψ of a policy π, Eq. 14 allows us to instantaneously evaluate π π under any reward that can be represented as a linear combination of the features φ (Eq. 13). This has been exploited in the past for transfer between tasks (Barreto et al., 2017, 2018). 4.3 Properties of the Eigenvectors of the Successor Representation As aforementioned, the SR is present in several RL algorithms, either explicitly or implicitly. An important result for this paper is that the eigenvectors of the SR are equivalent to proto- value functions (PVFs; Mahadevan, 2005; Machado et al., 2018). We rely on this result to be able to also discuss algorithms originally presented under the PVFs formalism. The properties of the eigenvectors of the SR (i.e., PVFs) are particularly relevant to option discovery methods. A discussion of PVFs and the formal equivalence result between them and the eigenvectors of the SR is available in Appendix A. PVFs, and consequently the eigenvectors of the SR, capture temporal properties of an environment, with different eigenvectors capturing different time-scales of diffusion, a hallmark of Fourier analysis. This can be seen in Figure 4, which depicts the first three PVFs in the four-room domain. The first eigenvectors capture longer time-scales, such as in Figures 4b and 4c, in which the biggest difference is seen between the two states that are furthest apart: the different diagonals in the environment. On the other hand, eigenvectors with corresponding larger eigenvalues1 exhibit shorter time-scales, such as in Figure 4d, in which the period of the curves depicted is already shorter—the distance between the states 1. When using proto-value functions, the order of the eigenvectors is flipped, explaining why eigenoptions use the eigenvectors with corresponding lowest eigenvalues (see Theorem 2 in Appendix B). 10
Temporal Abstraction in RL with the Successor Representation (a) Four-room (b) First PVF (c) Second PVF (d) Third PVF Figure 4: First three PVFs in the (a) four-room domain. Gray squares represent walls and white squares represent accessible states. Four actions are available: up, down, right, and left. The transitions are deterministic and the agent is not allowed to move into a wall. (b-d) These plots depict the first, second, and third eigenvectors associated with each state. The axes are rotated for clarity. The bottom left corner of the four-room domain is the state closer to the reader. with largest and smallest values is smaller. Similar to value functions, PVFs are smooth, with the value of each state being a function of its neighbors. The methods we discuss below heavily benefit from such properties. 5 Temporally-Extended Exploration To exemplify how the SR can be used for option discovery, we now discuss different methods that instantiate the ROD cycle using the SR as representation. In this section, we focus on discovering options useful for temporally-extended exploration. Specifically, we consider options that can be used by the agent, alongside primitive actions, when following a random walk. When an option is selected, instead of a primitive action, the agent acts according to the option’s policy until its termination condition is satisfied. Temporally-extended exploration with options is based on the intuition that agents explore the environment more effectively if they operate at a higher-level of abstraction. When acting according to options’ policies, agents exhibit more directed behavior in contrast to the aimless dithering commonly observed when selecting primitive actions uniformly at random (Dabney et al., 2021; Jinnai et al., 2019b; Machado and Bowling, 2016; Machado et al., 2017). Intuitively, if one needs to explore, say, a building, it makes more sense to do so in terms of rooms than in terms of motor twitches. In fact, such an approach was used as a solution for the exploration problem in one of the recent high-profile success stories in artificial intelligence: the deployment of a supervised learning algorithm to navigate superpressure balloons in the stratosphere (Bellemare et al., 2020). In this section, we discuss eigenoptions (Machado et al., 2017, 2018) and covering op- tions (Jinnai et al., 2019b, 2020). They instantiate the ROD cycle in different ways while using the SR as representation. These methods, and others (e.g., Bar et al., 2020), use the eigenspectrum of the learned representation to guide the option discovery process. They are representative of a class of methods that is motivated by the fact that the eigenvectors 11
Machado, Barreto, Precup, and Bowling Figure 5: Second eigenvector of the SR and the two corresponding eigenoptions in the four- room domain. The actions of the deterministic policy are depicted as arrows and the state in which the option terminates is depicted in red (in the corners). of the SR naturally encode the diffusion properties of the environment due to their close relationship to proto-vaue functions, as discussed in the previous section. In this and in the next section, we use the differences between eigenoptions and covering options to highlight (and evaluate) some of the overall choices one can make when designing option discovery methods. Specifically, we discuss different ways of designing the intrinsic reward function that guides learning of the options’ policy, the definitions of the options’ initiation set and termination condition, and how these choices impact the design of online versions of these algorithms. 5.1 Eigenoptions Eigenoptions are options defined by the eigenvectors of the SR.2 Each eigenvector assigns an intrinsic reward to every state in the environment. An eigenoption is an option, defined with respect to a specific eigenvector, that takes the agent to the state with largest (or smallest) value. Intuitively, what an eigenoption does is to ensure that there is an option that directly takes the agent to the state that was originally difficult to get to. This description becomes clearer with an example. In the four-room domain, the second largest eigenvector of the SR, defined w.r.t. a uniform random policy, is depicted in Figure 5 (the top eigenvector of the SR is constant). In this environment, the two states that are furthest apart are the states diagonally opposed in the corners, which is what the depicted eigenvector captures. The corresponding eigenoptions take the agent to one of those states. Learning the eigenoptions’ policies. To specify an option we need to define its policy, initiation set, and termination condition. An eigenoption’s policy is defined by the intrinsic reward function re(s, s(cid:48)), obtained from the eigenvector e ∈ Rd of the SR. It is defined as re(s, s(cid:48)) = e(cid:62)(cid:0) φ(s(cid:48)) − φ(s)(cid:1) , (15) 2. Machado et al. (2017) originally defined eigenoptions in terms of PVFs. Later, Machado et al. (2018) used the equivalence between PVFs and the eigenvectors of the SR to generalize eigenoptions to the setting in which the representation, that is, the SR, is learned online. 12
Temporal Abstraction in RL with the Successor Representation where φ(s) denotes the feature representation of state s. Notice that, in the tabular case, if we define φ(s) to be an |S|-dimensional one-hot encoding of state s, re(s, s(cid:48)) becomes re(s, s(cid:48)) = e(s(cid:48)) − e(s). Importantly, the sign of an eigenvector is arbitrary. Thus, as aforementioned, this reward function can be interpreted as incentivizing the agent to either go to the highest or to the lowest point of the graph shown in Figure 5. In this example, these correspond to the top right and bottom left states in the environment. We learn the option’s policy in a newly defined MDP Me = (cid:104)S, A ∪ {⊥}, re, p, γ(cid:105), where the state space and the transition probability kernel remain unchanged. The reward function is defined as in Eq. 15 and the action set is augmented by the action terminate (⊥), which allows the agent to leave Me at no cost, returning to the original MDP in the same state it was when it left Me. The discount factor can be chosen arbitrarily, although it impacts the timescale the option encodes by defining how myopic the agent will be w.r.t. re. In this formulation, eigenoptions ignore the reward signal provided by the original MDP, but it is not difficult to imagine extensions in which this is not the case (c.f., Liu et al., 2017; Sutton et al., 2022). With Me, we define the state-value function ve(s), for policy π, as the expected value of π the cumulative discounted intrinsic reward if the agent starts in state s and follows policy π until termination. We define the action-value function qe(s, a) similarly. The optimal value π function for any intrinsic reward function obtained through e is then described as ve(s) = max ve (s) and qe(s, a) = max qe (s, a). ∗ π ∗ π π π The option’s policy, πe, is the optimal policy w.r.t. the intrinsic reward function re, i.e., ∗ i πe(s) = arg max qe(s, a). ∗ ∗ a∈A Thus, finding the option’s policy πe becomes a traditional RL problem, with a different re- ∗ ward function. Importantly, the reward received for transitioning from one state to another is rarely zero, avoiding challenging exploration issues caused by sparse non-zero rewards. Defining the eigenoptions’ initiation sets and termination conditions. When defining the MDP to learn the option’s policy, we augment the agent’s action set with the terminate action so the agent can terminate the option. The termination condition is deterministic, with eigenoptions terminating when the agent is unable to accumulate further positive intrinsic rewards. This happens when the agent reaches the state with largest value assigned by the corresponding eigenvector (or a local maximum when γ < 1). Any subsequent sum of rewards will be at most zero. We formalize this condition by defining (cid:26) qe(s, a), if a ∈ A, qe+(s, a) = π π 0, if a = ⊥, where qe+ denotes the state-action value function, defined by π and re, augmented by the π terminate action, which has value zero. When the terminate action is selected, control is returned to the higher level policy. In summary, because we break ties in favour of the 13
Machado, Barreto, Precup, and Bowling terminate action, an option following a policy πe terminates in state s when qe(s, a) ≤ 0 π for all a ∈ A. The initiation set is defined to be the complement of the set of states in which an option terminates, i.e., all states in which there exists an action a ∈ A s.t. qe(s, a) > 0. In summary, π an eigenoption consists of a policy πe+, which is augmented by the terminate action, πe+(s) = arg max qe+(s, a). (16) π a∈A∪{⊥} The termination and initiation sets are implicitly defined. That is, for an eigenoption ωe, β (s) = 1 if qe(s, ·) ≤ 0, and β (s) = 0 if there is an action a ∈ A such that qe(s, a) > 0. ωe π ωe π In this second case, s ∈ I . In practice, this means the option terminates in states that are ωe assigned the (locally) largest values in the eigenvector; the option can be initialized in all other states. Algorithm 5 and 6, in Appendix C, summarize the presentation of eigenoptions when computed in both closed-form and online. Importantly, for any eigenoption, there is always at least one state in which it terminates. The theorem below formalizes this result. Theorem 1 (Machado et al. 2017) Let ω = (cid:104)I , π , β (cid:105) denote an eigenoption. In a ω ω ω finite and ergodic MDP with γ ∈ [0, 1), there is at least one state s ∈ S such that β (s) = 1. ω Proof See Appendix B. This result is due to the fact that the reward function resembles a potential function (Ng et al., 1999). It is not clear if it would be possible to obtain such a natural termination criteria if the agent maximized, for example, the feature itself, as in re(s) = e(cid:62)φ(s). Eigenoptions have several interesting prop- erties that allow them to improve exploration. One of them is that different eigenoptions have different durations, effectively letting the agent operate at different time scales. We exem- plify this in Figure 6, where we show how the Linear regression eigenoptions discovered first tend to be longer (i.e., those discovered from eigenvectors with corresponding larger eigenvalues). We overlay a linear regression of the data to emphasize this trend. Besides their duration, eigenoptions can be easily sequenced, and they are task- independent because, as the SR, they do not de- Figure 6: Avg. length of different eigen- pend on information related to the reward func- options by the order they are tion. Nevertheless, there could be twice as many discovered. The linear regres- eigenoptions as states in the environment and it sion emphasizes the downward is not clear how to choose the number of desired trend on the option length. options. Covering options (Jinnai et al., 2019b), discussed next, are an attempt to address this issue. 14
Temporal Abstraction in RL with the Successor Representation Figure 7: Second eigenvector of the graph Laplacian and corresponding covering options in the four-room domain. The actions of the deterministic policy are depicted as arrows and the state in which the option terminates is depicted in red. 5.2 Covering Options Covering options are options defined by the bottom eigenvector of the graph Laplacian (i.e., the first PVF), that is, the eigenvector with the smallest corresponding eigenvalue.3 They have the explicit goal of minimizing the environment’s expected cover time—the number of steps required for a random walk to visit every state (Broder and Karlin, 1989). The intuition behind them is similar to the one behind eigenoptions, as they exploit the fact that the bottom eigenvector of the graph Laplacian captures the states that are furthest apart in the environment. Nevertheless, a covering option is defined as a point option (Jinnai et al., 2019a) connecting only two states. It explicit adds an edge to the graph representing the underlying MDP to connect the two furthest vertices in that graph, in an attempt to shrink its diameter. They are obtained with an iterative procedure in which, after the discovery of each option, the environment’s underlying graph is updated and the option discovery procedure is executed again. Covering options are naturally defined by multiple iterations of the ROD cycle. Eigenoptions can be seen as adding multiple edges to the graph, connecting every state in the initiation set to one of the terminal states. Covering options are discovered one at a time and connect only two states. The description of covering options becomes clearer with an example. In the four-room domain, the bottom eigenvector of the graph Laplacian, defined w.r.t. a uniform random policy, is depicted in Figure 7, as well as its corresponding covering options—note the eigenvector is equivalent to the one depicted in Figure 5. In this environment, the two states that are furthest apart are the states diagonally opposed in the corners. Covering options connect those two states, allowing the agent to easily navigate between them. Learning the covering options’ policies. Formally, a covering option’s policy is defined by the intrinsic reward function re(s, s(cid:48)) obtained from the non-constant eigenvector e ∈ Rd of the graph Laplacian that has the smallest corresponding eigenvalue. It is defined as (cid:40) 1, if arg max e = s(cid:48), re(s, s(cid:48)) = i i 0, otherwise. 3. The eigenvector of the graph Laplacian with corresponding smallest eigenvalue is constant, so we refer to the second smallest one here. 15
Machado, Barreto, Precup, and Bowling In words, arriving at the state in which the eigenvector has the corresponding largest value leads to a reward of 1. The same reward function is applied to the negation of the eigenvector so the agent can also learn an option that leads to the state with smallest value. This reward function is arguably simpler than the reward function used by eigenoptions, but it does not provide the agent with a gradient to follow when learning the option’s policy, which might lead to exploration issues. We further discuss the pros and cons of each choice in the next sections. Defining the covering options’ initiation sets and termination conditions. Because these are point options, their initiation sets and termination conditions are trivially defined. The initiation set has a single state, the one with corresponding smallest value in the considered eigenvector. Covering options terminate with probability 1 when they reach the state with corresponding largest value. Formally, for a covering option ωe, (cid:40) (cid:40) 1, if arg max e = s, 1, if arg min e = s, i i i i β (s) = I (s) = ωe ωe 0, otherwise. 0, otherwise. In which we overloaded the notation to have I (s) denoting whether state s belongs to the ωe initiation set of option ωe or not. As before, we evaluate ±e. This can obviously lead to problems when reaching an individual state is unlikely (e.g., large/continuous state-spaces). Recently, Jinnai et al. (2020) extended this notion to a region of the state-space. Iterative discovery of covering options. While the formulation of eigenoptions does not prescribe the number of options to be used, there always exist exactly two covering options associated with an SR. In order to get more covering options, one must compute an updated SR induced by the newly-added options and then use its dominant eigenvectors to compute two new options. This process can be repeated multiple times, resulting in an iterative procedure in which two covering options are added at each iteration. Thus, while eigenoptions work well with a single iteration of the ROD cycle, learning multiple options in parallel, covering options require a much lighter iteration, but several iterations of the ROD cycle. Algorithm 7 and 8, in Appendix C, summarize the presentation of covering options when computed in both closed-form and online. Importantly, every iteration is guaranteed to improve the upper bound of the expected number of steps an agent would need, when following a uniform random policy, in order to visit every state in the environment. We refer the reader to the work by Jinnai et al. (2019b) for the formal statement behind this result. Covering options provide a simpler approach for option discovery. This approach dis- covers a fixed number of options at each iteration, the intrinsic reward function the agent needs to maximize is trivial, as well as the definitions of the initiation set and termination condition. Nevertheless, it is not clear that this simplicity implies better empirical perfor- mance. Is it empirically better to use the full spectrum of the graph Laplacian or only the bottom eigenvector? Do covering options face an exploration issue when learning the option’s policy? Are point options more effective than options defined over most of the state space? In the next section, we empirically evaluate these different choices option discovery methods have. 16
Temporal Abstraction in RL with the Successor Representation 6 Evaluation of Temporally-Extended Exploration with Options Machado et al. (2017, 2018) and Jinnai et al. (2019b, 2020) have already presented empirical evidence about the efficacy of the approaches discussed in the previous section. Thus, we focus instead on comparing components of these approaches. These comparisons elicit fundamental questions surrounding option discovery, such as the impact of different initiation sets, how to best use a limited number of samples observed by the agent, and on the trade-offs posed by different numbers of options available to the agent at a given moment. We evaluate these choices in the context of temporally-extended exploration, when using options to provide persistent behaviors inside uniformly random policies in both closed-form and online settings. This analysis also allows us to highlight different choices that can be made when instantiating the ROD cycle. Because we are interested in the agent’s exploration capabilities in a given environment, irrespective of a specific reward function, we use the diffusion time as the main evaluation metric. The diffusion time reflects the expected number of decisions4 required to navigate between any two states while following a random walk (Dayan and Hinton, 1992; Machado et al., 2017). A small expected number of decisions implies that the agent is more likely to reach any state with a random policy. The diffusion time captures the agent’s ability to learn about the structure of the environment, and it is particularly relevant in settings without a single, fixed task, such as continual (Brunskill and Li, 2014; Mankowitz et al., 2018), multitask (Teh et al., 2017), and transfer learning (Taylor and Stone, 2009). Moreover, the diffusion time allows us to summarize more easily a large number of results. While we will use this metric throughout most of the paper, in Section 6.2 we also evaluate how eigenoptions and covering options impact the speed at which agents accumulate rewards. In tabular domains, we can easily compute the diffusion time with dynamic program- ming. We do so by defining an MDP in which the value function of a state s, under a uniform random policy, encodes the expected number of decisions required to navigate between state s and a chosen goal state. We compute the expected number of decisions between any two states by setting one as the goal and checking the value of the other. We then compute the expected number of decisions across the entire state space by averaging over all possible pairs of the initial state and the goal state. The MDP in which the value function of state s encodes the expected number of decisions from s to a goal state has γ = 1 and a reward function of +1 at every decision that does not lead to the goal state. Policy evaluation computes the expected number of decisions the agent will take before arriving to the goal state. When computing the diffusion time, we iterate over all possible states, defining them as terminal states. We report both average and median as summary statistics of diffusion time. To provide a direct comparison between eigenoptions and covering options, in Section 6.1 we evaluate the agent’s diffusion time induced by these approaches. In Section 6.2, we check how the insights obtained from this comparison impact reward maximization. In Section 6.3, we evaluate the impact of different approaches for defining the options’ initiation sets and termination conditions, and of using the whole eigenspectrum of the successor 4. We use the term decisions instead of steps to estimate the likelihood that a sequence of random choices (i.e., options or primitive actions) will lead to the desired state. 17
Machado, Barreto, Precup, and Bowling 1400 1100 800 500 200 0 10 20 30 Number of Options emiT noisuffiD 107 1400 104 1100 0 10 20 30 800 500 200 0 10 20 30 Number of Options emiT noisuffiD 106 104 102 0 10 20 30 Eigenoptions Covering Options Primitive Actions Figure 8: Comparison between the agent’s average (left) and median (right) diffusion time in the four-room domain when using covering options and eigenoptions. The inset plots depict, in log-scale, the range in which the diffusion time lies. representation. We report our last set of experiments in Section 6.4, when we discuss how these algorithms behave in the online setting. We summarize our findings in Section 6.5. 6.1 Diffusion Time of Eigenoptions and Covering Options We report the diffusion time obtained by eigenoptions and covering options in Fig- ure 8. We use the four-room domain (Sutton et al., 1999), which we implemented with Gym-Minigrid (Chevalier-Boisvert et al., 2018). For simplicity, we consider deterministic transitions and we compute both sets of options in closed form. The average diffusion time we report for eigenoptions is similar to what Machado et al. (2017) reports. Covering options had not been evaluated under this metric yet. We observe that after a sufficient number of options becomes available, eigenoptions lead to a smaller diffusion time, although they lead to much worse performance with fewer options. Figure 8 shows that once options start to outperform primitive actions, given the same number of options, eigenoptions lead to a lower diffusion time. This suggests that it is better to use options derived from, say, the top ten eigenvectors of the SR, as done by eigenoptions, than to use the top eigenvector of the SR in ten successive iterations, as done by covering options.5 This is supported by other results that also show the benefits of looking beyond the first eigenvector of time-based representations (Bar et al., 2020). Aside from the number of eigenvectors they use, another difference between eigenoptions and covering options is with respect to how the options are defined. While an eigenoption is defined in the whole state space, the initiation set of a covering option has a single state. The main consequence of this is that at the beginning, before the options sufficiently connect the underlying graph, eigenoptions tend to create sink states. The agent needs to 5. In these experiments, for covering options, we used the eigenvectors of the Laplacian because we are computing them in closed form and also because, differently than the SR, it does not lead to eigenvectors with a complex component. We further discuss this issue in Section 6.4 and Appendix F. 18
Temporal Abstraction in RL with the Successor Representation take enough random actions to reach a state while also not choosing, by chance, options that move it away from the desired state. This explains the much worse performance of eigenoptions when fewer options are added. Point options have a less ubiquitous effect. There is a stark difference between the reported average and median diffusion time. The average diffusion time is heavily impacted by outliers while the median diffusion time is more representative of performance for a random pair of states. As options are added, most of the states become easier to reach. However, without enough options, it is very difficult to reach states that are far from the options’ terminal states. The average diffusion time captures this dichotomy. It has very high values at first because the options (mainly eigenoptions) make some states almost impossible to reach. The median is not impacted by these worst-case scenarios. It is particularly impressive to see that covering options, even with a single option, are already capable of reducing an agent’s median diffusion time. 6.2 Maximizing Rewards after Temporally-Extended Exploration A natural question to ask is how eigenoptions and covering options impact an agent’s ability to accumulate reward in a single, fixed task. We answer this question in the setting in which agents learn the values of primitive actions while being allowed to also act according to the options’ policies. In this setting, options do not incur an additional cost in terms of sample complexity (Brunskill and Li, 2014) because the agent does not learn state-option values. Nevertheless, the agent is still able to exhibit temporally extended-exploration when acting with respect to an option’s policy. Specifically, we use Q-learning (Watkins and Dayan, 1992) to learn the agent’s policy with (cid:15)-greedy exploration. When an exploratory step is chosen, the agent randomly chooses amongst all primitive actions and options with equal probability. When an option is selected, the agent acts according to the option’s policy until it terminates, while updating, off-policy, the value of each of the primitive actions taken. Additionally, this evaluation allows us to evaluate the setting in which the number of steps taken by the agent matter. While the diffusion time ignores the set of states visited by the agent when following an option, the off-policy updates we use do not. We evaluated our agents in the four-room domain with the agent having access to a varied number of options. We used ten tasks defined by different, randomly sampled, start and goal states. Episodes were at most 1,000 steps long and we evaluated the agents for 50 episodes. The agent observes a reward signal of 0 until reaching the goal, when it observes a reward signal of +1. The Q-learning parameters we use are α = 0.1, γ = 0.9, and (cid:15) = 0.05. We use the options pre-computed from the previous section, adding them to the agent’s option set according to their corresponding eigenvalue (or iteration), as previously discussed. Figure 9 depicts the performance of an agent augmented with eigenoptions or covering options. We chose a representative setting amongst the ten tasks. The results for the other nine tasks can be found in Appendix D. We observe that, for eigenoptions, as few as four options are enough to accelerate learning. We did not observe four eigenoptions hurting agent’s performance in any of the ten tasks we randomly sampled. On the other hand, surprisingly, covering options did not improve the agent’s performance. This was consistent across the ten tasks. We conjecture this is due to the sparse initiation set that reduces 19
Machado, Barreto, Precup, and Bowling 800 600 S 400 200 G 0 10 20 30 40 50 Number of Episodes (a) Task laoG ot spetS fo rebmuN 800 600 400 200 0 10 20 30 40 50 Number of Episodes (b) Eigenoptions laoG ot spetS fo rebmuN No options 4 options 8 options 16 options (c) Covering options Figure 9: Performance of Q-learning augmented with eigenoptions and covering options. In the task, S and G denote the start and goal state. Results are averaged across 50 runs and shaded regions denote a 99% confidence interval. See text for details. the effectiveness of covering options in the online setting because they are rarely sampled. Although this may be alleviated by the use of an agent that also learns option values, this strategy can have unforeseen consequences, such as a higher sample complexity for learning the optimal policy. These results can also be interpreted in light of Liu and Brunskill’s (2018) work, which characterizes the properties of an environment that make exploration hard or easy. Infor- mally, Liu and Brunskill show that, if, asymptotically, a random walk can explore well, then it is possible to obtain a polynomial sample complexity bound for finite sample exploration. We conjecture eigenoptions transform the problem to one where random walks are more effective, making the problem more amenable to (cid:15)-greedy exploration. Covering options, on the other hand, fail to make random walks more effective. This is due to the fact that each option is available in a single state, and, when the agent happens to be in that state, it still needs to sample the option instead of primitive actions. Recent extensions of covering options to problems that require function approximation define the initiation set as a region of the state space instead of a single state (Jinnai et al., 2020). 6.3 The Impact of Different Initiation Sets and Uses of the Eigenspectrum In this section, we quantify the impact that different choices have when designing op- tion discovery algorithms for temporally-extended exploration. We answer the following questions: • Is it better to have options that are broadly available, i.e., with large initiation sets? • Is it beneficial to use more than one eigenvector of the SR when discovering options? We ask these questions because eigenoptions and covering options are based on the same principles but, as previously discussed, their effectiveness is not the same. These questions are motivated by how these methods differ. We use a factorial design to evaluate the impact of these different choices, which are out- lined in Table 1. We use the diffusion time induced by the discovered options as evaluation 20
Temporal Abstraction in RL with the Successor Representation Single Options broadly Algorithm description iteration available (init. set) Covering options (CO) No No CO w/ Broad Initiation Set No Yes Point-based Eigenoptions Yes No Eigenoptions Yes Yes Table 1: The different dimensions of variation between eigenoptions and covering options. Covering options with a broad initiation set are defined such that the terminal state of these options is the same as in covering options, but the initiation set, instead of being a single state, is now every state that is not terminal. Conversely, point-based eigenoptions are eigenoptions that have a single start state, defined to be the state with smallest value in the corresponding eigenvector. metric. As aforementioned, the diffusion time is task-agnostic and it concisely describes the effectiveness of different sets of options across multiple potential tasks by assessing how op- tions capture relevant properties of the environment. We compare, given the same number of options, how the diffusion time induced by different methods varies. Figure 10 depicts the performance of the algorithms described in Table 1. These results show that a broad initiation set eventually leads to a smaller diffusion time but, until a minimum number of options is available, they hinder performance. Eigenoptions, for example, obtain the smallest diffusion time but requires more options before doing so—the same applies, in a smaller scale, to using covering options with a large (broad) initiation set. There is a bigger difference in the median diffusion time, as point-based options always reduce the median diffusion time while broad initiation sets can create attractor states that are difficult to escape from. Additionally, another trade-off to be considered is that, depending on how the options are used, a small initiation reduces the likelihood agents will have access to the corresponding options. In the setting we analyzed here, additional eigenvectors provide benefits when compared to using only the top eigenvector of the SR, even when we discard the cost of collecting more samples at each iteration. Notice that, because we use the closed-form solution, it is as if the agent had covered the whole state-space before starting the option discovery process. We explore the setting in which the agent does not cover the whole state-space before the option discovery step in Section 7, which is a setting in which the benefit of multiple iterations of the ROD cycle is clearer. We can also analyze other successes of temporally extended-exploration in light of the insights we obtained here. Dabney et al. (2021) recently introduced (cid:15)z-greedy, an extended form of (cid:15)-greedy exploration in which an agent, when taking an exploratory action, re- peats the sampled action for a random duration, often sampled from a zeta distribution. Such an approach achieves remarkable success in standard benchmarks such as Atari 2600 games. Despite not being based on the successor representation, such an approach gives us evidence to support our conclusions. In (cid:15)z-greedy exploration, the agent is allowed to 21
Machado, Barreto, Precup, and Bowling 3 2 1 4 1. Eigenoptions 2. Covering Options 3. Point-based Eigenoptions 5 5 4. Covering Options Broad Init. Set 5. Primitive Actions 2 4 1 3 Figure 10: Average (left) and median (right) diffusion time in the four-room domain induced by the options generated with the algorithms in Table 1. The inset plots depict, in log-scale, the range the diffusion time lies. See text for details. sample an extended sequence of actions at any time, corroborating the intuition that re- strictive initiation sets might prevent the agent from exploring the environment. Moreover, the duration in which the sampled action will be repeated is random, showing the benefits of varying time-scales, one of the features we get from using additional eigenvectors of the SR (c.f. Figure 6). Similarly, the temporally-extended exploration used when learning to control superpressure balloons in the stratosphere (Bellemare et al., 2020) is also based on options that are not constrained to a small region of the state space and they too have varying duration. The results in this section also raise the question of whether multiple iterations of the ROD cycle improve an agent’s ability to explore. As one can expect, this is indeed true, as we can see how, for covering options, additional iterations lead to better results. In Section 7, we provide a different illustration where we use the insights gained here to design an algorithm that performs multiple iterations of the ROD cycle. Before we do so though, we assess the impact of discovering options online, which is an important aspect of any iterative cycle. 6.4 Online Option Discovery The results so far were obtained in closed-form, which can only be achieved after the agent has thoroughly explored the environment. This allowed us to evaluate algorithmic ideas in a conceptually simpler way, without approximation errors. However, this is not a realistic setting. In this section, we evaluate the impact of using options discovered from online estimates of the SR, instead of assuming access to an adjacency matrix representing the environment. We use TD learning to estimate the SR from samples, which allows us to compute options before the environment has been exhaustively explored. Eigenoptions are robust to using online estimates of the SR, as one can see in Figure 11. This result is similar to Machado et al.’s (2018). To minimize the number of interactions with the environment, we re-use the data used to compute the SR to learn the eigenoptions’ 22
Temporal Abstraction in RL with the Successor Representation 1400 1100 800 500 200 0 10 20 30 Number of Options emiT noisuffiD 107 1400 104 1100 0 10 20 30 800 500 200 0 10 20 30 Number of Options emiT noisuffiD 100 episodes 500 episodes 1000 episodes Closed-form Primitive Actions Figure 11: Average (left) and median (right) diffusion time, in the four-room domain, induced by eigenoptions computed with the SR, online and in closed-form. Episodes were 1,000 steps long. We report, averaged across 50 different runs, the impact of using different number of episodes for computing the SR online. The inset plot on the left figure depicts, in log-scale, the range the diffusion time lies. policies (c.f. Algorithm 1). We use Q-learning to learn these policies. The agent always starts at the bottom left corner. Episodes were 1,000 steps long and we used a step-size η = 0.1 and γ = 0.9 to learn the SR. Surprisingly, covering options are not as effective in the setting in which the represen- tation is learned online. This is depicted in Figure 12. While the results for the median diffusion time are consistent with the other results in this section, the results for the average diffusion time are not. Covering options are not able to reduce the average diffusion time in the environment; in fact, they increase it substantially. The reason becomes clearer when we look at the first options discovered. When estimating the SR online,6 the agent rarely samples an option because it is available in a single state. This leads to similar options being discovered in multiple iterations. Moreover, covering options are less robust because they rely only on the maximum and minimum values of the top eigenvector of the SR to define the state in which an option is available. Using a single eigenvector adds to this brittleness because the agent cannot rely on other eigenvectors to capture different dimen- sions of the state space, or to correct for an option that captures the wrong timescale at which the agent should act. As an example, the top two eigenvectors of the SR capture the two diagonals of the four-room domain (c.f. Figure 7). If an agent tries to use the top eigenvector of the SR to capture, in two different iterations, these two diagonals, it may fail to do so if the random walk of the agent in the second iteration does not sample the option that navigates the dimension the first iteration captured. The mismatch between the closed-form as online solutions only increases in later iterations—Appendix E presents visualizations of eigenoptions and covering options learned online. 6. When the agent selects an option, we ignore individual transitions when estimating the SR, assuming the the agent “teleports” to the state in which the option terminates. This mimics the closed-form solution, which sees an option as creating an edge between two vertices. Moreover, we observed that estimating the SR from individual transitions leads to much worse results. 23
Machado, Barreto, Precup, and Bowling 1400 1100 800 500 200 0 10 20 30 Number of Options )01x( emiT noisuffiD 107 1400 104 1100 0 10 20 30 800 500 200 0 10 20 30 Number of Options emiT noisuffiD 100 episodes 500 episodes 1000 episodes Closed-form Primitive Actions Figure 12: Average (left) and median (right) diffusion time, in the four-room domain, in- duced by covering options computed with the SR, online and in closed-form. Episodes were 1,000 steps long. We report, averaged across 50 different runs, the impact of using different number of episodes for computing the SR online. The inset plot on the left figure depicts, in log-scale, the range the diffusion time lies. The y-axis in the left plot is scaled by 10 in comparison to Figure 11 to depict the average diffusion time induced by the covering options discovered online. Finally, using the SR to compute covering options can violate the symmetry assumption of Theorem 2. After the first iteration of covering options, in some states the agent has access to four primitive actions and one option, while in others only four actions are available. This mismatch is interesting from a theoretical point of view, but cannot be used to justify the poor performance we observed. At least in the four-room domain, this mismatch does not meaningfully impact the diffusion time induced by covering options discovered from the SR. The empirical analysis of the impact of this mismatch is available in Appendix F, where we compare the diffusion time induced by covering options when using the eigenvectors of the SR and proto-value functions, and we show it is minimal. 6.5 Summary In this section, we showed that eigenoptions and covering options reduce the diffusion time in a given environment, meaning that they capture properties of the environment that are useful when subsequent tasks are posed to the agent. Eigenoptions do so both when computed in closed-form and online, while covering options only do so when discovered in closed-form, the setting they were originally introduced. We also investigated the im- pact of the different choices these approaches make to understand their overall impact in temporally-extended exploration with options. Specifically, whether it is beneficial to dis- cover more than one option per iteration, with our results showing that discovering multiple options leads to a lower diffusion time, more robust solutions, and a more judicious sample use. Moreover, we evaluated different approaches for defining options’ initiation sets and termination conditions. Our results highlight an interesting trade-off: while making options 24
Temporal Abstraction in RL with the Successor Representation available in fewer states avoids the creation of sink states that increase the diffusion time, these options end up not being so useful to the agent when learning to maximize rewards. They can also be quite detrimental in the online setting. The results in this section also raise several interesting questions. In the next section we use the insights gained here to introduce an algorithm that illustrates multiple iterations of the ROD cycle, and in Sections 8 and 9 we explore the possibility of linearly combining eigenoptions, without additional sample complexity, in order to obtain more diverse behav- ior. Other questions we leave for future work revolve around, for example, combining the benefits of eigenoptions and covering options, maybe through an ensemble of both types of options; or on how to chain such options. 7 Iterative Option Discovery with the ROD Cycle So far, we have investigated settings in which either the agent has access to the true SR or it is able to learn an accurate estimate of the SR at each iteration of the ROD cycle. It is hard to see the benefits of multiple iterations of the ROD cycle in these settings because a single iteration is already enough for the agent to learn the SR accurately. We now look at the more challenging (and realistic) setting in which it is impossible for the agent to visit every state of the environment in a single iteration. This allows us to validate the claim that options can be used to generate a different distribution of state visitation, which leads to different representations, which further impacts subsequently discovered options, in a virtuous, never-ending, cycle. 7.1 Iterative Online Eigenoption Discovery Inspired by the results in the previous section, such as the importance of a large initia- tion set and the dangers of sink nodes, we introduce covering eigenoptions (CEO). CEO is an algorithm for option discovery that illustrates the benefits of multiple iterations of the ROD cycle. It combines the best design decisions of both eigenoptions and covering options for the online case. CEO discovers one eigenoption per iteration, slowly increasing the size of the option set. It uses the top eigenvector of the SR to define such an eigenoption but, differently than covering options, the SR it uses is defined only over primitive actions. At each time step, actions or options (if available) are randomly selected. Options are sampled with a smaller probability than primitive actions to ensure they do not dominate the agent’s behavior. At first, discovering one option per iteration might seem surprising in light of the results in the previous section suggesting that discovering more options per iteration is beneficial. However, such a choice stems from the difference in problem setting: now that options are sampled, it is important to acknowledge that any algorithm based on the ROD cycle will end up with a growing set of options. If too many options are added at each iteration, the probability of sampling any individual option becomes vanishingly small. This is in contrast with the previous evaluation that was mostly focused on how the discovered options change the topology of the environment. Finally, by discovering eigenoptions at each iteration, due to their large initiation set, we increase the likelihood that the options will actually be sampled, minimizing the chance an iteration is wasted, as sometimes happens with covering 25
Machado, Barreto, Precup, and Bowling options (c.f. Section 6.4). Algorithm 2, in the next page, depicts the pseudo-code for CEO. We explicitly discuss our choices for each each step of the ROD cycle below. Collect samples. The agent collects samples by randomly interacting with the environ- ment. In the first iteration, only primitive actions are available to the agent; later, options also become available. If an option is sampled, the agent acts according to that option’s policy until termination. We save the observed transitions in a data set. We consider tran- sitions in terms of primitive actions, even when they are induced by an option. Importantly, the probability of sampling an option, p , should be lower than the probability that a option primitive action is sampled to account for the fact that options have a longer duration. Learn representation. This step consists in learning the successor representation from the samples gathered by the agent. One important aspect to the success of the introduced algorithm is that it never throws away any data, meaning that it is constantly refining the SR instead of learning it from scratch at every iteration. Derive an intrinsic reward function from the learned representation. Having access to the SR learned online, the agent now derives an intrinsic reward function it will use to learn the option’s policy. We use the reward function defined by eigenoptions. Importantly, we only consider one direction for the eigenvector: to incentivize the agent to go to states that it has not visited much, it is important to pre-determine the direction of the eigenvector, otherwise one of the generated eigenoptions is the one that takes the agent to the place it has visited the most. Obviously, it is important to avoid such a behavior when options are sampled because we want to incentivize the agent to visit places it has not visited much. (cid:80) For this step, we choose the direction of the eigenvector such that e(i) < 0. Inspired by i covering options, we use only the top eigenvector of the SR. Learn to maximize intrinsic reward. We use Q-Learning to learn how to maximize the intrinsic reward defined above. Because the intrinsic reward is only defined in states which the agent has visited before, letting the agent learn from scratch how to maximize such reward might lead the agent to visit states in which the reward function is not defined. Thus, instead, we use the transitions in the data set we collected in the sample collection step. Such an approach saves agent-environment interactions and guarantees the agent will learn a policy taking only previously visited states into consideration. Define option. Finally, we define the option’s initiation set and termination condition following the eigenoptions description in Section 5, adding the new option to the option set. 7.2 Empirical Analysis As in the previous sections, we evaluate CEO in the four-room domain. However, instead of using episodes that are 1,000 steps long, we now consider episodes that are only 100 steps long. The agent starts every episode in the top right corner. In this setting, it is impossible for the agent to visit every state in the environment in a single episode. The agent interacts with the environment for a whole episode as part of the data collection step, with the agent using the collected data set to complete the other steps of the ROD cycle between episodes. In other words, each episode corresponds to a different iteration of the ROD cycle. 26
Temporal Abstraction in RL with the Successor Representation Algorithm 2: Covering Eigenoptions Input: η, α ; (cid:46) Step-sizes for learning the SR and the options’ policies o γ , γ ; (cid:46) Discount factor for the SR and the options’ policies SR o p ; (cid:46) Prob. of sampling an option instead of a primitive action option N ; (cid:46) Maximum number of interactions with the environment steps N ; (cid:46) Number of iterations of the ROD cycle iter D ← ∅ Ω ← ∅ for i ← 0 to N do iter (cid:46) Collect samples for j ← 0 to N do steps With prob. 1 − p randomly sample, uniformly, a primitive action a; option otherwise uniformly sample an option ω from Ω if primitive action was sampled then In state s, take action a and observe state s(cid:48) and reward r D ← D (cid:107) (s, a, r, s(cid:48)) ; (cid:46) Append transition to data set D else while β (s) (cid:54)= 1 do ω In state s, take action π (s) and observe state s(cid:48) and reward r ω D ← D (cid:107) (s, π (s), r, s(cid:48)) ; (cid:46) Append transition to data set D ω (cid:46) Learn representation Ψ ← Successor Representation(η, γ , D) SR (cid:46) Derive an intrinsic reward function from the learned representation e ← getTopEigenvector(Ψ) re(s, s(cid:48)) ← e(s(cid:48)) − e(s) ∀s, s(cid:48) ∈ S ; (cid:46) Define eigenpurpose (cid:46) Learn to maximize intrinsic reward Q ← Q-Learning(re, α , γ , D) ; (cid:46) Learn value function o o (cid:46) Define option I ← ∅; π (s) ← ⊥, β (s) ← 1 ∀s ∈ S ; (cid:46) Initialize option tuple ω ω ω for s in S do (cid:46) If s is not a terminal state for the eigenoption being learned: if ∃a ∈ A(s) s.t. Q(s, a) > 0 then I ← I ∪ {s} ω ω π (s) ← arg max Q(s, a) ω a β (s) ← 0 ω Ω ← Ω ∪ (cid:104)I , π , β (cid:105) ω ω ω 27
Machado, Barreto, Precup, and Bowling Random policy over Random policy over primitive primitive actions actions and covering eigenoptions Figure 13: Proportion of time spent in each state when randomly selecting actions over primitive actions only, and over primitive actions and covering eigenoptions. Reported numbers are averaged over 100 seeds. The result of each seed is cal- culated at the end of the last episode, which is defined by the time in which the agent has visited every state in the environment. The proportion is computed as the number of time steps the agent was in that particular state divided by the total number of interactions with the environment. We performed numerical simulations to estimate how many time steps, on average, CEO needs to visit every state in the environment at least once.7 This metric can be seen as a Monte Carlo estimate of the diffusion time when counting steps instead of decisions. We use this metric because it is not clear how to easily compute the diffusion time in closed form when taking into consideration the episodic nature of the problem. We use η = α = 0.1, o γ = γ = 0.99, and we sample options with 5% probability (p ), which is similar to SR o option what we did in Section 6.4, where options were potentially sampled only in the exploration step of Q-Learning with (cid:15)-greedy ((cid:15) = 0.05). We pass over D 100 times when learning the SR, and 1, 000 when learning the option policy, leveraging the off-policy aspect of our problem formulation. CEO needs, on average, 2,301.2 steps to visit every state at least once (n=100, SD=830.2, Mdn=2,069.5, Min=1,067, Max=5,793). In contrast, a uniform random policy needs 27,032.3 steps (n=100, SD=16,961.0, Mdn=22,397.0, Min=3,328, Max=95,118). CEO, by imple- menting multiple iterations of the ROD cycle, reduces the number of interactions an agent needs in order to visit every state by an order of magnitude! Importantly, when compared to a uniform random policy, CEO does not only visit every state quicker, but it also induces a more uniform visitation over the state space, as depicted in Figure 13. The behavior induced by CEO indeed supports the claims around the benefits of multiple iterations of the ROD cycle. Figure 14 depicts the first four iterations of CEO with a particular seed. In iteration 1, the agent collects samples with a random walk and it learns a representation such that it is incentivized to visit states it has not visited often. This leads to an option that takes the agent closer to the hallway between the top and bottom rooms 7. If the agent visits a state for the first time at the second time step of the second iteration, we consider the agent has visited that state at time step 101 + 3 = 104. 28
Temporal Abstraction in RL with the Successor Representation Top eigenvector of the Top eigenvector of the Iteration 2 Iteration 1 successor representation successor representation Learn Define Learn Define representation option representation option Collect samples All All visited visited states states Collect samples Collect Collect samples samples … All All visited visited states states Learn Define Learn Define representation option representation option Top eigenvector of the Top eigenvector of the Iteration 3 Iteration 4 successor representation successor representation Figure 14: Illustration of multiple iterations of the ROD cycle when instantiated with cov- ering eigenoptions. See text in Section 7 and Algorithm 2 for details. 29
Machado, Barreto, Precup, and Bowling on the right. In iteration 2, the agent ends up closer to the referred hallway as it eventually samples the learned option; a random walk then leads the agent to the bottom right room. The learned representation still puts a lot of mass in the states often visited in the first iteration, but the reward function is now defined in more states. Aside from the option that takes the agent to the aforementioned hallway, the agent now also discovers an option that, when in the top right room, takes the agent to the hallway between the top right and left rooms or, when in the bottom right room, takes the agent close to the hallway between the bottom right and left room. At the third iteration the agent then has access to two options. By chance, the agent eventually samples an option that takes it to the state close to the hallway between the top and left room, with the random walk then, by chance, taking the agent to the top left room. The induced SR at the end of these three iterations leads to the discovery of a third option that, from most states, take the agent to the state close to the hallway between the bottom right and left room. By chance, the agent ends up sampling that option in the fourth iteration, visiting the room that had not been visited yet. This is a clear example of the benefits of the ROD cycle where options discovered from previous iterations act as a scaffold for more complex behaviors discovered in subsequent iterations. We chose to depict this particular behavior of CEO for pedagogical purposes, although the agent does not always visit the four rooms in the first four iterations, it eventually does so, generally much faster than a uniform random policy. These results (and the proposed method) add to the growing list of approaches that can be seen as instantiations of the ROD cycle (e.g. Erraqabi et al., 2022; Jinnai et al., 2020; Hoang et al., 2021; Machado and Bowling, 2016). The illustration in this section is particularly useful for making the multiple iterations of the cycle, each step, and their outcome, very explicit. We further discuss some of these methods in Section 10. 8 Combining Options with the Option Keyboard In the previous sections we discussed the ROD cycle, a general framework for option discovery based on representation learning (c.f. Figure 1). We described two algorithms that instantiate the ROD cycle, one based on eigenoptions and one based on covering options, and we introduced an algorithm that combines both eigenoptions and covering options to benefit from multiple iterations of the ROD cycle. In all these cases, options are derived from the eigenvectors of the SR matrix. Options derived from the eigenvectors of the SR are particularly suitable for exploration because they induce complementary distributions that collectively cover the state space in a structured way. At first this suggests a straightforward strategy for exploration: compute one option per eigenvector and then use them together to explore the environment. The problem with this strategy is that, since each option must be learned, there is an inherent cost associated with adding them to the library of available behaviors. Moreover, adding the options associated with all the eigenvectors may be unnecessary, since some of them provide little benefit in terms of exploration in the presence of others. To illustrate this point, note that, even in a simple domain like four-room, this exploration scheme would result in more than 100 options. Our experiments clearly demonstrate that using this many options is not really necessary (c.f. Figures 10, 11, and 12), including those in Section 7. 30
Temporal Abstraction in RL with the Successor Representation This leads us to the second useful property of options induced by the eigenvectors of the SR: their associated eigenvalues provide a natural ordering of the options according to their time scale—this roughly corresponds to the option’s expected time before termination, as shown in Figure 6. Based on this observation, we can improve the exploration strategy outlined above: instead of adding the options all at once, we rank them based on their eigenvalues and add them one by one until they collectively form a good basis for exploration. This is the basic recipe underlying both eigenoptions and covering options. But can we do better? Can we use these options to not only go to the far end of the environment, but in pretty much any state in the environment? It has been argued that the ability to combine options may be key to extend the range of available behaviors without incurring the otherwise inevitable cost in terms of sample transitions (Sutton, 2016; Heess et al., 2016; Haarnoja et al., 2018). By exploiting com- positionality, one can potentially grow a finite number of options into a combinatorially larger counterpart without additional learning. In the context of eigenoptions and cover- ing options, this benefit manifests itself in two complementary ways. First, by combining higher-order options, it may be possible to approximately emulate their lower-order counter- parts, which will thus no longer need to be learned. Second, and perhaps more important, depending on how options are combined, they may give rise to new options whose behavior differ significantly from that of any option induced by the eigenvectors of the SR—i.e., the combined options might effectively extend the SR behavioral basis used for exploration, even when thinking about multiple iterations of the ROD cycle. The question then arises as to how to actually implement the combination of options. In this context, a natural choice is the option keyboard (Barreto et al., 2019). The option keyboard is particularly suitable to be used with options induced by the SR because it is itself based on the concept of SR, making the integration between the two approaches natural and transparent. Given a set of options evaluated under different rewards, the option keyboard provides a way to instantaneously generate options induced by any linear combination of the rewards, without any learning involved. Although simple, this way of combining options provides all the benefits mentioned above in the context of options induced by the eigenvectors of the SR. In the next section we elaborate on how to build and use the option keyboard using options derived from the SR. 8.1 Generalized Policy Evaluation and Generalized Policy Improvement The option keyboard is based on generalizations of the concepts of policy evaluation and pol- icy improvement introduced in Section 2. Simply put, generalized policy evaluation (GPE) is the computation of a policy’s value function under different reward functions. Given the value function of a set of policies under a specific reward function, generalized policy improvement (GPI) is the computation of a new policy whose performance is no worse, and generally better, than that of the original policies. GPE and GPI are strict generalizations of their standard counterparts, which are recovered as special cases (Barreto et al., 2020). To accommodate the generalization provided by GPE, we will use vr and qr to denote π π the state-value and action-value functions of policy π under reward r. We start by noting that the SR provides a particularly efficient form of GPE: as shown in Eq. 10, once we have the SR of a policy π, Ψ , we can evaluate it under any reward function r by simply π 31
Machado, Barreto, Precup, and Bowling making vr = Ψ r. As discussed in Section 4, SFs generalize the SR by replacing its features π π φ (s ) = 1 with arbitrary features φ : S × A (cid:55)→ Rd. Once we have the SFs of a policy i j {i=j} π, ψ , we can compute its value function under any linear combination of the features, π r(s, a) = φ(s, a)(cid:62)w, by making qr (s, a) = ψ (s, a)(cid:62)w, where w ∈ Rd (c.f. Eq. 13 and 14). π π Note that, because the features φ(s, a) can be any function, one can in principle use an intrinsic reward r (s, a) as the i-th feature: φ (s, a) = r (s, a). This will be important for i i i the option keyboard. GPI is the computation of a policy whose performance under a given reward is generally better than that of its precursors. The mechanics of GPI are actually very similar to that of its standard counterpart shown in Eq. 4: given policies π , π , ..., π , and their value 1 2 n functions under reward r, qr , qr , ..., qr , the GPI policy π(cid:48) is given by π1 π2 πn π(cid:48)(s) ∈ argmax max qr (s, a) for all s ∈ S. (17) a i πi Barreto et al. (2017) have shown that, starting from the execution of any action a ∈ A on any state s ∈ S, the GPI policy π(cid:48) will do at least as well as, and generally better than any of its precursors π . More formally, we have that qr (s, a) ≥ qr (s, a) for all i ∈ {1, 2, ..., n} i π(cid:48) πi and all (s, a) ∈ S × A. This result can also be extended to the case in which GPI is applied with approximations q˜r ≈ qr (Barreto et al., 2017). πi πi 8.2 Synthesizing Options with GPE and GPI Now that we have introduced the concepts of GPE and GPI, we describe how to use these operations to create options without any learning involved. For clarity, instead of presenting the option keyboard in its most general form, we will use the formalism of eigenoptions introduced in Section 5 (the adaptation to covering options is straightforward). Let e , e , ..., e be eigenvectors of the SR, Ψ , induced by a policy π. As per Eq. 15, 1 2 d π each e i gives rise to a reward function rei(s, s(cid:48)) = e(cid:62) i (cid:0) φ(s(cid:48)) − φ(s)(cid:1) , which in turn gives rise to an eigenoption ωei. As shown in Eq. 16, the eigenoption ωei can be compactly represented as a policy over an extended action space πei+. For ease of exposition, we will use πei+ to refer to ωei in this section. Suppose we have the value function of all the eigenoptions πei+ under all the rewards rej , that is, we have qrej for i, j ∈ {1, 2, ..., d}. We πei+ will now show how to instantaneously generate options associated with linear combinations of the eigenvectors e without any learning involved. i Let w ∈ Rd and let (cid:88) c = w e . (18) i i i We want to compute an approximation of the option induced by the reward rc(s, s(cid:48)) = c(cid:62)(cid:0) φ(s(cid:48)) − φ(s)(cid:1) without resorting to learning. First, we note that rc(s, s(cid:48)) = (cid:88) w e(cid:62)(cid:0) φ(s(cid:48)) − φ(s)(cid:1) = (cid:88) w rei(s, s(cid:48)). i i i i i Connecting the above with Eq. 13, we see that here we are using the intrinsic rewards rei as features. This view allows us to resort to Eq. 14 to compute the value function of the 32
Temporal Abstraction in RL with the Successor Representation eigenoptions πei+ under the reward rc as qrc (s, a) = (cid:88) w qrej (s, a). (19) πei+ j πei+ j Once we have qrc for all i ∈ {1, 2, ..., d}, we can use GPI defined in Eq. 17 to compute πei+ π˜c+(s) ∈ argmax max qrc (s, a) for all s ∈ S. (20) a∈A∪{⊥} i πei+ Note that, since π˜c+ is defined over an extended action space which also includes the termi- nate action ⊥, it gives rise to a well-defined option, including the initiation and termination sets (see discussion in Section 5). The option computed in Eq. 20 is an approximation of the option πc+ induced by c, that is, π˜c+ ≈ πc+. The advantage of using π˜c+ is that, unlike πc+, it can be obtained without any learning involved. Based on the results regarding GPI, we know that π˜c+ will perform at least as well as, and generally better than, any of the options πei+ under the reward rc. This means that the larger the number of options πei+ used in Eq. 20 the closer π˜c+ will be to πc+. In any case, it is worth noting that, since we are using options mostly to generate diverse behavior, an eventual sub-optimal performance of π˜c+ should not have a catastrophic effect. Putting it all together. We now summarize how the procedure above can be combined with eigenoptions to considerably enlarge the number of options used for exploration. Given a set of eigenvectors e , e , ..., e , the first thing we do is to compute the induced eigenop- 1 2 d tions ωe1, ωe2, ..., ωe d, which here we represent as policies defined over an augmented action space: πe1+, πe2+, ..., πe d+. Then, we evaluate each πei+ under the reward functions in- duced by the eigenvectors, that is, we compute qrej for i, j ∈ {1, 2, ..., d} (obviously, we πei+ can compute qrej while we learn the options πei+—see, for example, Barreto et al.’s (2019) πei+ Algorithm 3). Once we have the value functions qrej , the successive application of Eq. 18, πei+ 19 and 20 with any w ∈ Rd results in an option π˜c+ that approximates the option πc+ in- (cid:80) duced by c = w e . See Algorithm 3 for a presentation of this discussion in pseudo-code. i i i We can think of the process above as implementing a mapping from w ∈ Rd to an approximation of the corresponding option πc+, where c = (cid:80) w e . This means that we i i i immediately have at our disposal a potentially very large set of options induced by all possible instantiations of the vector w. If w happens to only have one non-zero element that is positive, we recover one of the eigenoptions πei+ induced by the eigevectors e i. For other instantiations of w we have options whose behavior can significantly deviate from that of the original eigenoptions (Barreto et al., 2019). In the next section we use experiments to illustrate the benefits of combining the option keyboard with eigenoptions. 9 Combining Eigenoptions with the Option Keyboard In this section, we demonstrate the synergy between eigenoptions and the option key- board. As mentioned above, the option keyboard allows one to extend a finite set of options to a combinatorially large counterpart without additional learning. Nevertheless, the option 33
Machado, Barreto, Precup, and Bowling Algorithm 3: Option Keyboard Input: Qre ; (cid:46) Matrix of Q-values for each reward-policy combination π ωe ωe; (cid:46) Set of options generated from intrinsic rewards e i w ; (cid:46) Weights to be used to combine behaviors Output: ω˜ ; (cid:46) Options generated by the OK from the base options (cid:46) GPE for ωei in ωe do Qrc ← w(cid:62)Qre π ωei π ωei I ← S ω˜ for s in S do (cid:46) GPI π+(s) ← arg max max Qrc (s, a) ω˜ a i π ωei (cid:46) Converts π+(s) into a regular option ω˜ if π+(s) = ⊥ then ω˜ π (s) ← random action in A ω˜ I ← I \ {s} ω˜ ω˜ β (s) ← 1 ω˜ else π (s) ← π+(s) ω˜ ω˜ β (s) ← 0 ω˜ ω˜ ← (cid:104)I , π , β (cid:105) ω˜ ω˜ ω˜ keyboard assumes an initial set of basis options is available beforehand, with existing results in the literature relying on handcrafted options as basis options. When considering discov- ered options, eigenoptions are natural candidates as basis options. They are autonomously discovered from the SR, the same object that makes the option keyboard computationally efficient, and they are generated by orthogonal vectors obtained from the agent’s behav- ior. Intuitively, using the option keyboard to combine eigenoptions is the equivalent of computing linear combinations of orthogonal bases of behaviors. We first present a qualitative analysis of the options generated by combining eigenoptions with the option keyboard. We present multiple eigenoptions and the options the option keyboard generates from them. We discuss the number of unique combinations and how diverse the generated options are. We then present a quantitative analysis focused on the diffusion time induced by these new options generated by the option keyboard. We conclude this section with a higher-level discussion about the benefits of combining eigenoptions with the option keyboard, and potential avenues for future work. 9.1 Options Combined through the Option Keyboard are Diverse We define the set of basis options to be the first ten eigenoptions and we linearly combine these options with the option keyboard. Because there are infinite possible weight com- binations, we constrain ourselves to {0, 1} combinations at first, meaning weights can be 34
Temporal Abstraction in RL with the Successor Representation Algorithm 4: OK-Eigenoptions Input: Ω ; (cid:46) Base options C ∈ R|Ω|×|S|×|A| ; (cid:46) Reward function used to generate each option in Ω γ ; (cid:46) Discount factor Output: Ω ; (cid:46) Options generated by the OK from the base options w (cid:46) Compute the matrix of Q-values induced by each intr. reward rej and option ωei for ωei in Ω do for rej in C do Q πre ωj ei ← PolicyEvaluation(π ωei , rej , γ) (cid:46) Using the option-keyboard, generate combination of eigenoptions with weights w Ω ← ∅ OK for all permutations w in [−1, 0, 1]|Ω| do Ω ← Ω ∪ Option Keyboard(Qre , w) OK OK π ωe 1st eigenoption 2nd eigenoption 3rd eigenoption 4th eigenoption 10th eigenoption Figure 15: Eigenoptions discovered in the open-room. As in previous results, their ordering is defined by the eigenvalues corresponding to the eigenvectors that gave rise to each option. Both directions of the eigenvectors are taken into consideration. either 0 or 1, and later, to {−1, 0, 1} combinations, as shown in Algorithm 4. We perform our experiments in the four-room domain and in an open 10 × 10 gridworld, which we name open-room (see Figure 15). The latter is particularly useful for this set of experiments because it allows us to build intuitions about the algebra of the options without being dis- tracted by walls and asymmetries. For reference, some of the eigenoptions used as basis options are depicted in Figures 15 and 27, the latter in Appendix E. In the open-room, when combining the first four eigenoptions in sets of two, the agent recovers cardinal directions, as shown in Figure 16. This is an interesting result because it shows a general method naturally discovering important abstractions. Besides that, we also observe the union of different options (e.g., going to the closest corner) and, once an eigenoption that takes the agent to the center of the room is available, the option keyboard generates combinations that together terminate in most states in the environment. In the four-room domain, we can draw similar conclusions, despite the asymmetric walls preventing behaviors as interpretable as those in the open-room. We see options that take the agent to specific walls/directions, to specific rooms, and to bottleneck states, as shown in Figure 17. 35
Machado, Barreto, Precup, and Bowling [1,0,0,1,0,0,0,0,0,0] [1,0,1,0,0,0,0,0,0,0] [0,1,0,1,0,0,0,0,0,0] [0,1,1,0,0,0,0,0,0,0] [0,0,0,0,0,0,1,0,0,1] [1,1,0,0,0,0,0,0,1,0] [1,1,0,0,0,0,0,1,0,0] [0,0,0,0,0,1,1,0,0,0] [0,0,1,0,0,1,0,0,0,1] [1,0,0,1,0,1,0,0,0,1] Figure 16: Options obtained by combining eigenoptions with the option keyboard. The weights used to generate them are above the option, where the i-th entry corre- sponds to the weight given to the i-th eigenoption. Figure 17: Options obtained by combining eigenoptions with the option keyboard. The weights used to generate them are above the option, where the i-th entry corre- sponds to the weight given to the i-th eigenoption. 36
Temporal Abstraction in RL with the Successor Representation 150 100 50 0 0 5 10 Number of Basis Options snoitpO weN fo rebmuN 10 × 10 Gridworld 150 115 100 62 55 50 33 1 2 5 8 9 10 0 0 5 10 Number of Basis Options snoitpO weN fo rebmuN Four-room Domain 137 72 34 1 2 5 7 8 11 21 Figure 18: Number of unique options generated by combining eigenoptions. We consider two options to be the same if they have the same set of terminal states. In the open-room, for example, the 1st eigenoption leads to a single option: itself. Adding a 2nd eigenoption only leads to an extra option because the 1st and 2nd eigenoptions stem from the same eigenvector, thus they cancel each other when combined. Adding a 3rd eigenoption leads to two extra options (combination of the 1st and 3rd, as well as the 2nd and 3rd), totalling 5 options (instead of 3). This difference becomes starker as more basis options become available. The option keyboard also leads to a combinatorial explosion of new options, even when we only consider {0, 1} combinations. This is shown in Figure 18, which reports the number of unique options generated by the option keyboard. The number of new options is not 2n, where n is the number of basis options, because two options can be added together to cancel each other, for example, when they are derived from both directions of the same eigenvector. Additionally, different combinations can lead to the same option. So far we have shown that combining eigenoptions with the option keyboard leads to interesting, semantically meaningful, options, as well as a large number of options. We conclude this section showing these options are diverse. We do so by looking at the frequency at which these options terminate in different states. Figures 19 and 20 depict heatmaps contrasting the set of terminal states induced by eigenoptions and those induced by the combinations of those eigenoptions. The numbers in each tile report the number of times, across the considered options, that the corresponding state is a terminal state. Colors represent the relative frequency of termination across all states. It is reassuring to see that the few basis options depicted on the left of Figures 19 and 20 can be combined to generate the diverse behaviors depicted on the right. In these environments, even when considering only {0, 1} combinations, ten options are enough for the option keyboard to generate options that visit most states in the environment. Eigenoptions are orthogonal bases of behavior that span most of the behaviors one would be interested in. In the open-room, for example, eigenoptions terminating in 16 states end up being combined to terminate in 96 states. In the next section we show this diversity in fact allows the agent to better explore the environment. 37
Machado, Barreto, Precup, and Bowling Eigenoptions Options from eigenoptions combined with the OK Figure 19: Frequency options terminate in each state in the open-room domain. Eigenoptions Options from eigenoptions combined with the OK Figure 20: Frequency options terminate in each state in the four-room domain. 9.2 Options Combined through the Option Keyboard Improve Exploration In this section, we show that the diversity introduced by the option keyboard in fact impacts an agent’s ability to explore the environment. Figures 21 and 22 depict the diffusion time induced by the first ten eigenoptions and by the options generated by the option keyboard when using the same eigenoptions as basis options. We term OK-Eigenoptions [0, 1] the set of options generated by {0, 1} combinations of eigenoptions, and OK-Eigenoptions [-1, 0, 1] the set of options generated by {−1, 0, 1} combinations, which we discuss later. We compare the diffusion time induced by these different option sets based on the number of basis options (i.e., eigenoptions) in them. This might seem unfair at first, as more options are used by OK-Eigenoptions, but this is exactly one of the benefits of the option keyboard: with an almost negligible computational cost, and no additional agent- environment interactions, a large combinatorial counterpart of the original options becomes available to the agent. Shortly, using the option keyboard to augment an agent’s option set can drastically reduce the number of eigenoptions the agent needs to effectively explore the environment. In the open-room and four-room domains, the agent needs 10 and 12 eigenoptions to make the induced exploration more effective than that obtained by a uniform random policy. When augmenting the agent with the options generated by the option keyboard, this number is reduced by 30% and 42%, respectively. Also, the median diffusion time is not affected 38
Temporal Abstraction in RL with the Successor Representation 4 1. Eigenoptions 2. OK- Eigenoptions [0,1] 3 2 3. OK-Eigenoptions [-1,0,1] 3 2 1 4. Primitive Actions 1 4 Figure 21: Average and median diffusion time in the open-room domain. Each curve de- picts a function of the number of primitive options the agent has access to, but the actual number of options used may vary, as described in the text. The per- formance of primitive actions is not depicted in the plot on the right because it is out of the reported range. The scale on the left is 100 times bigger than in the other plots due to the big impact individual options have at first. 4 3 2 1. Eigenoptions 2. OK- Eigenoptions [0,1] 3. OK-Eigenoptions [-1,0,1] 4. Primitive Actions 2 1 3 1 Figure 22: Average and median diffusion time in the four-room domain. Each curve depicts a function of the number of primitive options the agent has access to, but the actual number of options used may vary, as described in the text. The perfor- mance of primitive actions is not depicted in the plot on the right because it is out of the reported range. The scale on the left is 100 times bigger than in the other plots due to the big impact individual options have at first. when adding the options generated by the option keyboard, suggesting that these additional options improve exploration by making eigenoptions more robust. Finally, it is important to stress that we obtained these results using only {0, 1} weights. A trivial improvement to what we have done so far is to generate only one (instead of two) eigenoption from each eigenvector of the SR. The option corresponding to the opposite direction of each eigenvector can be obtained with the option keyboard by assigning a −1 weight to the corresponding eigenoption. This immediately reduces the number of options to be learned by half! We use the green line in Figures 21 and 22 to make this point obvious. 39
Machado, Barreto, Precup, and Bowling Shortly, by leveraging the option keyboard in the simplest possible way we are able to show, in both environments, that 4 eigenoptions are enough to improve exploration beyond what the uniform random policy does. This is an important result because, for the first time, we are able to obtain results that match the intuition that four options should suffice in these environments. It was not clear how to discover (and combine) such options until now. 9.3 A Big Picture View of Combining Eigenoptions with the Option Keyboard The combination of eigenoptions and the option keyboard allow these approaches to heavily benefit from each other. Eigenoptions, by construction, provide a diverse set of behaviors to the agent, but dozens of options are necessary for better exploration. The option keyboard, on the other hand, combines different options to generate a large set of new behaviors, but only once basis options are provided. The combination of the eigenoptions and the option keyboard leads to more diverse behavior that improves exploration while drastically reducing the number of options that need to be discovered. The option keyboard is also able to drastically reduce the computational cost of learning eigenoptions. Our experiments demonstrated the potential of combining an option discovery method based on the SR with the option keyboard. There are multiple directions one could pursue in a future work. An interesting direction is how to define the weights used by the option keyboard. While we used {−1, 0, 1} weights, different weights are likely to be more expres- sive, generating even more diverse behaviors. From a theoretical perspective, it would be interesting to characterize the number of basis options needed by an agent in order to, for example, ensure that there is at least one combined option that terminates in each state. This number could also be used as a measure of task complexity (or similarity). From an algorithmic perspective, it would be valuable to understand the impact of using the option keyboard to combine options estimated online, and how it can be used with multiple iterations of the ROD cycle. It would also be useful to see these ideas evaluated in the function approximation case. An immediate challenge in this case would be to identify equivalent options, something trivially done in the tabular case. Scaling these ideas to more challenging domains might also require non-linear function approximation and representation learning, problems tackled by deep supervised learning. As previously mentioned, for clarity purposes, we decided to not discuss these ideas in this paper, but there are several advances and successes of this line of work in the deep supervised learning case, such as the introduction of optimization objectives that allow non-linear function approximators such as neural networks to approximate the eigenvectors of the graph Laplacian (Pfau et al., 2019; Wang et al., 2021; Wu et al., 2019), and their use for option discovery in problems which require function approximation (Jinnai et al., 2020). We discuss some of these ideas in the next section. 10 Related Work We have discussed the role of the SR for temporal abstraction in supervised learning, focusing on options for temporally-extended exploration. Additionally, we explored how the SR can be used to seamlessly combine different options without additional learning. It is unfeasible to thoroughly discuss even all the methods for options discovery that aim at 40
Temporal Abstraction in RL with the Successor Representation exploration, let alone the diverse set of methods that do not. Instead, here we outline only the main ideas behind these other approaches. Besides additional approaches, we also discuss existing extensions of the ideas we presented to the function approximation setting, and we conclude by drawing connections between the topics we discussed and recent results in neuroscience. Naturally, there are also other approaches, not based on the SR, that share similar intuitions to those described here, such as discovering task-agnostic options that terminate in a diverse set of states. They achieve this in various ways, such as maximizing metrics like empowerment, diversity, and entropy (e.g., Eysenbach et al., 2019; Gregor et al., 2017; Hansen et al., 2020). However, to focus on approaches based on the SR, we do not further discuss them here. 10.1 Additional Option Discovery Methods based on the SR Planning and faster credit assignment are common use cases for options. These ideas are often associated with bottleneck options, that is, options that lead to states that connect different closely connected regions of the environment. The SR has also been used by option discovery methods in this setting. Stachenfeld et al. (2014, 2017), for example, explicitly searched bottleneck states by using the eigenvectors of the SR to obtain an approximate solution to the k-way normalized min-cut problem. In this case, the eigenvectors of the SR are used in a different way from what we described so far, with positive and negative elements of an eigenvector approximating different partitions, a known result from spectral graph theory (Shi and Malik, 2000). Kulkarni et al. (2016), in the context of deep supervised learning, also discovered bottleneck options through normalized cuts. Another relevant idea in this context is the algorithm named successor options (Ramesh et al., 2019). This method clusters the SR vectors and defines the clusters centroids as the options’ terminal states. In this case, even though the method is not explicitly designed to seek for bottleneck states, the empirical evidence provided shows that successor options tend to find bottleneck states much more often than eigenoptions (see Figure 5 by Ramesh et al., 2019). In the context of bottleneck options and the framework presented in Section 3, Continual Curiosity-driven Skill Acquisition (CCSA; Kompella et al., 2017) is also relevant to our discussion. CCSA also discovers options that maximize an intrinsic reward obtained from a learned representation, which in this case is obtained from Slow Feature Analysis (SFA; Wiskott and Sejnowski, 2002). Importantly, Sprekeler (2011) has shown that, given a specific choice of adjacency function, proto-value functions are equivalent to SFA. SFA becomes an approximation of proto-value functions if the function space used in the SFA does not allow arbitrary mappings from the observed data to an embedding. Recall that one can see proto-value functions as a special case of the eigenvectors of the SR (Machado et al., 2018), making SFA, by transitivity, connected to the ideas discussed in this paper. In the context of options for temporally-extended exploration, Bar et al. (2020) recently introduced diffusion options, which were inspired by the ideas discussed here. Instead of looking at individual eigenvectors, this approach sets out to use the full eigenspectrum of the adjacency matrix. Specifically, it uses the eigenvalues as weights to linearly combine the right and left eigenvectors of the non-symmetric lazy random walk matrix, a variation of the graph Laplacian matrix we discussed in Sections 2 and 4. Despite the intuitive similarity 41
Machado, Barreto, Precup, and Bowling between the eigenvectors of the non-symmetric lazy random walk matrix and the eigenvec- tors of the SR, there is still not a formal connection between these two objects. However, it is interesting to note that Bar et al.’s empirical results corroborate what we present in this paper. Shortly, (1) these options provide temporal-extended exploration, (2) eigenoptions tend to be more effective than covering options, and (3) diffusion options also needs to use 10 − 20 eigenvectors to be effective in gridworld domains, similar to our results in Section 6. Finally, the eigenoption-critic (Liu et al., 2017) is also relevant to this overview. It combines eigenoptions to the option-critic architecture (Bacon et al., 2017) for a single online phase for option discovery and option learning, while also taking the reward generated by the environment into consideration. Moreover, it is applicable to settings in which non-linear function approximation is required, which we discuss in the next section. Importantly, it also extends eigenoptions to continuous state spaces with the Nystr¨om approximation. Successor features and the other extensions to the function approximation case, which we discuss in the next section, naturally allow us to extend ideas such as eigenoptions and covering options to continuous state spaces. The results in Figure 2, for example, were generated in a continuous state space. But the Nystro¨m method is another interesting ap- proach that has been used in the past to extend eigenoptions to the continuous case (c.f. Mahadevan and Maggioni, 2007, for details). Shortly, it interpolates the value of eigenvec- tors computed on observed states to novel states, addressing the issue that the exact same state is rarely visited twice in continuous state spaces. For the normalized Laplacian, this is achieved with 1 (cid:88) w(s, s k) e (s) = e (s ), i (cid:112) i k 1 − λ i d(s)d(s k) i:|s−s |<(cid:15) k where λ and e denote the i-th eigenvalue and eigenvector, (cid:15) is a threshold parameter, i i s is the state observed in the past that is closest to the new observation s; w(s, s ) are k k (cid:80) the weights from s to s , and d(s) = w(s, s ), is the degree of s. Note that k k:|s−s |<(cid:15) k k d(s) is determined when a new state s is encountered while d(s ) is computed in the graph k construction phase (Liu et al., 2017). 10.2 Function Approximation and the SR In this paper, we focused on the tabular case for conceptual clarity. Nevertheless, in several problems one cannot uniquely identify the states in the environment and function approximation is required. Scaling these ideas up has been an active topic of research, with solutions proposed to both the linear and non-linear function approximation cases. In the linear function approximation case, given a set of features φ(·), Machado et al. (2017) proposed to generate the options’ intrinsic reward function with the singular vectors of the matrix obtained from the difference between current and previous observa- tions. If all transitions in the graph are sampled once, for tabular representations, this matrix recovers the same options we obtain with the combinatorial Laplacian. This is formalized in Theorem 3 in Appendix B. In the non-linear function approximation case, when neural networks are used to approximate the value function, there are many more solutions to the problem of scaling up the ideas we presented here. A direct way of doing so is by having the network also 42
Temporal Abstraction in RL with the Successor Representation outputting successor features (e.g. Barreto et al., 2020; Borsa et al., 2019; Hansen et al., 2020; Hoang et al., 2021; Kulkarni et al., 2016; Liu and Abbeel, 2021; Machado et al., 2018, 2020). This is often done by training the neural network to also minimize the loss (cid:34) (cid:35) (cid:18) (cid:19)2 L(s, s(cid:48)) = E φ(s) + γψ(cid:0) φ(s(cid:48))(cid:1) − ψ(cid:0) φ(s)(cid:1) , (21) or one of its variations, where ψ denotes successor features. Sometimes it is assumed the representation φ(·) is known beforehand (e.g. Borsa et al., 2019), while other times it is defined as the output of an inner layer of the neural network (e.g., Hoang et al., 2021; Machado et al., 2018). The latter can be unstable and, because the representation is also being learned, one needs to be careful since φ(·) = 0 is a fixed point of this minimiza- tion problem. In this case, it is common to prevent gradients to backpropagate to the representation layer, and to introduce auxiliary tasks to shape the representation learning process (Jaderberg et al., 2017). Alternatively, researchers have also been using results from graph drawing theory (Ko- ren, 2003) to design loss functions that allow the network to explicitly estimate the eigen- vectors of the Laplacian (e.g., Erraqabi et al., 2022; Wang et al., 2021; Wu et al., 2019). The loss function originally proposed, with later methods introducing variations, was (cid:34) d (cid:35) 1 (cid:88) (cid:16) (cid:17)2 G(e , . . . e ) = E e (u) − e (v) 1 d k k 2 k=1 (cid:34) (cid:35) (cid:88) (cid:16) (cid:17)(cid:16) (cid:17) +βE e (u)e (u) − δ e (v)e (v) − δ , (22) j k jk j k jk j,k where e , . . . , e are the first d eigenvectors, u and v are different states, β is a penalty 1 d weight parameter, and δ is a soft constraint used to capture the orthogonality constraint jk between different eigenvectors. Intuitively, the loss function above has an attractive and a repulsive term, which is common in contrastive losses (e.g., Li et al., 2021). The first term, the attractive one, tries to ensure consecutive states are put together in the embedding, as in the SR. The repulsive term repels the embedding of states independently sampled from the stationary distribution of the induced Markov chain, trying to make eigenvectors orthogonal to each other. In the first term, the state u is assumed to be (uniformly) sampled from the state distribution and v is defined by the environment’s dynamics. In the second, v is randomly sampled. We did not depict this in the expectations to avoid cluttering the equations. Wu et al. presents a precise discussion and the accompanying derivations. This approach, based on the graph drawing objective, has also been used to scale covering options to the deep supervised learning case (Jinnai et al., 2020). This is what was used to generate Figure 2. Finally, there have also been attempts to scale up the ideas discussed here by gener- ating abstract transition graphs, with a neural network being used to generate the state abstractions (Hoang et al., 2021; Mendonc¸a et al., 2019). The work by Hoang et al. (2021) is particularly interesting as it can be seen as instantiating multiple iterations of the ROD cycle while dealing with function approximation. Shortly, Hoang et al. use a neural net- work to learn successor features, as outlined in Eq. 21, and they define successor feature 43
Machado, Barreto, Precup, and Bowling similarity, which is the dot-product between two SFs, to measure the distance between two different states; using those to define landmark states that the agent plans to visit. When in a landmark (abstract) state considered to be in the frontier of the agent’s experience, the agent starts to act randomly to further explore the environment. 10.3 The Relationship of the SR to Other Ideas in RL and Other Fields As mentioned in Section 4, the SR is directly related to several other ideas in supervised learning. As already discussed, it is related to proto-value functions (Machado et al., 2018) and slow-feature analysis (Sprekeler, 2011), and it can be seen as encoding the LSTD matrix (Lagoudakis and Parr, 2003). Moreover, the SR can be seen, for example, as a form of dual approach to value-function based methods (Wang et al., 2007). In this formalism, which has been shown to avoid the risk of divergence, one maintains an explicit representation of visit distributions instead of value functions. For exploration, it has been shown that the norm of the SR implicitly encodes state-visitation counts (Machado et al., 2020). It is particularly interesting to observe that methods introduced for computational rein- forcement learning are related to recent results in neuroscience and human behavior. From a neuroscience perspective, Stachenfeld et al. (2014, 2017) have suggested the SR is able to model activations in the hippocampus. Specifically, they argue that hippocampal place fields encode a predictive representation of the current and future states under the current transition distribution. Additionally, and directly related to the option discovery methods we presented here, Stachenfeld et al. (2014, 2017) present results demonstrating that the eigenvectors of the SR model grid cells activations, also suggesting that the entorhinal cortex may use that information to aid hierarchical supervised learning. This result is moti- vated by the fact that the eigenvectors of the SR allow one to capture natural boundaries, potentially enconding metric information about the space. From a planning perspective, they show how one can use the eigenvectors of the SR to identify bottleneck states, which are convenient waypoints for likely being traversed along many optimal trajectories (see Solway et al., 2014). On a higher level of abstraction, the SR has also been used to explain human behavior, specifically the notion of choice in humans. Momennejad et al. (2017), for example, has suggested the SR can be used to introduce a “subtler, more cognitive notion of habit”. They argue that the SR can be seen as an underlying computational procedure that allows humans to balance the flexibility of model-based methods and the efficiency of model-free methods. Even more directly related to the concepts we presented, Tomov et al. (2021) also discusses the SR in the context of human decision making, suggesting it can be used, alongside GPI, as a model for human decision making in multi-task scenarios. In summary, the ideas discussed in this paper around the SR, such as using it as a representation, for efficient transfer learning, or for option discovery, seem to model well data collected from intelligent, biological animals. Although this does not necessarily mean that these ideas are the correct way of tackling the problems discussed here, these results are encouraging evidence that it might be worth investigating these ideas further. 44
Temporal Abstraction in RL with the Successor Representation 11 Conclusion In this paper, we discussed the role of the successor representation (SR) when using tem- poral abstractions in supervised learning. We presented a general framework for option discovery, termed Representation-driven Option Discovery cycle, which follows a construc- tivist approach, and we examined two instantiations of this cycle, executed for different numbers of iterations. These instantiations use the eigenvectors of the SR to discover op- tions for temporally-extended exploration, thoroughly evaluating them and shedding light on decisions every option discovery method should make, such as on how to define the initi- ation set and termination condition of each option. Moreover, we discussed how the SR can also be used to address the inevitable trade-off every agent is subject to: to discover more options in order to have access to a more expressive set of behaviors, or to restrict them to facilitate learning. This is done through the option keyboard, which extends, without addi- tional learning, a finite set of options to a combinatorially large counterpart. The empirical evaluation of using the option keyboard to combine options discovered with the SR provides encouraging evidence of the synergy of these approaches, with this combination drastically reducing the computational cost of option learning while increasing the expressivity of the options available to the agent. The goal of this paper was to provide a unified perspective over different approaches for temporal abstraction in supervised learning, showing how they can all be cast as discovering options from the SR, and how these ideas have evolved throughout the time. Naturally, there are several extensions of these approaches in order to scale them up to problems with large state spaces. Moreover, there is increasing evidence that the SR also plays an important role, at different levels of abstraction, in human and animal decision- making. We believe using the SR as the main substrate for temporal abstraction is a promising research direction and we hope this paper serves as a catalyst to this line of work. In particular, we believe that a cycle such as the one described here might end up being a core component of agents that are capable to continually learn and to acquire increasingly complex skills. Intelligent agents should be constantly acquiring new data, and this data should allow the agent to refine its representation of the world, which in turn should serve to inform which temporal abstractions an agent should learn, further empowering the agent’s ability to collect new data, in a virtuous cycle of discovery. Acknowledgements This work was partially developed while Marlos C. Machado was at Google Research, Brain Team. The authors would like to thank Tom Schaul, Adam White, and the anonymous reviewers for their thorough feedback on an earlier draft; and Dale Schuurmans, Yuu Jinnai, Marc G. Bellemare, and Patrick Pilarski for useful discussions. Marlos C. Machado, Doina Precup, and Michael Bowling are supported by a Canada CIFAR AI Chair. 45
Machado, Barreto, Precup, and Bowling Appendix A. Proto-value Functions and their Equivalence to the Eigenvectors of the SR As mentioned in the main paper, the SR is present in several RL algorithms, either explic- itly or implicitly. An important result for this paper is that the eigenvectors of the SR are equivalent to proto-value functions (PVFs; Mahadevan, 2005; Machado et al., 2018).8 Be- cause these properties were first discussed in the PVFs literature, we further discuss PVFs and their properties here. Proto-value funtions (PVFs; Mahadevan, 2005) were originally introduced as represen- tations that reflect the geometry of the environment. Specifically, they are basis functions based on the notion of diffusion models (Coifman et al., 2005; Kondor and Lafferty, 2002), which capture how information flows in the environment by modeling it as a graph con- necting states that are one action away from each other. This is motivated by the fact that value functions can be seen as the result of rewards diffusing through the state space, governed by the environment dynamics (Mahadevan and Maggioni, 2007). Formally, PVFs are the eigenvectors of a symmetric diffusion operator such as the normalized Laplacian, L = D− 1 2 (D − W)D− 1 2 , (23) where W is the graph’s adjacency matrix and D the diagonal matrix whose entries are the row sums of W. In the simplest setting, for states s and s , the ij-th entry of matrix W is i j (cid:26) 1, if p(s |s , a) > 0, W = j i ij 0, otherwise, for any action a ∈ A. Notice the matrix W can potentially be extended to a weight matrix. These diffusion models are tightly related to the random walk diffusion model D−1W. A diffusion model works as a surrogate that is easier to estimate than the full transition matrix, while being useful for value function approximation because we can represent the value function as a linear combination of the eigenvectors of the transition matrix, as shown in Eq. 3. See the work by Mahadevan and Maggioni (2007) for a detailed discussion. The formal result of equivalence between PVFs and the eigenvectors of the SR is below. Theorem 2 (Machado et al. 2018) Let n be the number of rows (and columns) of ma- trix P , and let i and j denote indices such that i + j = n + 1. The i-th eigenvalue of π the SR, defined w.r.t. a uniform random policy, and the j-th eigenvalue of the normalized Laplacian are related as follows when in a symmetric and deterministic environment: (cid:104) (cid:105) λ = 1 − (1 − λ−1 )γ−1 . PVF,j SR,i The i-th eigenvector of the SR, e , and the j-th eigenvector of the normalized Laplacian, SR,i e , are related as follows: PVF,j e = (γ−1D1/2)e . PVF,j SR,i 8. This holds when the SR is defined w.r.t. the uniform random policy in a deterministic and symmetric environment. The cardinality of the action set should also be the same across all states. 46
Temporal Abstraction in RL with the Successor Representation Proof See Appendix B. Notice that while the eigenvectors of the normalized Laplacian and the eigenvectors of the SR are the same, the order in which they appear is flipped. The eigenvectors with corresponding lowest eigenvalues, when using PVFs, are equivalent to the eigenvectors with corresponding largest eigenvalues when using the SR. Importantly, the eigenvectors of the SR are equivalent to PVFs in a symmetric and deterministic MDP, that is, when every transition is reversible and action outcomes are deterministic. Thus, while the SR reduces to PVFs in a more restrictive case, it is also applicable to the more general case, allowing us to more easily capture stochasticity and asymmetries in the environment. As discussed in the main paper, PVFs capture properties of the dynamics environment and this is the main motivation behind several option discovery methods. Importantly, PVFs were originally introduced as basis functions for function approximation and they ended up not being widely adopted. This paper did not study the use of PVFs as basis functions. This is a major difference from how PVFs were used in the past. In this paper we advocate for the use of these concepts for the discovery and combination of temporal abstractions. Moreover, the singular vectors of the SR can already be seen as a generaliza- tion of PVFs. Thus, the shortcomings of PVFs in the past should not be carried over when considering their use for temporal abstraction. Appendix B. Theoretical Results We present the theoretical results below for completeness. These results were obtained in other works and we present them, as well as their proofs, in their original version. We cite them accordingly before each theorem. We refer the reader to the works by Machado et al. (2017, 2018) for further details. Lemma 1 (Machado et al. 2017) Suppose (I + A) is a non-singular matrix, with ||A|| ≤ 1. We have: 1 ||(I + A)−1|| ≤ . 1 − ||A|| Proof (I + A)(I + A)−1 = I I(I + A)−1 + A(I + A)−1 = I (I + A)−1 = I − A(I + A)−1 ||(I + A)−1|| = ||I − A(I + A)−1|| ≤ ||I|| + ||A(I + A)−1|| ≤ 1 + ||A||||(I + A)−1|| 47
Machado, Barreto, Precup, and Bowling ||(I + A)−1|| − ||A||||(I + A)−1|| ≤ 1 (1 − ||A||)||(I + A)−1|| ≤ 1 1 ||(I + A)−1|| ≤ if ||A|| ≤ 1. 1 − ||A|| Where the first inequality is due to the fact that ||A + B|| ≤ ||A|| + ||B|| and the second inequality comes from the fact that ||AB|| ≤ ||A|| · ||B||. Lemma 2 (Machado et al. 2017) The induced infinity norm of (I − γT)−1T is bounded by 1 ||(I − γT)−1T|| ≤ . ∞ (1 − γ) Proof ||(I − γT)−1T|| ≤ ||(I − γT)−1|| ||T|| because ||AB|| ≤ ||A|| · ||B|| ∞ ∞ ∞ ∞ ∞ ∞ 1 ||(I − γT)−1T|| ≤ ||T|| Lemma 1 ∞ ∞ 1 − || − γT|| ∞ 1 ||(I − γT)−1T|| ≤ ||T|| because ||λBs|| = |λ|||B|| ∞ ∞ 1 − γ||T|| ∞ 1 ||(I − γT)−1T|| ≤ ∞ (1 − γ) Theorem 1 (Machado et al. 2017) Let ω = (cid:104)I , π , β (cid:105) denote an eigenoption. In a ω ω ω finite and ergodic MDP with γ ∈ [0, 1), there is at least one state s ∈ S such that β (s) = 1. ω Proof We can write the Bellman equation in the matrix form: v = r + γP v, for a fixed π policy π, where v is a finite column vector with one entry per state encoding its value function. From Equation 15 we have r = P w − w with w = Φe, where e denotes the π eigenvector of interest and Φ ∈ R|S|×d denotes the matrix representing the d-dimensional feature representation for each state. We use r : S → R for simplicity. This function can be seen as the expected reward in a given state. With that we have: v = P w − w + γP v π π v + w = P w + γP v π π = P w + γP v + γP w − γP w π π π π = (1 − γ)P w + γP (v + w) π π v + w − γP (v + w) = (1 − γ)P w π π (I − γP )(v + w) = (1 − γ)P w π π v + w = (1 − γ)(I − γP )−1P w π π 48
Temporal Abstraction in RL with the Successor Representation where the last step is true because (I − γP )−1 is guaranteed to be nonsingular since π ||P || ≤ 1, where ||P || = sup ||P v|| . By the Neumann series we have (I − π π v:||v||∞=1 π ∞ γP )−1 = (cid:80)∞ γnP n. Using the induced norm we have: π n=0 π ||v + w|| = (1 − γ)||(I − γP )−1P w|| ∞ π π ∞ ||v + w|| ≤ (1 − γ)||(I − γP )−1P || ||w|| because ||Ax|| ≤ ||A|| · ||x|| ∞ π π ∞ ∞ 1 ||v + w|| ≤ (1 − γ) ||w|| Lemma 2 ∞ ∞ (1 − γ) ||v + w|| ≤ ||w|| ∞ ∞ We can shift w by any finite constant without changing the reward, that is, P w − w = π (cid:80) P (w + δ) − (w + δ) because P 1δ = 1δ since P = 1. Therefore, we can as- π π j πi,j sume w ≥ 0. Let s∗ = arg max w , so that w = ||w|| . Clearly v ≤ 0, otherwise s s∗ s∗ ∞ s∗ ||v + w|| ≥ |v + w | = v + w > w = ||w|| , arriving at a contradiction. ∞ s∗ s∗ s∗ s∗ s∗ ∞ Theorem 2 (Machado et al. 2018) Let n be the number of rows (and columns) of ma- trix P , and let i and j denote indices such that i + j = n + 1. The i-th eigenvalue of π the SR, defined w.r.t. a uniform random policy, and the j-th eigenvalue of the normalized Laplacian are related as follows when in a symmetric and deterministic environment: (cid:104) (cid:105) λ = 1 − (1 − λ−1 )γ−1 . PVF,j SR,i The i-th eigenvector of the SR, e , and the j-th eigenvector of the normalized Laplacian, SR,i e , are related as follows: PVF,j e = (γ−1D1/2)e . PVF,j SR,i Proof Let λ , e denote the i-th eigenvalue and eigenvector of the SR, respectively. Using i i the fact that the SR converges to (I − γP )−1 (through the Neumann series), we have: π (I − γP )−1e = λ e π i i i e = λ (I − γP )e i i π i (I − γP )e = λ−1e π i i i (I − γP )γ−1e = λ−1γ−1e π i i i γ−1e − P e = λ−1γ−1e i π i i i P e = γ−1e − λ−1γ−1e π i i i i = (1 − λ−1)γ−1e i i Ie − P e = Ie − (1 − λ−1)γ−1e i π i i i i (I − P )e = [γ − (1 − λ−1)]γ−1e π i i i (I − P )γ−1e = [1 − (1 − λ−1)γ−1]γ−1e π i i i (I − P )γ−1e = λ(cid:48) γ−1e (24) π i j i 49
Machado, Barreto, Precup, and Bowling (I − D−1W)γ−1e = λ(cid:48) γ−1e i j i (D−1(D − W))γ−1e = λ(cid:48) γ−1e i j i D1/2(D−1(D − W))γ−1e = λ(cid:48) γ−1D1/2e i j i D−1/2(D − W)γ−1e = λ(cid:48) γ−1D1/2e i j i D−1/2(D − W)D−1/2D1/2γ−1e = λ(cid:48) γ−1D1/2e i j i LD1/2γ−1e = λ(cid:48) γ−1D1/2e i j i Lemma 3 (Machado et al. 2017) In the tabular case, if all transitions in the MDP have been sampled once, T(cid:62)T = 2L. Proof Let t and tt denote the entries in the i-th row and j-th column of matrices T ij ij and T(cid:62)T. We can write tt as: ij (cid:88) tt = t × t . (25) ij ik jk k In the tabular case, t has three possible values: ij • t = +1, meaning that the agent arrived in state j at time step i, ij • t = −1, meaning that the agent left state j at time step i, ij • t = 0, meaning that the agent did not arrive nor leave state j at time step i. ij We decompose T(cid:62)T in two matrices, K and Z, such that T(cid:62)T = K + Z. Here Z is a diagonal matrix such that z = tt , for all i; and K contains all elements from T(cid:62)T that ii ii lie outside the main diagonal. When computing the elements of Z we have i = j. Thus z = (cid:80) t2 . Because we ii k ik square all elements, we are in fact summing over all transitions leaving (−12) and arriving (12) in state i, counting the node’s degree twice. Thus, Z = 2Ds. When not computing the elements in the main diagonal, for the element tt , we add ij all transitions that leave state i arriving in state j (−1 × 1), and those that leave state j arriving in state i (1 × −1). We assume each transition has been sampled once, thus: (cid:26) −2, if the transition between states i and j exists, tt = ij 0, otherwise. Therefore, we have K = −2W and T(cid:62)T = K + Z = 2(D − W). Theorem 3 (Machado et al. 2017) Consider the singular value decomposition of the matrix T s.t. T = UΣV, with each row of T consisting of the difference between observa- tions, i.e., φ(s(cid:48)) − φ(s). In the tabular case, if all transitions in the MDP have been sampled once, the orthonormal vectors of L = D − W are the columns of V(cid:62). 50
Temporal Abstraction in RL with the Successor Representation Proof Given the SVD decomposition of a matrix A = UΣV(cid:62), the columns of V are the eigenvectors of A(cid:62)A (Strang, 2005). We know that T(cid:62)T = 2L, where L = D − W (Lemma 3). Thus, the columns of V are the eigenvectors of T(cid:62)T, which can be rewritten as 2(D − W). Therefore, the columns of V are also the eigenvectors of L. Appendix C. Pseudo-code for the Discussed Algorithms Algorithm 5 and 6 summarize eigenoption discovery. They present in an algorithm box the algorithm outlined in Section 5. Of note, is the assumption that the function eigen- decomposition returns eigenvectors sorted by their eigenvalues—we return the eigenvalues from this function only for clarity, but they are not used anywhere. The matrix E contains all eigenvectors. In the closed-form algorithm, we assume access to the transition matrix P . In the online case, we use U to denote a uniform distribution and we use || to represent the operation of appending a transition to the data set. To be able to write the covering options algorithm in closed-form more succinctly, we assumed access to a getInducedTransitionMatrix function that receives as input the ma- trix P that contains the underlying transition dynamics, as well as the option set Ω, and returns a new transition matrix induced by such options. For simplicity, we also define a getTopEigenvector function, and we abstract away the fact that we use both directions of each eigenvector (we do the same for eigenoptions). In the online case, notice we store temporally extended transitions when an option is sampled, as if the agent had teleported from one state to another. 51
Machado, Barreto, Precup, and Bowling Algorithm 5: Eigenoptions Closed-Form Input: γ , γ ; (cid:46) Discount factor for the SR and the options’ policies SR o P ∈ R|S|×|S| ; (cid:46) Transition matrix Output: Ω ; (cid:46) Set of eigenoptions (cid:46) Learn representation Ψ ← (I − γ P )−1 SR λ, E ← eigendecomposition(Ψ) Ω ← ∅ for e in E do (cid:46) Derive intrinsic reward function from learned representation re(s, s(cid:48)) ← e(s(cid:48)) − e(s) ∀s, s(cid:48) ∈ S ; (cid:46) Define eigenpurpose (cid:46) Learn to maximize intrinsic reward Q ← PolicyIteration(re, P, γ ) ; (cid:46) Learn value function o (cid:46) Define option I ← ∅; π(s) ← ⊥, β(s) ← 1 ∀s ∈ S ; (cid:46) Initialize option tuple for s in S do (cid:46) If s is not a terminal state for the eigenoption being learned: if ∃a ∈ A(s) s.t. Q(s, a) > 0 then I ← I ∪ {s} π(s) ← arg max Q(s, a) a β(s) ← 0 Ω ← Ω ∪ (cid:104)I, π, β(cid:105) 52
Temporal Abstraction in RL with the Successor Representation Algorithm 6: Eigenoptions Online Input: η, α ; (cid:46) Step-sizes for learning the SR and the options’ policies o γ , γ ; (cid:46) Discount factor for the SR and the options’ policies SR o N ; (cid:46) Maximum number of interactions with the environment steps Output: Ω ; (cid:46) Set of eigenoptions D ← ∅ (cid:46) Collect samples for i ← 0 to N do steps (cid:16) (cid:17) a ← U A(s) s ; (cid:46) Choose action randomly In state s, take action a and observe state s(cid:48) D ← D (cid:107) (s, a, s(cid:48)) ; (cid:46) Append transition to data set D (cid:46) Learn representation Ψ ← Successor Representation(η, γ , D) SR λ, E ← eigendecomposition(Ψ) Ω ← ∅ for e in E do (cid:46) Derive intrinsic reward function from learned representation re(s, s(cid:48)) ← e(s(cid:48)) − e(s) ∀s, s(cid:48) ∈ S ; (cid:46) Define eigenpurpose (cid:46) Learn to maximize intrinsic reward Q ← Q-Learning(re, α , γ ) ; (cid:46) Learn value function o o (cid:46) Define option I ← ∅; π(s) ← ⊥, β(s) ← 1 ∀s ∈ S ; (cid:46) Initialize option tuple for s in S do (cid:46) If s is not a terminal state for the eigenoption being learned: if ∃a ∈ A(s) s.t. Q(s, a) > 0 then I ← I ∪ {s} π(s) ← arg max Q(s, a) a β(s) ← 0 Ω ← Ω ∪ (cid:104)I, π, β(cid:105) 53
Machado, Barreto, Precup, and Bowling Algorithm 7: Covering Options Closed-Form Input: γ , γ ; (cid:46) Discount factor for the SR and the options’ policies SR o P ∈ R|S|×|S| ; (cid:46) Transition matrix N ; (cid:46) Number of iterations iter Output: Ω ; (cid:46) Set of covering options Ω ← ∅ for i in N do iter (cid:46) Learn representation T ← getInducedTransitionMatrix(P, Ω) Ψ ← (I − γ T )−1 SR (cid:46) Derive intrinsic reward function from learned representation e ← getTopEigenvector(Ψ) re(s, s(cid:48)) ← 0 ∀s, s(cid:48) ∈ S ; (cid:46) Define reward function re(s, arg max e) = 1 ∀s ∈ S (cid:46) Learn to maximize intrinsic reward Q ← PolicyIteration(re, P, γ ) ; (cid:46) Learn value function o (cid:46) Define option I ← ∅; π(s) ← ⊥, β(s) ← 1 ∀s ∈ S ; (cid:46) Initialize option tuple I ← arg min e π(s) ← arg max Q(s, a) a β(arg max e) ← 1 Ω ← Ω ∪ (cid:104)I, π, β(cid:105) 54
Temporal Abstraction in RL with the Successor Representation Algorithm 8: Covering Options Online Input: η, α ; (cid:46) Step-sizes for learning the SR and the options’ policies o γ , γ ; (cid:46) Discount factor for the SR and the options’ policies SR o N ; (cid:46) Maximum number of interactions with the environment steps N ; (cid:46) Number of option discovery iterations iter Output: Ω ; (cid:46) Set of covering options Ω ← ∅ for i in N do iter D ← ∅ (cid:46) Collect samples for i ← 0 to N do steps ω ← U (cid:0)A(s) ∪ Ω(cid:1) ; (cid:46) Choose action (or option) randomly if ω ∈ A(s) then In state s, take action a and observe state s(cid:48) else Act according to sampled option ω from s until termination in s(cid:48) D ← D (cid:107) (s, ω, s(cid:48)) ; (cid:46) Append transition to data set D (cid:46) Learn representation Ψ ← Successor Representation(η, γ , D) SR (cid:46) Derive intrinsic reward function from learned representation e ← getTopEigenvector(Ψ) re(s, s(cid:48)) ← 0 ∀s, s(cid:48) ∈ S ; (cid:46) Define reward function re(s, arg max e) = 1 ∀s ∈ S (cid:46) Learn to maximize intrinsic reward Q ← Q-Learning(re, α , γ ) ; (cid:46) Learn value function o o (cid:46) Define option I ← ∅; π(s) ← ⊥, β(s) ← 1 ∀s ∈ S ; (cid:46) Initialize option tuple I ← arg min e π(s) ← arg max Q(s, a) a β(arg max e) ← 1 Ω ← Ω ∪ (cid:104)I, π, β(cid:105) 55
Machado, Barreto, Precup, and Bowling Appendix D. Additional Results for Return Maximization with Eigenoptions and Covering Options Task Eigenoptions Covering options G S S G G S Figure 23: Performance of Q-learning augmented with eigenoptions and covering options in additional tasks as described in Section 6.2. In the task, S and G denote the start and goal state. Results are averaged across 50 runs and shaded regions denote a 99% confidence interval. See text for details. 56
Temporal Abstraction in RL with the Successor Representation Task Eigenoptions Covering options G S G S G S Figure 24: Performance of Q-learning augmented with eigenoptions and covering options in additional tasks as described in Section 6.2. In the task, S and G denote the start and goal state. Results are averaged across 50 runs and shaded regions denote a 99% confidence interval. See text for details. 57
Machado, Barreto, Precup, and Bowling Task Eigenoptions Covering options G S G S G S Figure 25: Performance of Q-learning augmented with eigenoptions and covering options in additional tasks as described in Section 6.2. In the task, S and G denote the start and goal state. Results are averaged across 50 runs and shaded regions denote a 99% confidence interval. See text for details. 58
Temporal Abstraction in RL with the Successor Representation Appendix E. Eigenoptions and Covering Options Learned Online 100 episodes 500 episodes 1000 episodes closed-form 1st eigenvector 2nd eigenvector 3rd eigenvector 4th eigenvector Figure 26: Top four eigenvectors of the SR when learning eigenoptions online for a varied number of episodes. We randomly selected one of the fifty runs to plot. The agent interacted with the environment for 1,000 steps in each episode. 59
Machado, Barreto, Precup, and Bowling 100 episodes 500 episodes 1000 episodes closed-form 1st eigenvector 2nd eigenvector 3rd eigenvector 4th eigenvector Figure 27: First four eigenoptions discovered online for a varied number of episodes, com- puted from the eigenvectors in Figure 26. We plot only one eigenoption per eigenvector (instead of two). Policies were learned off-policy from the data col- lected when learning the SR. 60
Temporal Abstraction in RL with the Successor Representation 100 episodes 500 episodes 1000 episodes closed-form 1st iteration 2nd iteration 3rd iteration 4th iteration Figure 28: Top four eigenvectors of the SR when learning covering options online for a varied number of episodes. We randomly selected one of the fifty runs to plot. The agent interacted with the environment for 1,000 steps in each episode. 61
Machado, Barreto, Precup, and Bowling 100 episodes 500 episodes 1000 episodes closed-form 1st iteration 2nd iteration 3rd iteration 4th iteration Figure 29: First four covering options discovered online for a varied number of episodes, computed from the eigenvectors in Figure 28. We plot only one covering option per eigenvector (instead of two). Policies were learned off-policy from the data collected when learning the SR. Notice how the covering options discovered online in the second and third iteration overlap, and how none of the first four options capture the NW-SE diagonal when discovered online. 62
Temporal Abstraction in RL with the Successor Representation Appendix F. Impact of using the Eigenvectors of the SR instead of the Eigenvectors of the Laplacian when Discovering Covering Options The theoretical guarantees we discussed for the equivalence between the eigenvectors of the graph Laplacian and the eigenvectors of the SR do not hold in later iterations of covering options. In the four-room domain, for example, after the first iteration of covering options, in some states the agent has access to four primitive actions and one option, while only four actions are available in the other states. This creates an asymmetry that violates one of the assumptions of Theorem 2. One of the consequences of this is that, in practice, when estimating the SR instead of the graph Laplacian, we sometimes observe eigenvectors with a non-zero complex part. We empirically evaluated, in closed-form, the impact of using the SR to learn covering options, ignoring its complex component, which is what we actually used in Section 6.4. We used the same parameters we used when learning eigenoptions, also learning the options’ policies off-policy. The results are depicted in Figure 30. Despite some variations, at least in the four-room domain, when computing covering options in closed-form, the mismatches between the eigenvectors of the Laplacian and of the SR do not lead to very different results.9 1400 1100 800 500 200 0 10 20 30 Number of Options emiT noisuffiD 104 1400 103 1100 102 0 10 20 30 800 500 200 0 10 20 30 Number of Options emiT noisuffiD Covering Options (Laplacian) Covering Options (SR) Primitive Actions Figure 30: Average (left) and median (right) diffusion time in the four-room domain induced by covering options when computed with the SR and the eigenvectors of the graph Laplacian, in closed-form. The inset plot on the left figure depicts, in log-scale, the range the diffusion time lies. 9. Because the environment we consider is ultimately symmetric, to avoid asymmetries in the SR, after we estimate the SR matrix Ψ, we actually compute the eigendecomposition of the matrix (Ψ + Ψ(cid:62))/2. Alternative solutions for dealing with the asymmetry of time-based representations include using a singular value decomposition (Machado et al., 2018) or applying spectral analysis on the result of a polar decomposition (Bar et al., 2020). We consider this problem to be outside the scope of our paper. 63
Machado, Barreto, Precup, and Bowling References Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Con- ference on Artificial Intelligence (AAAI), 2017. Amitay Bar, Ronen Talmon, and Ron Meir. Option discovery in the absence of rewards with manifold analysis. In International Conference on Machine Learning (ICML), 2020. Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrin- sically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1): 49–73, 2013. Andr´e Barreto, Will Dabney, R´emi Munos, Jonathan Hunt, Tom Schaul, David Silver, and Hado van Hasselt. Successor features for transfer in supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Andr´e Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel J. Mankowitz, Augustin Zidek, and R´emi Munos. Transfer in deep supervised learning using successor features and generalised policy improvement. In International Conference on Machine Learning (ICML), 2018. Andr´e Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Aygu¨n, Philippe Hamel, Daniel Toyama, Jonathan Hunt, Shibl Mourad, David Silver, and Doina Pre- cup. The option keyboard: Combining skills in supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Andr´e Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup. Fast reinforce- ment learning with generalized policy updates. National Academy of Sciences, 117(48): 30079–30087, 2020. Marc G. Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C. Machado, Subhodeep Moitra, Sameera S. Ponda, and Ziyu Wang. Autonomous navi- gation of stratospheric balloons using supervised learning. Nature, 588:77–82, 2020. Richard E. Bellman. Dynamic Programming. Princeton University Press, 1957. Diana Borsa, Andr´e Barreto, John Quan, Daniel J. Mankowitz, Hado van Hasselt, R´emi Munos, David Silver, and Tom Schaul. Universal successor features approximators. In International Conference on Learning Representations (ICLR), 2019. Andrei Z. Broder and Anna R. Karlin. Bounds on the cover time. Journal of Theoretical Probability, 2(1):101–120, 1989. Emma Brunskill and Lihong Li. PAC-inspired option discovery in lifelong supervised learning. In International Conference on Machine Learning (ICML), 2014. Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld envi- ronment for OpenAI gym. https://github.com/maximecb/gym-minigrid, 2018. 64
Temporal Abstraction in RL with the Successor Representation R. R. Coifman, S. Lafon, A. B. Lee, M. Maggioni, B. Nadler, F. Warner, and S. W. Zucker. Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps. National Academy of Sciences, 102(21):7426–7431, 2005. Will Dabney, Georg Ostrovski, and Andr´e Barreto. Temporally-extended (cid:15)-greedy explo- ration. In International Conference on Learning Representations (ICLR), 2021. Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4):613–624, 1993. Peter Dayan and Geoffrey E. Hinton. Feudal supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), 1992. Akram Erraqabi, Marlos C. Machado, Mingde Zhao, Sainbayar Sukhbaatar, Alessandro Lazaric, Ludovic Denoyer, and Yoshua Bengio. Temporal abstractions-augmented tem- porally contrastive learning: An alternative to the Laplacian in RL. In Conference on Uncertainty in Artificial Intelligence (UAI), 2022. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations (ICLR), 2019. Ronan Fruit and Alessandro Lazaric. Exploration-exploitation in MDPs with options. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2017. Karol Gregor, Danilo Rezende, and Daan Wierstra. Variational intrinsic control. In Inter- national Conference on Learning Representations (ICLR), Workshop track, 2017. Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for hierarchical supervised learning. In International Conference on Machine Learning (ICML), 2018. Steven Hansen, Will Dabney, Andr´e Barreto, David Warde-Farley, Tom Van de Wiele, and Volodymyr Mnih. Fast task inference with variational intrinsic successor features. In International Conference on Learning Representations (ICLR), 2020. Nicolas Heess, Gregory Wayne, Yuval Tassa, Timothy P. Lillicrap, Martin A. Riedmiller, and David Silver. Learning and transfer of modulated locomotor controllers. CoRR, abs/1610.05182, 2016. Christopher Hoang, Sungryull Sohn, Jongwook Choi, Wilka Carvalho, and Honglak Lee. Successor feature landmarks for long-horizon goal-conditioned supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), 2021. Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray Kavukcuoglu. supervised learning with unsupervised auxil- iary tasks. In International Conference on Learning Representations (ICLR), 2017. Yuu Jinnai, David Abel, David Ellis Hershkowitz, Michael L. Littman, and George D. Konidaris. Finding options that minimize planning time. In International Conference on Machine Learning (ICML), 2019a. 65
Machado, Barreto, Precup, and Bowling Yuu Jinnai, Jee Won Park, David Abel, and George D. Konidaris. Discovering options for exploration by minimizing cover time. In International Conference on Machine Learning (ICML), 2019b. Yuu Jinnai, Jee Won Park, Marlos C. Machado, and George D. Konidaris. Exploration in supervised learning with deep covering options. In International Conference on Learning Representations (ICLR), 2020. Varun Raj Kompella, Marijn F. Stollenga, Matthew D. Luciw, and Ju¨rgen Schmidhuber. Continual curiosity-driven skill acquisition from high-dimensional video inputs for hu- manoid robots. Artificial Intelligence, 247:313–335, 2017. Risi Kondor and John D. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In International Conference on Machine Learning (ICML), 2002. George D. Konidaris and Andrew G. Barto. Building portable options: Skill transfer in rein- forcement learning. In International Joint Conference on Artificial Intelligence (IJCAI), 2007. George D. Konidaris and Andrew G. Barto. Skill discovery in continuous supervised learning domains using skill chaining. In Advances in Neural Information Processing Systems (NeurIPS), 2009. Yehuda Koren. On spectral graph drawing. In International Computing and Combinatorics Conference (COCOON), 2003. Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J. Gershman. Deep successor supervised learning. CoRR, abs/ 1606.02396, 2016. Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–1149, 2003. Siyuan Li, Lulu Zheng, Jianhao Wang, and Chongjie Zhang. Learning subgoal represen- tations with slow dynamics. In International Conference on Learning Representations (ICLR), 2021. Hao Liu and Pieter Abbeel. APS: Active pretraining with successor features. In Interna- tional Conference on Machine Learning (ICML), 2021. Miao Liu, Marlos C. Machado, Gerald Tesauro, and Murray Campbell. The eigenoption- critic framework. CoRR, abs/1712.04065, 2017. Yao Liu and Emma Brunskill. When simple exploration is sample efficient: Identify- ing sufficient conditions for random exploration to yield PAC RL algorithms. CoRR, abs/1805.09045, 2018. Marlos C. Machado. Efficient Exploration in supervised learning through Time-Based Representations. PhD thesis, University of Alberta, Canada, 2019. Marlos C. Machado and Michael Bowling. Learning purposeful behaviour in the absence of rewards. CoRR, abs/1605.07700, 2016. 66
Temporal Abstraction in RL with the Successor Representation Marlos C. Machado, Marc G. Bellemare, and Michael Bowling. A Laplacian framework for option discovery in supervised learning. In International Conference on Machine Learning (ICML), 2017. Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray Campbell. Eigenoption discovery through the deep successor representation. In International Conference on Learning Representations (ICLR), 2018. Marlos C. Machado, Marc G. Bellemare, and Michael Bowling. Count-based exploration with the successor representation. In Conference on Artificial Intelligence (AAAI), 2020. Sridhar Mahadevan. Proto-value functions: Developmental supervised learning. In In- ternational Conference on Machine Learning (ICML), 2005. Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes. Journal of Machine Learning Research (JMLR), 8:2169–2231, 2007. Daniel J. Mankowitz, Augustin Z´ıdek, Andr´e Barreto, Dan Horgan, Matteo Hessel, John Quan, Junhyuk Oh, Hado van Hasselt, David Silver, and Tom Schaul. Unicorn: Continual learning with a universal, off-policy agent. CoRR, abs/1802.08294, 2018. Timothy A. Mann and Shie Mannor. Scaling up approximate value iteration with options: Better policies with fewer iterations. In International Conference on Machine Learning (ICML), 2014. Matheus R. F. Mendonc¸a, Artur Ziviani, and Andr´e Barreto. Laplacian using abstract state transition graphs: A framework for skill acquisition. In Brazilian Conference on Intelligent Systems (BRACIS), 2019. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep supervised learning. CoRR, abs/1312.5602, 2013. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Ku- maran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep supervised learning. Nature, 518:529–533, 2015. Ida Momennejad, Evan Russek, Jin Hyun Cheong, Matthew Botvinick, Nathaniel Daw, and Samuel Gershman. The successor representation in human supervised learning. Nature Human Behaviour, 1:680––692, 2017. Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning (ICML), 1999. 67
Machado, Barreto, Precup, and Bowling David Pfau, Stig Petersen, Ashish Agarwal, David Barrett, and Kimberly Stachenfeld. Spec- tral inference networks: Unifying spectral methods with deep learning. In International Conference on Learning Representations (ICLR), 2019. Jean Piaget. The Origins of Intelligence in Children. W. W. Norton & Company, 1963. Doina Precup. Temporal Abstraction in supervised learning. PhD thesis, University of Massachusetts Amherst, 2000. Rahul Ramesh, Manan Tomar, and Balaraman Ravindran. Successor options: An option discovery framework for supervised learning. In International Joint Conference on Artificial Intelligence (IJCAI), 2019. Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transac- tions on Pattern Analysis and Machine Intelligence (T-PAMI), 22(8):888–905, 2000. Alec Solway, Carlos Diuk, Natalia C´ordova, Debbie Yee, Andrew G. Barto, Yael Niv, and Matthew M. Botvinick. Optimal behavioral hierarchy. PLOS Computational Biology, 10 (8):1–10, 2014. Henning Sprekeler. On the relation of slow feature analysis and Laplacian eigenmaps. Neural Computation, 23(12):3287–3302, 2011. Kimberly Stachenfeld, Matthew Botvinick, and Samuel Gershman. Design principles of the hippocampal cognitive map. In Advances in Neural Information Processing Systems (NeurIPS), 2014. Kimberly Stachenfeld, Matthew Botvinick, and Samuel Gershman. The hippocampus as a predictive map. Nature Neuroscience, 20:1643–1653, 2017. Peter Stone, Richard S. Sutton, and Gregory Kuhlmann. supervised learning for RoboCup soccer keepaway. Adaptive Behaviour, 13(3):165–188, 2005. Gilbert Strang. Linear Algebra and Its Applications. Brooks Cole, 2005. Richard Sutton. Toward a new view of action selection: The subgoal keyboard. Slides presented at the Barbados Workshop on supervised learning, 2016. URL http: //barbados2016.rl-community.org/RichSutton2016.pdf?attredirects=0&d=1. Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9–44, 1988. Richard S. Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in supervised learning. Artificial Intelligence, 112 (1–2):181 – 211, 1999. Richard S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2011. 68
Temporal Abstraction in RL with the Successor Representation Richard S. Sutton, Marlos C. Machado, G. Zacharias Holland, David Szepesvari, Finbarr Timbers, Brian Tanner, and Adam White. Reward-respecting subtasks for model-based supervised learning. CoRR, abs/2202.03466, 2022. Matthew E. Taylor and Peter Stone. Transfer learning for supervised learning domains: A survey. Journal of Machine Learning Research (JMLR), 10:1633–1685, 2009. Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Momchil S. Tomov, Eric Schulz, and Samuel J. Gershman. Multi-task supervised learning in humans. Nature Human Behaviour, 5:764–773, 2021. Nicholay Topin, Nicholas Haltmeyer, Shawn Squire, John Winder, Marie desJardins, and James MacGlashan. Portable option discovery for automated learning transfer in object- oriented Markov decision processes. In International Joint Conference on Artificial In- telligence (IJCAI), 2015. Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Jun- hyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, R´emi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wu¨nsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Ko- ray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in StarCraft II using multi-agent supervised learning. Nature, 575:350–354, 2019. Kaixin Wang, Kuangqi Zhou, Qixin Zhang, Jie Shao, Bryan Hooi, and Jiashi Feng. Towards better Laplacian representation in supervised learning with generalized graph drawing. In International Conference on Machine Learning (ICML), 2021. Tao Wang, Michael Bowling, and Dale Schuurmans. Dual representations for dynamic pro- gramming and supervised learning. In IEEE International Symposium on Approximate Dynamic Programming and supervised learning (ADPRL), 2007. Christopher J. C. H. Watkins and Peter Dayan. Technical note: Q-learning. Machine Learning, 8(3-4), 1992. Laurenz Wiskott and Terrence J. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural Computation, 14(4):715–770, 2002. Yifan Wu, George Tucker, and Ofir Nachum. The Laplacian in RL: Learning representations with efficient approximations. In International Conference on Learning Representations (ICLR), 2019. 69
