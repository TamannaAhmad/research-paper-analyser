SYNTHESIZING OPTIMAL PARALLELISM PLACEMENT AND REDUCTION STRATEGIES ON HIERARCHICAL SYSTEMS FOR DEEP LEARNING Ningning Xie 1 Tamara Norman 2 Dominik Grewe 2 Dimitrios Vytiniotis 2 ABSTRACT We present a novel characterization of the mapping of multiple parallelism forms (e.g. data and model parallelism) onto hierarchical accelerator systems that is hierarchy-aware and greatly reduces the space of software-to-hardware mapping. We experimentally verify the substantial effect of these mappings on all-reduce performance (up to 448×). We offer a novel syntax-guided program synthesis framework that is able to decompose reductions over one or more parallelism axes to sequences of collectives in a hierarchy- and mapping-aware way. For 69% of parallelism placements and user requested reductions, our framework synthesizes programs that outperform the default all-reduce implementation when evaluated on different GPU hierarchies (max 2.04×, average 1.27×). We complement our synthesis tool with a simulator exceeding 90% top-10 F1-score, which therefore reduces the need for massive evaluations of synthesis results to determine a small set of optimal programs and mappings. 1 INTRODUCTION To facilitate efficient training of large-scale deep learning models, numerous parallelism techniques have been success- fully employed. Common forms of parallelism include data parallelism (Krizhevsky et al., 2012), where each device has a copy of the full model to process a portion of the train- (a) Combining (b) Reduction (c) Reduction ing data, and model parallelism (Dean et al., 2012), which parameter sharding along the axis of along the axis of partitions a training model over available devices, such as and data parallelism parameter sharding data parallelism parameter sharding (Shoeybi et al., 2020) and pipeline par- Figure 1: Parallelism combination allelism (Huang et al., 2019). More recent studies explore combinations of parallelism forms to maximize training throughput (Jia et al., 2019; Narayanan et al., 2021), where each form of parallelism is referred to as a parallelism axis. prominent (Sergeev & Balso, 2018; Goyal et al., 2018). While the aforementioned forms of parallelism and their combinations have greatly improved training throughput, To reduce communication overhead, one particular chal- they may still incur significant communication cost. For lenge posed by multiple parallelism axes is parallelism example, in the simplest form of data parallelism, parameter placement. That is, how we map parallelism over devices gradients for each device must be reduced and replicated for decides which devices communicate with each other along each iteration (Amodei et al., 2016), which is typically im- each parallelism axis, and therefore decides the communi- plemented using the collective operation AllReduce (Thakur cation overhead. For example, Figure 1a presents a com- et al., 2005). State-of-the-art parameter sharding for trans- bination of parameter sharding and data parallelism, for formers (Shoeybi et al., 2020) introduces sharded layers which reduction along the axis of parameter sharding (or where each involves several AllReduce operations. Com- data parallelism), referred to as the reduction axis, is shown munication overhead is especially important for distributed in Figure 1b (or Figure 1c, respectively). Now, suppose we deep learning, as the more devices we have, computation map each box in the figure to devices. In that case, different time reduces, and the communication cost becomes more mappings correspond to different reduction device groups, which can have a significant impact on the communication 1University of Cambridge 2DeepMind. Correspondence to: overhead depending on the network topology. Ningning Xie <nx213@cam.ac.uk>. In this work, we present P 2, a tool for parallelism placement Submitted to the 5 th MLSys Conference, Santa Clara, CA, USA, and placement-aware synthesis of recduction strategies. In 2022. Copyright 2022 by the author(s). particular, we offer the following contributions: 1202 voN 61 ]LP.sc[ 2v84501.0112:viXra
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning (b) (cid:20)1 2 2 1(cid:21) (c) (cid:20)1 2 1 2(cid:21) (d) (cid:20)1 1 2 2(cid:21) (a) [(rack, 1), (server, 2), (CPU, 2), (GPU, 4)] 1 1 1 4 1 1 2 2 1 2 1 2 Figure 2: (a): A system. (b), (c), (d): Possible (non-exhaustive) parallelism placements for (a) under data parallelism of size 4 and 4 parameter shards. For clarity, we show only the 16 GPUs but omit interconnects. Device marker n/m indicates data batch n and parameter shard m. • Parallelism placement synthesis: Given the parallelism synthesized reduction outperforms AllReduce with up axes, the reduction axes, and a hierarchical system to 2.04× speedup (average 1.27×). topology, P 2 automatically synthesizes hierarchical parallelism placements, where a parallelism placement • Simulation: P 2 synthesizes all mapping and hierarchy- is modelled as a parallelism matrix mapping from par- aware reduction strategies, but evaluating hundreds or allelism axes to the system hierarchy (Section 3.1). thousands of them to identify the best can be expensive. The notion of parallelism matrices greatly reduces the We therefore introduce a simulator for predicting the space of parallelism placements contrary to a naive end-to-end performance of a parallelism matrix and re- implementation. duction strategy (Section 5). The simulator is aware of the network topology including different bandwidths • Reduction strategy synthesis: For each parallelism for different interconnects and networks (e.g., NVLink placement, P 2 utilizes the system hierarchy to fur- and ethernet / data-center network in GPU topologies), ther synthesize a wide variety of reduction strategies predicting with reasonable F1-score the communica- to implement reductions using common collective op- tion overhead for each parallelism placement and re- erations. To achieve this, we introduce: (a) a formal duction strategy. The validation – over all mappings semantics for collectives (Section 3.2) based on Hoare and synthesized programs for each mapping, and for triples (Hoare, 1969); (2) a domain-specific language each of the two GPU systems we considered – demon- (DSL) that can express possibly simultaneous reduc- strates that the simulator has 52%, 72%, and 92% of tions amongst groups of devices based on the system top-1, top-5 and top-10 F1-score, respectively, making hierarchy (Section 3.3); and (b) a lowering of our DSL it practical for identifying a much smaller subset of into sequences of collective operations. We use the programs for actual evaluation. formal semantics to guide a syntax-directed synthesis procedure on our DSL. P 2 is helpful for ML practitioners to speed up their models by improving placement and synthesizing reduction strate- • Synthesis hierarchy: We show how the parallelism ma- gies tailored to their system hierarchies. For instance, we trix, which determines a candidate parallelism place- have used P 2 to improve Resnet-50 (He et al., 2016) data- ment, can be put to good use by the synthesizer to mas- parallel training by 15% across 4 nodes, each with 8 V100 sively reduce the space of programs considered without GPUs. (See Section 4 for the details of this system.) missing any semantically valid programs – provably (Section 3.4). 2 OVERVIEW • Evaluation: We evaluate the parallelism matrices and reduction strategies synthesized by P 2 on two differ- This section outlines the key design in P 2. First, a system ent GPU systems available on Google Cloud Platform consists of two entities: (1) a hardware hierarchy, where (GCP) (Section 4). We use collective operations as each level has a name and a cardinality; and (2) a set of implemented by NVIDIA’s NCCL communication li- switched interconnects. The system hierarchy is expected brary (NVidia, 2021), exposed through XLA. The eval- to reflect how devices are arranged. Figure 2a describes uation demonstrates (1) the impact of parallelism place- an example system with 16 GPUs (Cho et al., 2019). The ment: the performance of a single AllReduce across hierarchy is one-dimensional: a rack has 2 servers, each different parallelism matrices differs up to 448.5×; with 2 CPUs connecting 4 GPUs. Interconnects specify and (2) the effectiveness of custom reduction strate- how devices are connected with each other and the latency gies: for 69% of all parallelism mapping matrices, a and bandwidth constraints. In this case, we have exactly
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning one kind of interconnect in each level, but, in general, the interconnect topology can be more complex: there can be multiple interconnects in one level, and an interconnect can connect devices (and other interconnects) across levels. (a) AllReduce (b) AllReduce-AllReduce (c) Reduce-AllReduce-Broadcast Figure 3: Example reduction strategies. 2.1 Parallelism Placement Parallel placement decides which parts of a partitioned pro- gram will execute on which parts of a system. However, synthesizing all arbitrary device mappings, as well as run- ning experiments with them, can be extremely expensive if (a) ReduceScatter - AllReduce (b) AllReduce - AllReduce implemented naively. For example, if we have data paral- lelism of size 4 and 4 parameter shards for the system in Figure 4: Semantically invalid reduction. (a): Reduce data Figure 2a, then there will be (4 ∗ 4)! > 244 possibilities to that should not be reduced. (b): Reduce the same data twice. decide which partitioned program maps to which GPU. To explore the search space efficiently, the critical idea of P 2 is to partition parallelism axes over the system hierarchy 2.2 Reduction Strategy to generate topology-aware parallelism placements, while For each parallelism matrix, P 2 further synthesizes still being able to systematically generate a wide range of topology-aware reduction strategies using common collec- parallelism placements. Specifically, a result of parallelism tive operations, which allows us to find the optimal reduction placement synthesis is a parallelism matrix, where each strategy for any given parallelism matrix. element is a parallelism factor representing the number of a specific level in the hierarchy that a parallelism form splits To illustrate the idea, consider the parallelism matrix in Fig- the computation across. Figures 2b, 2c and 2d show exam- ure 2d, and the goal is to reduce along parameter sharding. ples of parallelism matrices synthesized by P 2, where we As shown in Figure 3a, an obvious choice to perform the have data parallelism of size 4 and 4 parameter shards. In reduction is a single AllReduce within reduction groups. (cid:2) (cid:3) Figure 2b, the first row 1 2 2 1 corresponds to a factoriza- However, such reduction may be suboptimal, as it does not tion of data parallelism on each system level. Specifically, utilize the topology of the system. Figure 3b and 3c show we first assign all data parallelism (of size 4) into 1 rack two reduction strategies, among others, synthesized by P 2. (each with data parallelism of size 4/1 = 4). Then each Figure 3b first performs a step of AllReduce which commu- rack assigns data parallelism of size 4 into 2 servers (each nicates over only S0, and then AllReduce that communicates with data parallelism of size 4/2 = 2). Next, each server as- over S0/S1/S2. Figure 3c first performs Reduce that puts signs data parallelism of size 2 into 2 CPUs (each with data the reduction result in the root device, then AllReduce be- parallelism of size 2/2 = 1). Finally, each CPU assigns data tween root devices, and finally Broadcast that broadcasts parallelism of size 1 into 1 GPU. The second row (cid:2) 1 1 1 4(cid:3) data from the root device. Of particular interest in these corresponds to a factorization of parameter sharding: each two reduction strategies is that no one is strictly better than rack, server, and CPU gets assigned all parameter shards (of the other, as the communication overhead depends on the size 4), and each CPU then assigns 4 parameter shards into network: 3c takes more steps, but has fewer data to be trans- 4 GPUs, each GPU level with 4/4 = 1 shard. Therefore, ferred over S1/S2, which may outperform 3b if S0 has high in the resulting placement, each CPU corresponds to one bandwidth while communication over S1/S2 is expensive. replica (data parallelism) where each GPU has one parame- P 2 gives us a systematic way to synthesize and compare ter shard. We can interpret Figure 2c and 2d accordingly. a wide range of topology-aware reduction strategies. In Note how parallelism matrices decide communication re- particular, synthesized reduction strategies can outperform a quirements. Consider reduction along parameter sharding single step AllReduce, with speedup up to 2.05×. However, (i.e., reduce devices n/m with the same n but different m). synthesizing reduction strategies also imposes challenges, In Figure 2b, this can be done by communication over only which we outline in the rest of this section. S0, while in 2c, half of the data can be reduced by only S0, but the rest of the reduction requires communication over 2.3 Formalism of Collective Operations S0/S1/S2. We discuss the impact of parallelism placements To synthesize reduction strategies, we first need to formal- on communication cost in detail in Section 4. ize the semantics of collective operations, since not all se- quences of operationally valid collective operations corre- spond to semantically correct implementations of the end-to- end reduction requested by the user. For example, consider
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning (cid:2) (cid:3) (cid:20) (cid:21) column-based 1 1 1 2 2 1 2 2 1 1 1 2 2 (cid:2) (cid:3) row-based 1 1 2 2 1 2 1 2 2 1 2 1 2 (cid:2) (cid:3) reduction axis 1 2 1 2 3 (cid:2) (cid:3) Figure 5 Figure 6: Device structured as the reduction axis   column-based 1 4 7 2 5 8 3 6 9 1 2 3 (cid:2) (cid:3) row-based 1 2 3 4 5 6 7 8 9 4 5 6 (cid:2) (cid:3) reduction axes 1 2 3 7 8 9 7 8 9 (cid:2) (cid:3) collapsed 7 16 27 the (incomplete) reduction steps given in Figure 4 for the re- quested reduction across parameter shards. Both programs Table 1: Synthesis hierarchy (reduction axes highlighted) can be executed successfully, e.g., by NCCL (NVidia, 2021). Unfortunately, they are both semantically invalid. In particu- lar, we consider reduction steps which result in device states all ”GPUs” connected to the same ”CPU”. Now, an impor- that can never reach the final desired state to be semanti- tant design decision to consider is which hierarchy to use cally invalid. Specifically, in 4a, the first ReduceScatter for synthesis, as it decides what kind of instructions we can will reduce, among others, device A0 and A1 (recall that produce. One obvious choice is the hardware hierarchy, i.e., GPUs are named in Figure 2a), and put the first half of the (cid:2) (cid:3) 1 2 2 4 for our running example (we ignore level names result on A0 and the second half on A1. Then the second as that can be randomly generated). But the system hierar- AllReduce will reduce A0 and A1 – so the first and the sec- chy is not fine-grained enough for the requested reduction: ond half of the result get reduced while they should not! for example, the reduction in Figure 3 requires reducing half Now, we can never reach the desired final state. 4b is also of the GPUs connected to a CPU. To do that, we need to invalid as it reduces the data on A0 and C0 twice. take parallelism axes into consideration. In this case, the P 2 provides a concise and novel formalism of common (cid:20) 1 1 2 2(cid:21) parallelism matrix is , which splits GPU into 2 collective operations (Section 3.2) that captures semantic 1 2 1 2 correctness and rules out semantically invalid programs, by 2, allowing us to reduce 2 GPUs connected to a CPU. massively reducing the synthesis space. Specifically, each However, to form a synthesis hierarchy from the parallelism device state is defined as a state matrix describing what kind matrix, we have two options (Table 1): 1 puts parallelism of data a device has. The semantics of collective operations factors by columns, which essentially expands the system is defined with Hoare triples (Hoare, 1969), where a collec- hierarchy corresponding to the parallelism matrix; or 2 tive takes the state of each device as a pre-condition and puts factors by rows, which expands the parallelism axes. In returns a new state as a post-condition. this work, we prove that 2 is more expressive than 1 , i.e., 2 can generate all semantically valid reduction strategies 2.4 Reduction Communication Patterns that can be generated by 1 . The result might be some- what counter-intuitive, as 1 seems a natural way to expand Even though the formalism of collective operations rules the system hierarchy. The critical insight is that as 2 puts out semantically invalid reduction steps, the search space of parallelism axes consecutively, it can more easily generate reduction strategies is still quite large. One reason is that semantically correct reduction, while 1 can more easily we need to decide which devices form a reduction group for reduce devices laid out consequently but those reduction each reduction step. For example, the first step in Figure 3b can be semantically invalid as it partitions parallelism axes. reduces over {A0, A1}, {A2, A3} (among others). We may It turns out we can further optimize the synthesis hierarchy. randomly generate all possible groups, but that would sig- (cid:2) (cid:3) In particular, note that we reduce along the axis 1 2 1 2 , nificantly increase the search space. Also, many of them (cid:2) (cid:3) but 2 includes the full matrix (i.e., including 1 1 2 2 ). would be immediately thrown away after semantic checks. With a full matrix, we can generate reduction like Figure 5, To synthesize reduction strategies effectively, P 2 uses a which, however, is not useful for this specific case, as we domain-specific language (Section 3.3) that explores the should not reduce device A0 and A2. The key observation hierarchy to generate hierarchical communication patterns. here is that each reduction group is essentially structured The reduction language, together with the synthesis hierar- according to the reduction axis (cid:2) 1 2 1 2(cid:3) , and this structure chy (explained next), can model many common communi- is repeated for the rest of the matrix, as shown in Figure 6. cation patterns, including those in Figure 3 and 4. Based on this observation, P 2 uses the synthesis hierarchy 3 formed by parallelism factors from only the reduction 2.5 Synthesis Hierarchy axis and then lowers synthesized programs to the full system hierarchy. We prove that 3 , while largely reducing the To generate hierarchical reduction communication patterns, search space, is actually more expressive than 2 . the DSL reduction instruction needs to know the synthesis hierarchy. For example, a possible instruction is to reduce Exploration with multiple reduction axes. The same obser-
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning vation applies for reduction over multiple axes. An example reduced from device 0 and 1 reduced from device 1 and 2 is given in the second half of Table 1. In this case, the reduc- 3 data chunks (cid:2) (cid:3) tion axes based synthesis hierarchy is 1 2 3 7 8 9 . Note reduced from device 2 and 3 that some parallelism factors are from the same hardware level: 1 and 7, 2 and 8, and 3 and 9. Since for switched Figure 7: A device state. Assume we have in total 4 devices networks, splitting hardware hierarchies does not bring ben- (i.e., device 0,1,2 and 3), so each device state is a 4 × 4 efits in most cases, we can collapse parallelism factors of matrix. s[i][j] is colored if s[i][j] = 1. The device state has the same hardware hierarchies. In this example, the final 3 non-empty rows, meaning that it has 3 data chunks. Each synthesis hierarchy is (cid:2) 7 16 27(cid:3) . data chunk describes where the data is reduced from. For example, the first data chunk is the reduction result between the original first data chunk of device 0 and 1. 3 PROGRAM SYNTHESIS We now present the program synthesis algorithm in P 2. 3.1 Parallelism Placement original ith chunk to the reduction result. Figure 7 gives an example. A state context G maps devices to their states. Parallelism placement partitions parallelism axes over the system hierarchy. With the novel notion of the parallelism Note finally that Reduce and Broadcast typically take a root matrix and its interpretation (Section 2.1), synthesizing par- device to reduce to or broadcast from. Since we focus on allelism matrices is straightforward. Consider hierarchical systems, we always use the first device in a H = (cid:2) h 0 · · · h n(cid:3) is the system hierarchy (e.g., (cid:2) 1 2 2 4(cid:3) ), reduction group as the root without loss of generality. P = (cid:2) p · · · p (cid:3) is the parallelism axes (e.g., (cid:2) 4 4(cid:3) ), 0 m then a parallelism matrix is   subject to: x x . . . x 0,0 0,1 0,n m Semantics Figure 8 defines the semantics of collective   . . . . . . . . . . . .   (cid:89) x i,j = h j, j = 0, ..., n (1) operations, which is closely based on Hoare rules (Hoare, 1969). Each reduction takes the form of a Hoare triple x x · · · x m,0 m,1 m,n i=0 (cid:89)n { G 1 } C { G 2 }, which means that from the pre-condition x i,j = p i, i = 0, ..., m (2) state G 1, a step of reduction C yields to the post-condition state G . Explanations of auxiliary functions are given in j=0 2 the figure. To better illustrate the semantics, the right of Equation (1) requires the product of a column to be equiva- Figure 8 provides examples of each collective operation. lent to the corresponding system hierarchy cardinality, while Equation (2) requires the product of a row to be equivalent At a high level, these rules capture the constraints for to the corresponding parallelism axis. a reduction step to be semantically correct. Rule R- ALLREDUCE first checks that the data contained in each device (denoted as rows representing the non-empty rows) 3.2 Collective Operations should have the same data chunks. Moreover, columns in This section defines the semantics of collective operations. any specific chunk should be disjoint. Both constraints are In this work we focus on the common ones: AllReduce, essential for the reduction result to be valid: we should not ReduceScatter, AllGather, Reduce and Broadcast. reduce data from different chunks or reduce the same data twice (as discussed in Section 2.3). Finally, we generate the resulting state (cid:93)s for each device by adding up all matri- i Notations We first define the notations. ces. Rule S-REDUCESCATTER and S-REDUCE are similar d device to rule S-ALLREDUCE, except that S-REDUCESCATTER s ∈ Bk×k device state scatters the reduction result over devices, where scatter raises an error if the number of data chunks in s is not di- G := d : s state context i i C := AllReduce | ReduceScatter visible by the number of devices; and S-REDUCE puts the result only in the first device and clears up the rest of the | AllGather | Reduce | Broadcast devices. Rule S-ALLGATHER simply needs all data rows to We use d to denote a device, whose state s is represented as be disjoint. Rule S-BROADCAST overrides the data of every a boolean matrix of dimensions k × k; k being the number device with the data from the first one. As an optimization, of devices. In particular we treat the data as being split in the rule enforces information increase, i.e., the data to be k chunks. The ith row of a state matrix represents the ith broadcasted must be as informative as data in other devices chunk. s[i][j] = 1 means that device j has contributed its and more informative than at least one other device.
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning { G } C { G } (Reduction: from the pre-condition state G , C yields to the post-condition state G ) before after 1 2 1 2 R-ALLREDUCE ∀i j, s i.rows = s j.rows ∀i j k, i (cid:54)= j =⇒ s i[k] (cid:13)(cid:63) s j[k] s = (cid:93)s i { d : s } AllReduce { d : s } i i i R-REDUCESCATTER ∀i j, s i.rows = s j.rows ∀i j k, i (cid:54)= j =⇒ s i[k] (cid:13)(cid:63) s j[k] s = (cid:93)s i s(cid:48) i = scatter(s, i)[i] { d : s } ReduceScatter { d : s(cid:48) } i i i i R-ALLGATHER ∀i j, i (cid:54)= j =⇒ s i.rows (cid:13)(cid:63) s j.rows ∀i j, |s i.rows| = |s j.rows| s = (cid:93)s i { d : s } AllGather { d : s } i i i R-REDUCE ∀i j, s i.rows = s j.rows ∀i j k, i (cid:54)= j =⇒ s i[k] (cid:13)(cid:63) s j[k] s = (cid:93)s i i(cid:54)=0 { d : s } Reduce { d : s, d : {} } i i 0 i (cid:13)(cid:63) disjoint rows non-empty rows R-BROADCAST ∀i, s ≤ s ∃i, s < s (cid:93) addition | · | length i 0 i 0 scatter(s, i) scatters non-empty rows in s over devices i { d i : s i } Broadcast { d i : s 0 } Figure 8: Semantics of collective operations. with the right presents examples of each operation. For those examples, we have in total 4 devices .e., device 0,1,2, and 3), so each device state is a 4 × 4 matrix. We assume the reduction happens between only device 0 and 1. The pre-condition states of device 0 (top) and 1 (bottom) are on the left, and after a step of reduction, their states turn into the post-condition states on the right. 3.3 Reduction Programs {A0, A1, A2, A3}, {B0, B1, B2, B3}, {C0, C1, C2, C3}, {D0, D1, D2, D3}. Now, if the form is InsideGroup, then we perform reduction We now turn to our reduction language which is built on top within each reduction group. If the form is Parallel(e), we of the formalism of collective operations. perform reduction over the first device in each group, the second device in each group, etc, if they connect to the program ∈ [reduction] reduction ∈ slice × f orm × C same e. Thus, Parallel(server) generates {A0, B0}, {A1, B1}, slice := e etc., whereas Parallel(rack) generates {A0, B0, C0, D0} etc. Master generates the device groups in the same way as f orm := InsideGroup | Parallel(e) | Master(e) Parallel, but only reduces over the first device group. A reduction strategy is represented as a program, which Note that Table 2 presents device groups for reduction over is essentially a list of reduction instructions. A reduction the system hierarchy [(rack, 1), (server, 2), (CPU, 2), (GPU, instruction consists of a slice, a f orm, and a collective 4)]. As we discussed in Section 2.5, reduction over specific operation C. We use e to represent a level in the synthesis parallelism axes will use the synthesis hierarchy formed by hierarchy. The slice chooses a level. The f orm has three parallelism factors, and we will get reduction groups for patterns: InsideGroup, Parallel(e), and Master(e). Inside a reduction, the e carried in the form must be an ancestor of that particular reduction axis like {A0, A1}, {A2, A3} etc. the one carried in the slice. The slice and the form together Supposing slice and f orm derive the device groups G , i decide the device groups that will perform the operation C. which are disjoint by construction, we define the semantics of a reduction instruction as: It turns out that slice and f orm are quite expressive and can encode many common hierarchical communi- (slice, form) derives G { G } C { G(cid:48) } cation patterns. Table 2 demonstrates several examples i i i { G , G } (slice, form, C) { G(cid:48), G } using the system hierarchy in Figure 2a. Specifically, a i i slice divides devices into reduction groups, and f orm where each device group participating in the reduction gets decides the reduction form happening for the reduction the device states updated according to the semantics of groups. For example, consider that the slice is CPU, collective operations, and devices not participating in the then we get reduction groups within each CPU, i.e., reduction have their states unchanged. A reduction program
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning slice form groups(slice, form) factor right before e i row-wisely, and let e(cid:48) 2 be e j. Then CPU InsideGroup {A0, A1, A2, A3}, {B0, B1, B2, B3}, (e(cid:48) 2, Parallel(e(cid:48) 1), C) is a desired reduction instruction. {C0, C1, C2, C3}, {D0, D1, D2, D3} Parallel(server) {A0, B0}, {A1, B1}, {A2, B2}, {A3, B3}  x 1 1 x  x 1 1 x  Parallel(rack) {{ {AC A0 0 2, , , D B B0 20 , ,} C C, { 0 2, ,C D D1, 0 2D } }, ,1 { {} A A, { 1 3, ,C B B2, 1 3, ,D C C2 1 3} , ,, D D{C 1 3} }3 ,, D3} reduc ati xoe in s1  x x1 20 ,, , 00 0 x 21 ,1 x 21 ,2 x x1 20 , ,, 3 33    x x0 1 2, , ,0 0 0 x 21 ,1 x 21 ,2 x x10 2, ,,3 33 e(cid:48) 2 e(cid:48) 1 server M Insa is dt ee Gr( rr oa uc pk) { {A A0 0, , B A0 1, , C A0 2, , D A30 ,} B0, B1, B2, B3}, 1 1 1 x 3,3 e 2 1 1 1 x 3,3 {C0, C1, C2, C3, D0, D1, D2, D3} Parallel(rack) {A0, C0}, {A1, C1}, {A2, C2}, {A3, C3} {B0, D0}, {B1, D1}, {B2, D2}, {B3, D3} rack InsideGroup {A0, A1, A2, A3, B0, B1, B2, B3, C0, C1, C2, C3, D0, D1, D2, D3} 3.5 Program Synthesis for Reduction Programs Table 2: Hierarchical communication patterns for Figure 2a. So far, we have given the constraint for generating paral- lelism matrices (Section 3.1) and how we can obtain a syn- thesis hierarchy from a parallelism matrix (Section 3.4). The then iteratively applies each reduction: last missing piece is how to synthesize reduction programs. i∈n To formalize the synthesis problem, we need an initial pre- program = reduction { G } reduction { G } i i i+1 condition state as the beginning state and a post-condition { G } program { G } 0 n+1 state as the final desired state. Initially, every device only holds its own data, and therefore device i has 1 in the ith 3.4 Synthesis Hierarchy column, and 0 in any other position. In the final desired In Section 2.5, we have proposed and compared different state, a device should have 1 in all columns corresponding synthesis hierarchies for synthesizing reduction programs: to devices in its reduction group, and 0 in any other position. An extra indirection is caused from using the reduction axis (cid:2) (cid:3) (a) System hierarchy ( 1 2 2 4 ) parallelism factors as the synthesis hierarchy (Section 3.4), (cid:2) (cid:3) (b) Column-based parallelism factors ( 1 1 1 2 2 1 2 2 ) which only includes part of the system, and then lowers (cid:2) (cid:3) (c) Row-based parallelism factors ( 1 1 2 2 1 2 1 2 ) the program to the full system. Therefore, our goal is to (cid:2) (cid:3) (d) Reduction axis parallelism factors ( 1 2 1 2 ) synthesize a program, whose lowering L subjects to: P 2 uses (d). Here, we formally prove the theorem that justi-     i i j fies our choice. First, every reduction instruction essentially  0 . . . 1 . . . 0   0 . . . 1 . . . 0 . . . 1 . . . 0  d ae pc ri od ge rs at mhe cd ae nv bic ee lg or wo eu rp es dG toan ad seth qe ueo np ce era (t Gio 1n , CC 1. )T , h (Ger 2e ,f Co 2re ),, d i :    0. . . ·. ·.. · 1. . . .. ... . 0. . .     L d i :    0. . . ·. ·.. · 1. . . .. ... . 0. . . ·. ·.. · 1. . . ·. ·.. · 0. . .     ..., (G , C ). Since (d) includes only the reduction axis, n n lowering for (d) applies the generated grouping patterns to supposing d reduces with devices j. i non-reduction axes when forming device groups. Given the syntax and the semantics of reduction programs, Definition 3.1 (Expressive power of synthesis hierarchy). we use syntax-guided program synthesis (Alur et al., 2013) A synthesis hierarchy is more expressive than (≥) another, if to synthesize programs in increasing order of program size. every valid lowered program L synthesized using the latter can be synthesized using the former. 4 EXPERIMENTS Theorem 3.2. (d) ≥ (c) ≥ (b) ≥ (a). We implement P 2 to synthesize parallelism matrices and re- duction programs, and lower the programs into sequences of Proof. For space reasons, we show one example that il- XLA collective operations, which in turn result in sequences lustrates the key proof strategy, and we refer the reader of NCCL calls on the XLA GPU backend. We measure the to the appendix for the full proof. Consider proving execution time of the compiled programs. The experiments (c) ≥ (b), where (b) synthesizes a valid reduction step aim to answer the following research questions: (e , Parallel(e ), C). For the reduction to be valid, every 2 1 reduction group must be partitioned only over the reduc- RQ1 What is the impact of parallelism placement on tion axis. Therefore, all non-reduction parallelism factors reduction algorithms? column-wisely between e (exclusive) and e (inclusive) 1 2 RQ2 Are our various techniques for taming the search can only be 1. An example is given below on the left. space effective so that we can quickly enumerate a wide Now we construct a reduction instruction for (c) that ex- variety of reduction programs? presses the same reduction. Suppose the reduction step in (b) covers parallelism factors e , ..., e on the reduction RQ3 Given a parallelism placement, can we find reduc- i j axis. Let e(cid:48) be the level corresponding to the parallelism tion strategies that outperform the default implementation 1
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning Reduction on Reduction on Parallelism Parallelism the 0th axis the 1st axis axes matrix Ring Tree Ring Tree 4 nodes, each with 16 A100 (cid:2) (cid:3) (cid:2)(cid:2) (cid:3) (cid:2) (cid:3)(cid:3) A1 2 32 1 2 4 8 0.12 0.17 8.74 9.89 (cid:2)(cid:2) (cid:3) (cid:2) (cid:3)(cid:3) A2 2 1 2 16 37.16 36.94 4.81 3.41 (cid:2) (cid:3) (cid:2)(cid:2) (cid:3) (cid:2) (cid:3)(cid:3) B1 4 16 1 4 4 4 0.15 0.20 17.70 19.03 (a) 2 nodes, each with 16 A100 GPUs sharing one NVSwitch and B2 (cid:2)(cid:2) 2 2(cid:3) (cid:2) 2 8(cid:3)(cid:3) 28.77 19.81 8.39 4.99 one NIC, and all NICs are connected in a data center (cid:2)(cid:2) (cid:3) (cid:2) (cid:3)(cid:3) B3 4 1 1 16 56.13 89.70 0.18 0.22 (cid:2) (cid:3) (cid:2)(cid:2) (cid:3) (cid:2) (cid:3)(cid:3) C1 8 8 1 8 4 2 0.17 0.21 33.92 41.06 (cid:2)(cid:2) (cid:3) (cid:2) (cid:3)(cid:3) C2 2 4 2 4 16.52 9.18 15.68 9.43 (cid:2)(cid:2) (cid:3) (cid:2) (cid:3)(cid:3) C3 4 2 1 8 34.05 41.23 0.17 0.21 4 nodes, each with 8 V100 (cid:2) (cid:3) (cid:2)(cid:2) (cid:3) (cid:2) (cid:3)(cid:3) E1 8 4 1 8 4 1 0.28 0.39 21.74 30.42 (cid:2)(cid:2) (cid:3) (cid:2) (cid:3)(cid:3) E2 2 4 2 2 14.25 15.48 10.98 7.34 (cid:2)(cid:2) (cid:3) (cid:2) (cid:3)(cid:3) E3 4 2 1 4 14.84 19.90 2.96 0.43 (b) 2 nodes, each with 8 V100 GPUs forming a ring via NVLink and connected via PCIe switches. Each node consists of two CPUs Table 3: Reduction time in seconds of running AllReduce. (each owning 4 GPUs) with one NIC to the DCN. A shared NIC connecting the two CPUs is a modeling simplification – in reality cross-domain communication is through shared memory. cases, and we put the full experiment results in the appendix. Figure 9: System topology models for 2 nodes. For the experiments, we run on both 2 and 4 nodes. 4.1 Synthesizing Parallelism Placement Result 1 (RQ 1): The performance of AllReduce differs significantly among parallelism matrices, up to 448.5×. (i.e., AllReduce), and if so what is their form? The experiment results are given in Table 3. For a particular The experiments ran on two different GPU system config- parallelism axis (e.g., A), we compare the reduction time urations available on Google Cloud Platform (GCP) (see for difference parallelism matrices (e.g., A1 and A2) with Figure 9): (i) NVIDIA A100, where each node consists of each NCCL algorithm and the reduction axis. Notably, for 16 GPUs sharing one NVSwitch and one NIC connecting to reducion on the 0th axis and with the Tree algorithm, B3 the data center network; and (ii) NVIDIA Tesla V100, where (89.70s) is slower than B1 (0.20s) by 448.5×. each node consists of 8 GPUs forming a ring via NVLink; each pair of GPUs are connected via PCIe switches, and The difference is due to the fact that different parallelism each of the two CPUs of the node has 4 GPUS in its PCIe matrices lead to different data placement. In B1, the first (cid:2) (cid:3) domain. We experiment with both NCCL ring reduction and row of the the matrix ( 1 4 ) means that devices to be re- tree reduction (Sanders et al., 2009), set by NCCL ALGO. duced are inside a single node, where the local NVSwitch can perform the reduction efficiently. For B3, the first row We run experiments with 2 and 4 nodes. For A100, the (cid:2) (cid:3) ( 4 1 ) puts reduction groups across nodes, going through (cid:2) (cid:3) (cid:2) (cid:3) system hierarchy is 2 16 or 4 16 . For V100, since the the slow data-center network. However, B3 can still be use- NVLink ring connects all 8 GPUs, and the NVLink ring ful for a diffferent reduction: since it puts the 1st reduction has much higher bandwidth than PCIe bridges, we put 8 axis inside a single node, for a reduction on the 1st axis, B3 GPUs inside one layer, and so with 2 or 4 nodes, the system (0.22s on Tree) is 86.5× faster than B1 (19.03s). In prac- (cid:2) (cid:3) (cid:2) (cid:3) hierarchy is 2 8 or 4 8 , respectively. Each GPU carries tice, models with multiple parallelism forms (e.g., Shoeybi a large amount of data ((229× nodes) of float32) to reduce et al. (2020)) involve reductions across both axes, and the the impact of latency, and each program runs 10 times to selection of a mapping should take all of them into account. reduce the impact of network noise. For each system, we synthesize parallelism mappings and 4.2 Synthesizing Reduction Programs reduction programs for (1) a single parallelism axis; (2) all Now we turn to the reduction programs synthesized for each combinations of two parallelism axes, with reduction on one parallelism matrix. Table 4 presents experiment results. of the axes; and (3) three parallelism axes, with reduction on the first and the third axes. We can easily scale to more Result 2 (RQ 2): Our pruning techniques are effective for axes, though up to three axes are quite common in practical the synthesizer to achieve fast synthesis time. settings, and many observations can already be illustrated. With our formalism, a program cannot be arbitrarily large, Next, we discuss the results and insights from the experi- since our carefully crafted semantics of collective operations ments. For space reasons, we present only representative enforces a form of information increase for every operation.
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning Programs AllReduce Optimal NCCL Parallelism Synthesis outperforming (bold if the (bold if Parallelism matrix Speedup algo axes time (s) AllReduce / total optimal overall programs AllReduce) optimal) 2 nodes, each with 16 A100 (cid:2) (cid:3) (cid:2)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3) F1 Ring 8 4 0.03 14/47 1 8 2 2 0.17 0.17 1× (cid:2)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3) F2 2 4 1 4 16.84 9.19 1.83× 4 nodes, each with 16 A100 (cid:2) (cid:3) (cid:2)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3) G1 Tree 4 16 0.04 10/53 1 4 4 4 0.20 0.17 1.17× (cid:2)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3) G2 4 1 1 16 89.70 56.13 1.60× (cid:2) (cid:3) (cid:2)(cid:2) (cid:3)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3) H1 Ring 16 2 2 0.97 25/235 1 16 2 1 2 1 4.79 4.63 1.03× (cid:2)(cid:2) (cid:3)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3) H2 2 8 2 1 1 2 4.91 3.10 1.58× (cid:2) (cid:3) (cid:2)(cid:2) (cid:3)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3) I1 Ring 2 2 16 0.93 29/235 2 1 2 1 1 16 4.82 2.99 1.61× (cid:2)(cid:2) (cid:3)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3) I2 1 2 2 1 2 8 5.28 4.77 1.11× (cid:2) (cid:3) (cid:2)(cid:2) (cid:3)(cid:3) J1 Tree 64 1.16 5/47 4 16 5.75 4.74 1.21× 4 nodes, each with 8 V100 (cid:2) (cid:3) (cid:2)(cid:2) (cid:3)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3) K1 Ring 8 2 2 0.24 17/188 2 4 2 1 1 2 4.80 2.35 2.04× (cid:2)(cid:2) (cid:3)(cid:2) (cid:3)(cid:2) (cid:3)(cid:3) K2 1 8 2 1 2 1 4.40 4.40 1× (cid:2) (cid:3) (cid:2)(cid:2) (cid:3)(cid:3) L1 Ring 32 0.06 11/47 4 8 4.83 3.45 1.4× Table 4: Reduction time in seconds for running AllReduce and the synthesized optimal reduction strategy (reduction on the 0th axis for parallelism axes of size 1 and 2, and on the 0th and 2rd axes for parallelism axes of size 3). In our experiments, we set 5 as the program size limit for Table 4 shows that when cross-node communication is the synthesizer, which turns out to be sufficient to generate needed the optimal program tends to outperform AllReduce. interesting reduction patterns. With this setup, the longest For example, the speedup is 1.84× in F2, and 2.04× in K2. synthesis time is under 2 seconds (for up to 235 programs). For 69% of all mappings across both systems, synthesized Increasing the size limit makes the synthesis slightly slower, programs outperform AllReduce by 1.27× on average. but, for most cases, does not generate new programs. We present common optimal reduction programs applied Result 3 (RQ 3): If the reduction axes can be put within to our running examples (Section 2) in Figure 10. (i) Fig- one node, then a single step AllReduce inside that node is ure 10i first reduces local data to a root device, performs the most performant reduction due to fast local bandwidth. AllReduce between root devices, and broadcasts the result from the root device to each device. (ii) Figure 10ii first We observe this result from the difference between F1 and performs ReduceScatter between local devices, and then F2. F1 assigns the reduction axis to the GPU level, and thus AllReduce between remote devices, and finally AllGather AllReduce is the most performant reduction, outperforming between local devices. Both reduction programs utilize the F2, which requires cross-node reduction, by 99.06×. topology, by performing local communication first, which Result 4 (RQ3): Synthesized programs can help mitigate is often more efficient due to local high bandwidth. Now, the impact of parallelism placement. the data to be reduced across nodes in the intermediate step is significantly smaller. The final step is again local commu- Consider G1 and G2. As discussed before, G2’s AllReduce nication. Thus, the reduction programs have overall better (89.7s) is 448.5× slower than G1 (0.20s). Synthesized performance than AllReduce. It turns out that both reduc- programs have helped bridge the gap: G2’s optimal program tion programs have been recently proposed: program (i) is only 330.2× slower. However, the performance difference has been used in Goyal et al. (2018); Jia et al. (2018a), and here is significant and the help is limited. The case of H1 program (ii) has been proposed by Cho et al. (2019). and H2 is more interesting: H1’s AllReduce is 1.03× slower than H2, but its optimal program is 1.49× faster than H2! Furthermore, the experiments suggest that program (ii) is more often the optimal one and outperforms (i) by a larger On the other hand, it is also possible that synthesis aggra- speedup. Specifically, when (i) is optimal, it outperforms vates the impact of parallelism placement. For example, for (ii) by only about 1.1× (up to 1.12×); when (ii) is optimal, I1 and I2, the performance difference jumps from 1.10× for it outperforms (i) by about 1.3× (up to 2.73×). AllReduce to 1.60× for the optimal program. Result 5 (RQ3): For reduction across nodes, a topology- aware reduction program tends to outperform a single step AllReduce, with speedup on average 1.28×, upto 2.04×.
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning Top-1 Top-2 Top-3 Top-5 Top-6 Top-10 A100 46.8% 71.0% 72.6% 74.2% 90.3% 94.4% V100 60.5% 67.1% 71.1% 76.3% 76.3% 88.1% Total 52.0% 69.5% 72.0% 75.0% 85.0% 92.0% (i) Reduce-AllReduce-Broadcast (ii) ReduceScatter-AllReduce-AllGather Table 5: Prediction F1-score. Figure 10: Common optimal reduction programs same trend as the V100 experiments (11a), and are very close to the absolute performance for A100 (11b). The main reason for reduced absolute F1-score in V100 is the imperfect modeling of cross-domain communication. Note that there exist a few programs for which the simulation result is notably slower than the experiments, mostly due to XLA optimizations. For example, program 10 and 11 (in 11a) are all 2 steps of AllReduce, which XLA optimizes to a single step AllReduce. We do not extend P 2 with any optimizations, as optimized programs are themselves valid synthesizable programs; in this specific example program 9. (cid:2) (cid:3) (a) 4 nodes of V100 with NCCL Ring and parallelism axes 2 16 , P 2 successfully predicts the performance of program 9. reduction on the 1st axis. Synthesis 0.12s, and simulation 0.54s. Table 5 summarizes the F1-score over all experiments for the two GPU systems. Overall, P 2 delivers 52% top-1 F1-score, 75% top-5 F1-score, and 92% top-10 F1-score. 6 RELATED WORK Parallelism forms. Recent work has explored combinations of parallelism forms. Jia et al. (2018b) propose layer-wise parallelism that allows each network layer to use a differ- ent parallelization strategy. FlexFlow (Jia et al., 2019) uses guided randomized search to find a fast parallelism combina- (cid:2) (cid:3) tion. Narayanan et al. (2021) combine pipeline, tensor and (b) 4 nodes of A100 with NCCL Tree and parallelism axes 4 2 8 , reduction on 0th and 2rd axes. Synthesis 2.86s, simulation 3.09s. data parallelism to scale transformer training to thousands of GPUs, extending previous model parallelism work (Shoeybi Figure 11: Simulation results, in increasing order of experi- et al., 2020). ZeRO-DP (Rajbhandari et al., 2020) includes ment time. Measurements are ’•’ and solid, and simulations three optimization stages partitioning over optimizer state, are ’×’ and translucent. Colors denote parallelism matrices. gradient, and parameters. To our best knowledge, no prior work has discussed parallelism placement, and typically commit to a specific placement (e.g. model parallelism 5 SIMULATION within a host, batch parallelism across (Narayanan et al., 2021).) which can get involved when multiple axes oc- In this section, we apply simulation to the system topologies cur. Finally, there exists a rich body of work on operator in Figure 9, and show that P 2 can identify near-optimal mapping, e.g. (Addanki et al., 2019; Gao et al., 2018; Mirho- programs, to reduce the need for evaluation over hundreds seini et al., 2017), but the focus there does not include the or thousands of synthesized mappings and strategies. structured forms of parallelism and reductions we address. Assumptions. We use 100Gbps NICS, which we assume Program synthesis for communication. For a given topol- were utilized at 60%, yielding an effective 8GB/s. For ogy, SCCL (Cai et al., 2021) synthesizes optimal collective the PCIe switches we assumed 32GB/s. For the V100 algorithms as a sequence of sends, but the work has focused NVLink ring we assume 135GB/s in each direction – an on the single-node setting. SCCL takes a more fine-grained optimistic 90% of the nominal uni-directional bandwidth system topology as a graph with bandwidth constraints on (150GB/s) (Jia et al., 2018c). For the A100 NVLink switch, GPUs and edges, and uses an SMT solver to synthesize algo- we assume 270GB/s uni-directional bandwidth – again 90% rithms. It is possible for P 2 to use SMT, but we found that of the nominal value (300GB/s in each direction) (NVIDIA). the structure we imposed on the problem already enables Results. Figure 11 shows that P 2 predictions follow the fast enumerative syntax-guided synthesis. Blink (Wang
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning et al., 2020) performs the synthesis based on an approxi- Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., mate spanning-tree packing algorithm for intra-node com- Le, Q. V., Mao, M. Z., Ranzato, M., Senior, A., Tucker, munication, but always uses program (i) (Figure 10ii) for P., Yang, K., and Ng, A. Y. Large scale distributed deep inter-node communication. PLink (Luo et al., 2020) probes networks. In Proceedings of the 25th International Con- the network locality and groups nodes by physical affinity, ference on Neural Information Processing Systems - Vol- and performs (i) for intra-group reduction. By contrast, P 2 ume 1, NIPS’12, pp. 1223–1231, Red Hook, NY, USA, synthesizes hierarchical strategies using collectives. 2012. Curran Associates Inc. Reduction strategies. BlueConnect (Cho et al., 2019) pro- Gao, Y., Chen, L., and Li, B. Spotlight: Optimizing de- pose program (ii) (Figure 10i) for hierarchical systems, vice placement for training deep neural networks. In which is later generalized by FlexReduce (Lee et al., 2020) Dy, J. G. and Krause, A. (eds.), Proceedings of the 35th to asymmetric topologies. On the other hand, P 2 systemati- International Conference on Machine Learning, ICML cally generates and compares a wide range of hierarchical 2018, Stockholmsma¨ssan, Stockholm, Sweden, July 10-15, reduction strategies for different parallelism placements. 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1662–1670. PMLR, 2018. URL http:// 7 CONCLUSION proceedings.mlr.press/v80/gao18a.html. GCP. Gpus on compute engine. https://cloud. We have presented a framework to synthesize structured google.com/compute/docs/gpus. Accessed: forms of parallelism mappings and hierarchy-aware reduc- 2021-10-07. tion strategies. Our tool can be useful for speeding up ML models, and also for establishing projections about commu- Goyal, P., Dolla´r, P., Girshick, R., Noordhuis, P., nication costs when investigating new system hierarchies. Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch sgd: Training MNIST REFERENCES in 1 hour, 2018. Addanki, R., Venkatakrishnan, S. B., Gupta, S., Mao, H., He, K., Zhang, X., Ren, S., and Sun, J. Deep resid- and Alizadeh, M. Placeto: Learning Generalizable ual learning for image recognition. In 2016 IEEE Device Placement Algorithms for Distributed Machine Conference on Computer Vision and Pattern Recogni- Learning. Curran Associates Inc., Red Hook, NY, USA, tion, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2019. 2016, pp. 770–778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/ Alur, R., Bodik, R., Juniwal, G., Martin, M. M., 10.1109/CVPR.2016.90. Raghothaman, M., Seshia, S. A., Singh, R., Solar- Hoare, C. A. R. An axiomatic basis for computer program- Lezama, A., Torlak, E., and Udupa, A. Syntax-guided ming. Commun. ACM, 12(10):576–580, October 1969. synthesis. IEEE, 2013. ISSN 0001-0782. doi: 10.1145/363235.363259. Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, Battenberg, E., Case, C., Casper, J., Catanzaro, B., Cheng, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Q., Chen, G., et al. Deep speech 2: end-to-end speech Efficient training of giant neural networks using pipeline recognition in english and mandarin. In Proceedings parallelism. Advances in neural information processing of the 33rd International Conference on International systems, 32:103–112, 2019. Conference on Machine Learning-Volume 48, pp. 173– 182, 2016. Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., Xie, L., Guo, Z., Yang, Y., Yu, L., et al. Highly scal- Cai, Z., Liu, Z., Maleki, S., Musuvathi, M., Mytkowicz, able deep learning training system with mixed-precision: T., Nelson, J., and Saarikivi, O. Synthesizing optimal Training MNIST in four minutes. arXiv preprint collective algorithms. In Proceedings of the 26th ACM arXiv:1807.11205, 2018a. SIGPLAN Symposium on Principles and Practice of Par- allel Programming, PPoPP ’21, pp. 62–75, New York, Jia, Z., Lin, S., Qi, C. R., and Aiken, A. Exploring hidden dimensions in accelerating convolutional neural NY, USA, 2021. Association for Computing Machinery. networks. In Dy, J. and Krause, A. (eds.), Proceed- ISBN 9781450382946. doi: 10.1145/3437801.3441620. ings of the 35th International Conference on Machine Cho, M., Finkler, U., and Kung, D. Blueconnect: Novel Learning, volume 80 of Proceedings of Machine Learn- hierarchical all-reduce on multi-tired network for deep ing Research, pp. 2274–2283. PMLR, 10–15 Jul 2018b. learning. In Proceedings of the 2nd SysML Conference, URL https://proceedings.mlr.press/v80/ 2019. jia18a.html.
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning Jia, Z., Maggioni, M., Staiger, B., and Scarpazza, D. P. Sanders, P., Speck, J., and Tra¨ff, J. L. Two-tree algorithms Dissecting the NVIDIA volta GPU architecture via mi- for full bandwidth broadcast, reduction and scan. Parallel crobenchmarking. CoRR, abs/1804.06826, 2018c. URL Computing, 35(12):581–594, 2009. http://arxiv.org/abs/1804.06826. Sergeev, A. and Balso, M. D. Horovod: fast and easy Jia, Z., Zaharia, M., and Aiken, A. Beyond data and model distributed deep learning in tensorflow, 2018. parallelism for deep neural networks. In Talwalkar, A., Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, Smith, V., and Zaharia, M. (eds.), Proceedings of Machine J., and Catanzaro, B. Megatron-lm: Training multi- Learning and Systems, volume 1, pp. 1–13, 2019. billion parameter language models using model paral- lelism, 2020. Krizhevsky, A., Sutskever, I., and Hinton, G. E. MNIST classification with deep convolutional neural networks. Thakur, R., Rabenseifner, R., and Gropp, W. Optimization Advances in neural information processing systems, 25: of collective communication operations in mpich. The 1097–1105, 2012. International Journal of High Performance Computing Applications, 19(1):49–66, 2005. Lee, J., Hwang, I., Shah, S., and Cho, M. Flexreduce: Flexible all-reduce for distributed deep learning on asym- Wang, G., Venkataraman, S., Phanishayee, A., Thelin, J., metric network topology. In 2020 57th ACM/IEEE De- Devanur, N. R., and Stoica, I. Blink: Fast and generic col- sign Automation Conference (DAC), pp. 1–6, 2020. doi: lectives for distributed ML. In Dhillon, I. S., Papailiopou- 10.1109/DAC18072.2020.9218538. los, D. S., and Sze, V. (eds.), Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, Luo, L., West, P., Krishnamurthy, A., Ceze, L., and Nel- USA, March 2-4, 2020. mlsys.org, 2020. URL https: son, J. Plink: Discovering and exploiting locality for //proceedings.mlsys.org/book/299.pdf. accelerated distributed training on the public cloud. In Dhillon, I. S., Papailiopoulos, D. S., and Sze, V. (eds.), Proceedings of Machine Learning and Systems 2020, ML- Sys 2020, Austin, TX, USA, March 2-4, 2020. mlsys.org, 2020. URL https://proceedings.mlsys.org/ book/293.pdf. Mirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y., Kumar, N., Norouzi, M., Bengio, S., and Dean, J. Device placement optimization with reinforce- ment learning. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, pp. 2430–2439. JMLR.org, 2017. Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Pat- wary, M., Korthikanti, V. A., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., and Za- haria, M. Efficient large-scale language model training on gpu clusters using megatron-lm. In The International Conference for High Performance Computing, Network- ing, Storage, and Analysis, 2021. NVIDIA. Nvidia nvlink and nvswitch. https://www. nvidia.com/en-gb/data-center/nvlink/. Accessed: 2021-10-07. NVidia. The nvidia collective communication library (nccl), 2021. URL https://developer.nvidia.com/ nccl. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models, 2020.
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning A EXPERIMENTS Parallelism Reduce Synthesis Simulation time Programs Parallelism matrix AllReduce Optimal Speedup axes axes time Ring Tree Ring Tree Ring Tree Ring Tree Ring Tree 2 nodes each with 16 A100 [32] [0] 0.244 0.458 0.860 9/47 3/47 [2 16] 4.74 3.57 3.18 2.83 1.49 1.26 [2 16] [0] 0.004 0.014 0.019 0/6 1/6 [[1 2] [2 8]] 0.12 0.17 0.12 0.15 1 1.13 [[2 1] [1 16]] 36.69 37.18 36.69 37.18 1 1 [1] 0.065 0.019 0.208 10/50 1/50 [[2 1] [1 16]] 0.18 0.22 0.18 0.20 1 1.1 [[1 2] [2 8]] 8.44 4.82 4.77 4.82 1.77 1 [4 8] [0] 0.026 0.125 0.137 11/50 5/50 [[1 4] [2 4]] 0.15 0.20 0.15 0.17 1 1.18 [[2 2] [1 8]] 28.91 19.61 18.48 18.5 1.56 1.06 [1] 0.030 0.135 0.149 12/50 3/50 [[2 2] [1 8]] 0.17 0.21 0.17 0.18 1 1.17 [[1 4] [2 4]] 16.04 9.15 8.97 9.01 1.79 1.02 [8 4] [0] 0.033 0.136 0.151 14/50 2/50 [[1 8] [2 2]] 0.17 0.21 0.17 0.19 1 1.11 [[2 4] [1 4]] 16.84 9.26 9.19 9.10 1.83 1.02 [1] 0.026 0.124 0.136 11/50 7/50 [[2 4] [1 4]] 0.15 0.20 0.15 0.17 1 1.18 [[1 8] [2 2]] 28.78 18.93 18.48 18.00 1.56 1.05 [16 2] [0] 0.067 0.195 0.214 10/50 2/50 [[1 16] [2 1]] 0.18 0.22 0.18 0.19 1 1.16 [[2 8] [1 2]] 8.86 5.34 5.05 5.21 1.75 1.02 [1] 0.005 0.011 0.012 0/6 1/6 [[2 8] [1 2]] 0.11 0.17 0.11 0.14 1 1.21 [[1 16] [2 1]] 36.84 36.82 36.84 36.82 1 1 4 nodes each with 16 A100 [64] [0] 1.161 1.868 2.01 6/47 5/47 [[4 16]] 5.18 5.75 4.29 4.74 1.21 1.21 [2 32] [0] 0.006 0.019 0.022 0/6 1/6 [[1 2] [4 8]] 0.12 0.17 0.12 0.15 1 1.13 [[2 1] [2 16]] 37.16 36.94 37.04 36.94 1.003 1 [1] 0.463 0.160 1.27 18/94 10/94 [[2 1] [2 16]] 4.81 3.41 3.05 3.07 1.58 1.11 [[1 2] [4 8]] 8.74 9.89 6.91 8.00 1.26 1.24 [4 16] [0] 0.043 0.259 0.287 12/53 10/53 [[1 4] [4 4]] 0.15 0.20 0.15 0.17 1 1.18 [[2 2] [2 8]] 28.77 19.81 18.24 18.55 1.57 1.07 [[4 1] [1 16]] 56.13 89.70 55.99 56.13 1.003 1.60 [1] 0.133 0.619 0.704 21/97 9/97 [[4 1] [1 16]] 0.18 0.22 0.18 0.19 1 1.16 [[2 2] [2 8]] 8.39 4.99 4.81 4.82 1.74 1.04 [[1 4] [4 4]] 17.70 19.03 13.38 14.85 1.32 1.28 [8 8] [0] 0.091 0.512 0.590 20/97 7/97 [[1 8] [4 2]] 0.17 0.21 0.17 0.18 1 1.17 [[2 4] [2 4]] 16.52 9.18 9.21 9.18 1.79 1 [[4 2] [1 8]] 34.05 41.23 28.86 29.79 1.18 1.38 [1] 0.084 0.508 0.598 19/97 17/97 [[4 2] [1 8]] 0.17 0.21 0.17 0.18 1 1.17 [[2 4] [2 4]] 15.68 9.43 8.92 9.22 1.76 1.02 [[1 8] [4 2]] 33.92 41.06 27.93 29.36 1.21 1.40 [16 4] [0] 0.149 0.631 0.712 21/97 17/97 [[1 16] [4 1]] 0.18 0.22 0.18 0.2 1 1.1 [[2 8] [2 2]] 8.81 5.42 5.01 5.25 1.76 1.03 [[4 4] [1 4]] 18.30 20.13 14.13 14.90 1.30 1.35 [1] 0.042 0.261 0.297 13/53 5/53 [[4 4] [1 4]] 0.15 0.20 0.15 0.18 1 1.11 [[2 8] [2 2]] 28.68 18.47 19.09 18.41 1.50 1.003 [[1 16] [4 1]] 57.13 85.22 56.23 55.63 1.02 1.53 [32 2] [0] 0.483 1.183 1.30 20/94 15/94 [[2 16] [2 1]] 4.74 3.99 3.14 3.13 1.51 1.27 [[4 8] [1 2]] 9.37 10.41 7.23 7.71 1.30 1.35 [1] 0.008 0.020 0.022 0/6 1/6 [[4 8] [1 2]] 0.11 0.17 0.11 0.15 1 1.13 [[2 16] [2 1]] 37.18 37.10 37.18 37.10 1 1 [16 2 2] [0 2] 0.968 2.36 2.55 25/188 21/188 [[1 16] [2 1] [2 1]] 4.79 3.69 4.63 2.71 1.03 1.36 [[2 8] [2 1] [1 2]] 4.91 3.97 3.10 2.93 1.58 1.35 [[2 8] [1 2] [2 1]] 9.05 10.29 9.03 9.46 1.002 1.09 [[4 4] [1 2] [1 2]] 9.14 10.32 7.08 7.87 1.29 1.31 [8 2 4] [0 2] 1.107 2.88 3.08 28/235 22/235 [[1 8] [2 1] [2 2]] 4.80 3.62 4.64 2.67 1.03 1.36 [[2 4] [2 1] [1 4]] 4.82 3.87 3.12 3.08 1.54 1.26 [[2 4] [1 2] [2 2]] 8.91 9.68 8.91 9.29 1 1.04 [[4 2] [1 2] [1 4]] 9.19 10.24 7.02 7.69 1.31 1.33 [[1 8] [1 2] [4 1]] 9.21 10.37 5.50 8.72 1.7 1.19 [4 2 8] [0 2] 1.143 2.86 3.09 32/235 24/235 [[2 2] [2 1] [1 8]] 4.74 3.54 3.04 2.99 1.56 1.18 [[1 4] [2 1] [2 4]] 4.77 3.77 4.48 3.54 1.06 1.06 [[4 1] [1 2] [1 8]] 8.73 9.81 7.00 8.12 1.25 1.21 [[1 4] [1 2] [4 2]] 9.12 9.98 8.65 8.80 1.05 1.13
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning [[2 2] [1 2] [2 4]] 9.12 10.36 9.02 9.77 1.01 1.06 [2 2 16] [0 2] 0.927 2.32 2.51 29/188 16/188 [[2 1] [2 1] [1 16]] 4.82 3.91 2.99 3.00 1.61 1.30 [[1 2] [2 1] [2 8]] 5.28 4.29 4.77 3.66 1.11 1.17 [[2 1] [1 2] [2 8]] 9.32 9.61 7.10 7.97 1.31 1.21 [[1 2] [1 2] [4 4]] 9.81 9.79 9.81 9.37 1 1.04 2 nodes each with 8 V100 [16] [0] 0.058 0.158 0.178 12/47 0/47 [[2 8] ] 4.58 2.30 2.41 2.3 1.90 1 [2 8] [0] 0.0035 0.014 0.134 0/6 0/6 [[1 2] [2 4]] 9.37 8.44 9.37 8.44 1 1 [[2 1] [1 8]] 14.53 14.55 14.53 14.55 1 1 [1] 0.0257 0.206 0.128 6/50 2/50 [[2 1] [1 8]] 0.28 0.40 0.28 0.30 1 1.33 [[1 2] [2 4]] 6.59 3.70 6 3.70 1.10 1 [4 4] [0] 0.0181 0.243 0.161 0/50 8/50 [[1 4] [2 2]] 12.55 13.01 12.55 13.01 1 1 [[2 2] [1 4]] 13.11 16.11 13.11 13.5 1 1.19 [1] 0.0182 0.181 0.116 10/50 2/50 [[2 2] [1 4]] 2.96 0.43 2.29 0.43 1.29 1 [[1 4] [2 2]] 10.95 7.42 7.52 7.26 1.46 1.02 [8 2] [0] 0.0272 0.137 0.304 1/50 4/50 [[1 8] [2 1]] 0.28 0.40 0.28 0.30 1 1.33 [[2 4] [1 2]] 14.24 15.47 14.2 14.48 1.003 1.07 [1] 0.0036 0.009 0.023 0/6 1/6 [[2 4] [1 2]] 0.32 0.33 0.32 0.33 1 1 [[1 8] [2 1]] 14.51 14.81 14.51 14.47 1 1.02 4 nodes each with 8 V100 [32] [0] 0.209 0.507 0.770 11/47 7/47 [[4 8] ] 4.83 4.57 4.83 3.66 1 1.25 [2 16] [0] 0.0043 0.024 0.027 0/6 1/6 [[1 2] [4 4]] 9.17 8.42 9.17 8.42 1 1 [[2 1] [2 8]] 14.47 14.52 14.47 14.51 1 1.0007 [2 16] [1] 0.121 0.536 0.621 10/94 7/94 [[2 1] [2 8]] 4.37 2.26 2.42 2.25 1.81 1.004 [[1 2] [4 4]] 7.18 8.10 7.16 7.10 1.003 1.14 [4 8] [0] 0.027 0.303 0.356 1/53 9/53 [[1 4] [4 2]] 12.60 13.00 12.60 13.00 1 1 [[2 2] [2 4]] 13.69 16.07 13.69 13.56 1 1.19 [[4 1] [1 8]] 22.21 30.55 22.04 21.49 1.001 1.42 [4 8] [1] 0.096 0.421 0.498 15/97 11/97 [[4 1] [1 8]] 0.28 0.40 0.28 0.30 1 1.33 [[2 2] [2 4]] 6.60 3.82 5.78 3.74 1.14 1.02 [[1 4] [4 2]] 12.94 15.47 11.21 12.12 1.15 1.28 [8 4] [0] 0.103 0.579 0.701 2/97 13/97 [[1 8] [4 1]] 0.28 0.39 0.28 0.3 1 1.3 [[2 4] [2 2]] 14.25 15.48 14.23 14.49 1.001 1.07 [[4 2] [1 4]] 14.84 19.90 1.84 15.33 1 1.30 [1] 0.0435 0.213 0.241 11/53 2/53 [[4 2] [1 4]] 2.96 0.43 2.29 0.43 1.29 1 [[2 4] [2 2]] 10.98 7.34 7.58 7.34 1.45 1 [[1 8] [4 1]] 21.74 30.42 21.74 21.71 1 1.40 [16 2] [0] 0.168 0.597 0.719 12/94 7/94 [[2 8] [2 1]] 4.47 2.25 2.44 2.25 1.83 1 [[4 4] [1 2]] 15.00 17.58 14.99 15.01 1.0007 1.17 [1] 0.004 0.016 0.019 0/6 1/6 [[4 4] [1 2]] 0.32 0.33 0.32 0.33 1 1 [[2 8] [2 1]] 14.53 14.89 14.53 14.66 1 1.02 [2 2 8] [0 2] 0.229 1.105 14/188 [[1 2] [2 1] [2 4]] 4.29 4.29 1 [[2 1] [2 1] [1 8]] 4.57 2.4 1.90 [[2 1] [1 2] [2 4]] 7.17 6.95 1.03 [[1 2] [1 2] [4 2]] 9.41 9.41 1 [8 2 2] [0 2] 0.243 1.184 17/188 [[1 8] [2 1] [2 1]] 4.40 4.40 1 [[2 4] [2 1] [1 2]] 4.80 2.35 2.04 [[4 2] [1 2] [1 2]] 9.36 9.35 1.001 [[2 4] [1 2] [2 1]] 15.02 14.95 1.005
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning B PROOF OF EXPRESSIVENESS BETWEEN Here, the use of x ensures that we form the same i0 SYNTHESIS HIERARCHIES reduction groups, and then we use x jm (instead of x j0) to get the first (second/etc) device from each reduction Recall our definitions of synthesis hierarchies: group that connects to the same x . As x is the jm jm (cid:2) (cid:3) last parallelism factor for x , this makes sure that we (a) System hierarchy ( 1 2 2 4 ) j (cid:2) (cid:3) only connect devices that connect to the same x . (b) Column-based parallelism factors ( 1 1 1 2 2 1 2 2 ) j (cid:2) (cid:3) (c) Row-based parallelism factors ( 1 1 2 2 1 2 1 2 ) Case 3 : (a) uses (x , Master(x ), C). (d) Reduction axis parallelism factors ((cid:2) 1 2 1 2(cid:3) ) i j Then (b) can use (x , Master(x ), C) to express the i0 jm Note that while we can assume all system hierarchies (i.e., same reduction. The case is similar as the above one. (a), and thus (b) and (c)) start with a level of 1, e.g., [(rack, 1), (server, 2), (CPU, 2), (GPU, 4)], it may not be the case for (d). To make it a complete synthesis hierarchy, we will append a (root, 1) as the root of (d). B.2 Part 2 Our goal is to prove B.2.1 Semantically valid reduction Theorem 3.2. (d) ≥ (c) ≥ (b) ≥ (a). Lemma B.2 (Parallel reduction). Consider a synthesis hi- We split our goal into three parts: (1) (b) ≥ (a) (Ap- erarchy (cid:2) x 0 x 1 · · · x j · · · x i · · · x n(cid:3), and the device groups pendix B.1 Lemma B.1); (2) (c) ≥ (b) (Appendix B.2 formed by (x i, Parallel(x j), C), we have: Lemma B.7); (2) (d) ≥ (c) (Appendix B.3 Lemma B.8), and prove them separately. 1. a device group to be reduced has size x × ... × x , j+1 i and we have x × x × ... × x × x × ... × x During the proof, we often use parallelism factors (x) and 0 1 j i+1 n number of such groups. their corresponding levels (e) interchangeably. 2. a device group has been partitioned over x , ..., x . j+1 i B.1 Part 1 Lemma B.1. (b) ≥ (a). Proof. We can derive the observation from the semantics of Parallel. In particular, we first form reduction groups for Proof. This lemma is intuitive, as (b) expands (a), so can devices connected to the same x . So each reduction group i be used to express any communication patterns that can be has size size = x × ... × x . 1 i+1 n formed by (a). Then, because of Parallel, we reduce all first (second/etc) Suppose (a) is (cid:2) x x · · · x (cid:3) , devices in the reduction group if they are connected to the 0 1 n and the parallelism axes has size 0..m, same x . Since each x owns size = x × ... × x j j 2 j+1 i then (b) is (cid:2) x · · · x x · · · x · · · x · · · x (cid:3) , different reduction groups, the device group we form is 00 0m 10 1m n0 nm m exactly of size . And for each x , we have size of such (cid:89) 2 j 1 with x = x . device groups. Since we have size = x ×...×x different ij i 3 0 j x , we have in total size × size device groups. j=0 j 3 1 Now we show that every reduction instruction given by (a), The second observation is similar. In particular, since device a reduction instruction can be formed by (b) expressing the groups are formed by each x , which owns size different j 2 same reduction. x s with different x , ..., x , the device groups we form i j+1 i are exactly partitioned over x , ..., x . j+1 i Case 1 : (a) uses (x , InsideGroup, C). i Then (b) can use (x , InsideGroup, C) to express the i0 same reduction. Lemma B.3 (Partitioning over reduction axes). Given re- duction axes, for a reduction instruction to be semantically To see why this is true, note that x , forall j, repre- ij valid, all device groups to be reduced must only be parti- sents the same system hierarchy as x . So x and i i0 tioned over the reduction axes. InsideGroup form the same device groups as x and i InsideGroup. Proof. Suppose we want to reduce over reduction axes A, Case 2 : (a) uses (x , Parallel(x ), C). i j and we reduce between d and d that have been, possibly j j Then (b) can use (x , Parallel(x ), C) to express the among others, partitioned over B (different from A). Then, i0 jm same reduction. after reduction both devices contain data that differs in B.
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning Now the desired final state (where data should only be re- Then since x(cid:48) is part of x , ..., x , we can find within i+1 n duced if they have different A) becomes unreachable for x , ..., x a device d , that has only different B with the i+1 n 0B both devices, as we can never recover the state of the device very first device d . 0 since according to the semantics, once a row has grown it Suppose d reduces with some device d in this master will never get reduced back. 0 1 reduction. Since the reduction is valid, d and d differ only 0 1 Corollary B.4 (Semantically valid parallel reduction). Con- in A. sider a synthesis hierarchy (cid:2) x 0 x 1 · · · x j · · · x i · · · x n(cid:3), Then in the same way we find d dB to d 0, we can find a given some reduction axes, a reduction instruction device d , that is only different with d in B. 1B 1 (x , Parallel(x ), C) is only semantically valid if all of i j x , ..., x are either 1, or on the reduction axes. However, note that this is a master reduction, and d 0 and j+1 i d belong to the same x , ..., x group, so the master 0B i+1 n reduction will only reduce d and d , but only d and d . 0 1 0B 1B Proof. By Lemma B.2, we know each device group is parti- tioned over x , ..., x . However, Lemma B.3 shows that Now we can show that the final desired state becomes un- j+1 i for each device group to be semantically valid, they can reachable. In particular, note that d 0B and d 1B will never only be partitioned over the reduction axes. Therefore, all get reduced: every reduction that reduces d 0B and d 1B will xs in x j+1, ..., x i. should either be 1, or on the reduction reduce d 0 and d 1 as well (Figure 6 is useful here). But since axes. d 0 and d 1 has been reduced already, re-reducing the devices is an invalid reduction step. So we will never be able to Lemma B.5 (Semantically valid InsideGroup reduction). reduce d and d again. And thus the master reduction 0B 1B Consider a synthesis hierarchy (cid:2) x 0 x 1 · · · x j · · · x i · · · x n(cid:3), is invalid. given some reduction axes, a reduction instruction (x , InsideGroup, C) is only semantically valid if all of i x i+1, ..., x n are either 1, or on the reduction axes. Lemma B.7. (c) ≥ (b). Proof. Similar as Lemma B.2, except in this case we know Proof. Case 1 (b) has (e 2, Parallel(e 1), C). that we are reducing devices over x i+1, ..., x n. Based on Corollary B.4, all non-reduction parallelism factors column-wisely between e (exclusive) and e 1 2 (inclusive) can only be 1. Lemma B.6 (Semantically valid master reduction). Con- Now we construct a reduction instruction for (c) that sider a synthesis hierarchy (cid:2) x 0 x 1 · · · x j · · · x i · · · x n(cid:3), expresses the same reduction. Suppose the reduction given some reduction axes, a reduction instruction step in (b) covers parallelism factors e , ..., e on the i j (x i, Master(x j), C) is only semantically valid if all of reduction axis. Note that if the reduction step in (b) x j+1, ..., x n are either 1, or on the reduction axes. does not cover any parallelism factors, that means it forms groups of one device, and in that case this does not form a reduction and won’t be generated by P 2. Proof. Note that in this case we require all parallelism fac- tors up until x (as with InsideGroup, rather than x as with Let e(cid:48) be the level right before e row-wisely in n i 1 i Parallel). The trickiness here is that the case of Master is the synthesis hierarchy (c), and let e(cid:48) be e . Then 2 j different from Parallel: while in Parallel we know that we (e(cid:48) , Parallel(e(cid:48) ), C) is a desired reduction instruction. 2 1 will reduce in parallel everything that is not in the range Indeed, we can derive from Lemma B.2 that this re- of Parallel (Lemma B.2), with Master we reduce only the duction instruction forms the same device groups as first reduction group. So it is important to guarantee that we (b). form exactly the same first inner reduction groups. The example from the paper is repeated below, with We prove the result by contradiction. Suppose x , ..., x the reduction axis highlighted. j+1 n contains a parallelism factor x(cid:48) that is not 1 nor is on the reduction axes. Then there are two possibilities.  x 0,0 1 1 x 0,3 x 0,0 1 1 x 0,3  e 1 x 1,0 1 1 x 1,3 x 1,0 1 1 x 1,3 e(cid:48) 1 (1) x(cid:48) is part of x j+1, ..., x i, i.e., it is covered by the reduc-  x 2,0 x 2,1 x 2,2 x 2,3 x 2,0 x 2,1 x 2,2 x 2,3 e(cid:48) 2 tion. Then we will reduce devices of different x(cid:48). According 1 1 1 x 3,3e 2 1 1 1 x 3,3 to lemma B.3, the reduction is invalid. (2) x(cid:48) is part of x , ..., x , i.e., it is not covered by the Case 2 (b) has (e, InsideGroup, C). i+1 n reduction, but it affects the reduction groups we form. Sup- This case can be reasoned in a similar way as the pre- pose reduction axes are A, and x(cid:48) is on level B, with B (cid:54)= A. vious case, by using Lemma B.5.
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning We construct a reduction instruction for (c) that ex- Case 1 (c) has (e, InsideGroup, C). presses the same reduction. Suppose the reduction We construct a reduction instruction for (d) that ex- step in (b) covers parallelism factors e , ..., e on the i j presses the same reduction. reduction axis. Let e(cid:48) be the level corresponding to the parallelism factor right before e row-wisely. Then If the reduction covers the whole parallelism factor, i (e(cid:48), InsideGroup, C) is a desired reduction instruction. then (root, InsideGroup, C) is a desired reduction in- struction. Below we give an example, with the reduction axis highlighted. x x x x  0,0 0,1 0,2 0,3  x 0,0 1 1 1  x 0,0 1 1 1  e x 1,0 x 1,1 1 1  e x 1,0 1 1 1  x 1,0 1 1 1  e(cid:48) x 2,0 x 2,1 x 2,2 x 2,3  x 2,0 x 2,1 x 2,2 x 2,3 x 2,0 x 2,1 x 2,2 x 2,3 1 1 1 1 1 1 1 1 1 1 1 1 If the reduction covers some (but not all) parallelism On the other hand, we can also show a counterex- factors on the reduction axis, then e itself is on the ample of (b) ≥ (c), where in the following hierarchy, (e(cid:48), InsideGroup, C) is a valid reduction in (c) but there reduction aixs. is no way in (b) that can simulate the same reduction. Then (e, InsideGroup, C) is a desired reduction instruc- tion. x x x x  0,0 0,1 0,2 0,3 x 1,0 x 1,1 x 1,2 x 1,3 e(cid:48) x x x x  x x x x  0,0 0,1 0,2 0,3 2,0 2,1 2,2 2,3 x x 1 1 1 1 1 1 e  x1 2, ,0 0 x 21, ,1 1 x 2,2 x 2,3  1 1 1 1 Similar counterexamples can also be shown for other cases. Again, if the reduction does not cover any parallelism factors on the reduction axis, then it forms reduction Case 3 (b) has (e , Master(e ), C). 2 1 groups of size 1, and thus this does not form a reduction This case can be reasoned in a similar way as the pre- and won’t be generated by P 2. vious case, by using Lemma B.6. Below we also show a counterexample of (c) ≥ (d), We construct a reduction instruction for (c) that ex- where in the following hierarachy, (e, InsideGroup, C) presses the same reduction. The choice of e(cid:48) and e(cid:48) 1 2 is a valid reduction in (d) but there is no way in (c) is the same as Case 1. Then (e(cid:48) , Parallel(e(cid:48) ), C) is a 2 1 that can simulate the same reduction since x can be 3 desired reduction instruction. arbitrary numbers. Below we give an example, with the reduction axis highlighted. x x x x  0,0 0,1 0,2 0,3  x 1 1 1  x 1 1 1  x 1,0 x 1,1 1 1  e 1  x x10 2, ,,0 00 x 21 ,1 x 21 ,2 x 21 ,3   x x0 1 2, , ,0 0 0 x 21 ,1 x 21 ,2 x1 2,3  e(cid:48) 2e(cid:48) 1 e x x2 3, ,0 0 x x2 3,, 11 x x2 3, ,2 2 x x2 3, ,3 3 1 1 1 1 e 2 1 1 1 1 Case 2 (c) has (e , Parallel(e ), C). 2 1 Similar as the previous case, we construct a reduction B.3 Part3 instruction for (d) that expresses the same reduction. Suppose the reduction step in (c) covers parallelism Lemma B.8. (d) ≥ (c). factors e , ..., e on the reduction axis. Let e(cid:48) be the i j 1 level in the synthesis hierarchy (d) right before e row- Proof. Most reasoning is the same as Lemma B.7. For i wisely. If the reduction covers the whole parallelism each case of a (c) reduction instruction, we show how we factor, then e(cid:48) would be the root. Let e(cid:48) be e . Then construct the reduction instruction for (d) that expresses the 2 j (e(cid:48) , Parallel(e(cid:48) ), C) is a desired reduction instruction. same reduction. Remember that we attach a (root, 1) to 2 1 synthesis hierarchy (d). In the following example, e(cid:48) = root. 1
Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning x x0, 1 x x  x 1 1 x  0,0 0,2 0,3 0,0 0,3 e 1x 1,0 x 1,1 1 1  x 1,0 x 1,1 1 1  x 2,0 x 2,1 x 2,2 x 2,3 x 2,0 x 2,1 x 2,2 x 2,3 e(cid:48) 2 1 1 1 x 3,3 e 2 1 1 1 x 3,3 Case 3 (c) has (e , Master(e ), C). 2 1 This case is exactly the same as the case for Parallel.
