Wide Neural Networks Forget Less Catastrophically Seyed Iman Mirzadeh * 1 Arslan Chaudhry 2 † Dong Yin 2 Huiyi Hu 2 Razvan Pascanu 2 Dilan Gorur 2 Mehrdad Farajtabar 2 † Abstract et al., 2019). To overcome these inefficiencies, fields, such as continual learning (CL) (Ring, 1995) or lifelong learn- A primary focus area in continual learning re- ing (Thrun, 1995) are gaining a lot of attention recently. search is alleviating the “catastrophic forgetting” One of the key challenges in continual learning models is problem in neural networks by designing new al- the abrupt erasure of previous knowledge, referred to as gorithms that are more robust to the distribution catastrophic forgetting (CF) (McCloskey and Cohen, 1989). shifts. While the recent progress in continual learning literature is encouraging, our understand- Alleviating catastrophic forgetting has attracted a lot of at- ing of what properties of neural networks con- tention lately, and many interesting solutions are proposed to tribute to catastrophic forgetting is still limited. partly overcome the issue (e.g., Toneva et al., 2019; Nguyen To address this, instead of focusing on continual et al., 2019; Hsu et al., 2018; Li et al., 2019; Wallingford learning algorithms, in this work, we focus on the et al., 2020). These solutions vary in degree of complexity model itself and study the impact of “width” of from simple replay-based methods to complicated regular- the neural network architecture on catastrophic ization or network expansion-based methods. Unfortunately, forgetting, and show that width has a surprisingly however, there is not much fundamental understanding of significant effect on forgetting. To explain this the intrinsic properties of neural networks that affect contin- effect, we study the learning dynamics of the net- ual learning performance through catastrophic forgetting or work from various perspectives such as gradient forward/backward transfer (Mirzadeh et al., 2020b). More- orthogonality, sparsity, and lazy training regime. over, most of the continual learning works focus on the We provide potential explanations that are con- algorithmic side of alleviating the forgetting rather than the sistent with the empirical results across different architecture being used throughout the learning experience. architectures and continual learning benchmarks. This motivated our work to study one of the most important aspects of neural network architectures: the structure of the neural networks (e.g., width and depth). 1. Introduction In contrast to the orthodox importance of the depth for Machine learning is relying more and more on training network performance, such as in AlexNet (Krizhevsky et al., large models on large static datasets to reach impressive 2012), Inception (Szegedy et al., 2016), ResNets (He et al., results (Kaplan et al., 2020; Lazaridou et al., 2021; Hom- 2016) or GPT family of models (Brown et al., 2020), we baiah et al., 2021). However, the real world is changing study how the width of the network affects the continual over time, and new information is becoming available at learning performance – aligned with the recent research on an unprecedented rate (Lazaridou et al., 2021; Hombaiah infinite width networks and the neural tangent kernel (NTK) et al., 2021). In such real-world problems, the learning regime (Jacot et al., 2018; Lee et al., 2019; Arora et al., agent is exposed to a continuous stream of data, with poten- 2019; Chizat et al., 2019). We empirically demonstrate that tially changing data distribution, and it has to absorb new increasing the width alone reduces catastrophic forgetting information efficiently while not being able to iterate on significantly, while it’s not the case for depth. previous data as freely as wanted due to time, sample, com- Concretely, Figures 1 and 2 depict our findings on two pute, privacy, or environmental complexity issues (Parisi popular continual learning benchmarks. Figure 1 shows that *Work done during an internship at DeepMind. †Equal advis- by increasing the width of a 2-layer MLP from 32 to 2048 ing. 1Washington State University 2DeepMind. Correspondence the amount of forgetting of task 1 (after learning the 5th to: Seyed Iman Mirzadeh <seyediman.mirzadeh@wsu.edu>, task) decreases from 62 to 48 percent. On Split CIFAR-100, Mehrdad Farajtabar <farajtabar@google.com>. by multiplying the width of a WideResNet-10 by 8 we can Proceedings of the 39 th International Conference on Machine decrease forgetting of the first task (after learning the 20th Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- task) from 42 to 31 percent. right 2022 by the author(s). 2202 luJ 41 ]GL.sc[ 3v62511.0112:viXra
Wide Neural Networks Forget Less Catastrophically 65 60 55 50 32 128 512 2048 Width )1ksaT(gnittegroF Replay Buffer Size 45 125 250 40 35 30 1 2 4 8 Width (a) MLP on Rotated MNIST )1ksaT(gnittegroF Replay Buffer Size 100 400 65 60 55 50 2 4 6 8 Depth (b) WRN on Split CIFAR-100 Figure 1. Amount of forgetting w.r.t. increasing width. Wider networks forget less. The boost in performance gained by only increasing the width is comparable to employing alternative methods spe- cially developed to tackle CF with moderately sized re- play buffers or regularization schemes. For example, it can be seen that in Figure 1a, wider networks without replay achieve comparable or better performance than the network with width 32 and employing replay buffers of sizes of 125 and 250. A similar level of performance improvement can be observed in Figure 1b, where the baseline WideResNet (width factor equal to 1) is trained with replay buffer sizes of 100 and 400. The ability of the wider networks to be less forgetful points towards looking at forgetting through the lens of over-parametrization (Kaplan et al., 2020; Lazaridou et al., 2021; Hombaiah et al., 2021). It is noteworthy that increasing width is not aimed to achieve state-of-the-art performance in alleviating forget- ting. Our goal is not to compete with computationally cheaper and performant rehearsal or regularization-based methods (Chaudhry et al., 2018; Farajtabar et al., 2020; Mirzadeh et al., 2021) but to provide a perspective on how over-parametrization and especially the width of the neu- ral network can affect CF. One may wonder that a similar over-parametrization achieved through depth might have a similar positive effect on continual learning performance. Figure 2 shows that this is not the case and that achieving over-parametrization through depth has no or even negative effect on mitigating forgetting. Contributions. We study the implications of different neu- ral network structures and over-parametrization (i.e., width and depth) for continual learning. We show that increased width can alleviate the forgetting and provide simplistic the- oretical and empirical justifications such as lazy training regime and orthogonalization that can explain the benefits of using wide neural networks in continual learning. 2. Related Work In continual learning (Ring, 1995), also referred to as life- long learning (Thrun, 1995), an agent, constrained in time and memory, receives a sequence of tasks. The goal of the agent is to remember the useful knowledge of the past tasks to solve new tasks efficiently. Catastrophic forget- )1ksaT(gnittegroF width = 128 50 45 40 35 30 10 16 22 28 Depth (a) MLP on Rotated MNIST )1ksaT(gnittegroF width = 2 (b) WRN on Split CIFAR-100 Figure 2. Amount of forgetting w.r.t. increasing depth. ting (McCloskey and Cohen, 1989; McClelland et al., 1995; Goodfellow et al., 2013)—a sudden erasure of previous knowledge when exposed to new tasks—is identified as one of the key challenges in continual learning. Recently, a lot of algorithmic progress has been made in mitigating catastrophic forgetting in neural networks. The progress can broadly be classified into three categories. Regularization-based methods (Rebuffi et al., 2016; Kirk- patrick et al., 2017; Zenke et al., 2017; Nguyen et al., 2018; Aljundi et al., 2018; Yin et al., 2020) attempt to identify important parameters or features of the past tasks, and mod- ify the training objective of the new task such that those parameters or features do not drift significantly. While regularization-based methods provide strong results for a small number of tasks, the importance of parameters drifts when the number of tasks is large (Chaudhry et al., 2018; Titsias et al., 2019). Expansion-based methods (Rusu et al., 2016; Fernando et al., 2017; Aljundi et al., 2017; Rosenbaum et al., 2018; Chang et al., 2018; Xu and Zhu, 2018; Ferran Alet, 2018; Li et al., 2019; Veniat et al., 2021) add new network modules or experts for new tasks. By construction, expansion-based methods can have zero-forgetting, but their memory com- plexity can increase super-linearly with the number of tasks, making them undesirable when the pool of tasks is large. We note that even though the expansion-based works may have the model as their main subject, the main objective of these works is designing algorithms that can efficiently use a model (e.g., if the model should be expanded or not), and they do not explicitly analyze the structure of the model (e.g., width). For instance, while Yoon et al. (2018) show that expanding the network capacity thorough width is help- ful, they have not studied the impact of increasing capacity when the depth increases, nor why increasing the width is helpful. Overall, in this work, we are interested in under- standing the impacts of network structure (e.g., width and depth) on continual learning performance by focusing on the model itself rather than the algorithm that works with the model. Perhaps the strongest are the replay-based methods that store a small subset of data from previous tasks in the replay buffer, and either use it directly in the form of experience replay with new task (Shin et al., 2017; Kirichenko et al.,
Wide Neural Networks Forget Less Catastrophically 2021; Rolnick et al., 2019; Riemer et al., 2018; Chaudhry show that large pre-trained models perform significantly et al., 2019; Chaudhry et al., 2020; Balaji et al., 2020) or use better than the models trained from scratch in continual it as a set of optimization constraints (Lopez-Paz and Ran- learning scenarios. zato, 2017; Chaudhry et al., 2018; Farajtabar et al., 2020). While these methods provide algorithmic improvements 3. Main Experiments on top of standard neural network training (empirical risk In this section, we study the impact of width on both average minimization), there is still limited understanding as to what F1-score and average forgetting. Further detailed experi- causes forgetting in the first place. Some works have looked ments and results will be provided in Sections 4 and 5. at the forgetting from the lens of network representations. For example, Javed and White (2019); Beaulieu et al. (2020) 3.1. Experimental Setup showed that meta-learned representations forget less when trained continually. On the other hand, Mehta et al. (2021) The experimental setup, such as benchmarks, network archi- showed that pre-trained over-parameterized networks forget tectures, continual learning setting (e.g., number of tasks, considerably less than randomly initialized networks when episodic memory size, and training epochs per task), hyper- trained continually. Similarly, few works have studied the parameters, and evaluation metrics are chosen to be similar forgetting from the neural network’s training dynamics to several other studies (Chaudhry et al., 2019; Farajtabar perspective. Mirzadeh et al. (2021) showed that the optima et al., 2020; Mirzadeh et al., 2021). To ensure that the of the multitask learning and continual learning solutions reported results are not more favorable to a specific architec- are linearly connected in the parameter space. Contrary ture, we use a grid of hyper-parameters, and for each model, to these works, we look at catastrophic forgetting from the we report the results with the best hyperparameters. For all lens of network architecture, specifically with respect to experiments, we report the average and standard deviation its width and depth and show that wider networks forget over five runs with different random seeds for network ini- considerably less. tialization. In addition to random seed for initialization, for Split CIFAR-100, we use three different seeds for each run A flurry of recent papers in deep learning theory has taken a where each seed corresponds to the random order in which huge step in understanding the training behavior of neural classes are selected for each task. We provide more details networks. In particular, Jacot et al. (2018) first introduced about our experimental design in Appendix A. the concept of neural tangent kernel (NTK) by studying the training dynamics of neural networks with square loss in the Benchmarks. We report our findings on two standard infinite width or “over-parameterized” regime. The result- continual learning benchmarks: Rotated MNIST and Split ing neural network can be approximated by a linear model. CIFAR-100. In Rotated MNIST, each task is generated by Based on the NTK framework, other works (Allen-Zhu the continual rotation of the MNIST images for degrees 0, et al., 2019; Du et al., 2019; Zou et al., 2020) have proved 22.5, 45, 67.5, and 90, respectively, constituting 5 differ- the convergence of the SGD on various other settings and ent tasks. For Split CIFAR-100 each task contains the data proved the NTK to be a powerful framework to study neu- from 5 random classes (without replacement), resulting in ral networks training dynamics. Later Chizat et al. (2019) 20 tasks. Finally, in both benchmarks, each model trains on developed a parallel idea under the framework called “lazy each task for five epochs. We have selected Rotated MNIST training” where the weights stay near their initial values dur- as a domain-incremental benchmark and Split CIFAR-100 ing the training. In this regime, the system becomes almost as a task-incremental benchmark to represent two common linear, and the dynamics of SGD within this region can be scenarios in continual learning. characterized via properties of the associated NTK. In the Networks. For the Rotated MNIST benchmark, we use a context of continual learning, Doan et al. (2021) have stud- two-layer MLP with ReLU non-linearities in each layer and ied forgetting of projection-based algorithms in the NTK vary the width. For the Split CIFAR-100 benchmark, we regime using the similarity of the tasks. use a WideResNet (WRN) with the depth of 10 (Zagoruyko Finally, several recent works have explored the intersec- and Komodakis, 2016) and different width factors which tion of neural network architectures and continual learning. multiply the width of the layers (i.e., the number of convolu- Mirzadeh et al. (2022) provides a comprehensive analysis tional channels) in the original WideResNet by 1, 2, 4, 8 . on the significant role of architectures and architectural de- All models are trained using the SGD optimiz{ er, and th} e cisions in continual learning. They show that that different best result for each model is reported. architecture families have different learning and retention Evaluations. Following previous work (Chaudhry et al., capabilities, and compare the merits and flaws of various 2018; Riemer et al., 2018; Mirzadeh et al., 2021), we report architectures. For the pre-training setup, Ramasesh et al. the following metrics: (2022) studies the effect of scaling in continual learning and (1) Average F1-score: The average validation F1-score
Wide Neural Networks Forget Less Catastrophically Table 1. MLP on Rotated MNIST Table 2. WideResNet-10 on Split CIFAR-100 Average Average Learning Joint Average Average Learning Joint Width Width F1-score Forgetting F1-score F1-score F1-score Forgetting F1-score F1-score 32 65.9 1.00 36.9 1.27 95.5 0.85 91.2 0.57 1 47.1 2.60 37.3 2.62 81.1 4.37 81.4 0.62 128 70.8 ± 0.68 31.5 ± 0.92 96.0 ± 0.90 93.4 ± 0.66 2 49.7 ± 1.51 34.9 ± 1.72 82.9 ± 5.17 83.6 ± 0.13 512 72.6 ± 0.27 29.6 ± 0.36 96.4 ± 0.73 94.1 ± 0.77 4 53.8 ± 2.74 33.8 ± 2.16 83.9 ± 6.53 84.4 ± 0.64 2048 75.2 ± 0.34 26.7 ± 0.50 96.6 ± 0.61 94.0 ± 0.45 8 59.7 ± 2.33 29.4 ± 2.52 87.5 ± 4.15 84.8 ± 0.49 ± ± ± ± ± ± ± ± after the model has been continually trained for T tasks, is 4. Analysis defined as: T In this section, we overview the reasons we believe con- 1 (cid:88) A = a (3.1) tribute to the reduction in catastrophic forgetting when T T T,i i=1 widening the network. Although wide networks have higher capacity, similar to how it has been shown that wider net- where, a t,i is the validation F1-score on the dataset of task works generalize well because of implicit regularization i after the model finished learning task t. imposed by gradient descent (Jacot et al., 2018; Lee et al., (2) Average Forgetting: The average forgetting is calculated 2019), we argue that the behavior of catastrophic forgetting as the difference between the peak F1-score and the final cannot be reduced to capacity alone and we need to integrate F1-score of each task, after the continual learning experi- learning dynamics. Here, we first construct a simple exam- ence is finished. For a continual learning benchmark with T ple for a regression problem with squared loss to demon- tasks, it is defined as: strate this point. Informally, we have the following claim. Claim 4.1 (informal). Consider learning problems with 1 T (cid:88)−1 input space Rd, output space R, and squared loss. Let F = T 1 max t∈{1,...,T −1} (a t,i − a T,i) (3.2) be the class of linear models that maps the input to tF he1 − i=1 output and be the class of two-layer linear networks 2 F (i.e., no nonlinear activation). Then, there exist two tasks (3) Learning F1-score: The F1-score for each task directly such that when we train task 2 using gradient descent, if we after it is learned. The learning F1-score provides a good use model class , the amount of forgetting for task 1 is 1 F representation of the plasticity of a model and can be calcu- strictly zero; whereas if we use model class , the amount 2 F lated using: of forgetting can be positive. T 1 (cid:88) LA = a (3.3) We provide details of the construction of the two tasks in T T i,i i=1 Appendix C. As we can see, the two model classes 1 and F have exactly the same capacity, i.e., = , since 2 1 2 F F F there is no nonlinearity in . However, due to the use of (4) Joint F1-score: The F1-score of the model trained on 2 F gradient descent algorithm, the amount of forgetting for task the combined data of all tasks together. 1 is different for the two model classes when training on task 2. Moreover, in Appendix C, we show that if the size of 3.2. Main Results a hidden layer in is not large enough, then the forgetting 2 F Tables 1 and 2 show the main results in terms of average could be more severe. Intuitively, if the additional layer F1-score, average forgetting, and learning F1-score for MLP becomes a bottleneck that projects the input to a very low on Rotated MNIST and WideResent on Split CIFAR-100 dimensional space, the information can be lost. with varying widths. It can be seen from the tables that wider While this example is a specially constructed one, we be- networks improve both average F1-score and forgetting. lieve it can be helpful towards understanding the benefits It is important to note that reduced forgetting is not due to of wide neural networks in continual learning. In particular, stabilizing the model dogmatically at the cost of plasticity. it shows that we have to take the commonly used gradi- Increased average F1-score means that the wider models ent descent algorithm into account when investigating this not only remember previous tasks better, but keep their phenomenon. In Appendix B, we provide further empirical competitive performance on the current task too. This can results regarding the significance of the width of the first be further consolidated by observing the learning F1-score layer. In the following, we dive deeper in this phenomenon (LA) of the wider networks. and provide several potential explanations.
Wide Neural Networks Forget Less Catastrophically 90 80 70 60 50 2 3 4 5 Tasks (t) neewtebelgnA L dna L 1 t ∇ ∇ 90 85 80 75 Width 70 32 512 128 2048 65 2 5 10 15 20 Tasks (t) (a) MLP on Rotated MNIST neewtebelgnA L dna L 1 t ∇ ∇ Width 1 4 2 8 (b) WRN on Split CIFAR-100 Figure 3. The angle between the gradients at the optimum of task 1, ∇L (w∗), and between the optimum of subsequent tasks t, ∇L (w∗), 1 1 t t for t > 1. Wider networks remain closer to 90 degree (more orthogonal). 4.1. Increased Gradient Orthogonality Hence, the one-step forgetting is small if we have near- orthogonal gradients for the two tasks. The next question is Earlier works in continual learning (Farajtabar et al., 2020; whether widening the network increases the gradient orthog- Chaudhry et al., 2018; Lopez-Paz and Ranzato, 2017; onality. Empirically, to study the directions of the gradients Riemer et al., 2018) have focused on explicitly forcing the for different tasks, in Figure 3, we plot the angle between gradients of the new tasks to be orthogonal to those of the the gradient vector at the minimum of the first task and that previous ones to reduce forgetting among the tasks. Here, of the subsequent tasks for various width factors. For this we want to look at how the gradient directions of different figure, we concatenate the gradients from all the layers into tasks evolve as the continual learning experience progresses one vector. It can be seen from the figure that the gradients while the networks’ width increases. We first establish why at the minimum of subsequent tasks become more orthogo- orthogonalizing the gradients of different tasks is a good nal to the gradients at the minimum of the first task as we continual learning strategy. increase the width. This trend also holds when the gradi- Let us consider a two-task continual learning problem, and ents of other tasks are used as reference (i.e.) the gradients study the amount of forgetting for task 1 induced by one at the minimum of tasks > 2 also become orthogonal to gradient descent step on task 2. Let Rp be the model the gradients at the minimum of task 2. Refer to the ap- parameter space and L (w) : WR b⊆ e the training loss pendix for these results. Since the gradients of different t W (cid:55)→ function of the t-th task. Assuming that all the loss functions tasks generally become more orthogonal for the wider net- are differentiable, according to mean value theorem, we works, the reduction in forgetting can be attributed to this have the following simple observation. orthogonalization. Claim 4.2. Let w(cid:48) = w η L (w). Then there exists 2 ξ [0, 1] such that − ∇ 4.2. Increased Gradient Sparsity ∈ L (w(cid:48)) L (w) = η L (w ξη L (w)), L (w) . Another scenario where forgetting may be reduced is that 1 1 1 2 2 − − (cid:104)∇ − ∇ ∇ (cid:105) (4.1) during the training of a new task, we only need to update a small number of model parameters, and thus the network is As we can see, the left hand side of Eq. 4.1 is the increase stable and forgets less about the previous tasks. In fact, ac- in the loss of task 1, and the right hand side is related to cording to Claim 4.2, we can establish the following simple the inner product of the gradient of L at w ξη L (w), corollary. 1 2 − ∇ and the gradient of L 2 at w. Therefore, if the gradients of Claim 4.4. Suppose that L (w) B for all w , 2 2 the two tasks are more orthogonal to each other, then when and let b(w) := sup (cid:107)∇ (cid:107) L≤ (w) . Let w∈ (cid:48) W = training on new tasks, the model forgets less about previous w η L (w). Then(cid:107) ww(cid:101) e− hw a(cid:107) v2≤ e ηB (cid:107)∇ 1 (cid:101) (cid:107)∞ 2 task. Formally, we have the following direct corollary. − ∇ L (w(cid:48)) L (w) ηb(w) L (w) , Claim 4.3. Suppose that for any w , w with w 1 1 2 0 1 2 1 − ≤ (cid:107)∇ (cid:107) ∈ W (cid:107) − w 2 (cid:107)2 ≤ η sup w∈W (cid:107)∇L 2(w) (cid:107)2, we have where (cid:107) · (cid:107)0 denotes the number of non-zero elements in a vector. L (w ), L (w ) (cid:15)/η, 1 1 2 2 |(cid:104)∇ ∇ (cid:105)| ≤ for some (cid:15) > 0, and let w(cid:48) = w η L (w). Then, we have Therefore, increased gradient sparsity may lead to less for- 2 − ∇ L (w(cid:48)) L (w) (cid:15). getting. Figure 4 confirms this hypothesis by demonstrating 1 1 − ≤
Wide Neural Networks Forget Less Catastrophically 1.0 0.8 0.6 0.4 0.2 0.0 0.000 0.002 0.004 0.006 0.008 0.010 Gradient Magnitude Distribution oitaR 1.0 width 2048 0.8 512 128 32 0.6 0.4 0.2 0.0 0.000 0.002 0.004 0.006 0.008 0.010 Gradient Magnitude Distribution (a) MLP on Rotated MNIST oitaR width 8 4 2 1 (b) WRN on Split CIFAR-100 Figure 4. Histogram of the magnitude of the elements of the gradient vector. Wider networks have sparser gradients (the histogram is more skewed towards the left). the histogram of the absolute value of the entries of the suggests the following intuitive claim: The forgetting may gradient. The gradient norm is constructed by calculating be less severe if the model moves a shorter distance D in t the gradients of all layers on the empirical loss of task 2, the parameter space. after learning task 1 and before starting task 2. From the Interestingly, for overparametrized neural networks, the dis- figure, it can be seen that the wider networks have sparser tance that the model parameter moves during SGD training gradients. This implies a smaller number of parameters can indeed be small. In their seminal papers, Jacot et al. needs to change to adapt to new tasks resulting in a reduced (2018) and Lee et al. (2019) connect learning in neural net- forgetting on already learned tasks. works to kernel methods and show that at the infinite width limit, the kernel doesn’t change throughout the training, and 4.3. Lazy Training Regime the neural network evolves as a linear model under SGD In the previous two sections, we showed that the amount training. Moreover, Chizat et al. (2019) show that these of forgetting induced by a single gradient descent step on a neural networks operate in lazy regime where w∗ win 2 (cid:107) − (cid:107) new task can be reduced by increased gradient orthogonal- is very small. Since wider networks are closer to the infinite ity/sparsity. In this section, we take a different perspective width limit, the moving distance D t may be smaller for and study the entire training process of a task. In the fol- wider networks and thus the forgetting is less severe. lowing, we denote by win and w∗ the initial and end model t t To confirm this intuition, in Figure 5, we plot the distance parameters for task t, respectively. Note that in the continual between the optimum of the first task and that of the subse- learning scenario, the initializer for a subsequent task is in quent tasks as the continual learning experience progress. In fact the solution to the previous task. Thus, win = w∗ . t t−1 the figure, each task is trained for 5 epochs. From the figure, Let us consider the amount of forgetting of task t 1 at the it can be seen that, for both MLPs and ResNets, the relative end of the training of task t, measured by the increa− se in loss distance of the parameters for the wider networks is sub- function L , i.e., L (w∗) L (w∗ ). Suppose that stantially smaller than that of the less wide networks. This the loss funt− c1 tion is dit f− fe1 rent tia− ble,t b− y1 met a− n1 value theorem, confirms that lazy training of wider networks is indeed ben- there exists ξ [0, 1] such that eficial for continual learning and reduces forgetting signifi- ∈ cantly. In the same vein, Mirzadeh et al. (2020b;a) observed L (w∗) L (w∗ ) that changing the learning regime (i.e., imposing a learning t−1 t − t−1 t−1 = L ((1 ξ)w∗ + ξw∗)(cid:62)(w∗ w∗ ), rate decay, decreasing batch size, employing dropout, etc.) ∇ t−1 − t−1 t t − t−1 also leads to a small change in parameters from the optimum which yields the following upper bound on the amount of of previous tasks resulting in less forgetting. However, their forgetting: work was concerned about the optimization properties of neural networks rather than architectural ones. L (w∗) L (w∗ ) t−1 t − t−1 t−1 D sup L (w) , (4.2) 4.4. The Case for Depth t t−1 2 ≤ {w:(cid:107)w−w t∗ −1(cid:107)2≤Dt} (cid:107)∇ (cid:107) In Section 3, we saw that the width of the network has a where D := w∗ w∗ is the distance the the model beneficial effect when it comes to catastrophic forgetting. parametet r mo(cid:107) vest d− urint g−1 th(cid:107) e2 training of task t. This result We do not see a similar monotonic reduction in forgetting
Wide Neural Networks Forget Less Catastrophically 0.6 0.4 0.2 0.0 5 10 15 20 25 (Task 1) (Task 2) (Task 3) (Task 4) (Task 5) Epochs wmorfecnatsiDevitaleR 1 Width 2.0 32 128 512 1.5 2048 1.0 0.5 0.0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 Epochs (a) MLP on Rotated MNIST wmorfecnatsiDevitaleR 1 Width 1 2 4 8 (b) WRN on Split CIFAR-100 Figure 5. The distance between w∗, solution of task 1, and w∗, solution of the t-th task, for t > 1 over the course of training. Wider 1 t networks remain closer to the solution of the first task. when we increase the depth of the network in our experi- where in the second line, we use sub-ordinate and sub- ments. In fact, we observe that with increasing depth, the multiplicative properties of induced matrix norms, and in forgetting increases, as shown in Figure 2. We now provide the third line, we assumed that ReLU activation is used and an intuitive analysis for why and when the network depth thus Σ 1. This analysis shows that the gradient norm i (cid:107) (cid:107) ≤ hurts the performance of a continual learning model. on the earlier layers is proportional to the product of matrix norms of higher layer weights. For a 2-norm, the gradi- Inspired by the exploding gradients phenomena in recurrent ent magnitude could be bounded by the product of largest neural networks (Bengio et al., 1994; Pascanu et al., 2013), singular values (Λ ) of the intermediate weight matrices: we show that deeper networks may cause larger gradient i updates in the earlier layers. Larger gradients mean that the (cid:13) (cid:13) ∂L (cid:13) (cid:13) (cid:13) (cid:13) ∂L (cid:13) (cid:13) K (cid:89)−1 solution for the subsequent tasks will not be found in the (cid:13) (cid:13) ∂h (cid:13) (cid:13) ≤ (cid:13) (cid:13) ∂h (cid:13) (cid:13) . (cid:107)Λ i+1 (cid:107) . (4.3) l K vicinity of the current task, which would entail increased i=l forgetting. To establish the larger gradients for deeper net- Although, for Λ > 1 this bound in unusable, as it is an works, we use the standard argument (Bengio et al., 1994) i upper bound which grows exponentially, and becomes more for exploding gradients. Suppose that the network consists and more loose, with K (the number of layers) increasing, of K feed-forward layers parameterized by weight matrices however, it does potentially point to a scenario when the W K and non-linearities σ( ). For ease of exposition, { i }i=1 · gradients on the earlier layers could increase with the depth. we ignore the biases and normalization layers in the network. This can happen when the singular values of all the interme- The gradient on any intermediate layer l can be written as: diate weight matrices are greater than 1, i.e., Λ > 1, i i ∀ ∈ ∂L = ∂L . ∂h K = ∂L . K (cid:89)−1 ∂h i+1 { thl a, t· · th· i, sK ind− ee1 d} i. sI tn heth ce as su ep fp ol re tm heen dt aa tr ay seF tsig au nr de a1 r1 c, hw itee cs th uo rew s ∂h ∂h ∂h ∂h ∂h l K l K i i=l considered in this work. A large gradient on earlier layers ∂L K (cid:89)−1 can then contribute to a larger forgetting of previous tasks. = . Σ W , ∂h K i+1 i+1 Figure 6a shows the norm of the gradients of layer 1 for i=l different MLP depths. It can be seen from the figure that the where Σ is the diagonal Jacobian of the non-linearity at i gradient norm on the earlier layers increases with the depth. the i-th layer. Note, if a ReLU non-linearity is used, then For example, when the network is trained for task 5, the Σ = 1, unless all the activations in the layer are negative. (cid:107) i (cid:107) gradient norm on layer 1 for an 8-layer network is almost The norm of the gradient on layer l can then be bounded as: three times as that of the 2-layer network. In contrast, as depicted in Figure 6b, increasing the width has a minimal (cid:13) (cid:13) (cid:13) (cid:13) ∂∂ hL l (cid:13) (cid:13) (cid:13) (cid:13) = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ∂∂ hL K . K (cid:89)−1 Σ i+1W i+1(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) o thr ae tv then e dd ee ec pre la es ai rn ng ine gff ce oc mt o mn ug nr ia tydi he an st dn eo vr em lo. pW ede nn uo mte eh roe ure s i=l strategies to avoid exploding gradients (Goodfellow et al., (cid:13) (cid:13) K−1 (cid:13) ∂L (cid:13) (cid:89) 2016), and that extensively studying those is not the purpose ≤ (cid:13) (cid:13) ∂h (cid:13) (cid:13) . (cid:107)Σ i+1 (cid:107) (cid:107)W i+1 (cid:107) here. We use the exploding gradient analysis to understand K i=l the negative effect of depths in our experiments. (cid:13) (cid:13) K−1 (cid:13) ∂L (cid:13) (cid:89) ≤ (cid:13) (cid:13) ∂h (cid:13) (cid:13) . (cid:107)W i+1 (cid:107) , K i=l
Wide Neural Networks Forget Less Catastrophically 15.0 12.5 10.0 7.5 5.0 2.5 0.0 2 3 4 5 Tasks Learned )h(∂/L∂ 1 1 || || width = 128 15.0 Depth 12.5 2 4 10.0 6 8 7.5 5.0 2.5 0.0 2 3 4 5 Tasks Learned (a) Gradient norm for different depths )h(∂/L∂ 1 1 || || Width 32 128 512 2048 (b) Gradient norm for different widths (cid:13) (cid:13) Figure 6. MLP on Rotated MNIST: The gradient norm in the first layer (i.e.,(cid:13) ∂L1 (cid:13)) grows faster in deeper networks (a) while this is not (cid:13) ∂h1 (cid:13) the case when the width increases (b). Table 3. Comparing shallow and wide vs. deep and thin MLP Table 4. Comparing shallow and wide vs. deep and thin WRN models on Rotated MNIST. Each row in a subdivision of the models on Split CIFAR-100. Each row in a subdivision of the table has roughly a comparable number of parameters. Shallower table has roughly a comparable number of parameters. Shallower and wider networks have higher average F1-score and smaller and wider networks have higher average F1-score and smaller forgetting than deeper and thinner networks. forgetting than deeper and thinner networks. Average Average Joint Average Average Joint Width Depth Parameters Depth Width Params F1-score Forgetting F1-score F1-score Forgetting F1-score 128 8 217.35 K 68.9 1.07 35.4 1.34 94.1 0.73 10 4 1.69 M 53.8 2.74 33.8 2.16 83.4 0.64 256 2 269.32 K 71.1 ± 0.43 31.4 ± 0.48 93.9 ± 0.65 28 2 1.61 M 46.6 ± 2.56 37.1 ± 2.47 83.6 ± 0.54 ± ± ± ± ± ± 256 8 664.08 K 70.4 0.61 32.1 0.75 94.78 0.67 ± ± ± 10 8 3.72 M 59.7 2.33 29.4 2.52 84.8 0.49 512 2 669.70 K 72.6 0.27 29.6 0.36 94.08 0.77 ± ± ± ± ± ± 16 4 3.24 M 50.1 2.59 37.0 2.77 85.1 0.45 ± ± ± 28 3 3.58 M 49.4 1.82 36.2 1.98 84.7 0.92 ± ± ± To this end, we compare several architectures on both Ro- In conclusion, we have observed that wider networks have tated MNIST and Split CIFAR-100 benchmarks. Table 3 sparser gradients with more orthogonal gradients across and 4 show that on both benchmarks, wider and shallower tasks. In addition, the training dynamics of wide networks models outperform their thinner and deeper counterparts. become more similar to the lazy training regime. Finally, Note that to make sure that the number of parameters are the gradient norm in wide and shallow models does not roughly the same, we have added some new models (e.g., increase as fast as in deeper and thinner models. MLP with a width of 256). 5. Additional Experiments 5.2. Interacting with CL Algorithms We have seen that increasing width automatically reduces This section provides further empirical results on the role forgetting without the need to employ any continual learn- of width and depth in continual learning. More specifically, ing algorithm. One may wonder if the effect of width on fixing the number of parameters, the wider and shallower performance remains additive even when a specialized CL models, outperform deeper and thinner ones. Moreover, algorithm is employed. we show that the benefits of increasing the width can be complementary to the benefits of algorithms. In Figure 7, we compare two replay-based methods, ER (Riemer et al., 2018) and A-GEM (Chaudhry et al., 2018), for various width factors using both MLPs and 5.1. Width versus Depth WideResNets. We use the replay buffer of sizes 125 We have already seen in Figure 1 and 2 that increasing (MNIST) and 100 (CIFAR-100) for both ER and A-GEM. width is beneficial while increasing depth has no such effect. The figure shows that, similar to vanilla fine-tuning, by in- However, it is interesting to answer the following question creasing width, average F1-score improves, and forgetting in a more direct manner: Given a fixed budget on the total gets reduced across the board. Hence, we conclude that the number of parameters, how would a wider and shallower benefit of increasing the width is additive to the benefits model perform compared to a thinner and deeper one? brought by continual learning algorithms.
Wide Neural Networks Forget Less Catastrophically 80.0 77.5 75.0 72.5 70.0 32 128 512 2048 Width ycaruccA eygarevA 35 Methods A-GEM 30 ER 25 gnittegroF egarevA (a) MLP on Rotated MNIST 65 60 55 50 45 40 1 2 4 8 Width ycaruccA eygarevA 26 24 Methods A-GEM 22 ER 20 18 gnittegroF egarevA width to tackle catastrophic forgetting as it is undoubtedly an inefficient decision in terms of computation time, energy and cost. By showing the connection between width and CF, our purpose here is to enhance our basic understanding of how over-parametrization, both in terms of width and depth, interacts with catastrophic forgetting. The extent of over-parametrization should be an important factor when designing continual learning benchmarks. We hope this work acts as a stepping stone for studying the interaction between the inherent properties of neural networks and catastrophic forgetting. In addition to vary- ing architecture sizes, modern neural networks employ sev- eral training tricks that are not well studied with respect to catastrophic forgetting. Understanding these effects, such as activation functions, normalization schemes, skip con- nections, etc., remains an interesting future work. Very recently, Mirzadeh et al. (2022) have studied the signifi- cance of architectures in continual learning. They have extended our main results for other architectures (e.g., Vi- sion Transformers (Dosovitskiy et al., 2021)) on large-scale (b) WRN on Split CIFAR-100 benchmarks. In addition, they show that the role of ar- Figure 7. Average F1-score (blue) and forgetting (red) of CL meth- chitecture in continual learning can be as important as the ods for varying widths. Similar to the naive fine-tuning case, in the algorithms, which suggests the intersection of continual presence of other CL algorithms, wider networks perform better. learning and neural network architecture is an interesting and under-explored research direction. In Appendix B.7, we provide additional results for more al- Acknowledgments gorithms such as Mode Connectivity SGD (Mirzadeh et al., The authors are grateful to Amal Rannen-Triki, Timothy 2021), LwF (Li and Hoiem, 2018), and EWC (Kirkpatrick Nguyen, Hooman Shahrokhi, and Anonymous Reviewers et al., 2017) on Rotated MNIST. We refer the reader to for their valuable comments and feedback on this work. (Mirzadeh et al., 2022) for additional results on other bench- marks and architectures such as CNNs, ResNets (He et al., 2016), and Vision Transformers (Dosovitskiy et al., 2021). References Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and 6. Discussion and Conclusion Tuytelaars, T. (2018). Memory aware synapses: Learning what (not) to forget. In Proceedings of the European In this paper, we studied a previously unexplored connec- Conference on Computer Vision (ECCV), pages 139–154. tion between the width of the network and catastrophic forgetting (CF)—an important bottleneck for continual Aljundi, R., Chakravarty, P., and Tuytelaars, T. (2017). Ex- learning (CL) systems. Through different experiments, pert gate: Lifelong learning with a network of experts. In we hypothesize that high orthogonality between the CVPR, pages 7120–7129. gradients of different tasks induced by the wider networks, increased gradient sparsity, and a lazy training regime, Allen-Zhu, Z., Li, Y., and Liang, Y. (2019). Learning and i.e., the decreased distance of updated parameters from generalization in overparameterized neural networks, go- their initialization are the main factors alleviating catas- ing beyond two layers. Advances in neural information trophic forgetting. We highlighted the contrasts between processing systems. over-parametrization with increasing width to increasing depth and argued that increasing depth might have negative Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R., and consequences for forgetting while acknowledging its Wang, R. (2019). On exact computation with an infinitely importance for learning representations. wide neural net. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, The scope of current work is to show a surprising and, at pages 8141–8150. the same time, very intuitive correlation between width and forgetting. One should be cautious of utilizing increasing Balaji, Y., Farajtabar, M., Yin, D., Mott, A., and Li, A.
Wide Neural Networks Forget Less Catastrophically (2020). The effectiveness of memory replay in large scale Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. (2019). Gradi- continual learning. arXiv preprint arXiv:2010.02418. ent descent finds global minima of deep neural networks. In International Conference on Machine Learning, pages Beaulieu, S., Frati, L., Miconi, T., Lehman, J., Stanley, 1675–1685. PMLR. K. O., Clune, J., and Cheney, N. (2020). Learning to con- tinually learn. In ECAI 2020 - 24th European Conference Farajtabar, M., Azizan, N., Mott, A., and Li, A. (2020). on Artificial Intelligence. Orthogonal gradient descent for continual learning. In International Conference on Artificial Intelligence and Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning Statistics, pages 3762–3773. PMLR. long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157–166. Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A. A., Pritzel, A., and Wierstra, D. (2017). Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, Pathnet: Evolution channels gradient descent in super J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., neural networks. arXiv preprint arXiv:1701.08734. Askell, A., et al. (2020). Language models are few-shot learners. In Advances in Neural Information Processing Ferran Alet, Tomas Lozano-Perez, L. P. K. (2018). Modular Systems 33: Annual Conference on Neural Information meta-learning. arXiv preprint arXiv:1806.10166v1. Processing Systems 2020, NeurIPS 2020. Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Chang, M., Gupta, A., Levine, S., and Griffiths, T. L. (2018). learning. MIT press. Automatically composing representation transformations as a means for generalization. In ICML workshop Neural Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., Abstract Machines and Program Induction v2. and Bengio, Y. (2013). An empirical investigation of catastrophic forgetting in gradient-based neural networks. Chaudhry, A., Gordo, A., Dokania, P. K., Torr, P. H. S., and arXiv preprint arXiv:1312.6211. Lopez-Paz, D. (2021). Using hindsight to anchor past knowledge in continual learning. In Thirty-Fifth AAAI He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep Conference on Artificial Intelligence, AAAI 2021. residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern Chaudhry, A., Khan, N., Dokania, P. K., and Torr, P. H. recognition, pages 770–778. (2020). Continual learning in low-rank orthogonal sub- spaces. In Advances in Neural Information Processing Hombaiah, S. A., Chen, T., Zhang, M., Bendersky, M., Systems. and Najork, M. (2021). Dynamic language models for continuously evolving content. In KDD ’21: The 27th Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, ACM SIGKDD Conference on Knowledge Discovery and M. (2018). Efficient lifelong learning with a-gem. In Data Mining. International Conference on Learning Representations. Hsu, Y.-C., Liu, Y.-C., and Kira, Z. (2018). Re-evaluating Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., continual learning scenarios: A categorization and case Dokania, P. K., Torr, P. H. S., and Ranzato, M. (2019). for strong baselines. arXiv preprint arXiv:1810.12488. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486. Jacot, A., Gabriel, F., and Hongler, C. (2018). Neural tan- gent kernel: convergence and generalization in neural Chizat, L., Oyallon, E., and Bach, F. (2019). On lazy train- networks. In Proceedings of the 32nd International Con- ing in differentiable programming. In NeurIPS 2019-33rd ference on Neural Information Processing Systems, pages Conference on Neural Information Processing Systems, 8580–8589. pages 2937–2947. Javed, K. and White, M. (2019). Meta-learning represen- Doan, T., Bennani, M. A., Mazoure, B., Rabusseau, G., and tations for continual learning. In Advances in Neural Alquier, P. (2021). A theoretical analysis of catastrophic Information Processing Systems, pages 1820–1830. forgetting through the ntk overlap matrix. In International Conference on Artificial Intelligence and Statistics. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, Amodei, D. (2020). Scaling laws for neural language D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, models. arXiv preprint arXiv:2001.08361. M., Heigold, G., Gelly, S., et al. (2021). An image is worth 16x16 words: Transformers for image recognition Kirichenko, P., Farajtabar, M., Rao, D., Lakshminarayanan, at scale. In 9th International Conference on Learning B., Levine, N., Li, A., Hu, H., Wilson, A. G., and Pas- Representations, ICLR, 2021. canu, R. (2021). Task-agnostic continual learning with
Wide Neural Networks Forget Less Catastrophically hybrid probabilistic models. In ICML Workshop on Invert- Mirzadeh, S. I., Farajtabar, M., and Ghasemzadeh, H. ible Neural Networks, Normalizing Flows, and Explicit (2020a). Dropout as an implicit gating mechanism for Likelihood Models. continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Kirkpatrick, J. N., Pascanu, R., Rabinowitz, N. C., Veness, Workshops, pages 232–233. J., and et. al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy Mirzadeh, S. I., Farajtabar, M., Gorur, D., Pascanu, R., of Sciences of the United States of America, 114 13:3521– and Ghasemzadeh, H. (2021). Linear mode connectivity 3526. in multitask and continual learning. In International Conference on Learning Representations. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). MNIST classification with deep convolutional neural Mirzadeh, S. I., Farajtabar, M., Pascanu, R., and networks. Advances in neural information processing Ghasemzadeh, H. (2020b). Understanding the role of systems, 25:1097–1105. training regimes in continual learning. In Advances in Lazaridou, A., Kuncoro, A., Gribovskaya, E., Agrawal, D., Neural Information Processing Systems 33: Annual Con- Liska, A., Terzi, T., Gimenez, M., d’Autume, C. d. M., ference on Neural Information Processing Systems 2020, Ruder, S., Yogatama, D., et al. (2021). Pitfalls of static NeurIPS 2020. language modelling. arXiv preprint arXiv:2102.01951. Nguyen, C. V., Achille, A., Lam, M., Hassner, T., Ma- Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl- hadevan, V., and Soatto, S. (2019). Toward understand- Dickstein, J., and Pennington, J. (2019). Wide neural ing catastrophic forgetting in continual learning. arXiv networks of any depth evolve as linear models under gra- preprint arXiv:1908.01091. dient descent. Advances in neural information processing systems, 32:8572–8583. Nguyen, C. V., Li, Y., Bui, T. D., and Turner, R. E. (2018). Variational continual learning. In 6th International Con- Li, X., Zhou, Y., Wu, T., Socher, R., and Xiong, C. (2019). ference on Learning Representations, ICLR 2018. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In Proceedings of Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., and Wermter, the 36th International Conference on Machine Learning, S. (2019). Continual lifelong learning with neural net- ICML, Proceedings of Machine Learning Research. works: A review. Neural Networks, 113:54–71. Li, Z. and Hoiem, D. (2018). Learning without forgetting. Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the IEEE Transactions on Pattern Analysis and Machine In- difficulty of training recurrent neural networks. In Interna- telligence, 40:2935–2947. tional conference on machine learning, pages 1310–1318. Lopez-Paz, D. and Ranzato, M. (2017). Gradient episodic PMLR. memory for continual learning. In Advances in Neural Information Processing Systems, pages 6467–6476. Ramasesh, V. V., Lewkowycz, A., and Dyer, E. (2022). Effect of scale on catastrophic forgetting in neural net- McClelland, J. L., McNaughton, B. L., and O’Reilly, R. C. works. In International Conference on Learning Repre- (1995). Why there are complementary learning systems sentations. in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning Rebuffi, S.-A., Kolesnikov, A. I., Sperl, G., and Lampert, and memory. Psychological review, 102(3):419. C. H. (2016). icarl: Incremental classifier and representa- tion learning. 2017 IEEE Conference on Computer Vision McCloskey, M. and Cohen, N. J. (1989). Catastrophic inter- and Pattern Recognition (CVPR), pages 5533–5542. ference in connectionist networks: The sequential learn- ing problem. Psychology of Learning and Motivation, Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, 24:109–165. Y., and Tesauro, G. (2018). Learning to learn without forgetting by maximizing transfer and minimizing inter- Mehta, S. V., Patil, D., Chandar, S., and Strubell, E. (2021). ference. In International Conference on Learning Repre- An empirical investigation of the role of pre-training in lifelong learning. ICML CL Workshop,. sentations. Mirzadeh, S. I., Chaudhry, A., Yin, D., Nguyen, T., Pascanu, Ring, M. B. (1995). Continual learning in reinforcement R., Gorur, D., and Farajtabar, M. (2022). Architecture environments. PhD thesis, University of Texas at Austin, matters in continual learning. ArXiv, abs/2202.00275. TX, USA.
Wide Neural Networks Forget Less Catastrophically Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., and Xu, J. and Zhu, Z. (2018). Reinforced continual learning. Wayne, G. (2019). Experience replay for continual learn- In Advances in Neural Informatio Processing Systems 31: ing. In Advances in Neural Information Processing Sys- Annual Conference on Neural Information Processing tems 32: Annual Conference on Neural Information Pro- Systems 2018, NeurIPS 2018. cessing Systems 2019, NeurIPS 2019. Yin, D., Farajtabar, M., Li, A., Levine, N., and Mott, A. Rosenbaum, C., Klinger, T., and Riemer, M. (2018). Rout- (2020). Optimization and generalization of regularization- ing networks: Adaptive selection of non-linear functions based continual learning: a loss approximation viewpoint. for multi-task learning. In International Conference on arXiv preprint arXiv:2006.10974. Learning Representations. Yoon, J., Yang, E., Lee, J., and Hwang, S. J. (2018). Lifelong learning with dynamically expandable networks. In Sixth Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, International Conference on Learning Representations. H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and ICLR. Hadsell, R. (2016). Progressive neural networks. arXiv preprint arXiv:1606.04671. Zagoruyko, S. and Komodakis, N. (2016). Wide residual networks. In British Machine Vision Conference 2016. Shin, H., Lee, J. K., Kim, J., and Kim, J. (2017). Continual British Machine Vision Association. learning with deep generative replay. In Advances in Neu- ral Information Processing Systems, pages 2990–2999. Zenke, F., Poole, B., and Ganguli, S. (2017). Continual learning through synaptic intelligence. In Proceedings of Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wo- the 34th International Conference on Machine Learning- jna, Z. (2016). Rethinking the inception architecture for Volume 70, pages 3987–3995. JMLR. computer vision. In Proceedings of the IEEE confer- ence on computer vision and pattern recognition, pages Zou, D., Cao, Y., Zhou, D., and Gu, Q. (2020). Gradient 2818–2826. descent optimizes over-parameterized deep relu networks. Machine Learning, 109(3):467–492. Thrun, S. (1995). A lifelong learning perspective for mobile robot control. In Intelligent robots and systems, pages 201–214. Elsevier. Titsias, M. K., Schwarz, J., Matthews, A. G. d. G., Pascanu, R., and Teh, Y. W. (2019). Functional regularisation for continual learning using gaussian processes. arXiv preprint arXiv:1901.11356. Toneva, M., Sordoni, A., Combes, R. T. d., Trischler, A., Bengio, Y., and Gordon, G. J. (2019). An empirical study of example forgetting during deep neural network learning. In 7th International Conference on Learning Representations, ICLR 2019. Veniat, T., Denoyer, L., and Ranzato, M. (2021). Efficient continual learning with modular networks and task-driven priors. In ICLR. Wallingford, M., Kusupati, A., Alizadeh-Vahid, K., Wals- man, A., Kembhavi, A., and Farhadi, A. (2020). In the wild: From ml models to pragmatic ml systems. ArXiv, abs/2007.02519. Wortsman, M., Ramanujan, V., Liu, R., Kembhavi, A., Rastegari, M., Yosinski, J., and Farhadi, A. (2020). Su- permasks in superposition. In Advances in Neural Infor- mation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020.
Wide Neural Networks Forget Less Catastrophically: Supplementary Material A. Experimental Setup Details In this section, we discuss our experimental setup, including the design choices for benchmarks, architectures, and hyper- parameters. A.1. Design Choices BENCHMARKS We have chosen the rotated MNIST benchmark since it is a common benchmark in the research literature and, unlike the permuted MNIST, has a meaningful shift across tasks. The number of 5 tasks on MNIST is also typical in the literature (Farajtabar et al., 2020; Mirzadeh et al., 2020b). While the rotation of 10 degrees per task is more common in the literature, we have increased the rotation to 22.5 degrees per task make the benchmark more difficult and the performance gap between different models become more visible (Mirzadeh et al., 2021). While the rotated MNIST benchmark is domain-incremental, we have chosen the split CIFAR-100, which is a common task-incremental benchmark in the literature. Moreover, the split CIFAR-100 has 20 tasks, which helps support our arguments when the number of tasks increases. Our goal was to show our results in various setups such as domain versus task incremental, small versus large number of tasks, and single-head versus multi-head learning. ARCHITECTURES On the MNIST benchmark, MLP is a popular architecture in the literature (Wortsman et al., 2020; Kirkpatrick et al., 2017). However, on the split CIFAR-100, ResNet-18 is a more popular choice (Chaudhry et al., 2021; Mirzadeh et al., 2021). However, WideResNets (WRN) have two essential properties that are more suitable for this study: first, they share similar architectural properties with ResNets (e.g., skip-connections, batch normalization layers, etc.), and second, they have the nice property of ease of scaling both width and depth. We note that for the experiments that involved increasing depth, we have not chosen the thinnest models as a starting point. For instance, in Fig. 2, we have used MLP with a width of 128 and WideResNet with a width of 2. The reason is to make sure that the models are not too small, so by increasing the depth, the number of parameters increases in a meaningful manner. A.2. Hyperparameters For all experiments, we use a grid to ensure that one specific set of hyper-parameters is not favoring a specific model. Moreover, the training regime plays an important role in continual learning performance (Mirzadeh et al., 2020b). 1. learning rate: [0.001, 0.01 (MNIST), 0.05 (CIFAR), 0.1] 2. batch size: [16, 32 (CIFAR), 64 (MLP)] 3. SGD momentum: [0.0 (MNIST), 0.8 (CIFAR)] 4. weight decay: [0.0 (MNIST), 0.0001 (CIFAR)] For each set of hyper-parameters, we run the model using 5 different seeds for random initialization of the models. Moreover, to ensure that the specific ordering of tasks in the split CIFAR-100 is not playing a role in the results, we also use 3 additional seeds to shuffle the ordering of tasks.
Wide Neural Networks Forget Less Catastrophically B. Additional Results B.1. Evolution of the Average F1-score Throughout the Learning Experience While tables 1 and 2 provide the “final” average F1-score across different widths, it will be interesting to also compare the “evolution” of the average F1-score in throughout of learning experience. Fig. 8a and 8b show this during the learning. There are a few interesting observations we can make from these results. First, the wider models perform consistently better during the learning experience. Second, the performance gap between wide and thin models is not large initially; however, as learning progresses, the gap increases. Finally, it shows that the decrease in the average F1-score due to forgetting still happens for wide models. This points out to our discussion in Sec. 6 that even though increasing width has beneficial effects, it is not the ultimate solution to continual learning. However, it will be interesting to understand the reasons behind these beneficial effects and use them in future works to design better continual learning algorithms, and this is the main objective of this study. 95 90 85 80 75 70 65 1 2 3 4 5 Tasks Learned ycaruccAnoitadilaVegarevA 80 70 60 Width 32 128 50 512 2048 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Tasks Learned (a) MLP on Rotated MNIST ycaruccAnoitadilaVegarevA Width 1 2 4 8 (b) WideResNet on Split CIFAR-100 Figure 8. Evolution of the average F1-score during the continual learning experience: while the gap is not initially large, it grows as the number of tasks increases. We note that while wider models learn task 1 with higher F1-score, the gap between wide and thin models grows as the number of tasks increases. For instance, in Fig. 8b, the F1-score of WRN-10-8 on task 1 is roughly 4% higher than WRN-10-1. However, when the learning ends, the F1-score gap between WRN-10-8 and WRN-10-1 grows to more than 12%. While part of this gap can be explained by the fact that WRN-10-8 is a better learner, we note that the majority of the gap comes from the fact that WRN-10-8 has substantially smaller forgetting, as shown in Table. 2. B.2. Varying Width In Claim 4.1 in Sec. 4, we have theoretically studied the importance of width for a simple case. Here, we provide additional empirical evidence for the Rotated MNIST benchmark where we use a 3-layer MLP model and measure continual learning metrics for different widths (64, 256, 1024) at each layer. As shown in Table. 5, we show that the width of the first layer plays a more important role compared to the second and third layers, which is consistent with our analysis. While the models that have a width of 1024 in their first layer have more parameters, we note that we have already discussed in Sec. 5 that simply increasing the number of parameters is not necessarily helpful for reducing the forgetting. B.3. Lazy Training Regime in Deeper Models In Sec. 4.3, we have discussed the role of parameter changes in the network. In Fig. 5a, we have provided empirical evidence each parameter of wide models changes less during the learning. However, one important argument could be that the norm of parameters increases as the number of parameters increases, and this might be the reason behind the result that the relative distance of wider models is smaller. However, we show in Fig. 9b that this is not the case, and the relative distance for deeper models does change as depth increases. Fig. 9a is similar to Fig. 5a and was put here for the ease comparison.
Wide Neural Networks Forget Less Catastrophically Table 5. Performance metrics for different 3-layer MLP models on Rotated MNIST. Average Average Learning L1 L2 L3 Params F1-score Forgetting F1-score 64 64 64 59.2 K 67.9 0.28 37.2 0.56 97.2 0.37 ± ± ± 64 64 256 73.6 K 68.5 0.99 37.1 0.18 97.5 0.67 ± ± ± 64 64 1024 131.2 K 68.1 0.64 37.2 0.82 97.8 0.28 ± ± ± 64 256 64 84.0 K 67.7 0.58 37.6 0.79 97.7 0.29 ± ± ± 64 1024 64 183.1 K 68.2 0.55 36.9 0.72 97.8 0.31 ± ± ± 256 256 64 283.8 K 71.9 0.61 32.8 0.84 97.8 0.28 ± ± ± 256 256 256 335.1 K 71.8 0.54 32.9 0.63 98.1 0.33 ± ± ± 256 256 1024 540.2 K 71.4 0.68 33.6 0.83 98.0 0.37 ± ± ± 1024 64 64 874.2 K 73.1 0.94 29.8 1.10 97.1 0.72 ± ± ± 1024 64 1024 946.2 K 73.9 0.69 29.4 0.76 97.4 0.52 ± ± ± 1024 1024 64 1.92 M 74.3 0.40 28.5 0.51 97.1 0.67 ± ± ± 1024 1024 256 2.12 M 74.4 0.35 28.4 0.50 97.1 0.66 ± ± ± 1024 1024 1024 2.91 M 75.1 0.41 27.8 0.52 97.3 0.60 ± ± ± 0.6 0.4 0.2 0.0 5 10 15 20 25 (Task 1) (Task 2) (Task 3) (Task 4) (Task 5) Epochs wmorfecnatsiDevitaleR 1 depth = 2 Width 32 0.6 128 512 2048 0.4 0.2 0.0 5 10 15 20 25 (Task 1) (Task 2) (Task 3) (Task 4) (Task 5) Epochs (a) MLP on Rotated MNIST (different widths) wmorfecnatsiDevitaleR 1 width = 128 Depth 2 4 6 8 (b) MLP on Rotated MNIST (different depths) Figure 9. The distance between w∗, solution of task 1, and w∗, solution of the t-th task, for t > 1 over the course of training. Wider 1 t networks remain closer to the solution of the first task, but deeper models do not show the same property. B.4. Detailed Results for Other Methods In Fig. 7 of Sec. 5, we have shown the average forgetting and F1-score of the A-GEM and Experience Replay(ER) algorithms to illustrate that the gain due to increasing width also happens in the presence of other popular continual learning algorithms. Tables 6 and 7 show the exact numbers for the mentioned figures, and we can see that by increasing width, the average F1-score increases and average forgetting decreases consistently. Moreover, we have included the data from tables 1 and 2 for ease of comparison. An interesting observation from these results could be that adding replay buffer or gradient manipulation by the A-GEM algorithm has an additive effect to the gain due to width. B.5. Orthogonality Between Task 2 and Subsequent Tasks To show that our conclusion in Section 4 about orthogonality of gradients holds for the next tasks, in this section, we provide a similar figure to Fig. 3 but for task 2. In Fig. 10, we plot the angle between the gradient vector at the minimum of the second task and that of the subsequent tasks for various width factors. We can see that for wider models, the trend of gradients being orthogonal continues after learning the second task as well.
Wide Neural Networks Forget Less Catastrophically Table 6. Rotated MNIST: the gain due to width for different CL Table 7. Split CIFAR-100: the gain due to width for different CL algorithms. Algorithms. Width Method Average F1-score Average Forgetting Width Method Average F1-score Average Forgetting 32 Naive 65.9 1.00 36.9 1.27 1 Naive 47.1 2.60 37.3 2.62 ± ± ± ± 128 Naive 70.8 0.68 31.5 0.92 2 Naive 49.7 1.51 34.9 1.72 ± ± ± ± 512 Naive 72.6 0.27 29.6 0.36 4 Naive 53.8 2.74 33.8 2.16 ± ± ± ± 2048 Naive 75.2 0.34 26.7 0.50 8 Naive 59.7 2.33 29.4 2.52 ± ± ± ± 32 ER 68.9 1.08 34.6 1.29 1 ER 51.6 1.74 24.7 0.86 ± ± ± ± 128 ER 75.6 0.55 27.7 0.72 2 ER 57.1 0.98 23.1 1.74 ± ± ± ± 512 ER 77.3 1.00 25.9 1.28 4 ER 59.0 1.10 20.9 2.45 ± ± ± ± 2048 ER 78.6 0.41 24.6 0.45 8 ER 63.8 0.37 18.8 0.49 ± ± ± ± 32 A-GEM 70.2 1.50 33.0 1.91 1 A-GEM 54.2 0.93 21.5 0.03 ± ± ± ± 128 A-GEM 74.1 0.52 29.6 0.73 2 A-GEM 57.9 1.32 20.0 1.11 ± ± ± ± 512 A-GEM 76.8 0.62 26.6 0.74 4 A-GEM 60.6 0.24 18.4 0.79 ± ± ± ± 2048 A-GEM 78.8 0.34 24.3 0.50 8 A-GEM 62.8 0.24 17.7 0.06 ± ± ± ± 90 85 80 75 70 65 60 3 4 5 Tasks (t) neewtebelgnA L dna L 2 t ∇ ∇ 90 85 80 Width 75 32 512 128 2048 70 5 10 15 20 Tasks (t) (a) MLP on Rotated MNIST neewtebelgnA L dna L 2 t ∇ ∇ Width 1 4 2 8 (b) WideResNet on Split CIFAR-100 Figure 10. The degree between the gradients of task 2 and the gradients of subsequent tasks B.6. Singular Values of Deeper Models In Eq. (4.3) of Sec. 4.4, we have discussed that one condition that impacts our analysis, is the singular values of all intermediate layers. Fig. 11a shows the singular values for our shallowest model with depth of 2, and Fig. 11b shows the values for the deepest model. We can see that the top singular values are all greater than 1. We note that to make the figures easier to read, we show the top-10 singular values at the end of tasks 1, 2, and 5. Finally, we would like to provide some clarifications on our analysis in Sec. 4.4. We note that the bound provided for the gradient norm is unusable for Λ > 1, as it is an upper bound which grows exponentially – becomes more and more loose – i with K (the number of layers). The purpose of providing the analysis is to potentially point to a scenario when the gradients on the earlier layers could increase with the depth. This can happen when the singular values of all the intermediate weight matrices are greater than 1, i.e., Λ > 1, i l, , K 1 . In Fig. 11, we empirically see that this indeed is the case for i ∀ ∈ { · · · − } Rotated MNIST tested with varying MLP depths. B.7. Interacting With Additional Algorithms In Sec. 5.2, we have shown that similar to the naive finetuning case, when using A-GEM (Chaudhry et al., 2018), and Experience Replay, wide networks perform better for both Rotated MNIST and Split CIFAR-100 benchmarks. Here, we extend the result for other algorithms on Rotated MNIST (with 5 tasks). We use Mode Connectivity SGD (MC- SGD) (Mirzadeh et al., 2021), which is the state-of-the-art algorithm in continual learning, in addition to Learning without Forgetting (LwF) (Li and Hoiem, 2018) and Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) methods.
Wide Neural Networks Forget Less Catastrophically 3.0 2.5 2.0 1.5 1 2 5 Tasks Learned seulaVralugniS width=128, depth=2 2.0 1.8 Layers 1 1.6 2 1.4 1.2 1 2 5 Tasks Learned (a) Rotated MNIST: depth = 2 seulaVralugniS width=128, depth=8 Layers 1 2 3 4 5 6 7 8 (b) Rotated MNIST: depth = 8 Figure 11. Top-10 singular values of each layer during the learning: top singular values are larger than 1. 90 85 80 75 70 65 60 32 128 512 2048 Width ycaruccAegarevA MCSGD LwF 40 ER A-GEM EWC 30 20 10 0 32 128 512 2048 Width (a) Rotated MNIST: Average F1-score gnittegroFeygarevA EWC A-GEM ER LwF MCSGD (b) Rotated MNIST: Average Forgetting Figure 12. Rotated MNIST: Similar to the naive fine-tuning case, in the presence of other CL algorithms, wider networks perform better. Fig. 12 shows that for all algorithms, by increasing the width, the average F1-score increases and average forgetting decreases. However, interestingly, the performance gain varies across algorithms, and methods such as MC-SGD and LwF benefit more from the increased width compared to other algorithms. An interesting future direction is to study this phenomenon in more detail. C. Task Construction in Claim 4.1 Consider a learning problem where the input space is Rd and the output space is R, and two model classes, , 1 2 Rd R , where is parameterized by w Rd: F F ⊆ 1 { (cid:55)→ } F ∈ := f : f (x) = x, w , w Rd , 1 w w F { (cid:104) (cid:105) ∈ } and is parameterized by U Rh×d and v Rh: 2 F ∈ ∈ := f : f (x) = v(cid:62)U x, U Rh×d, v Rh . 2 U,v F { ∈ ∈ } This means that is the set of linear models, and is the set of two-layer linear networks. Clearly, we have = , 1 2 1 2 F F F F i.e., they have the same capacity, since there is no non-linearity used in . For any model f or , and data point 2 1 2 (x, y) Rd R, we use the quadratic loss function (cid:96)(f ; x, y) = 1 (y fF (x))2. ∈ F F ∈ × 2 − Now consider two tasks with training data points (x , y ) n and (x , y ) n . We denote by X = [x x ](cid:62) Rn×d and Y = [y y ](cid:62) R{ n th1 e,i da1 t, ai m}i a= t1 rix and{ lab2 e, li ve2 c,i to} ri= fo1 r task t (t = 1, 2), rest pec- t,i t,n t t,i t,n · · · ∈ · · · ∈ tively. Define the training loss function: n 1 (cid:88) L (f ) = (cid:96)(f ; x , y ), t = 1, 2. t n t,i t,i i=1
Wide Neural Networks Forget Less Catastrophically For either model class, the learner first trains the model with data for task 1, and obtains model f and then after training 1 ends, the data for task 1 are all removed. The learner then trains the model with data for task 2. Let f be the model that the 2 learner obtains after training on task 2. The forgetting metric is defined as fgt(f , f ) := L (f ) L (f ). 2 1 1 2 1 1 − We will consider the over-parmeterized regime where d n. We know that as long as X X(cid:62) is invertable, there exist (cid:29) 1 1 models with zero loss on task 1. In this case, let us assume that the model is sufficiently trained on task 1, i.e., at the end of task 1, we have L (f ) = 0 for both model classes. Then we train the model on task 2 using gradient descent for T steps 1 1 with learning rate η. Let w0 and wT be the initial and end parameters for model class in task 2 training, respectively, and 1 F let (U 0, v0), (U T , vT ) be the initial and end parameters for model class in task 2 training, respectively. Then we have 2 F the following claim. Claim C.1 (Claim 4.1 restated). There exist two tasks such that when we train task 2 using T steps of gradient descent, if we use model class , the amount of forgetting is strictly zero, i.e., 1 F fgt(f , f ) = 0; wT w0 whereas if we use model class , the amount of forgetting can be positive, i.e., 2 F fgt(f , f ) 0. UT ;vT U0,v0 ≥ In particular, if h n, and the matrix X (U 0)(cid:62) Rn×h has rank h, and vT = v0, then the forgetting for model class 1 2 ≤ ∈ (cid:54) F is positive, i.e., fgt(f , f ) > 0. UT ;vT U0,v0 Proof. We construct the two tasks in the following way. We assume that the input features of the two tasks satisfy x , x = 0 for all 1 i, j n, 1,i 2,j (cid:104) (cid:105) ≤ ≤ i.e., X X(cid:62) = 0. This means that the data points in the two tasks are in two orthogonal subspaces. 1 2 When training on task 2, the learner use gradient descent algorithm with respect to the parameters. Here, we rewrite the loss function such that it becomes a function of the parameters. For model class , we let 1 F 1 L1(w) := Y X w 2, 2 2n (cid:107) 2 − 2 (cid:107)2 and for function class , we let 2 F 1 L2(U, v) := Y X U (cid:62)v 2. 2 2n (cid:107) 2 − 2 (cid:107)2 Consider model class . With learning rate η, the parameter update rule for model class is 1 1 F F η wt+1 = wt (X(cid:62)X wt X(cid:62)Y ). − n 2 2 − 2 2 Since X X(cid:62) = 0, we know X (wt+1 wt) = 0. Let w0 be the model parameter that we have at the beginning of the 1 2 1 − training of task 2. Then after T steps training on task 2, we have X wT = X w0. Thus the forgetting for function class 1 1 1 F is zero, i.e., fgt(f , f ) = 0. (C.1) wT w0 Now consider model class . The parameter update rule is 2 F η U t+1 = U t (vt(vt)(cid:62)U tX(cid:62)X vtY (cid:62)X ) − n 2 2 − 2 2 η vt+1 = vt (U tX(cid:62)X (U t)(cid:62)vt U tX(cid:62)Y ). − n 2 2 − 2 2
Wide Neural Networks Forget Less Catastrophically Suppose that the second task starts with f with parameters U 0 and v0. We assume that task 1 is well trained, so 1 L (U 0, v0) = 0. Since X(cid:62)X = 0, we know that X ((U t+1)(cid:62) (U t)(cid:62)) = 0, thus X (U t)(cid:62) = X (U 0)(cid:62) for any t. 1 1 2 1 − 1 1 Therefore 1 fgt(f , f ) = X (U T )(cid:62)vT Y 2 UT ,vT U0,v0 2n (cid:107) 1 − 1 (cid:107)2 1 = X (U 0)(cid:62)vT Y 2 2n (cid:107) 1 − 1 (cid:107)2 1 = X (U 0)(cid:62)(vT v0) 2 0. 2n (cid:107) 1 − (cid:107)2 ≥ One can also easily observe that if h n, and the matrix X (U 0)(cid:62) Rn×h has rank h, and vT = v0, then 1 ≤ ∈ (cid:54) fgt(f , f ) > 0. UT ,vT U0,v0
