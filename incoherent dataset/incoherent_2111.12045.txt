Adaptive Multi-Goal Exploration Jean Tarbouriech Omar Darwiche Domingues Pierre Ménard Meta AI & Inria Scool Inria Scool OvGU Magdeburg Matteo Pirotta Michal Valko Alessandro Lazaric Meta AI DeepMind Meta AI Abstract This general setting is often called unsupervised RL, self- supervised RL or intrinsically motivated RL. A notewor- thy instance of it is unsupervised goal-conditioned RL We introduce a generic strategy for provably (GC-RL, see e.g., Colas et al., 2020, for a survey). In efficient multi-goal exploration. It relies on this framework, the agent must learn a goal-conditioned AdaGoal, a novel goal selection scheme that policy, which learns a distribution over actions condi- leverages a measure of uncertainty in reach- tioned not only on the current state but also on a goal ing states to adaptively target goals that are state that it must reach as quickly as possible (in ex- neither too difficult nor too easy. We show pectation). For the goal-conditioned policy to be able how AdaGoal can be used to tackle the objec- to reach a variety of goals in the unknown environment, tive of learning an ε-optimal goal-conditioned the agent must autonomously set its own goals (via policy for the (initially unknown) set of goal e.g., a curriculum) and learn to effectively reach them. states that are reachable within L steps in ex- pectation from a reference state s in a reward- 0 Deep GC-RL. Recently, GC-RL has been exten- free Markov decision process. In the tabular sively studied in the context of deep RL (e.g., Schaul case with S states and A actions, our algo- et al., 2015; Andrychowicz et al., 2017; Florensa et al., rithm requires O(cid:101)(L3SAε−2) exploration steps, 2018; Warde-Farley et al., 2019; Nair et al., 2018; Colas which is nearly minimax optimal. We also et al., 2019; Zhao et al., 2019; Hartikainen et al., 2020; readily instantiate AdaGoal in linear mix- Ecoffet et al., 2021; Pong et al., 2020; Zhang et al., ture Markov decision processes, yielding the 2020b; Pitis et al., 2020). GC-RL has notably been first goal-oriented PAC guarantee with linear shown to be a powerful framework to tackle navigation function approximation. Beyond its strong problems (Florensa et al., 2018), game playing (Ecoffet theoretical guarantees, we anchor AdaGoal in et al., 2021, on Montezuma’s Revenge) or real-world goal-conditioned deep supervised learning, robotic manipulation tasks (Pong et al., 2020). At a both conceptually and empirically, by con- high-level, these deep GC-RL methods follow the same necting its idea of selecting “uncertain” goals algorithmic structure that alternates between: to maximizing value ensemble disagreement. (GS) Goal Selection: select a candidate goal state; (PE) Policy Execution: deploy an explorative policy conditioned on this goal for a fixed number of 1 INTRODUCTION steps. When the extrinsic reward signal is absent or non- Given this simple and unifying algorithmic structure, informative, a supervised learning (RL) agent needs the core differences between the methods lie in the to explore the environment driven by objectives other goal sampling distribution — e.g., uniform (Kaelbling, than reward maximization (see e.g., Chentanez et al., 1993; Schaul et al., 2015; Andrychowicz et al., 2017), 2005; Oudeyer and Kaplan, 2009; Singh et al., 2010). proportional to rarity (Pong et al., 2020), or targeting goals of increasing complexity (Florensa et al., 2018; Proceedings of the 25th International Conference on Artifi- Colas et al., 2019; Zhang et al., 2020b) — and in ways cial Intelligence and Statistics (AISTATS) 2022, Valencia, to take advantage of each policy execution as much as Spain. PMLR: Volume 151. Copyright 2022 by the au- possible to speed up the learning — e.g., goal relabeling thor(s). (Andrychowicz et al., 2017) or encoding goal states in 2202 beF 42 ]GL.sc[ 2v54021.1112:viXra
Adaptive Multi-Goal Exploration lower-dimensional representations (Pong et al., 2020). lower bound on the sample complexity. The specific choice of (GS) and (PE) steps directly • We introduce AdaGoal, a novel goal selection influence the learning speed as well as the quality of scheme for unsupervised goal-conditioned RL. It the goal-conditioned policy returned by the algorithm. relies on a simple optimization problem that adap- tively targets goal states of intermediate difficulty. While these GC-RL approaches effectively leverage deep RL techniques and are able to achieve impressive It also provides an algorithmic stopping rule and a results in complex domains, they often lack substantial set of candidate goal states that the agent is confi- dent it can reliably reach. We identify some generic theoretical understanding and guarantees, even when conditions that make AdaGoal theoretically sound. restricted to the tabular setting. • We design AdaGoal-UCBVI, an instantiation of AdaGoal in tabular MDPs, and prove that it Theory of GC-RL. On the other hand, GC-RL has achieves nearly minimax optimal sample complexity, been rather sparsely analyzed through a theoretical improving over existing bounds that only hold in lens. Notably, Lim and Auer (2012); Tarbouriech et al. special cases. (2020b) study the incremental autonomous exploration • Leveraging a similar algorithmic and proof structure, setting, where the agent is restricted to identifying and learning to efficiently reach only the goal states that we readily design AdaGoal-UCRL·VTR for linear- mixture MDPs. This yields the first goal-oriented are incrementally attainable from a reference initial PAC1 guarantee with linear function approximation. state. Meanwhile, Tarbouriech et al. (2021a) study the problem of goal-free cost-free exploration (i.e., finding a • We anchor AdaGoal in deep GC-RL, both conceptu- near-optimal goal-reaching policy for any starting state, ally and empirically, by connecting its idea of select- goal state and cost function) under the assumption that ing “uncertain” goals to a practical approximation the Markov decision process (MDP) is communicating of maximizing value ensemble disagreement (Zhang (i.e., any state is reachable from any other state). et al., 2020b). While the existing theoretical GC-RL approaches come 1.1 Additional Related Work with rigorous guarantees on the learning performance, their algorithmic designs are quite involved, far from Theory of unsupervised exploration. A recent the interpretable alternation of (GS) and (PE) steps line of work theoretically analyzes unsupervised explo- (see Appendix A for details, where we identify three ration in RL (in other settings than GC-RL). Some specific shortcomings). This makes it hard to extract approaches (e.g., Hazan et al., 2019; Tarbouriech and relevant practical insights and to make them amenable Lazaric, 2019; Cheung, 2019; Tarbouriech et al., 2020c) to a deep RL implementation. learn to induce a desired state(-action) distribution (e.g., maximally entropic), yet they do not analyze how Objective. In light of these conceptual and algorith- this can be used to solve downstream tasks. In finite- mic discrepancies between the two branches of theoret- horizon MDPs, Jin et al. (2020) propose a paradigm ical and deep GC-RL, we seek to: composed of: an exploration phase, where the agent interacts with the environment without the supervision Design a strategy for unsupervised goal-conditioned RL of reward signals; followed by a planning phase, where with both the agent is required to compute a near-optimal policy 1) a simple and interpretable algorithmic structure for some given reward function. If the reward func- whose conceptual idea can be adapted to deep RL, and tion can be chosen arbitrarily (including adversarially), 2) strong learning guarantees under suitable the problem is called reward-free exploration (RFE) assumptions on the environment (e.g., tabular, linear). (Jin et al., 2020; Kaufmann et al., 2021; Ménard et al., 2021a; Zanette et al., 2020; Wang et al., 2020; Chen Contributions. Our main contributions can be sum- et al., 2021b; Zhang et al., 2021a,b). If there is only marized as follows: a finite number of rewards that are fixed a priori yet • We formalize the multi-goal exploration (MGE) ob- unknown during exploration, it is called task-agnostic jective of minimizing the number of exploration steps exploration (TAE) (Zhang et al., 2020a; Wu et al., 2021b,a). Our MGE problem bears some resemblance (i.e., the sample complexity) required to learn a near- to TAE in the sense that we also consider a finite num- optimal goal-conditioned policy for all the goal states ber of unknown tasks to solve (specifically, finding a that are reachable within a given number of steps near-optimal goal-conditioned policy for the unknown in expectation from the initial state. We formally motivate the need of an available reset action to the 1Recall that the probably approximately correct (PAC) initial state so as to solve the objective in a reason- learning setting provides sample complexity guarantees to able number of exploration steps, and we derive a find a near-optimal policy at the fixed initial state.
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric set of reliably reachable goal states). However, TAE (as Let D ∈ [0, +∞], D ∈ [0, +∞] be the MDP’s (possibly 0 well as RFE) is limited to the finite-horizon setting and infinite) s -diameter and diameter, respectively, i.e., 0 does not extend to goal-reaching tasks, which belong to the category of stochastic shortest path problems. D 0 (cid:44) max V (cid:63)(s 0 → g), D (cid:44) max V (cid:63)(s → g). g∈S s,g Single-goal exploration (a.k.a. SSP). A special We denote by G ⊆ S the goal space, which corresponds case of multi-goal exploration is when there is a single to the set of goal states that the agent may condition goal state that is extrinsically fixed throughout learning its goal-conditioned policy on (in the absence of prior (i.e., there is no (GS)). This is known as the stochastic knowledge on the goal space we simply set G = S). shortest path (SSP) problem (Bertsekas, 1995). A In environments with arbitrary dynamics, there may recent line of work has studied online learning in SSP be some goal states in G that are too difficult for the in the context of regret minimization (Tarbouriech agent to reliably reach in a reasonable number of ex- et al., 2020a; Rosenberg et al., 2020; Cohen et al., 2021; ploration steps, or even completely unreachable from Tarbouriech et al., 2021c; Jafarnia-Jahromi et al., 2021; s . Consequently, we consider the high-level objective Chen et al., 2021a; Vial et al., 2021; Min et al., 2021). 0 of learning an accurate goal-conditioned policy for all However, as argued by Tarbouriech et al. (2021b), a the goal states that are reliably reachable from s . standard regret-to-PAC conversion does not hold in 0 general in SSP, which implies that these techniques Definition 2 (Reliably L-reachable goal states G L). cannot be directly applied to our case. See Section 5 For any threshold L ≥ 1, we define a goal state g ∈ G for further discussion. to be reliably L-reachable if V (cid:63)(s 0 → g) ≤ L, and we denote by G the set of such goal states, i.e., L (cid:110) (cid:111) 2 MULTI-GOAL EXPLORATION G (cid:44) g ∈ G : V (cid:63)(s → g) ≤ L . L 0 The agent interacts with an environment modeled by a We thus seek to learn a goal-conditioned policy that is Markov decision process (MDP) M that has no extrin- accurate in reaching the goals in G . A challenge in sic reward (i.e., it is reward-free), a finite state space L solving this objective comes from the fact that the set S with S (cid:44) |S| states and a finite action space A with of goals of interest G is initially unknown and it has A (cid:44) |A| actions. Let s ∈ S be a designated initial L 0 to be discovered online at the same time as learning state. We denote by p(s(cid:48)|s, a) the probability transition their corresponding optimal policy. The threshold L from state s to state s(cid:48) by taking action a. can be interpreted as the user’s exploration radius of A deterministic stationary policy π : S → A is a map- interest around s . In the absence of a pre-specified 0 ping between states to actions and we denote by Π threshold, the agent can build its own curriculum for L the set of all possible policies. We measure the perfor- to guide its learning process. mance of a policy in navigating the MDP and define Since in environments with arbitrary dynamics the the shortest-path distance as follows. agent may get stuck in a state without being able to Definition 1. For any policy π ∈ Π and a pair of return to s , we introduce the following “reset” assump- 0 states (s, s(cid:48)) ∈ S2, let V π(s → s(cid:48)) ∈ [0, +∞] be the tion.3 In Lemma 5 we will formally motivate its role expected number of steps it takes to reach s(cid:48) starting in solving our learning objective. from s when executing policy π, i.e.,2 Assumption 3. The action space contains a known (cid:104) (cid:105) V π(s → s(cid:48)) (cid:44) E ω π(s → s(cid:48)) , action a reset ∈ A such that p(s 0|s, a reset) = 1 for any state s ∈ S. ω π(s → s(cid:48)) (cid:44) inf (cid:110) i ≥ 0 : s i+1 = s(cid:48) (cid:12) (cid:12) s 1 = s, π(cid:111) , Consider as input an exploration radius L ≥ 1, an where the expectation is w.r.t. the random sequence of F1-score level ε ∈ (0, 1] and a confidence level δ ∈ (0, 1). states generated by executing π in M starting from We now formally define our exploration objective. state s. For any state g ∈ S, let V (cid:63)(s 0 → g) ∈ [0, +∞] Definition 4 (Multi-Goal Exploration — MGE). An be the shortest-path distance from s to g, i.e., 0 algorithm is said to be (ε, δ, L, G)-PAC for MGE if V (cid:63)(s 0 → g) (cid:44) min V π(s 0 → g). • it stops after some (possibly random) number of π∈Π exploration steps τ that is less than some polynomial 2Note that V π(s → s(cid:48)) corresponds to the value func- tion (a.k.a. expected cost-to-go) of policy π in a stochastic 3This setting should be contrasted with the finite-horizon shortest-path setting (SSP, Bertsekas, 1995) with initial setting, where each policy resets automatically after H steps, state s, goal state s(cid:48) and unit cost function. If the policy π or assumptions on the MDP dynamics such as ergodicity reaches s(cid:48) from s with probability 1, it is said to be proper, or bounded diameter, which guarantee that it is always otherwise it holds that V π(s → s(cid:48)) = +∞. possible to find a policy navigating between any two states.
Adaptive Multi-Goal Exploration in the relevant quantities (S, A, L, ε−1, log δ−1) with Ω((1 − γ)−3SAε−2) (Azar et al., 2013) — and online probability at least 1 − δ, stationary finite-horizon MDPs — i.e., Ω(H3SAε−2) • it returns a set of goal states X and a set of policies (Domingues et al., 2021b). This correspondence is not {π } such that P(cid:0) C ∩ C (cid:1) ≥ 1 − δ, where we surprising as L captures the “range” of the MGE prob- (cid:98)g g∈X 1 2 define the conditions lem, similar to the effective horizon 1/(1 − γ) or the horizon H. Also recall that both discounted MDPs (cid:110) (cid:111) and finite-horizon MDPs are subclasses of goal-oriented C 1 (cid:44) ∀g ∈ X , V π(cid:98)g (s 0 → g) − V (cid:63)(s 0 → g) ≤ ε , MDPs, i.e., SSP-MDPs (Bertsekas, 1995). (cid:110) (cid:111) C (cid:44) G ⊆ X ⊆ G . 2 L L+ε MGE under linear function approximation. Lemma 6 shows that the MGE sample complexity must The objective is to build an (ε, δ, L, G)-PAC algorithm for which the MGE sample complexity, that is the scale with SA in the worst case, which may be pro- hibitive in the case of large state-action spaces. This number of exploration steps τ , is as small as possible. motivates us to further analyze MGE under linear Remark 1. Since the goal set G is unknown, it may L function approximation. In particular, we focus on the not be possible to exactly identify it within a reasonable linear mixture MDP setting (Ayoub et al., 2020; Zhou number of exploration steps. Thus we allow the learner et al., 2021), which assumes that the transition proba- to output a larger set X of candidate goals and policies. bility is a linear mixture of d signed basis measures. Nonetheless, we constrain an (ε, δ, L, G)-PAC algorithm for MGE to return a set X that is at most contained Definition 7 (Linear Mixture MDP, Ayoub et al., 2020; in the slightly larger set G (i.e., X ⊆ G ). Zhou et al., 2021). The unknown transition probability L+ε L+ε R G em ⊆a Srk is2. anCo en qs uid ale ir tyth ia ft MG = isS c. omTh me un nt ih ce ati in nc glu (s ii .o en ., p φ ii (s s(cid:48)a |s,li an )e ,a ir .ec .,om p(b si (cid:48)n |sa ,t aio )n (cid:44)o (cid:80)f d d i=s 1ig φn ie (d s(cid:48)|b sa ,s ai )s θ i(cid:63)m .ea Msu eare ns - L D to < L + (n∞ ot) ea tn hd atif ut nh de e( run Ak sn suow mn p) tiD on0 3is , l Dow ≤er Dor e +qu 1a )l . w suh mile m, afo tir oa nn (cid:80)y V s(cid:48)∈: SS φ→ i(s[ (cid:48)0 |s, ,1 a], )Vi ∈ (s(cid:48)[ )d] i, s(s c, oa m) p∈ uS ta× bleA . , Ft ohe r Moreover, denote by S→ the set of incrementa0 lly L- simplicity, let φ (cid:44) [φ 1, . . . , φ d](cid:62), θ(cid:63) (cid:44) [θ 1(cid:63), . . . , θ d(cid:63)](cid:62) reachable states defined L by Lim and Auer (2012, Defini- and ψ V (s, a) (cid:44) (cid:80) s(cid:48)∈S φ(s(cid:48)|s, a)V (s). Without loss of tion 5). Then in the special case where S→ = G , the generality, we assume (cid:107)θ(cid:63)(cid:107) 2 ≤ B, (cid:107)ψ V (s, a)(cid:107) 2 ≤ 1 for L L MGE objective of Definition 4 is equivalent to the AX(cid:63) all V : S → [0, 1] and (s, a) ∈ S × A. objective proposed by Tarbouriech et al. (2020b, Defi- nition 5) in the incremental autonomous exploration 3 OUR AdaGoal APPROACH setting introduced by Lim and Auer (2012). In Algorithm 1, we introduce the common algorithmic MGE vs. reset-free MGE. Lemma 5 establishes structure based on AdaGoal. We use it to design an exponential separation between MGE and reset- AdaGoal-UCBVI that tackles the MGE problem in free MGE (i.e., MGE without Assumption 3). This tabular MDPs, and AdaGoal-UCRL·VTR that tackles motivates the use of Assumption 3 to solve our learning the MGE problem in linear mixture MDPs. Both follow objective in a reasonable number of exploration steps. the goal-conditioned structure described in Section 1. Lemma 5. MGE can be solved in poly(S, L, ε−1, A) The agent sets a horizon of H = Ω(L log Lε−1) and steps. On the other hand, there exists an MDP and a splits its learning interaction in algorithmic episodes of goal space where any algorithm requires at least Ω(D) length H. At the beginning of each algorithmic episode, steps to solve reset-free MGE, where D is exponentially it (GS) selects a candidate goal state and (PE) deploys larger than L, S, A, ε−1. an explorative (i.e., optimistic) policy conditioned on this goal for H steps before resetting to s . It alternates 0 MGE lower bound. We now give a worst-case lower between these two steps until an adaptive stopping rule bound on the MGE problem (details in Appendix B). is met, at which point the algorithm terminates.4 Lemma 6. For any algorithm that is (ε, δ, L, G)-PAC for MGE for any MDP and goal space G, there exists (PE) step. Goal-conditioned finite-horizon Q- functions (Kaelbling, 1993; Schaul et al., 2015) are an MDP and a goal space where the algorithm requires, (cid:50) in expectation, at least Ω(L3SAε−2) exploration steps maintained optimistically. At each episode k and to stop. episode step h ∈ [H], Q k,h(s, a, g) approximates (from below) the number of (expected) steps required to Remark 3. We can relate the dependencies in reach any goal g ∈ G starting from any state-action Lemma 6 with the lower bound of the time steps needed to identify an ε-optimal policy in both γ- 4Indeed recall from Definition 4 that the algorithm must discounted MDPs with a generative model — i.e., adaptively decide when to terminate its learning interaction.
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric Algorithm 1: AdaGoal-based algorithmic struc- pair (s, a) ∈ S × A and executing the optimal goal- ture. Blue text denotes AdaGoal-UCBVI spe- reaching policy for H − h steps. Intuitively, the Q- cific steps and purple text denotes AdaGoal- functions will gradually increase, more so for goal states UCRL·VTR specific steps. that the agent struggles to reach. This is essentially done by initializing the Q-functions optimistically (i.e., 1 Input: Exploration radius L ≥ 1, F1-score level at 0), considering that the cost (i.e., negative reward) ε ∈ (0, 1], confidence level δ ∈ (0, 1). is +1 (resp. 0) per time step if the conditioned goal is 2 Input: Number of states S, number of actions A. not reached (resp. reached), and carefully subtracting 3 Input: Dimension of feature mapping d, bound B an exploration bonus to maintain optimism. Given on (cid:96) -norm of θ(cid:63). 2 a goal g ∈ G selected at the beginning of episode k, 4 Input: Goal space G ⊆ S (otherwise set G = S). the (PEk ) step simply amounts to deploying an explo- Set as horizon 5 rative policy conditioned on g , that is, a policy π H (cid:44) (cid:100)5(L + 2) log (cid:0) 10(L + 2)/ε(cid:1) / log(2)(cid:101). that greedily minimizes the cuk rrent Q-functions, ik .e,h ., 6 Initialize: algorithmic episode index k = 1, π (s) ∈ arg min Q (s, a, g ). distance estimates D (g) = 1[g (cid:54)= s ], error k,h a∈A k,h k 1 0 estimates E 1(g) = H1[g (cid:54)= s 0], goal-conditioned (GS) step. To elect a relevant sequence of candi- fi fon rit ae l- lh (o gr ,i szo , an , Q h)-v ∈al Gue ×s Q S 1 ×,h A(s, ×a, [Hg) ].= 1[s (cid:54)= g], d g(cid:50) oa ate l g so ela el cs t( ig ok n) k s≥ ch1, emwe ein bt ar so ed duc oe nA ad sa iG mo pa lel, ca on na std ra ap inti ev de while stopping rule (1) is not met do optimization problem. It relies on the agent’s ability 7 ○i Goal selection rule: to compute two types of goal-conditioned quantities 8 9 Select as goal state for any goal g ∈ G and episode k ≥ 1: g k ∈ arg max E k(g) • a distance estimate from s 0 to g, denoted by D k(g); g∈G • an error of estimating this distance, denoted by E (g). k subject to: D k(g) ≤ L. Conveniently, the distance estimates can be simply in- stantiated as D (g) (cid:44) min Q (s , a, g). Formally, ○ii Policy execution rule: k a∈A k,1 0 we require the following properties of D and E to hold 10 For a duration of H steps, run the optimistic (with high probability): goal-conditioned policy πk such that at gk • Property 1: D is an optimistic distance estimate, step h, πk (s) ∈ arg min Q (s, a, g ) (note thag tk D,h (g ) = min a∈A Q k, (h s , a, g k )). i.e., k k a∈A k,1 0 k 11 Then execute action a reset and increment D (g) ≤ D(cid:63)(g), ∀k ≥ 1, ∀g ∈ G, episode index k += 1. k H 12 ○iii Update and check stopping rule: where D H(cid:63)(g) (cid:44) min π E(cid:2) ω π(s 0 → g) ∧ H(cid:3) denotes 13 Update estimates Q k, D k, E k according to the shortest-path distance from s 0 to g truncated (8), (9), (10) using samples collected so far. at H steps. Note that D(cid:63)(g) ∈ [0, H] and that H 14 Update estimates Q k, D k, E k according to lim H→+∞ D H(cid:63)(g) = V (cid:63)(s 0 → g). (29), (30), (31) using samples collected so far. • Property 2: E is an upper bound on the prediction 15 Stop the algorithm if error, i.e., max E (g) ≤ ε. (1) g∈G: Dk(g)≤L k |D H(cid:63)(g) − D k(g)| ≤ E k(g), ∀k ≥ 1, ∀g ∈ G. end 16 Given these two goal-conditioned quantities, AdaGoal 17 Let κ (cid:44) inf (cid:8) k ∈ N : max g∈G: Dk(g)≤L E k(g) ≤ ε(cid:9). selects at episode k a candidate goal that solves the following constrained optimization problem: 18 Output: Goal states X κ (cid:44) {g ∈ G : D κ(g) ≤ L}, and for every g ∈ X , a deterministic, κ non-stationary policy π (cid:98)g that at time step i and g k ∈ arg max E k(g) (2a) state s selects action a according to: g∈G  subject to: D k(g) ≤ L. (2b) arg min Q (s, a, g)  if i ≡a h∈A (moκ d,h H + 1) for h ∈ [H], π (a|s, i) (cid:44) (cid:98)g  a re is fet i ≡ 0 (mod H + 1). I gn oat ler stp ar te et wat iti hon h. ighT esh te pra eg de in ctt ios neq eu rre on rti Eal aly mos nel gec tt hs osa e whose distance estimate D is not too large. If the agent is confident that a goal g is either too easy or too hard to reach, it will assign a low prediction error E(g). As
Adaptive Multi-Goal Exploration Figure 1: Goal sampling frequency of AdaGoal- UCBVI over 1000 episodes of length H = 50 (with L = 40). The grid-world has S = 52 states, starting state s = (0, 0) (top left), A = 5 actions 0 (4 cardinal ones and a ). The 4 states of the reset bottom right room can only be accessed from s 0 by any cardinal action with probability η = 0.001 (their associated V (cid:63)(s → ·) thus scale with η−1). 0 a result, the objective function in (2a) adaptively sam- the samples collected so far, and the algorithm checks ples goal states on the frontier of the learning process. whether a stopping rule (1) based on AdaGoal is trig- The constraint in (2b), although it is not required for gered, in which case it terminates. This occurs when the final sample complexity result, further tightens the the prediction errors E of all the goal states that meet goal selection process. Indeed, for any k ≥ 1, let the AdaGoal constraint (2b) are below the prescribed X (cid:44) (cid:8) g ∈ G : D (g) ≤ L(cid:9) , ε (cid:44) max E (g). F1-score level ε. These states then form the set of can- k k k k didate goal states output by Algorithm 1, along with g∈Xk their associated optimistic goal-reaching policies. Then, if as a warm-up we take the limit H → +∞, injecting Properties 1 and 2 in (2a-2b) entails that Empirical validation. In Figure 1 (see Appendix E G ⊆ X ⊆ G . (3) for details), we empirically study the sequence of goals L k L+εk selected by AdaGoal-UCBVI during learning. We de- We thus see that the constraint in (2b) does not remove sign a two-room grid-world with a very small probabil- valid goals in G from the set X of candidate goal states L k ity of reaching the second room. We see that AdaGoal to sample. Second, decreasing ε has the dual impact of k is able to discard the states from the second room and making the set of candidates goals X closer to G and k L target as goals the states in the first room that are improving their distance estimates, which motivates the goal selection scheme in (2a). In practice, we consider “furthest” away from s 0, which effectively correspond to the fringe of what the agent can reliably reach. a finite truncation H (line 5 in Algorithm 1), thus we need to account for the bias ρ (cid:44) V (cid:63)(s → g) − D(cid:63)(g), g 0 H which can be arbitrarily large for goals g that are hard 4 SAMPLE COMPLEXITY or impossible to reach. Fortunately, our AdaGoal GUARANTEES strategy will be able to gradually discard such states, hence the final MGE sample complexity will not pay Guarantee for AdaGoal-UCBVI. We first for such terms. In fact, we will later see that the choice bound the MGE sample complexity of AdaGoal- (cid:50) of horizon H = Ω(L log Lε−1) ensures that ρ g = O(ε) UCBVI. For simplicity we consider that G = S, i.e., for all the (unknown) goal states of interest g ∈ G L. the goal space spans the entire state space (the results trivially extend to any G ⊆ S). Choice of Q, D, E. A key algorithmic design is how to build and update the goal-conditioned Q-functions Theorem 8. AdaGoal-UCBVI is (ε, δ, L, S)-PAC for and the estimates D and E. In Appendices C and D, we MGE and, with probability at least 1−δ, for ε ∈ (0, 1/S] will carefully construct them with exact bonus-based its MGE sample complexity is of order5 O(cid:101)(L3SAε−2). estimates for both tabular MDPs and linear mixture Lemma 6 and Theorem 8 imply that the MGE sample MDPs. As suggested by the algorithms’ names, the complexity of AdaGoal-UCBVI is nearly minimax estimates of the former are inspired from BPI-UCBVI (Ménard et al., 2021a), an algorithm for best policy optimal for small enough ε and up to logarithmic terms. identification in finite-horizon tabular MDPs, while In the absence of a pre-specified exploration radius L, those of the latter are inspired from UCRL-VTR (Ay- the agent can build its own curriculum for L (i.e., oub et al., 2020), an algorithm for regret minimization design a sequence of increasing L’s) to guide its learning. in finite-horizon linear mixture MDPs. Since all our In this case, the total sample complexity is (up to a estimates are optimistic, we see that Algorithm 1 relies logarithmic factor) the same of AdaGoal-UCBVI run on the principle of optimism in the face of uncertainty with the final value of L, as stated below. both for the goal selection and the policy execution. Corollary 9. The successive execution of AdaGoal- Finally, in Section 6, we propose a way to instantiate UCBVI for an increasing sequence L ∈ {2, 22, ..., 2f } Q, D, E for a practical implementation in deep RL. 5The notation O(cid:101) in Theorem 8 hides poly-log terms in Adaptive stopping rule. At the end of each ε−1, S, A, L, δ−1. See Lemma 23 in Appendix C.3 for a (cid:50)algorithmic episode, the estimates are updated using more detailed bound that includes the poly-log terms.
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric with f ∈ N∗ is (ε, δ, L , S)-PAC for MGE and, with To the best of our knowledge, Theorem 11 yields the f probability at least 1 − δ, for ε ∈ (0, 1/S] its MGE sam- first goal-oriented PAC guarantee with linear function ple complexity is of order O(cid:101)(L3 f SAε−2), where L f = 2f . approximation. The algorithm’s choice of E, D, Q re- lies on two regression-based goal-conditioned estima- Finally, we can investigate the special case where M tors, one standard “value-targeted” estimator inspired is communicating and the objective is to learn an ε- from UCRL-VTR (Ayoub et al., 2020) and one novel optimal goal-conditioned policy for every goal state. “error-targeted” estimator, see Appendix D. We ex- Since the s -diameter D is unknown, we can use as an pect that the bound of Theorem 11 can be refined 0 0 initial subroutine the GOSPRL algorithm (Tarbouriech using tighter Bernstein-based estimates, for instance et al., 2021a) to compute an estimate D(cid:101) such that inspired from UCRL-VTR+ (Zhou et al., 2021), which D 0 ≤ D(cid:101) ≤ 2D 0 in O(cid:101)(D 03S2A) time steps. Then we can we leave as future work. Note that the O(cid:101) notation in execute AdaGoal-UCBVI with L = D(cid:101), which leads to Theorem 11 contains a log(G) factor (which appears the following guarantee. when performing a union bound argument over all Corollary 10. Assume that the MDP M has a finite goals g ∈ G), and the computational complexity of and unknown s -diameter D . Then the above strategy AdaGoal-UCRL·VTR scales with G (since the algo- 0 0 is (ε, δ, D , S)-PAC for MGE and, with probability at rithm maintains goal-conditioned estimates). We point 0 least 1−δ, for ε ∈ (0, 1/S], its MGE sample complexity out that here we still consider that the MDP has a is of order O(cid:101)(D3SAε−2). finite number of states S. This is to be expected given 0 the way a goal is currently modeled (at the granular Comparison with existing bounds. A different level of states), independently of how large the state although related problem is considered in Tarbouriech space is, where it may be very hard to visit specific et al. (2021a) in communicating MDPs, where the states. Learning in single- or multi-goal RL beyond a agent must find an ε-optimal goal-conditioned policy finite state space is an interesting direction of future for any arbitrary starting state, goal state and posi- investigation. Note that existing works on single-goal tive cost function. For small enough ε, their bound exploration (i.e., SSP regret minimization) with linear in the unit-cost case scales as O(cid:101)(D4ΓSAε−2), where Γ function approximation (Vial et al., 2021; Min et al., is the branching factor which in the worst case is S. 2021) also assume that the state space is finite. We see that Corollary 10 improves over Tarbouriech et al. (2021a) by a factor D 0 as well as a factor Γ ≤ S. 5 ANALYSIS OVERVIEW Although the latter considers a more demanding cost- free objective (i.e., for any positive cost function), it The proofs of Theorems 8 and 11 (see Appendices C is unable to avoid its superlinear dependence in S and D) are decomposed in the same following key steps. when instantiated in our scenario of unit cost functions, since the algorithm’s design is to estimate uniformly well the transition kernel. Finally, DisCo (Tarbouriech (cid:46) Key step (cid:172): Optimism and gap bounds. We prove that Properties 1 and 2 hold with high probability, et al., 2020b) designed for incremental autonomous exploration can solve MGE in the special case where i.e., (i) the quantities D are optimistic estimates of the optimal goal-conditioned finite-horizon value functions S→ = G , where S→ denotes the set of incrementally LL -reachaL ble statesL (see Lim and Auer, 2012, Defini- D H(cid:63) and (ii) the quantities E are valid upper bounds to the goal-conditioned finite-horizon gaps. tion 5). However, the bound of DisCo would translate to O(cid:101)(L5ΓSAε−2), which is also worse than Theorem 8. (cid:46) Key step (cid:173): Bounding the cumulative gap Guarantee for AdaGoal-UCRL·VTR. We bounds. We make explicit a function f M (depending now bound the MGE sample complexity of Algorithm 1 on the MDP M) that is strictly decreasing in K with (cid:50) in linear mixture MDPs (Definition 7). Since the state f (K) → 0, such that with high probability, M K→∞ space S may be large, we consider that the known goal space is in all generality a subset of it, i.e., G ⊆ S, (cid:88)K (cid:88)K where G (cid:44) |G| denotes the cardinality of the goal space. D H(cid:63)(g k) − D k(g k) ≤ E k(g k) ≤ K · f M(K), (4) k=1 k=1 Theorem 11. In linear mixture MDPs, for ε ∈ (0, 1], AdaGoal-UCRL·VTR is (ε, δ, L, G)-PAC for MGE for any number of algorithmic ep √isodes K. Specifically, a pn led, cow mi pth lexp itr yob ia sb oil fit oy rda et r6le O(cid:101)as (cid:0)t L41 d− 2ε−δ 2, (cid:1)i ,ts whM erG e E d is sa tm he- w foe r e Ast da ab Gli osh alt -h Ua Ct Bf M VI( ,K a) nd= fO(cid:101) M(cid:0) (KK )H =2S O(cid:101)A (cid:0)√+ KH H2S 3d2 2A +(cid:1) dimension of the feature mapping. H2d3/2(cid:1) for AdaGoal-UCRL·VTR. (4) resembles a 6The notation O(cid:101) in Theorem 11 hides poly-log terms in no-regret property of the exploration algorithm that re- ε−1, G, d, L, δ−1. ceives as input the sequence of goals (g ) prescribed k k≥1
Adaptive Multi-Goal Exploration by AdaGoal and performs the (PE) step. Indeed, in- near-optimal goal-reaching behavior (i.e., SSP value tuitively, the aim of the (PE) step is to improve the function) by: (i) first computing a near-optimal policy estimation of D (g ) and make it closer to D(cid:63)(g ), i.e., π in the finite-horizon reduction (using the stopping k k H k (cid:101) to decrease the prediction error E (g ). rule (1)), (ii) and then expanding π into an infinite- k k (cid:101) horizon policy via the reset action every H time steps (cid:46) Key step (cid:174): Bounding the sample complexity. to get our desired candidate policy. To bound κ the episode index at which Algorithm 1 Now, importantly, the above reasoning only holds for terminates, we combine (4) and the termination con- the goals in G , which is an unknown set. This is dition (1) to simultaneously lower and upper bound L+ε where our AdaGoal strategy comes into the picture, as (with high probability) the cumulative errors E as it provides a simple and computable sufficient condition κ−1 for a goal to belong to G . (cid:88) L+ε ε · (κ − 1) ≤ E (g ) ≤ (κ − 1) · f (κ − 1). k k M Lemma 12. With probability at least 1 − δ, if a goal k=1 state g ∈ G satisfies D (g) ≤ L and E (g) ≤ ε for an k k Inverting this functional inequality in κ yields that κ episode k ≥ 1, then g ∈ G . L+ε is finite and bounded as We are now ready to put everything together and prove κ ≤ f −1(ε) + 2. (5) M that AdaGoal-UCBVI and AdaGoal-UCRL·VTR are The sample complexity is (H + 1) · κ with H = O(cid:101)(L), (ε, δ, L)-PAC for MGE. The candidate goal states are thus AdaGoal-UCBVI (resp. AdaGoal-UCRL·VTR) X κ (cid:44) {g ∈ S : D κ(g) ≤ L}, with candidate policies stops in O(cid:101)(L3SAε−2 + L3S2Aε−1) (resp. O(cid:101)(L4d2ε−2)) π (cid:98)g (cid:44) (π gκ+1)|H . In what follows we reason with high time steps, with high probability. probability. Property 1 ensures that G L ⊆ X κ, while Lemma 12 entails that X ⊆ G . Finally, for any κ L+ε (cid:46) Key step (cid:175): Connecting to the original MGE g ∈ X κ, combining Property 2 and the termination objective. The key remaining step is to prove that condition (1) gives that π gκ+1 is ε/9-optimal in M g,H . As a result, the translation from the finite-horizon to the MGE objective is indeed fulfilled. goal-oriented objective (which holds since g ∈ G Remark 4. Algorithm 1 relies on a finite-horizon con- L+ε and by choice of the horizon H = Ω(L log Lε−1), see struction, with algorithmic episodes of length H. This relates to the reduction of SSP to finite-horizon studied Lemma 27) yields that V gπ(cid:98)g (s 0) ≤ V g(cid:63)(s 0) + ε, i.e., π (cid:98)g is in some SSP regret minimization works (Cohen et al., ε-optimal for the original SSP objective. This concludes the proofs of Theorems 8 and 11. 2021; Chen and Luo, 2021), which rely on the idea that an SSP problem can be approximated by a finite- horizon problem if the horizon is large enough w.r.t. T , 6 OPERATIONALIZING AdaGoal (cid:63) the optimal expected hitting time to the goal starting IN DEEP RL from any state. Two main differences arise in our MGE setting: (i) first, in these works, the goal state is fixed In this section, we present a way to operationalize throughout learning and T (cid:63) is assumed known, whereas the AdaGoal idea of targeting goals with high “un- we need to deal with goal selection and find the rel- certainty”. We show it can be implemented similar to evant goals of interest while having to discard those the deep RL algorithm of Zhang et al. (2020b), and that are poorly reachable or unreachable. (ii) Second, we investigate an aspect that was not considered in these works ensure that the empirical goal-reaching the latter paper, which pertains to the capability of performance of the algorithm’s non-stationary policy AdaGoal to adapt to an unknown target goal set (G ) L over the whole learning interaction is good enough (by given a goal set that is possibly misspecified (G). definition of the regret objective in SSP). As such, they do not show that the expected performance of some First, we notice that Q and D in Algorithm 1 can be learned in practice with a goal-conditioned value- candidate policy is good enough (i.e., the SSP value based neural network (Schaul et al., 2015). Meanwhile, function is small enough) – in fact, they do not even explicitly prove that the executed policies are proper. the predictions errors E can be approximated by the disagreement between an ensemble of goal-conditioned The latter property may actually not be possible to ob- Q-functions. Interestingly, this approach has already tain since standard regret-to-PAC conversion may not work in SSP (Tarbouriech et al., 2021b). In our MGE been investigated in the deep GC-RL algorithm VDS (Zhang et al., 2020b), which considers a similar goal- objective, the key difference lies in the availability of proposal module to prioritize goals that maximize the the reset action (Assumption 3), as we will now see. epistemic uncertainty of the Q-function of the current Our analysis builds on the following reasoning: given a policy, in order to sample goals at the frontier of the goal state in G , we can find a candidate policy with set of goals that the agent is able to reach. L+ε
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric 1 0.8 0.6 0.4 0.2 0 0 10 20 30 40 50 Epoch etaRsseccuS FetchPush-v1 1 0.8 0.6 0.4 Adaptive Uniform 0.2 0 0 10 20 30 40 50 Epoch etaRsseccuS FetchPickAndPlace-v1 Adaptive Uniform 1 0.8 0.6 0.4 0.2 0 0 10 20 30 40 50 Epoch etaRsseccuS FetchPush-v1 w/ Goal-Space Misspecification 1 0.8 0.6 0.4 0.2 0 0 10 20 30 40 50 Epoch etaRsseccuS FetchPickAndPlace-v1 w/ Goal-Space Misspecification Figure 2: Success rate evaluated on G with the latest policy trained on G . The shaded region represents confidence test train over 5 random seeds. The adaptive goal sampling scheme improves the learning performance over the uniform sampling of HER. This is especially the case in the presence of goal-space misspecification (bottom row), where the training goal space G (delimited in purple) is larger than the test goal space G (delimited in yellow). train test Specifically, we take an ensemble {Q } of J ran- In the above experimental set-up (which is in fact j 1≤j≤J domly initialized goal-conditioned Q-functions and we considered by most deep GC-RL works), the goal space instantiate E(g) as the standard deviation of the ensem- G seen at train time is the same as the goal space train ble’s Q values conditioned on goal g. Since computing G on which the agent is evaluated at test time, i.e., j test the maximum over g ∈ G in (2a) is expensive if the goal the white rectangular table. In the language of the space G is large, the procedure is replaced by first uni- previous sections, by relating G ↔ G and G ↔ train test formly sampling a set of candidate goals {g(n)}N ⊂ G, G , this means that the environment is considered n=1 L and then selecting a goal g(n) with probability communicating and G = G . However, in some cases, L there may be some misspecification in the goal space q (cid:44) E(g(n)) , E(g) (cid:44) std (cid:8) min Q (s , a, g)(cid:9) . seen during the learning interaction. This may occur if n (cid:80)N E(g(n(cid:48))) 1≤j≤J a∈A j 0 the agent is unaware of the goals of interest, in which n(cid:48)=1 case we have that G (cid:40) G, where we recall that G is L L Moreover, approximating H ≈ L renders the constraint a priori unknown. We design an experiment to model in (2b) always valid so it can be omitted. Hence, this this scenario by translating the x-y range of G by train approximation of AdaGoal exactly recovers the goal a factor of λ ≥ 1. Specifically, denoting by (x , y , z ) 0 0 0 sampling scheme of Zhang et al. (2020b), which pairs it the center of the table and letting r (cid:44) 0.15, we leave with Hindsight Experience Replay (HER, Andrychow- G unchanged yet we expand G ⊃ G as test train test icz et al., 2017) that performs uniform goal sampling. G (cid:44) (cid:8) (x + U(−r, r), y + U(−r, r), z )(cid:9) , test 0 0 0 We consider the multi-goal environments of FetchPush G (cid:44) (cid:8) (x + U(−λr, λr), y + U(−λr, λr), z )(cid:9) , and FetchPickAndPlace, which are sparse-reward sim- train 0 0 0 ulated robotic manipulation tasks from OpenAI Gym with λ = 10, λ = 5. In this sce- FetchPush FetchPickAndPlace (Plappert et al., 2018). We empirically compare the nario, Figure 2 (bottom row) shows that an adaptive performance of such an adaptive goal selection that goal sampling scheme is particularly pertinent. Intu- prioritizes “uncertain” goals and the performance of itively, it enables to discard the set of goals G \ G train test HER’s uniform goal selection (see Appendix F for im- that cannot be reached and thus hinder learning when plementation details). We observe in Figure 2 (top row) the agent conditions its behavior on them. This em- that the adaptive goal sampling scheme outperforms pirically corroborates AdaGoal’s (theoretically estab- the uniform one of HER, which is consistent with the lished) ability to adapt to an unknown target goal set results of Zhang et al. (2020b). (G ) given a goal set that is possibly misspecified (G). L
Adaptive Multi-Goal Exploration Acknowledgement objectives. In Advances in Neural Information Pro- cessing Systems, pages 724–734, 2019. We thank Evrard Garcelon, Andrea Tirinzoni and Yann Alon Cohen, Yonathan Efroni, Yishay Mansour, and Ollivier for helpful discussion. Aviv Rosenberg. Minimax regret for stochastic short- est path. Advances in Neural Information Processing References Systems, 34, 2021. Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Cédric Colas, Pierre Fournier, Mohamed Chetouani, Szepesvári. Improved algorithms for linear stochastic Olivier Sigaud, and Pierre-Yves Oudeyer. Curious: bandits. Advances in neural information processing intrinsically motivated modular multi-goal reinforce- systems, 24:2312–2320, 2011. ment learning. In International conference on ma- Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas chine learning, pages 1331–1340. PMLR, 2019. Schneider, Rachel Fong, Peter Welinder, Bob Mc- Cédric Colas, Tristan Karch, Olivier Sigaud, and Grew, Josh Tobin, Pieter Abbeel, and Wojciech Pierre-Yves Oudeyer. Intrinsically motivated goal- Zaremba. Hindsight experience replay. In Advances conditioned supervised learning: a short survey. in neural information processing systems, pages 5048– arXiv preprint arXiv:2012.09830, 2020. 5058, 2017. Omar Darwiche Domingues, Yannis Flet-Berliac, Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, Edouard Leurent, Pierre Ménard, Xuedong Shang, and Lin Yang. Model-based supervised learning and Michal Valko. rlberry-a supervised learning with value-targeted regression. In International Con- library for research and education, 2021a. ference on Machine Learning, pages 463–474. PMLR, Omar Darwiche Domingues, Pierre Ménard, Emilie 2020. Kaufmann, and Michal Valko. Episodic reinforce- Mohammad Gheshlaghi Azar, Rémi Munos, and ment learning in finite mdps: Minimax lower bounds Hilbert J Kappen. Minimax pac bounds on the revisited. In Algorithmic Learning Theory, pages sample complexity of supervised learning with a 578–598. PMLR, 2021b. generative model. Machine learning, 91(3):325–349, Adrien Ecoffet, Joost Huizinga, Joel Lehman, Ken- 2013. neth O Stanley, and Jeff Clune. First return, then Mohammad Gheshlaghi Azar, Ian Osband, and Rémi explore. Nature, 590(7847):580–586, 2021. Munos. Minimax regret bounds for reinforcement Carlos Florensa, David Held, Xinyang Geng, and Pieter learning. In Proceedings of the 34th International Abbeel. Automatic goal generation for reinforcement Conference on Machine Learning-Volume 70, pages learning agents. In International Conference on Ma- 263–272. JMLR. org, 2017. chine Learning, pages 1515–1528, 2018. Dimitri Bertsekas. Dynamic programming and optimal Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, control, volume 2. 1995. and Sergey Levine. Dynamical distance learning for Liyu Chen and Haipeng Luo. Finding the stochas- semi-supervised and unsupervised skill discovery. In tic shortest path with low regret: The adversarial International Conference on Learning Representa- cost and unknown transition case. In International tions, 2020. Conference on Machine Learning, pages 1651–1660. Elad Hazan, Sham Kakade, Karan Singh, and Abby PMLR, 2021. Van Soest. Provably efficient maximum entropy ex- Liyu Chen, Mehdi Jafarnia-Jahromi, Rahul Jain, and ploration. In International Conference on Machine Haipeng Luo. Implicit finite-horizon approximation Learning, pages 2681–2691, 2019. and efficient optimal algorithms for stochastic short- Mehdi Jafarnia-Jahromi, Liyu Chen, Rahul Jain, and est path. arXiv preprint arXiv:2106.08377, 2021a. Haipeng Luo. Online learning for stochastic shortest Xiaoyu Chen, Jiachen Hu, Lin F Yang, and Liwei path model via posterior sampling. arXiv preprint Wang. Near-optimal reward-free exploration for lin- arXiv:2106.05335, 2021. ear mixture mdps with plug-in solver. arXiv preprint Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and arXiv:2110.03244, 2021b. Tiancheng Yu. Reward-free exploration for rein- Nuttapong Chentanez, Andrew G Barto, and Satin- forcement learning. In International Conference on der P Singh. Intrinsically motivated reinforcement Machine Learning, pages 4870–4879. PMLR, 2020. learning. In Advances in neural information process- Leslie Pack Kaelbling. Learning to achieve goals. In ing systems, pages 1281–1288, 2005. IJCAI, pages 1094–1099. Citeseer, 1993. Wang Chi Cheung. Regret minimization for reinforce- Emilie Kaufmann, Pierre Ménard, Omar Darwiche ment learning with vectorial feedback and complex Domingues, Anders Jonsson, Edouard Leurent, and
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric Michal Valko. Adaptive reward-free exploration. In Schneider, Josh Tobin, Maciek Chociej, Peter Welin- Algorithmic Learning Theory, pages 865–891. PMLR, der, et al. Multi-goal supervised learning: Chal- 2021. lenging robotics environments and request for re- Abbas Kazerouni, Mohammad Ghavamzadeh, Yasin search. arXiv preprint arXiv:1802.09464, 2018. Abbasi, and Benjamin Van Roy. Conservative con- Vitchyr Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, textual linear bandits. In Advances in Neural Infor- Shikhar Bahl, and Sergey Levine. Skew-fit: State- mation Processing Systems, pages 3910–3919, 2017. covering self-supervised supervised learning. In In- Tor Lattimore and Marcus Hutter. Pac bounds for ternational Conference on Machine Learning, pages 7783–7792. PMLR, 2020. discounted mdps. In International Conference on Al- gorithmic Learning Theory, pages 320–334. Springer, Aviv Rosenberg, Alon Cohen, Yishay Mansour, and 2012. Haim Kaplan. Near-optimal regret bounds for Tor Lattimore and Csaba Szepesvári. Bandit algo- stochastic shortest path. In International Confer- rithms. Cambridge University Press, 2020. ence on Machine Learning, pages 8210–8219. PMLR, 2020. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver, and Daan Wierstra. Continuous control Silver. Universal value function approximators. In with deep supervised learning. arXiv preprint International conference on machine learning, pages arXiv:1509.02971, 2015. 1312–1320, 2015. Shiau Hong Lim and Peter Auer. Autonomous explo- Satinder Singh, Richard L Lewis, Andrew G Barto, ration for navigating in MDPs. In Conference on and Jonathan Sorg. Intrinsically motivated reinforce- Learning Theory, pages 40–1, 2012. ment learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, Pierre Ménard, Omar Darwiche Domingues, Anders 2(2):70–82, 2010. Jonsson, Emilie Kaufmann, Edouard Leurent, and Michal Valko. Fast active learning for pure explo- Jean Tarbouriech and Alessandro Lazaric. Active ex- ration in supervised learning. In International ploration in markov decision processes. In The 22nd Conference on Machine Learning, pages 7599–7608. International Conference on Artificial Intelligence PMLR, 2021a. and Statistics, pages 974–982, 2019. Pierre Ménard, Omar Darwiche Domingues, Xuedong Jean Tarbouriech, Evrard Garcelon, Michal Valko, Mat- Shang, and Michal Valko. Ucb momentum q-learning: teo Pirotta, and Alessandro Lazaric. No-regret explo- Correcting the bias without forgetting. In Interna- ration in goal-oriented supervised learning. In In- tional Conference on Machine Learning, pages 7609– ternational Conference on Machine Learning, pages 7618. PMLR, 2021b. 9428–9437. PMLR, 2020a. Yifei Min, Jiafan He, Tianhao Wang, and Quan- Jean Tarbouriech, Matteo Pirotta, Michal Valko, and quan Gu. Learning stochastic shortest path with Alessandro Lazaric. Improved sample complexity for linear function approximation. arXiv preprint incremental autonomous exploration in mdps. In arXiv:2110.12727, 2021. Advances in Neural Information Processing Systems, Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar volume 33, pages 11273–11284, 2020b. Bahl, Steven Lin, and Sergey Levine. Visual rein- Jean Tarbouriech, Shubhanshu Shekhar, Matteo forcement learning with imagined goals. Advances Pirotta, Mohammad Ghavamzadeh, and Alessandro in Neural Information Processing Systems, 31:9191– Lazaric. Active model estimation in markov decision 9200, 2018. processes. In Conference on Uncertainty in Artificial Pierre-Yves Oudeyer and Frederic Kaplan. What is Intelligence, pages 1019–1028. PMLR, 2020c. intrinsic motivation? a typology of computational Jean Tarbouriech, Matteo Pirotta, Michal Valko, and approaches. Frontiers in neurorobotics, 1:6, 2009. Alessandro Lazaric. A provably efficient sample col- Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, lection strategy for supervised learning. Advances and Jimmy Ba. Maximum entropy gain exploration in Neural Information Processing Systems, 34, 2021a. for long horizon multi-goal supervised learning. Jean Tarbouriech, Matteo Pirotta, Michal Valko, and In International Conference on Machine Learning, Alessandro Lazaric. Sample complexity bounds for pages 7750–7761. PMLR, 2020. stochastic shortest path with a generative model. Matthias Plappert, Marcin Andrychowicz, Alex Ray, In Algorithmic Learning Theory, pages 1157–1178. Bob McGrew, Bowen Baker, Glenn Powell, Jonas PMLR, 2021b.
Adaptive Multi-Goal Exploration Jean Tarbouriech, Runlong Zhou, Simon S Du, Mat- Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. teo Pirotta, Michal Valko, and Alessandro Lazaric. Nearly minimax optimal supervised learning for Stochastic shortest path: Minimax, parameter-free linear mixture markov decision processes. In Confer- and towards horizon-free regret. Advances in Neural ence on Learning Theory, pages 4532–4576. PMLR, Information Processing Systems, 34, 2021c. 2021. Daniel Vial, Advait Parulekar, Sanjay Shakkottai, and R Srikant. Regret bounds for stochastic shortest path problems with linear function approximation. arXiv preprint arXiv:2105.01593, 2021. Ruosong Wang, Simon S Du, Lin Yang, and Russ R Salakhutdinov. On reward-free reinforcement learn- ing with linear function approximation. In Advances in Neural Information Processing Systems, volume 33, pages 17816–17826, 2020. David Warde-Farley, Tom Van de Wiele, Tejas Kulka- rni, Catalin Ionescu, Steven Hansen, and Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. In International Conference on Learning Representations, 2019. Jingfeng Wu, Vladimir Braverman, and Lin F Yang. Gap-dependent unsupervised exploration for rein- forcement learning. arXiv preprint arXiv:2108.05439, 2021a. Jingfeng Wu, Lin Yang, et al. Accommodating picky customers: Regret bound and exploration complexity for multi-objective supervised learning. Advances in Neural Information Processing Systems, 34, 2021b. Andrea Zanette, Alessandro Lazaric, Mykel J Kochen- derfer, and Emma Brunskill. Provably efficient reward-agnostic navigation with linear value iter- ation. In Advances in Neural Information Processing Systems, volume 33, pages 11756–11766, 2020. Weitong Zhang, Dongruo Zhou, and Quanquan Gu. Reward-free model-based supervised learning with linear function approximation. Advances in Neural Information Processing Systems, 34, 2021a. Xuezhou Zhang, Yuzhe Ma, and Adish Singla. Task- agnostic exploration in supervised learning. Ad- vances in Neural Information Processing Systems, 33, 2020a. Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Au- tomatic curriculum learning through value disagree- ment. In Advances in Neural Information Processing Systems, pages 7648–7659, 2020b. Zihan Zhang, Simon Du, and Xiangyang Ji. Near optimal reward-free supervised learning. In In- ternational Conference on Machine Learning, pages 12402–12412. PMLR, 2021b. Rui Zhao, Xudong Sun, and Volker Tresp. Maximum entropy-regularized multi-goal reinforcement learn- ing. In International Conference on Machine Learn- ing, pages 7553–7562, 2019.
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric Appendix Table of Contents A SHORTCOMINGS OF ALGORITHMIC DESIGNS OF EXISTING THEORETICAL GC-RL METHODS 13 B PROOFS 14 C DETAILS OF AdaGoal-UCBVI AND ANALYSIS 18 D DETAILS OF AdaGoal-UCRL·VTR AND ANALYSIS 30 E ABLATION OF THE GOAL SELECTION SCHEME & PROOF OF CONCEPT EX- PERIMENT 34 F IMPLEMENTATION DETAILS OF SECTION 6 35 A SHORTCOMINGS OF ALGORITHMIC DESIGNS OF EXISTING THEORETICAL GC-RL METHODS Here we corroborate our discussion in Section 1 that the existing theoretical GC-RL approaches have quite involved algorithmic designs, far from the interpretable alternation of (GS) and (PE) steps, thus making them poorly amenable to a more practical implementation. We identify three specific algorithmic shortcomings that we describe below. Specifically, UcbExplore (Lim and Auer, 2012) suffers from Shortcomings 1 and 2, while both DisCo (Tarbouriech et al., 2020b) and GOSPRL (Tarbouriech et al., 2021a) suffer from Shortcoming 3. • Shortcoming 1: Cumbersome (PE) step. Once the method fixes a relevant goal state, it has an elaborate policy execution phase, by running numerous executions of the candidate optimistic goal-conditioned policy and checking after each execution whether an empirical performance check is verified to determine whether to continue the policy execution phase or not. In contrast, our AdaGoal-based approach has a simple (PE) step, which executes a single rollout of the candidate optimistic goal-conditioned policy for a fixed number of steps before resetting and selecting a (possibly) different, more relevant candidate goal. • Shortcoming 2: Possibly eliminatory (GS) step. The method establishes a strict separation between goal states that have been identified as reliably reachable by a policy and those that have not. In particular, the goal states belonging to the former category can never be selected again as candidate goals, even if the agent could later discover more promising policies leading to it. In contrast, our AdaGoal-based approach only implicitly targets goal states of intermediate difficulty (without relying on an explicit distinction between goal states that have already been identified as being reliably reachable and the other goal states). This also means that AdaGoal does not need to formalize a notion of “fringe” or “border”, which can make it more amenable to implementation. • Shortcoming 3: Exhaustive/non-adaptive stopping condition. The method has a predefined stopping condition, that poorly (or does not) adapts to the samples collected so far. For instance, the method explicitly fixes a minimum number of samples to have collected from every state-action pair of interest, or fixes a minimum number of times a candidate goal must be selected. In contrast, our AdaGoal-based approach relies on an adaptive, data-driven strategy to select goals and to terminate the algorithm. This may also pave the way for problem-dependent sample complexity guarantees.
Adaptive Multi-Goal Exploration B PROOFS Proof of Equation (3). Here we take the limit H → +∞. Let g ∈ G , then Property 1 entails that D (g) ≤ L k V (cid:63)(s → g) ≤ L, thus g ∈ X , therefore G ⊆ X . Now let g ∈ X , then by Property 2 and definition of ε , we 0 k L k k k have that V (cid:63)(s → g) ≤ D (g) + E (g) ≤ L + E (g) ≤ L + ε , therefore X ⊆ G . 0 k k k k k L+εk Proof of Equation (5). Fix any finite episode index K < κ, where κ denotes the (possibly unbounded) episode index at which AdaGoal-UCBVI terminates. By design of AdaGoal, we have that ε ≤ E (g ) for every k ≤ K. k k We assume that κ > 1 otherwise the result is trivially true. Define κ(cid:48) (cid:44) min{κ − 1, K} ≥ 1. Summing the inequality above yields ε·κ(cid:48) ≤ (cid:80)κ(cid:48) E (g ) ≤ κ(cid:48) ·f (κ(cid:48)). This implies that ε ≤ f (κ(cid:48)), in which case f −1(ε) ≥ κ(cid:48), k=1 k k M M M since f −1 is strictly decreasing (like f ). Since the last inequality holds for any finite K < κ, letting K → +∞ M M implies that κ is finite and bounded by f −1(ε) + 2. M B.1 Proof of Lemma 5 We assume throughout that L ≥ 2 and ε ∈ (0, 1]. On the one hand, Theorem 8 proves that AdaGoal- UCBVI is (ε, δ, L, G)-PAC for MGE with sample complexity of order O(cid:101)(L3SAε−2), therefore MGE is solvable in poly(S, L, ε−1, A) steps (up to poly-log factors). On the other hand, reset-free MGE is a special case of the cost-free goal-free exploration problem in communicating MDPs studied by Tarbouriech et al. (2021a), whose algorithm GOSPRL can solve it in poly(S, D, ε−1, A) steps (up to poly-log factors). Now we prove that there exists an MDP such that any algorithm requires at least Ω(D) steps to solve the reset-free MGE problem (i.e., without Assumption 3), where the MDP’s diameter D can be made exponentially larger than L, S, A, ε−1. To this end, we design in Figure 3 a communicating MDP M that does not satisfy Assumption 3, with A ≥ 4 a† actions (including a special action a† ∈ A) and S = 5 states, where S (cid:44) {s , g, x, s, s}, and all states apart from 0 (cid:101) s are reliably L-reachable from s . We define as goal space G (cid:44) {g}. We consider the problem of learning an 0 ε-optimal goal-reaching policy with goal state g from the starting state s , i.e., finding a policy π such that 0 V π(s → g) ≤ V (cid:63)(s → g) + ε, which is a sub-problem of the MGE objective. 0 0 Given η ∈ (0, 1) and an unknown action a† ∈ A, we define the following transition probabilities for all a ∈ A, 1 − ζ p(s (cid:101)|s 0, a) (cid:44) η, p(x|s 0, a) (cid:44) 1 − η, 1 − η ζ p(g|x, a) (cid:44) ζ, p(x|x, a) (cid:44) 1 − ζ, s 0 x g p(g|s (cid:101), a) (cid:44) 1[a = a†], p(s|s (cid:101), a) (cid:44) 1[a (cid:54)= a†], a = a† η/2 η η p(g|s, a) (cid:44) , p(s|s, a) (cid:44) 1 − , η 2 2 p(s |g, a) (cid:44) 1, s (cid:101) a (cid:54)= a† s 0 where we can set any ζ = O(1/L) such that g is reliably 1 − η/2 L-reachable from s 0 (i.e., V (cid:63)(s 0 → g)). It is easy to see Figure 3: Illustration of the MDP instance M a†. that the MDP’s diameter verifies D = αη−1 for a constant α > 0. Finally, we denote by F the family of MDPs of the form of Figure 3 parameterized by a† ∈ {1, . . . A}, i.e., F (cid:44) {M } . a† a†∈{1,...A} We define the event J that state s has never been visited by the agent by time t (recalling that it initially starts t (cid:101) at state s ), i.e., J (cid:44) {nt(s) = 0} (note that its probability is the same for all MDPs in F). We now fix an MDP 0 t (cid:101) M and denote by π(cid:63) its optimal policy, i.e., π(cid:63) ∈ arg min V π(· → g), which in particular selects action a† at a† π state s. First, we consider any deterministic algorithm A whose candidate policy π does not select action a† when (cid:101) (cid:98) it is in state s. Then it holds that (cid:101) (cid:88) 2 V π(cid:98)(s (cid:101) → g) = 1 + p(y|s 0, π (cid:98)(s (cid:101)))V π(cid:98)(y → g) ≥ 1 + V π(cid:98)(s → g) = 1 + , (6) η y∈S 2 V π(cid:98)(s (cid:101) → g) − V π(cid:63) (s (cid:101) → g) ≥ , η
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric V π(s → g) = 1 + ηV π(s → g) + (1 − η)V π(x → g), ∀π, 0 (cid:101) V π(cid:98)(s 0 → g) − V π(cid:63) (s 0 → g) = (1 − η)(cid:0) V π(cid:98)(x → g) − V π(cid:63) (x → g) (cid:1) + η(cid:0) V π(cid:98)(s (cid:101) → g) − V π(cid:63) (s (cid:101) → g) (cid:1) ≥ 2 > ε, (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) ≥0 ≥2/η (7) thus π has a sub-optimality gap larger than ε. This means that under the event J , where the algorithm cannot (cid:98) t know which is the favorable action a†, it holds that with probability at least 1 − 1 ≥ 3 , any deterministic A 4 algorithm’s candidate policy π does not select the action a† at state s thus its value function is not ε-optimal. (cid:98) (cid:101) Second, we note that we can easily extent to the case where A outputs stochastic actions at state s. Given that (cid:101) a† is unknown, in the best case scenario it can set π(·|s) = 1/A. Then we can retrace the reasoning above and (cid:98) (cid:101) replace (6) with V π(cid:98)(s (cid:101)) ≥ 1 + A A−1 η2 , and thus (7) with V π(cid:98)(s 0) − V π(cid:63) (s 0) ≥ 2 A A−1 > 1 ≥ ε since A > 2, which leads to the same result that π is not ε-optimal on at least one of the MDPs in F. (cid:98) Now we study how the probability of the event J evolves over time t, i.e. we bound the time required to visit s t (cid:101) at least once, which we denote by T(cid:101). Recall that s (cid:101) can only be reached with probability η from s 0 following any action. The random variable T(cid:101) can be seen as an upper bound of a random variable distributed geometrically with success probability η, thus Chernoff’s inequality entails that with probability at least 1 we have T(cid:101) ≥ 1 , i.e., 2 9η the event J for t = (cid:98)1/(9η)(cid:99) holds with probability at least 1 . t 2 Putting everything together, there exists an MDP in F such that with probability at least 1 , the number of time 4 steps required to output a candidate policy that is ε-optimal for goal state g is at least 1 = Ω(D), where η can 9η be made arbitrarily small, so in particular D can be exponentially larger than L. Hence in this MDP instance, no algorithm can solve MGE in poly(L) steps with overwhelming probability. While here we considered S = 5 for simplicity, note that the result easily holds for any S ≥ 5 by replacing the state x in the construction above with a set of S − 4 states; the only property that must be still verified is that g remains reliably L-reachable from s . 0 Therefore, for any L ≥ 2, S ≥ 5, A ≥ 4, ε ∈ (0, 1], there exists an MDP instance and a goal space for which any algorithm requires at least Ω(D) time steps to solve reset-free MGE, where the diameter D can be exponentially larger than L, S, A, ε−1, which concludes the proof of Lemma 5. B.2 Proof of Lemma 6 In the following, we will prove in Lemma 14 a lower bound on the expected number of exploration steps to find an ε-optimal SSP policy from s for a specific goal state g that is reliably L-reachable from s . Such BPI-SSP 0 0 objective corresponds to our MGE objective with goal space G = {g} and it thus induces a lower bound on the MGE problem, which will conclude the proof of Lemma 6. Notation. We largely follow the notations and definitions of Domingues et al. (2021b). We define an RL algorithm as a history-dependent policy π used to interact with the environment. In the BPI setting, where we eventually stop and recommend a policy, an algorithm is defined as a triple (π, τ, π ) where τ is a stopping time and π is a (cid:98)τ (cid:98)τ Markov policy recommended after τ time steps. We now write more formally our objective. Definition 13 (BPI-SSP). An algorithm (π, τ, π ) is (ε, δ, L)-PAC for best-policy identification in a stochastic (cid:98)τ shortest path problem satisfying Assumption 3 (BPI-SSP) if V (cid:63)(s ) ≤ L and if the policy π returned after τ time 0 (cid:98)τ steps satisfies, for the initial state s , 0 (cid:104) (cid:105) P π,M V π(cid:98)τ (s 0) − V ∗(s 0) ≤ ε ≥ 1 − δ, where we denote the goal state by g and the SSP value of any policy π at any state s by V π(s) (cid:44) E [inf{i ≥ 0 : s = g} | s = s], and V (cid:63)(s) (cid:44) min V π(s). π,M i+1 1 π Lemma 14 (BPI-SSP Lower Bound). There exist absolute constants L , S , A , ε , δ such that, for any L ≥ 0 0 0 0 0 L 0, A ≥ A 0, ε ≤ ε 0, δ ≤ δ 0 and S 0 ≤ S ≤ A L 3 −2, and for any algorithm (π, τ, π (cid:98)τ ) that is (ε, δ, L)-PAC for BPI-SSP in any finite MDP with S states and A actions, there exists an MDP M with a goal state belonging to G and an absolute constant β such that L L3SA (cid:18) 1 (cid:19) E [τ ] ≥ β log . π,M ε2 δ
Adaptive Multi-Goal Exploration Proof of Lemma 14. We first define our family of hard MDPs for S = 4 states, and the extension to any S states can be done as in Domingues 1 − q et al. (2021b) as explained later. Consider the hard MDP illustrated in Figure 4, where all states incur a cost of 1 apart from the goal state s g (a.k.a. “good” state). The agent s 0 stays in the initial state s with probability 1 − q, and goes 0 to a decision state s with probability q. For any action a d q taken in s , the agent reaches s with probability 1/2 and a d g 1 “bad” state s with probability 1/2, except if an action a(cid:63) is b chosen, that increases to 1/2 + ε the probability of reaching s d (cid:101) s g. From s b, the agent returns to the initial state s 0 with 1 − ε 1 + ε probability 1. The goal state s is absorbing, and the agent 2 (cid:101) 2 (cid:101) g stays there unless the reset action is taken, which brings the (a tg he ent arb ra oc wk st oo fs t0 h. eN reo st ee tt ah ca tt iot nhe frM omDP s sa tt ois sfies anA dss fu rm ompt sion to3 s b 1 2 1 2 s g cost = 0 d 0 g s are not represented in Figure 4 for visual convenience). 0 Moreover, we define the following parameters 1 L ε Figure 4: Illustration of our hard MDP. H (cid:44) (cid:100) − 1(cid:101), q (cid:44) 1/H, ε (cid:44) . (cid:101) 2 2(H + 1) Note that this hard MDP instance is inspired from hard MDPs used in prior lower-bound constructions (see e.g., Lattimore and Hutter, 2012; Domingues et al., 2021b), albeit with slight modifications. Indeed, a key difference with respect to the discounted MDP setting (Lattimore and Hutter, 2012) or the finite-horizon MDP setting (Domingues et al., 2021b) is that in our case, the agent has access to an anytime reset action (Assumption 3). This implies that we cannot do as prior works that rely on absorption properties of states in the MDP (e.g., the “good” and “bad” states s and s ) in order to compound errors and add an extra effective horizon term (either b g 1/(1 − γ) or H) in the sample complexity (i.e., to go from quadratic to cubic). The only absorption property we can rely on here is at the initial state s . It turns out that this will be sufficient in our setting to compound error 0 and go from L2 to L3 dependence. The intuition for this is that the SSP value function generates a cost of +1 at each time step until the goal state is reached, which compounds errors more than the usual reward-based value function in the sparse-reward MDP constructions of Lattimore and Hutter (2012); Domingues et al. (2021b). We consider a family of MDPs of the form of Figure 4, parameterized by a(cid:63) ∈ {1, . . . A}, where we denote by M a(cid:63) the MDP such that a(cid:63) increases the probability by ε (cid:101) of reaching the goal state s g from state s d. For any policy π we denote its SSP value (with goal state s g) at state s in M a(cid:63) by V aπ (cid:63)(s). We also define a reference MDP M , where ε = 0, that is, there is no special action increasing the probability of 0 (cid:101) reaching the goal state s g. We denote by P a(cid:63)[·] and E a(cid:63)[·] the probability measure and the expectation in the MDP M a(cid:63) by following the algorithm π and by P 0[·] and E 0[·] the corresponding operators in M 0. The optimal value function does not depend on the MDP parameter a(cid:63), and for any MDP M a(cid:63) we get V (cid:63)(s ) = 1 + (cid:16) 1 + ε(cid:17) + (cid:16) 1 − 1 − ε(cid:17)(cid:0) 1 + V (cid:63)(s )(cid:1) 0 (cid:101) (cid:101) 0 q 2 2 (cid:16) 1 (cid:17) 1 =⇒ V (cid:63)(s ) = + 1 . 0 q 1/2 + ε (cid:101) Note that by our choice of q, it importantly holds that V (cid:63)(s ) ≤ L, i.e., s ∈ G . 0 g L Meanwhile, the value function of the recommended policy π (cid:98)τ in M a(cid:63) is (cid:16) 1 (cid:17) 1 V π(cid:98)τ (s ) = + 1 . a(cid:63) 0 q 1/2 + ε · π (a(cid:63)|s ) (cid:101) (cid:98)τ d
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric As a result, (cid:16) 1 (cid:17) ε(1 − π (a(cid:63)|s )) (cid:16) 1 (cid:17) V π(cid:98)τ (s ) − V (cid:63)(s ) = + 1 (cid:101) (cid:98)τ d ≤ 4 + 1 ε ·(1 − π (a(cid:63)|s )), a(cid:63) 0 0 q (1/2 + ε)(1/2 + ε · π (a(cid:63)|s )) q (cid:101) (cid:98)τ d (cid:101) (cid:101) (cid:98)τ d (cid:124) (cid:123)(cid:122) (cid:125) =2ε and thus 1 V π(cid:98)τ (s ) − V (cid:63)(s ) < ε ⇐⇒ π (a(cid:63)|s ) > . a(cid:63) 0 0 (cid:98)τ d 2 We observe that this analysis of the suboptimality gap of π in terms of SSP value functions can be mapped (cid:98)τ to the one of Domingues et al. (2021b, Proof of Theorem 7) for their finite-horizon value functions, despite the different MDP constructions. This means that we can retrace the steps of Domingues et al. (2021b, Proof of (D) Theorem 7). In the following, we use the notation ≥ to signify that the inequality stems from following the same corresponding steps of Domingues et al. (2021b, Proof of Theorem 7). In particular, we similarly define the event (cid:110) 1 (cid:111) Eτ (cid:44) π (a(cid:63)|s ) > , a(cid:63) (cid:98)τ d 2 and since the algorithm is assumed to be (ε, δ, L)-PAC for BPI-SSP for any MDP, we have (cid:104) (cid:105) (cid:104) (cid:105) P a(cid:63) E aτ (cid:63) = P a(cid:63) V aπ(cid:98) (cid:63)τ (s 0) < V (cid:63)(s 0) + ε ≥ 1 − δ. We now proceed with lower bounding the expectation of the sample complexity τ in the reference MDP M . We 0 define τ (cid:88) (cid:88) N τ (cid:44) 1[S = s , A = a(cid:63)], N τ (cid:44) N τ . a(cid:63) t d t a(cid:63) t=1 a(cid:63) Following the steps of Domingues et al. (2021b, Proof of Theorem 7), we have that (cid:104) (cid:105) (D) (cid:104) (cid:105) (cid:16) 1 1 (cid:17) (D) (cid:104)(cid:16) (cid:104) (cid:105)(cid:17) (cid:16) 1 (cid:17) (cid:105) E N τ 4ε2 ≥ E N τ kl , + ε ≥ 1 − P Eτ log − log(2) , 0 a(cid:63) (cid:101) 0 a(cid:63) 2 2 (cid:101) 0 a(cid:63) δ thus (cid:104) (cid:105) 1 (cid:104)(cid:16) (cid:104) (cid:105)(cid:17) (cid:16) 1 (cid:17) (cid:105) E N τ ≥ 1 − P Eτ log − log(2) . 0 a(cid:63) 4ε2 0 a(cid:63) δ (cid:101) Summing all over MDP instances, we obtain following Domingues et al. (2021b, Proof of Theorem 7) that (cid:104) (cid:105) (cid:88) (cid:104) (cid:105) (D) A (cid:16) 1 (cid:17) E N τ = E N τ ≥ log . 0 0 a(cid:63) 8ε2 δ a(cid:63) (cid:101) While the proof of Domingues et al. (2021b, Theorem 7) can stop at this stage, our proof requires an additional step of linking this back to the sample complexity τ , since the latter is not defined in terms of number of episodes but in terms of number of time steps. For any i ≤ τ , denote by W (s → s ) the random variable of the number of time steps required to reach s i 0 d d starting from s for the i-th time — note importantly that this quantity is independent of the algorithm and of 0 the MDP parameter a(cid:63), and we can write E[W (s → s )] = E[W (s → s )] = 1 . Then from Wald’s equation we i 0 d 0 d q have (cid:34) (cid:88)Nτ (cid:35) 1 1 (cid:18) 1 (cid:19) L3A (cid:18) 1 (cid:19) E [τ ] ≥ E W (s → s ) = E[W (s → s )] · E [N τ ] ≥ · α A log ≥ α(cid:48) log , 0 0 i 0 d 0 d 0 q ε2 δ ε2 δ (cid:101) i=1 where α and α(cid:48) are absolute constants. Finally, as mentioned, the extension to any S to make appear the multiplicative dependence on S can be done by following the steps done in Domingues et al. (2021b, Proof of Theorem 7) (which relies on their Assumption 1, see their Theorem 10 for the relaxed statement). The idea of the construction is to consider not just 1 decision state s but S − 3 of them, where only one of them possesses the favorable action a(cid:63); intuitively this generates d SA actions instead of A (see e.g., Lattimore and Szepesvári, 2020, Section 38.7), thus leading to the additional S factor in the sample complexity.
Adaptive Multi-Goal Exploration C DETAILS OF AdaGoal-UCBVI AND ANALYSIS In this section, we focus on AdaGoal-UCBVI. In Section C.1, we introduce useful notation. In Section C.2, we define the exact choice of estimates D, E, Q used by AdaGoal-UCBVI (line 13 of Algorithm 1). Then in Section C.3, we provide the full proof of Theorem 8, by establishing the key steps followed in Section 4. Throughout the analysis we consider that G = S, ε ∈ (0, 1] and δ ∈ (0, 1). C.1 Notation Given a goal state g ∈ G, denote by M the unit-cost SSP-MDP which adds a self-loop at g to the original MDP g M, and denote by p its transition function and c its cost function. Formally, let g g (cid:26) p(s(cid:48)|s, a) if s (cid:54)= g c (s, a) (cid:44) 1[s (cid:54)= g], p (s(cid:48)|s, a) (cid:44) g g 1[s(cid:48) = g] if s = g. For any (possibly non-stationary) policy π = (π ) , let V π be its SSP value function (i.e., expected cost-to-go) h h≥1 g in M , i.e., g (cid:34)+∞ (cid:35) V gπ(s 0) (cid:44) E (cid:88) c g(s h, a h) (cid:12) (cid:12) s 1 = s 0, π, M g , h=1 where a (cid:44) π (s ) and s ∼ P (s , a ). Let π(cid:63) ∈ arg min V π and V (cid:63) (cid:44) V π g(cid:63) . We now define the set of h h h h+1 g h h g π g g g finite-horizon goal-conditioned models. Definition 15 (Finite-Horizon Goal-Conditioned Models). Fix a horizon H ≥ 1. For any goal state g ∈ G, denote by M the finite-horizon model that corresponds to starting from state s and interacting for H steps g,H 0 with the original MDP M in which state g is made absorbing. M admits as cost function c (cid:44) c and as g,H g g transition function p (cid:44) p . g g Remark 5. Note that for a given goal state g ∈ G, the cost function c is known to the agent, while the MDP g transitions p are unknown with some (known) goal-specific changes w.r.t. the original MDP M (namely, the g self-loop at g). An alternative way of framing the problem is that there is one single MDP with state space S × G, i.e., with state variable (s, g). For a finite-horizon policy π ∈ Π , denote by V π its finite-horizon value function at step 1 ≤ h ≤ H in the H g,h finite-horizon instance M , i.e., g,H (cid:34) H (cid:35) V π g,h(s 0) (cid:44) E (cid:88) c g(s h(cid:48), a h(cid:48)) (cid:12) (cid:12) s 1 = s 0, π, M g,H . h(cid:48)=h We define the corresponding optimal value function as V (cid:63) (cid:44) min V π . Observe that V (cid:63) (s ) = D(cid:63)(g) g,h π g,h g,1 0 H (notation used in Properties 1 and 2 of Section 3). Let (s , a , s ) be the state, action and next state observed by an algorithm at time step i. Let nk(s, a) (cid:44) i i i+1 (cid:80)kH 1[(s , a ) = (s, a)] be the number of times state-action pair (s, a) was visited in the first k episodes i=1 i i and nk(s, a, s(cid:48)) (cid:44) (cid:80)kH 1[(s , a , s ) = (s, a, s(cid:48))]. We define the empirical transitions as p k(s(cid:48)|s, a) (cid:44) i=1 i i i+1 (cid:98) nk(s, a, s(cid:48))/nk(s, a) if nk(s, a) > 0, and p k(s(cid:48)|s, a) (cid:44) 1/S otherwise. Also, pX(s, a) (cid:44) E [X(s(cid:48))] denotes (cid:98) s(cid:48)∼p(·|s,a) the expectation operator w.r.t. the transition probabilities p and π Y (s) (cid:44) Y (s, π (s)) denotes the composition h h with policy π at step h, so that pπ Z(s, a) (cid:44) E [Z(s(cid:48), π (s(cid:48)))]. Finally, we denote the clip function by h s(cid:48)∼p(·|s,a) h clip(x, y, z) (cid:44) max(min(x, z), y). Finally, in the analysis we denote by pk (s, a) the probability of reaching state-action pair (s, a) at step h under g,h policy πk in the true MDP. We also define the pseudo-counts as nk(s, a) (cid:44) (cid:80)H (cid:80)k pl (s, a), where g ∈ S denotes g the goal state selected by AdaGoal-UCBVI at the beginning of algorih t= h1 micl= ep1 isogld,h e l. l
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric C.2 Algorithmic Choices of D, E, Q for AdaGoal-UCBVI We generalize to our goal-conditioned scenario the estimates used by BPI-UCBVI (Ménard et al., 2021a), a recent algorithm designed for Best-Policy Identification (BPI) in finite-horizon non-stationary MDPs. First, we build the optimistic goal-conditioned Q-values and value functions in the finite-horizon models M for g ∈ S and g,H 0 h ≤ H as follows, Q(cid:101) (s, a) (cid:44) 1[s (cid:54)= g], g,h (cid:32) (cid:115) k k β(cid:63)(nk(s, a), δ) β(nk(s, a), δ) Q(cid:101) (s, a) (cid:44) clip 1[s (cid:54)= g] − 3 Var (V(cid:101) )(s, a) − 14H2 g,h p(cid:98)k g,h+1 nk(s, a) nk(s, a) (cid:33) − 1 p k(V k − V(cid:101) k )(s, a) + p kV(cid:101) k (s, a), 0, H , H (cid:98) ˜ g,h+1 g,h+1 (cid:98) g,h+1 k k k k V(cid:101) (s) (cid:44) min Q(cid:101) (s, a), V(cid:101) (g) (cid:44) 0, V(cid:101) (s) (cid:44) 0, g,h g,h g,h g,H+1 a∈A where we define the variance of V(cid:101) k g,h+1 with respect to p (cid:98)k(·|s, a) as Var p(cid:98)k (V(cid:101) k g,h+1)(s, a) (cid:44)(cid:80) s(cid:48) p (cid:98)k(s(cid:48)|s, a)(cid:0) V(cid:101) k g,h+1(s(cid:48)) − p (cid:98)kV(cid:101) k g,h+1(s, a)(cid:1)2, where β(n, δ) = O(cid:101)(S log(n/δ)) and β(cid:63)(n, δ) = O(cid:101)(log(n/δ)) are some exploration thresholds and V k is a pessimistic finite-horizon goal-conditioned value function; see Appendix C.3 for the complete definitions. g ˜ Let πk+1 be the greedy policy with respect to the lower bounds Q(cid:101) k . We recursively define the functions U k for g,h g,h g g ∈ S and h ≤ H as follows, U 0 (s, a) (cid:44) H1[s (cid:54)= g], g,h (cid:32) (cid:115) U k (s, a) (cid:44) clip 6 Var (V(cid:101) k )(s, a) β(cid:63)(nk(s, a), δ) + 36H2 β(nk(s, a), δ) g,h p(cid:98)k g,h+1 nk(s, a) nk(s, a) (cid:18) (cid:19) (cid:33) + 1 + 3 p kπk+1 U k (s, a), 0, H , H (cid:98) g,h+1 g,h+1 U k (g, a) (cid:44) 0, U k (s, a) (cid:44) 0, g,h g,H+1 We are now ready to define the distance and error estimates of AdaGoal-UCBVI (Algorithm 1) as follows k−1 Q (s, a, g) (cid:44) Q(cid:101) (s, a), (8) k,h g,h k−1 D (g) (cid:44) V(cid:101) (s ), (9) k g,1 0 E (g) (cid:44) πk U k−1 (s ) + 8ε . (10) k g,1 g,1 0 9 C.3 Proof of Theorem 8 In this section, we provide the full proof of Theorem 8, by establishing the key steps followed in Section 4. Appendices C.3.1, C.3.2 and C.3.3, which prove “key steps (cid:172), (cid:173), (cid:174)”, focus on more “standard” technical tools (e.g., high-probability events, variance-aware concentration inequalities); in particular building on the analysis of Ménard et al. (2021a) on the sample complexity of BPI in finite-horizon MDPs and extending it to our goal-conditioned scenario. Then Appendix C.3.4 proves “key step (cid:175)”, which focuses on the technical novelty of the AdaGoal goal selection scheme that is specific to the multi-goal exploration setting. We begin by stating a simple property that we will rely on throughout the analysis. Lemma 16. For any state-action pair (s, a) ∈ S × A and goal state g ∈ S, consider any vector Y ∈ RS such that Y (g) = 0, then pY (s, a) = p Y (s, a), where we recall that g (cid:26) p(s(cid:48)|s, a) if s (cid:54)= g p (s(cid:48)|s, a) (cid:44) g 1[s(cid:48) = g] if s = g. Proof. It is easy to see that (cid:88) pY (s, a) = p(s(cid:48)|s, a)Y (s(cid:48)) + p(g|s, a) Y (g) (cid:124) (cid:123)(cid:122) (cid:125) s(cid:48)∈S\{g} =0
Adaptive Multi-Goal Exploration (cid:88) = p (s(cid:48)|s, a)Y (s(cid:48)) + p (g|s, a) Y (g) g g (cid:124) (cid:123)(cid:122) (cid:125) s(cid:48)∈S\{g} =0 = p Y (s, a). g Thanks to the above observation, our analysis will not require to handle goal-conditioned (true or empirical) transition probabilities, and will only need to deal with the (true or empirical) transition probabilities of the original MDP M. C.3.1 Proof of “key step (cid:172)” Concentration events. Here we define the high-probability event U on which we condition our statements. We follow the notation of Ménard et al. (2021a, Appendix A) and define the three following favorable events: E the event where the empirical transition probabilities are close to the true ones, Ecnt the event where the pseudo-counts are close to their expectation, and E(cid:63) where the empirical means of the optimal goal-conditioned value functions are close to the true ones. Denoting by KL the Kullback-Leibler divergence, we set E (cid:44) (cid:26) ∀k ∈ N, ∀(s, a) ∈ S × A : KL(cid:0) p k(·|s, a), p(·|s, a)(cid:1) ≤ β(nk(s, a), δ) (cid:27) , (cid:98) nk(s, a) (cid:26) (cid:27) Ecnt (cid:44) ∀k ∈ N, ∀(s, a) ∈ S × A : nk(s, a) ≥ 1 n¯k(s, a) − βcnt(δ) , 2 (cid:40) E(cid:63) (cid:44) ∀k ∈ N, ∀h ∈ [H], ∀(s, a) ∈ S × A, ∀g ∈ S : (cid:32) (cid:115) (cid:33)(cid:41) (cid:12) (cid:12)(p (cid:98)k − p)V (cid:63) g,h+1(s, a)(cid:12) (cid:12) ≤ min H, 2Var p(V (cid:63) g,h+1)(s, a) β(cid:63)( nn kk (( ss ,, aa )), δ) + 3H β(cid:63)( nn kk (( ss ,, aa )), δ) · We define the intersection of these events as U (cid:44) E ∩ Ecnt ∩ E(cid:63). (11) We prove that for the right choice of the functions β the above event holds with high probability. Lemma 17. For the following choices of functions β, β(n, δ) (cid:44) log(3S2AH/δ) + S log(8e(n + 1)), βcnt(δ) (cid:44) log(cid:0) 3S2AH/δ(cid:1) , β(cid:63)(n, δ) (cid:44) log(3S2AH/δ) + log(8e(n + 1)), it holds that P(U) ≥ 1 − δ. Proof. The only difference with respect to the concentration inequalities of Ménard et al. (2021a, Appendix A) is that we need to take a union bound over the goal states g ∈ S when concentrating our optimal goal-conditioned value functions. We thus set δ ← δ/S in the choices of functions β compared to Ménard et al. (2021a). As a result, by Ménard et al. (2021a, Theorem 3 & 4 & 5) we have that P(E) ≥ 1 − δ , P(Ecnt) ≥ 1 − δ and P(E(cid:63)) ≥ 1 − δ , 3 3 3 respectively. Applying a union to the above three inequalities, we conclude that P(U) ≥ 1 − δ. We recall the definitions of the functions U k, Q(cid:101) k and V(cid:101) k in Appendix C.2. They rely on the pessimistic g g,h g,h finite-horizon goal-conditioned values V k defined as g ˜ (cid:32) (cid:115) Qk (s, a) (cid:44) clip 1[s (cid:54)= g] + 3 Var (V(cid:101) k )(s, a) β(cid:63)(nk(s, a), δ) + 14H2 β(nk(s, a), δ) ˜ g,h p(cid:98)k g,h+1 nk(s, a) nk(s, a)
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric (cid:33) + 1 p k(V k − V(cid:101) k )(s, a) + p kV k (s, a), 0, H , H (cid:98) ˜ g,h+1 g,h+1 (cid:98) ˜ g,h+1 V k (s) (cid:44) min Qk (s, a), V k (g) (cid:44) 0, V k (s) (cid:44) 0. g,h g,h g,h g,H+1 ˜ a∈A ˜ ˜ ˜ Finally, we define the following quantities (cid:32) (cid:32) (cid:115) Q˚k (s, a) (cid:44) max 1[s (cid:54)= g] + pV˚ (s, a), clip 1[s (cid:54)= g] + 3 Var (V(cid:101) k )(s, a) β(cid:63)(nk(s, a), δ) g,h g,h+1 p(cid:98)k g,h+1 nk(s, a) (cid:33)(cid:33) + 14H2 β(nk(s, a), δ) + 1 p k(V k − V(cid:101) k )(s, a) + p kV(cid:101) k (s, a), 0, H , nk(s, a) H (cid:98) ˜ g,h+1 g,h+1 (cid:98) g,h+1 V˚k (s) (cid:44) πk+1Q˚k (s, a), g,h g,h g,h V˚k (g) (cid:44) 0, g,h V˚k (s) (cid:44) 0. g,H+1 We have the following property, which is the equivalent of Ménard et al. (2021a, Lemma 6) and is proved likewise. Lemma 18. On the event U, for all (s, a, g, h) ∈ S × A × S × [H] and for every episode k, it holds that (cid:18) (cid:19) ˚k k πk+1 Q (s, a) ≥ max Q (s, a), Q g (s, a) , g,h g,h g,h ˜ (cid:18) (cid:19) ˚k k πk+1 V (s) ≥ max V (s), V g (s) . g,h ˜ g,h g,h We now derive “key step (cid:172)” by establishing that Properties 1 and 2 hold. Specifically, we show that (i) the functions V(cid:101) are optimistic estimates of the optimal goal-conditioned finite-horizon value functions and (ii) the g,1 functions U serve as valid upper bounds to the goal-conditioned finite-horizon gaps, as shown below. g,1 Lemma 19. On the event U, it holds that for every episode k and goal g ∈ S, πk+1 (cid:63) πk+1 k V g,1 (s ) − V (s ) ≤ V g,1 (s ) − V(cid:101) (s ) g,1 0 g,1 0 g,1 0 g,1 0 ≤ πk+1U k (s ). g,1 g,1 0 Proof. On the event U, using Lemma 18, we upper bound the goal-conditioned gap at episode t as πk+1 (cid:63) πk+1 k ˚k k V g,1 (s ) − V (s ) ≤ V g,1 (s ) − V(cid:101) (s ) ≤ V (s ) − V(cid:101) (s ). g,1 0 g,1 0 g,1 0 g,1 0 g,1 0 g,1 0 Next, following the same reasoning as in Ménard et al. (2021a, Proof of Lemma 2), we obtain by induction on h that for all state-action pairs (s, a) and goal states g, Q˚k (s, a) − Q(cid:101) k (s, a) ≤ U k (s, a). (12) g,h g,h g,h In particular for the initial layer h = 1 and initial state s = s , we get that 0 V˚k (s ) − V(cid:101) k (s ) = πk+1(Q˚k − Q(cid:101) k )(s ) ≤ πk+1U k (s ). g,1 0 g,1 0 g,1 g,1 g,1 0 g,1 g,1 0 C.3.2 Proof of “key step (cid:173)” Lemma 20. On the event U, for every goal state g ∈ S and episode k, it holds that (cid:118) π gk ,+ 11U k g,1(s 0) ≤ 24e13H(cid:117) (cid:117) (cid:116)(cid:88)H (cid:88) pk g,+ h1(s, a) β n(cid:63) ¯( kn¯ (sk ,(s a, )a ∨), 1δ) + 336e13H2 (cid:88)(cid:34) (cid:88)H pk g,+ h1(s, a) β n¯( kn¯ (k s( ,s a, )a ∨), δ 1) (cid:35) ∧ 1, h=1 s,a s,a h=1 where we recall that pk+1(s, a) denotes the probability of reaching (s, a) at step h under policy πk+1. g,h g
Adaptive Multi-Goal Exploration Proof. Similar to Ménard et al. (2021a, Steps 1 & 2 in proof of Theorem 2), we begin by upper-bounding U k (s, a) g,h for all (s, a, h, g, k). If nk(s, a) > 0, by definition of U k we have that g,h (cid:115) U k (s, a) ≤ 6 Var (V(cid:101) k )(s, a) β(cid:63)(nk(s, a), δ) + 36H2 β(nk(s, a), δ) + (cid:18) 1 + 3 (cid:19) p kπk+1 U k (s, a). g,h p(cid:98)k g,h+1 nk(s, a) nk(s, a) H (cid:98) g,h+1 g,h+1 (13) We now replace the empirical transition probabilities with the true ones. Using the Bernstein-type technical inequality of Ménard et al. (2021a, Lemma 10) and that 0 ≤ U k ≤ H, we get g,h (cid:115) (p k − p)πk+1 U k (s, a) ≤ 2Var (πk+1 U k )(s, a) β(nk(s, a), δ) + 2 H β(nk(s, a), δ) (cid:98) g,h+1 g,h+1 p g,h+1 g,h+1 nk(s, a) 3 nk(s, a) ≤ 1 pπk+1 U k (s, a) + 3H2 β(nk(s, a), δ) , H g,h+1 g,h+1 nk(s, a) where in the last line we used Var (πk+1U k )(s, a) ≤ Hπk+1 U k (s, a) and √ xy ≤ x + y for all x, y ≥ 0. p h+1 g,h+1 g,h+1 g,h+1 We then replace the variance of the upper confidence bound under the empirical transition probabilities by the variance of the optimal value function under the true transition probabilities. Using the technical lemmas of Ménard et al. (2021a, Lemma 11 & 12) that control the deviation in variances w.r.t. the choice of transition probabilities, we obtain that k k β(nk(s, a), δ) Var (V(cid:101) )(s, a) ≤ 2Var (V(cid:101) )(s, a) + 4H2 p(cid:98)k g,h+1 p g,h+1 nk(s, a) ≤ 4Var (V π gk+1 )(s, a) + 4Hp(V(cid:101) k − V π gk+1 )(s, a) + 4H2 β(nk(s, a), δ) p g,h+1 g,h+1 g,h+1 nk(s, a) ≤ 4Var (V π gk+1 )(s, a) + 4Hpπk+1 U k (s, a) + 4H2 β(nk(s, a), δ) , p g,h+1 g,h+1 g,h+1 nk(s, a) √ √ √ √ where we used (12) in the last inequality. Next, using x + y ≤ x + y, xy ≤ x + y, and β(cid:63)(n, δ) ≤ β(n, δ) leads to (cid:115) (cid:115) Var (V(cid:101) k )(s, a) β(cid:63)(nk(s, a), δ) ≤ 2 Var (V πgk+1 )(s, a) β(cid:63)(nk(s, a), δ) + (2H + 4H2) β(nk(s, a), δ) p(cid:98)k g,h+1 nk(s, a) p g,h+1 nk(s, a) nk(s, a) + 1 pπk+1 U k (s, a) H g,h+1 g,h+1 (cid:115) ≤ 2 Var (V πgk+1 )(s, a) β(cid:63)(nk(s, a), δ) + 6H2 β(nk(s, a), δ) p g,h+1 nk(s, a) nk(s, a) + 1 pπk+1 U k (s, a). H g,h+1 g,h+1 Combining these two inequalities with (13) yields (cid:115) U k (s, a) ≤ 12 Var (V πgk+1 )(s, a) β(cid:63)(nk(s, a), δ) + 36H2 β(nk(s, a), δ) g,h p g,h+1 nk(s, a) nk(s, a) + 6 pπk+1 U k (s, a) + 36H2 β(nk(s, a), δ) H g,h+1 g,h+1 nk(s, a) + (cid:18) 1 + 3 (cid:19) 1 pπk+1 U k + (cid:18) 1 + 3 (cid:19) 3H2 β(nk(s, a), δ) H H g,h+1 g,h+1 H nk(s, a) (cid:18) (cid:19) + 1 + 3 pπk+1 U k (s, a) H g,h+1 g,h+1 (cid:115) ≤ 12 Var (V πgk+1 )(s, a) β(cid:63)(nk(s, a), δ) + 84H2 β(nk(s, a), δ) p g,h+1 nk(s, a) nk(s, a)
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric (cid:18) (cid:19) + 1 + 13 pπk+1 U k (s, a). H g,h+1 g,h+1 Since by construction, U k (s, a) ≤ H, we have that for all nk(s, a) ≥ 0, g,h (cid:115) U k (s, a) ≤ 12 Var (V πgk+1 )(s, a)(cid:18) β(cid:63)(nk(s, a), δ) ∧ 1(cid:19) + 84H2(cid:18) β(nk(s, a), δ) ∧ 1(cid:19) g,h p g,h+1 nk(s, a) nk(s, a) (cid:18) (cid:19) + 1 + 13 pπk+1 U k (s, a). H g,h+1 g,h+1 Unfolding the previous inequality and using (1 + 13/H)H ≤ e13 we get (cid:115) πk+1U k (s ) ≤ 12e13 (cid:88)H (cid:88) pk+1(s, a) Var (V πgk+1 )(s, a)(cid:18) β(cid:63)(nk(s, a), δ) ∧ 1(cid:19) g,1 g,1 0 g,h p g,h+1 nk(s, a) h=1 s,a (cid:88)H (cid:88) (cid:18) β(nk(s, a), δ) (cid:19) + 84e13H2 pk+1(s, a) ∧ 1 . g,h nk(s, a) h=1 s,a Using that πk+1U k (s ) ≤ H, we can clip the above bound as follows g,1 g,1 0 (cid:115) πk+1U k (s ) ≤ 12e13 (cid:88)H (cid:88) pk+1(s, a) Var (V πgk+1 )(s, a)(cid:18) β(cid:63)(nk(s, a), δ) ∧ 1(cid:19) g,1 g,1 0 g,h p g,h+1 nk(s, a) h=1 s,a (cid:88)(cid:34) (cid:88)H (cid:18) β(nk(s, a), δ) (cid:19)(cid:35) + 84e13H2 pk+1(s, a) ∧ 1 ∧ 1. (14) g,h nk(s, a) s,a h=1 From the technical lemma of Ménard et al. (2021a, Lemma 8) that relates counts to pseudo-counts, β(nk(s, a), δ) β(n¯k(s, a), δ) ∧ 1 ≤ 4 , nk(s, a) n¯k(s, a) ∨ 1 thus we can replace the counts by the pseudo-counts in (14) as (cid:115) πk+1U k (s ) ≤ 24e13 (cid:88)H (cid:88) pk+1(s, a) Var (V πgk+1 )(s, a) β(cid:63)(n¯k(s, a), δ) g,1 g,1 0 g,h p g,h+1 n¯k(s, a) ∨ 1 h=1 s,a (cid:88)(cid:34) (cid:88)H β(n¯k(s, a), δ) (cid:35) + 336e13H2 pk+1(s, a) ∧ 1. (15) g,h n¯k(s, a) ∨ 1 s,a h=1 We now apply the law of total variance (see e.g., Azar et al., 2017 or Ménard et al., 2021a, Lemma 7) in order to further upper-bound the first sum in (15). In particular, by Cauchy-Schwarz inequality, we obtain (cid:115) (cid:88)H (cid:88) pk+1(s, a) Var (V πgk+1 )(s, a) β(cid:63)(n¯k(s, a), δ) g,h p g,h+1 n¯k(s, a) ∨ 1 h=1 s,a (cid:118) (cid:118) ≤ (cid:117) (cid:117) (cid:116)(cid:88)H (cid:88) pk g,+ h1(s, a)Var p(V π g,gk h+ +1 1)(s, a)(cid:117) (cid:117) (cid:116)(cid:88)H (cid:88) pk g,+ h1(s, a) β n(cid:63) ¯( kn¯ (sk ,(s a, )a ∨), 1δ) h=1 s,a h=1 s,a (cid:118) ≤ (cid:117) (cid:117) (cid:117) (cid:116)E πgk+1 (cid:32) (cid:88)H 1[s h (cid:54)= g] − V π g,gk 1+1 (s 0)(cid:33)2 (cid:118) (cid:117) (cid:117) (cid:116)(cid:88)H (cid:88) pk g,+ h1(s, a) β n(cid:63) ¯( kn¯ (sk ,(s a, )a ∨), 1δ) h=1 h=1 s,a (cid:118) (cid:117) (cid:117)(cid:88)H (cid:88) β(cid:63)(n¯k(s, a), δ) ≤ H(cid:116) pk+1(s, a) · g,h n¯k(s, a) ∨ 1 h=1 s,a Plugging this in (15) concludes the proof of Lemma 20.
Adaptive Multi-Goal Exploration We are now ready to derive “key step (cid:173)” which controls the cumulative gap bounds, see Equation 4. Lemma 21. On the event U, for any number of episodes K ≥ 1, it holds that K−1 √ (cid:88) πk+1 U k (s ) ≤ 48e13 K(cid:112) H2SA log(HK + 1)β(cid:63)(K, δ) + 1344e13H2SAβ(K, δ) log(HK + 1) gk+1,1 gk+1,1 0 k=0 (cid:112) + 48e13H2SA β(cid:63)(K, δ). Proof. Plugging in the bound of Lemma 20 yields K−1 (cid:88) πk+1 U k (s ) gk+1,1 gk+1,1 0 k=0 (cid:118) K (cid:88)−1 (cid:117) (cid:117)(cid:88)H (cid:88) β(cid:63)(n¯k(s, a), δ) K (cid:88)−1 (cid:88)(cid:34) (cid:88)H β(n¯k(s, a), δ) (cid:35) ≤ 24e13H (cid:116) pk+1 (s, a) + 336e13H2 pk+1(s, a) ∧ 1 gk+1,h n¯k(s, a) ∨ 1 g,h n¯k(s, a) ∨ 1 k=0 h=1 s,a k=0 s,a h=1 (cid:118) ≤ 24e13H(cid:112) β(cid:63)(HK, δ) K (cid:88)−1 (cid:117) (cid:117) (cid:116)(cid:88) n¯k+1(s, a) − n¯k(s, a) + 336e13H2β(HK, δ) (cid:88) K (cid:88)−1(cid:20) n¯k+1(s, a) − n¯k(s, a) (cid:21) ∧ 1, n¯k(s, a) ∨ 1 n¯k(s, a) ∨ 1 k=0 s,a s,a k=0 where we used that β(., δ) and β(cid:63)(., δ) are increasing. We define J (cid:44) {k ∈ [0, K − 1] : n¯k(s, a) < n¯k+1(s, a) − n¯k(s, a) − 1}. Applying Lemma 22 gives that (cid:118) (cid:115) (cid:88) (cid:117) (cid:117)(cid:88) n¯k+1(s, a) − n¯k(s, a) (cid:88) (cid:88) n¯k+1(s, a) − n¯k(s, a) (cid:116) ≤ ≤ 2SAH, n¯k(s, a) ∨ 1 n¯k(s, a) ∨ 1 k∈J s,a s,a k∈J (cid:124) (cid:123)(cid:122) (cid:125) ≤2H (cid:118) (cid:118) (cid:88) (cid:117) (cid:117) (cid:116)(cid:88) n¯k+1 n¯(s k, (a s,) a− ) ∨n¯k 1(s, a) ≤ √ K(cid:117) (cid:117) (cid:117)(cid:88) (cid:88) n¯k+1 n¯(s k, (a s,) a− ) ∨n¯k 1(s, a) ≤ 2(cid:112) KSA log(HK + 1), (cid:117) k∈/J s,a (cid:116) s,a k∈/J (cid:124) (cid:123)(cid:122) (cid:125) ≤4 log(HK+1) (cid:88) K (cid:88)−1(cid:20) n¯k+1(s, a) − n¯k(s, a) (cid:21) ∧ 1 ≤ 4SA log(HK + 1). n¯k(s, a) ∨ 1 s,a k=0 (cid:124) (cid:123)(cid:122) (cid:125) ≤4 log(HK+1) Putting everything together yields Lemma 21. Note that as opposed to Ménard et al. (2021a), we are in the setting of stationary transition probabilities (and cost functions), which is why we are able to shave a factor H in the main order term of the bound of the cumulative gap bounds (also recall that their sample complexity bound is in terms of exploration episodes and not exploration steps as ours). Lemma 22 (Technical lemma). For T ∈ N(cid:63) and (u t) t∈N(cid:63), for any sequence where u t ∈ [0, H] for some constant H > 0 and U (cid:44) (cid:80)t u , let Ω (cid:44) {t ∈ [0, T ] : U < u − 1} and ω (cid:44) max{t ∈ Ω}. Then it holds that t l=1 (cid:96) t t+1 (cid:114) (cid:88) u t+1 ≤ 2H, U ∨ 1 t t∈Ω (cid:88) u t+1 ≤ 4 log(U + 1), U ∨ 1 T +1 t t∈/Ω T (cid:20) (cid:21) (cid:88) u t+1 ∧ 1 ≤ 4 log(U + 1). U ∨ 1 T +1 t t=0 Proof. First, note that for any t ∈ Ω, ut+1 ≥ 1, therefore Ut∨1 (cid:114) ω (cid:88) u t+1 ≤ (cid:88) u t+1 ≤ (cid:88) u ≤ (cid:88) u = U = U + u ≤ u − 1 + u ≤ 2H. U ∨ 1 U ∨ 1 t+1 t+1 ω+1 ω ω ω+1 ω t t t∈Ω t∈Ω t∈Ω t=0
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric Second, if t ∈/ Ω, then 2U + 2 ≥ U + 1, therefore t t+1 u u U − U t+1 ≤ 4 t+1 ≤ 4 t+1 t , U ∨ 1 2U + 2 U + 1 t t t+1 which yields that (cid:88) u t+1 ≤ 4 (cid:88) U t+1 − U t ≤ 4 (cid:88)T (cid:90) Ut+1 1 dx ≤ 4 log(U + 1). U ∨ 1 U + 1 x + 1 T +1 t∈/Ω t t∈/Ω t+1 t=0 Ut Third, combining the two cases above and further noticing that 4 Ut+1−Ut = 4ut+1 ≥ 4ut+1 ≥ 1 for all t ∈ Ω, it holds that Ut+1+1 Ut+ut+1+1 2ut+1 (cid:20) (cid:21) u U − U t+1 ∧ 1 ≤ 4 t+1 t , U ∨ 1 U + 1 t t+1 thus (cid:88)T (cid:20) u t+1 (cid:21) ∧ 1 ≤ 4 (cid:88)T U t+1 − U t ≤ 4 (cid:88)T (cid:90) Ut+1 1 dx ≤ 4 log(U + 1). U ∨ 1 U + 1 x + 1 T +1 t=0 t t=0 t+1 t=0 Ut C.3.3 Proof of “key step (cid:174)” Lemma 23. The MGE sample complexity τ of algorithm AdaGoal-UCBVI can be bounded with probability at least 1 − δ by7 (cid:32) (cid:33) L3SA (cid:16) LSA (cid:17) (cid:16) L (cid:17) L3S2A (cid:16) LSA (cid:17) (cid:16) L (cid:17) τ = O · log3 · log3 + · log3 · log3 . ε2 εδ ε ε εδ ε Proof. We assume that the event U holds and fix a (finite) episode K < κ, where κ denotes the (possibly unbounded) episode index at which AdaGoal-UCBVI terminates. For any k ≤ K, denote by g the goal selected k by AdaGoal-UCBVI at the beginning of episode k, then by design of the stopping rule (1) and by choice of prediction error E (10), it holds that k ε ≤ πk U k−1 (s ) + 8ε . gk,1 gk,1 0 9 By summing the previous inequality for all k ≤ K and plugging in the bound of Lemma 21, we get that ε √ (cid:112) K ≤ 48e13 K H2SA log(HK + 1)β(cid:63)(K, δ) + 1344e13H2SAβ(K, δ) log(HK + 1) 9 (cid:112) + 48e13H2SA β(cid:63)(K, δ). We assume that κ > 1 otherwise the result is trivially true. Defining κ(cid:48) (cid:44) min{κ − 1, K} and using the definition of β and β(cid:63) given in Lemma 17, we get the following functional inequality in κ(cid:48) (cid:113) εκ(cid:48) ≤ x κ(cid:48)H2SA log(Hκ(cid:48))(cid:0) log(3S2AH/δ) + log(16eκ(cid:48))(cid:1) + x H2S2A log(3S2AH/δ) log2(Hκ(cid:48)), 1 2 for some absolute constants x , x . There remains to invert the above inequality to obtain an upper bound on 1 2 κ(cid:48). We use the auxiliary inequality of Lemma 24 instantiated with scalars B = x (cid:112) H2SA log(3S2AH/δ)/ε, 1 C = x H2S2A log(3S2AH/δ)/ε and α = 16eH. This yields that 2 (cid:18) H2SA (cid:18) HSA (cid:19) H2S2A (cid:18) HSA (cid:19)(cid:19) κ(cid:48) ≤ O log3 + log3 . (16) ε2 εδ ε δε 7We say that f = O(g) if there exists an absolute constant ι > 0 (independent of the MDP instance) such that f ≤ ιg.
Adaptive Multi-Goal Exploration Since (16) holds for κ(cid:48) = min{κ − 1, K} for any finite K < κ, letting K → +∞ implies that κ is finite and bounded as in (16). The last step is to relate the above bound on the number of algorithmic episodes κ to the MGE sample complexity of AdaGoal-UCBVI denoted by τ . Since the algorithmic episodes are of length H and separated by a one-step execution of the reset action a , it holds that τ ≤ (H + 1)κ. We finally plug in the choice of horizon reset H (cid:44) (cid:100)5(L + 2) log (cid:0) 10(L + 2)/ε(cid:1) / log(2)(cid:101) to conclude the proof of Lemma 23. Lemma 24 (An auxiliary inequality). For any positive scalars B, C ≥ 1 and α ≥ e, it holds for any X ≥ 2 that √ X ≤ B X log(αX) + C log2(αX) =⇒ X ≤ O(cid:0) B2 log2(αB) + C log2(αC)(cid:1) . √ √ Proof. On the one hand, assume that X ≤ B X log(αX), then X ≤ − X + B X log(αX). From the √2 2 √ technical lemma of Kazerouni et al. (2017, Lemma 8), − X + B X log(αX) ≤ 32B2 [log(4B αe)]2, thus √ 2 9 X ≤ 64B2 [log(4B αe)]2. On the other hand, assume that X ≤ C log2(αX). Using that log(x) ≤ xβ/β for all 9 x ≥ 0, β > 0, we get X ≤ √C(8α1/8X1/8)2 ≤ 64Cα1/4X1/4, thus X ≤ (64C√)4/3α1/3, thus X ≤ C log2(64α4/3C4/3). Now, assume that X ≤ B X log(αX)+ C log2(αX). Then X ≤ 2 max{B X log(αX), C log2(αX)}. From above we can bound each term separately, which concludes the proof. C.3.4 Proof of “key step (cid:175)” We finally establish “key step (cid:175)”, which focuses on the technical novelty of the AdaGoal goal selection scheme that is specific to the multi-goal exploration setting. Recall for any H ∈ N∗ the definition of the finite-horizon MDP M = {H, S, A, p , c }, where we recall that g,H g g p = p and c = c . We denote by π(cid:63) the optimal policy in M as well as V (s) the optimal value g g g g g,H g,H g,H,h function starting from state s at step h. We also define p¯πg,H (s (cid:54)= g|s = s) the probability of reaching state g g,H H 1 starting from state s with the policy π ∈ Π in the MDP M . When it is clear from the context we drop the H g,H dependence on the horizon H in the previous notations. The following lemma controls the probability of not reaching a goal in G with the optimal policy in the L+ε finite-horizon reduction MDP. Lemma 25. For g ∈ G , for all H ≥ 2(L + 2), for all s ∈ S, L+ε (cid:0) (cid:1) p¯π(cid:63) g,H (s (cid:54)= g|s = s) ≤ e− log(2)H/ 4(L+2) . g,H H 1 Proof. By induction it holds p¯π(cid:63) g,H (s (cid:54)= g|s = s) = (cid:88) p¯π(cid:63) g,H (s = s(cid:48)|s = s)p¯π(cid:63) g,H (s (cid:54)= g|s = s(cid:48)) g,H H 1 g,H H−M+1 1 g,H H H−M+1 s(cid:48)(cid:54)=g ≤ p¯π(cid:63) g,H (s (cid:54)= g|s = s) max p¯π(cid:63) g,H (s (cid:54)= g|s = s(cid:48)) g,H H−M+1 1 g,H H H−M+1 s(cid:48) (cid:98)H/M(cid:99) ≤ (cid:89) max p¯π(cid:63) g,H (s (cid:54)= g|s = s(cid:48)) . g,H H−jM H−(j+1)M+1 s(cid:48) j=0 Then thanks to the Markov inequality and the optimal Bellman equations solved by π(cid:63) we obtain g,H p¯π(cid:63) g,H (s (cid:54)= g|s = s(cid:48)) ≤ p¯π(cid:63) g,H (s (cid:54)= g|s = s(cid:48)) g,H H−jM H−(j+1)M+1 g,H H H−(j+1)M+1 V (cid:63) (s(cid:48)) g,H,H−(j+1)+1 ≤ M V (cid:63) (s(cid:48)) g,(j+1)M,1 = M ≤ V (cid:63) g,(j+1)M,1(s(cid:48)) ≤ V g(cid:63)(s(cid:48)) ≤ L + 2 , M M M
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric where the last inequality uses the existence of a resetting action (Assumption 3) and the fact that g ∈ G with L+ε ε ≤ 1. Choosing M = 2(L + 2) allows us to conclude (cid:0) (cid:1) p¯π(cid:63) g,H (s (cid:54)= g|s = s) ≤ e−(cid:98)H/M(cid:99) log(2) ≤ e− log(2)H/ 4(L+2) , g,H H 1 where in the last inequality we used (cid:98)x(cid:99) ≥ x/2 for x ≥ 1. We define the class of non-stationary, infinite-horizon policies that perform the reset action whenever the goal state is not reached after H steps. Definition 26 (Resetting policies). For any π, we denote by π|H the non-stationary policy that, until the goal is reached, successively executes the actions prescribed by π for H steps and takes action a , i.e., at time step i reset and state s it executes the following action: (cid:26) a if i ≡ 0 (mod H + 1), π|H (a|s, i) (cid:44) reset π(a|s, i) otherwise. We denote by Π|H the set of such resetting policies. We now establish two key lemmas. First, we show that, equipped with a near-optimal policy for the finite-horizon model M , expanding it into an infinite-horizon policy via the reset provides a near-optimal goal-reaching g,H policy in the original MDP M as long as the goal state g belongs to G and the horizon H is large enough. g O(L) Lemma 27. For g ∈ G and H ≥ 5(L + 2) log (cid:0) 10(L + 2)/ε(cid:1) / log(2), it holds that L+ε V (cid:63) (s ) ≤ V (cid:63)(s ) ≤ V (cid:63) (s ) + ε , g,1 0 g 0 g,1 0 9 and if a policy π is ε/9-optimal in M then (cid:101) g,H V gπ(cid:101)|H (s 0) ≤ V g(cid:63)(s 0) + ε . Proof. We have trivially V (cid:63) (s ) ≤ V (cid:63)(s ). Thanks to Lemma 25 it holds g,H,1 0 g 0 q(cid:63) (cid:44) p¯π(cid:63) g,H (s (cid:54)= g|s = s ) ≤ p¯π(cid:63) g,H (s (cid:54)= g|s = s ) ≤ ε ≤ 1 . (17) g,H g,H H+1 1 0 g,H H 1 0 10(L + 2) 30 Thanks to (17) and the definition of a resetting policy, we can conclude that V (cid:63)(s ) ≤ V π(cid:63) g,H|H (s ) = V (cid:63) (s ) + q(cid:63) (1 + V π(cid:63) g,H|H (s )) g 0 g 0 g,H,1 0 g,H g 0 q(cid:63) (cid:63) g,H (cid:63) = V (s ) + (1 + V (s )) g,H,1 0 1 − q(cid:63) g,H,1 0 g,H (cid:63) 30 ε ≤ V (s ) + (1 + L + ε) g,H,1 0 29 10(L + 2) (cid:63) ε ≤ V (s ) + . g,H,1 0 9 Thus it holds that V (cid:63) (s ) ≤ V (cid:63)(s ) ≤ V (cid:63) (s ) + ε . (18) g,H,1 0 g 0 g,H,1 0 9 For the second part of the lemma, first note that H V π g(cid:101) ,H,1(s 0) = (cid:88) p¯π g(cid:101) ,H (s h (cid:54)= g|s 1 = s 0) ≥ V π g(cid:101) ,H−L,1(s 0) + Lp¯π g(cid:101) ,H (s H (cid:54)= g|s 1 = s 0) , h=1 where in the inequality we used that p¯π g(cid:101) ,H (s h (cid:54)= g|s 1 = s 0) ≥ p¯π g(cid:101) ,H (s H (cid:54)= g|s 1 = s 0). Using successively the fact that π is ε -optimal in M , the inequality above and (18) we obtain (cid:101) 9 g,H V (cid:63)(s ) + ε ≥ V (cid:63) (s ) + ε g 0 9 g,H,1 0 9
Adaptive Multi-Goal Exploration ≥ V π(cid:101) (s ) g,H,1 0 ≥ V π g(cid:101) ,H−L,1(s 0) + Lp¯π g(cid:101) ,H (s H (cid:54)= g|s 0) ε ≥ V g(cid:63)(s 0) − 9 + Lp¯π g(cid:101) ,H (s H (cid:54)= g|s 0) . The previous sequence of inequalities entails that p¯π g(cid:101) ,H (s H (cid:54)= g) ≤ (2ε)/(9L). Now we can upper bound the value of the resetting extension of π (cid:101). Indeed, for q (cid:101) (cid:44) p¯π g(cid:101) ,H (s H+1 =(cid:54) g|s 1 = s 0) ≤ p¯π g(cid:101) ,H (s H (cid:54)= g|s 1 = s 0) we have using that π is ε -optimal in M with g ∈ G that (cid:101) 9 g,H L+ε V gπ(cid:101)|H (s 0) = V π g(cid:101) ,H,1(s 0) + 1 −q (cid:101) q (cid:0) 1 + V π g(cid:101) ,H,1(s 0)(cid:1) (cid:101) (cid:63) ε 2ε 1 (cid:16) ε (cid:17) ≤ V (s ) + + 1 + L + ε + g,H,1 0 9 9L 1 − 2/9 9 ≤ V (cid:63)(s ) + ε . g 0 The second key lemma that we prove is that any goal state that meets the constraint (2b) with small enough prediction error (2a) must belong to G . L+ε Restatement of Lemma 12. With probability at least 1 − δ, if a goal state g ∈ G satisfies D (g) ≤ L and k E (g) ≤ ε for an episode k ≥ 1, then g ∈ G . k L+ε Proof. Consider that the event U defined in (11) holds. Consider a goal state g such that D (g) ≤ L and E (g) ≤ ε k k at an episode k ≥ 1. Then V (cid:63) (s ) ( ≤i) V(cid:101) k (s ) + πk+1U k (s ) g,H,1 0 g,H,1 0 g,1 g,1 0 8ε (ii) = D (g) + E (g) − k k 9 (iii) ε ≤ L + , (19) 9 where (i) comes from Lemma 19, (ii) stems from the choice of D and E estimates and (iii) comes from the conditions on g. Following the steps of the proof of Lemma 25, we have that (cid:98)H/M(cid:99) p¯π(cid:63) g,H (s (cid:54)= g|s = s) ≤ (cid:89) max p¯π(cid:63) g,H (s (cid:54)= g|s = s(cid:48)), g,H H 1 g,H H−jM H−(j+1)M+1 s(cid:48) j=0 and that p¯π(cid:63) g,H (s (cid:54)= g|s = s(cid:48)) ≤ V (cid:63) g,(j+1)M,1(s(cid:48)) ≤ V (cid:63) g,H,1(s(cid:48)) ≤ 1 + V (cid:63) g,H,1(s 0) ≤ 1 + L + ε/9 , g,H H−jM H−(j+1)M+1 M M M M where the before last inequality uses the existence of a resetting action (Assumption 3) and the last inequality uses (19). Choosing M = 2(L + 2) gives (cid:0) (cid:1) p¯π(cid:63) g,H (s (cid:54)= g|s = s) ≤ e− log(2)H/ 4(L+2) . (20) g,H H 1 We now follow the steps of the proof of Lemma 27. Thanks to (20) and the choice of H it holds q(cid:63) (cid:44) p¯π(cid:63) g,H (s (cid:54)= g|s = s ) ≤ p¯π(cid:63) g,H (s (cid:54)= g|s = s ) ≤ ε ≤ 1 . (21) g,H g,H H+1 1 0 g,H H 1 0 10(L + 2) 30 Thanks to (21) and the definition of a resetting policy, we obtain that V (cid:63)(s ) ≤ V π(cid:63) g,H|H (s ) = V (cid:63) (s ) + q(cid:63) (1 + V π(cid:63) g,H|H (s )) g 0 g 0 g,H,1 0 g,H g 0
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric q(cid:63) (cid:63) g,H (cid:63) = V (s ) + (1 + V (s )) g,H,1 0 1 − q(cid:63) g,H,1 0 g,H (i) ε 30 ε (cid:0) ε (cid:1) ≤ L + + 1 + L + 9 29 10(L + 2) 9 2ε ≤ L + , 9 where (i) uses (19). Therefore we have that g ∈ G , which concludes the proof. L+ε We now have all the tools to prove that when AdaGoal-UCBVI terminates, it fulfills the MGE objective of Definition 4. Lemma 28. If the algorithm AdaGoal-UCBVI stops, it is (ε, δ, L)-PAC for MGE. Proof. Consider that the event U defined in (11) holds, and that the algorithm AdaGoal-UCBVI has stopped at episode κ. Recall that we define D (g) (cid:44) V(cid:101) κ (s ) and E (g) (cid:44) πκ+1U κ (s ) + 8ε . Denoting X (cid:44) {g ∈ S : κ g,1 0 κ g,1 g,1 0 9 κ D (g) ≤ L}, the stopping rule (Equation 1) implies that max E (g) ≤ ε. We now prove that G ⊆ X ⊆ G . κ g∈Xκ κ L κ L+ε On the one hand, it holds that D (g) = V(cid:101) κ (s ) ≤ V (cid:63) (s ) ≤ V (cid:63)(s ), κ g,1 0 g,1 0 g 0 which ensures that G ⊆ X . On the other hand, consider that g ∈ X , then D (g) ≤ L and E (g) ≤ ε, which L κ κ κ κ implies that g ∈ G from Lemma 12, therefore X ⊆ G . L+ε κ L+ε We now prove that the candidate policies of AdaGoal-UCBVI are near-optimal goal-reaching policies. Consider any g ∈ X . Combining the result of Lemma 19 and Equation 1, we obtain that κ V π gκ ,+ 11 (s ) − V (cid:63) (s ) ≤ πκ+1U κ (s ) ≤ E (g) − 8ε ≤ ε , g,1 0 g,1 0 g,1 g,1 0 κ 9 9 thus the policy πκ+1 is ε -optimal in M . As a result, denoting by π (cid:44) (πκ+1)|H the candidate policy of g 9 g,H (cid:98)g g AdaGoal-UCBVI, we have from Lemma 27 that V gπ(cid:98)g (s 0) ≤ V g(cid:63)(s 0) + ε , i.e., π is ε-optimal for the original SSP objective. Putting everything together, we have that (cid:98)g (cid:16) (cid:110) (cid:111)(cid:17) P {G L ⊆ X κ ⊆ G L+ε} ∩ ∀g ∈ X κ, V π(cid:98)g (s 0 → g) − V (cid:63)(s 0 → g) ≤ ε ≥ P(U) ≥ 1 − δ. which ensures that AdaGoal-UCBVI is (ε, δ, L)-PAC for MGE. C.4 Putting everything together Restatement of Theorem 8. AdaGoal-UCBVI is (ε, δ, L, S)-PAC for MGE and, with probability at least 1 − δ, for ε ∈ (0, 1/S] its MGE sample complexity is of order8 O(cid:101)(L3SAε−2). Proof. The result comes from combining Lemmas 23 and 28. 8The notation O(cid:101) in Theorem 8 hides poly-log terms in ε−1, S, A, L, δ−1. See Lemma 23 in Appendix C.3 for a more detailed bound that includes the poly-log terms.
Adaptive Multi-Goal Exploration D DETAILS OF AdaGoal-UCRL·VTR AND ANALYSIS In this section, we provide details on the AdaGoal-UCRL·VTR algorithm and the guarantee of Theorem 11 which bounds its MGE sample complexity in linear mixture MDPs. Recall that since the state space S may be large, we consider that the known goal space is in all generality a subset of it, i.e., G ⊆ S, where G (cid:44) |G| denotes the cardinality of the goal space. First of all, we extend the linear mixture definition (Definition 7) to handle our multi-goal setting. For any goal g ∈ G, we define (cid:18) θ(cid:63)(cid:19) (cid:18)1[s (cid:54)= g]φ(s(cid:48)|s, a)(cid:19) p (s(cid:48)|s, a) (cid:44) (cid:104)φ (s(cid:48)|s, a), θ(cid:63)(cid:105), with θ(cid:63) (cid:44) ∈ Rd+1, φ (s(cid:48)|s, a) (cid:44) ∈ Rd+1. g g g g 1 g 1[s = g]1[s(cid:48) = g] We see that by construction, (cid:26) p(s(cid:48)|s, a) if s (cid:54)= g p (s(cid:48)|s, a) = g 1[s(cid:48) = g] if s = g. D.1 Overview of AdaGoal-UCRL·VTR and Choice of E, D, Q in line 14 of Algorithm 1 Here we focus on the specificities of AdaGoal-UCRL·VTR in the linear mixture MDP setting (refer to Section 3 for the description of the algorithmic structure that is common to AdaGoal-UCBVI), i.e., we explain how to define the estimates D, E, Q in line 14 of Algorithm 1. At a high level, AdaGoal-UCRL·VTR uses two regression-based goal-conditioned estimators of the unknown parameter vector θ(cid:63) of each goal g ∈ G: g • Value-targeted estimator. The first estimator minimizes a ridge regression problem with the target being the past value functions. This is similar to the UCRL-VTR algorithm for linear mixture MDPs (Ayoub et al., 2020) and follow-up work (e.g., Zhou et al., 2021; Zhang et al., 2021a). This step is used to compute the distance estimates D (and Q) for AdaGoal. • Error-targeted estimator. The second estimator is novel and minimizes a ridge regression problem with the target being past “error functions”, that are computable upper bounds on the goal-conditioned gaps. This step is used to compute the prediction errors E for AdaGoal. (cid:46) Value-targeted estimator. First, AdaGoal-UCRL·VTR builds a goal-conditioned estimator θ for the unknown g parameter vector θ(cid:63) of each goal g ∈ G, as well as a goal-conditioned covariance matrix Σ of the feature mappings, g g which characterizes the uncertainty of the estimator θ . Similar to UCRL-VTR, θ is computed as the minimizer g g to a ridge regression problem with the target being the past value functions, i.e., k H θ g,k+1 ← a θr ∈g Rm d+i 1n λ(cid:107)θ(cid:107)2 + k(cid:88) (cid:48)=1 h(cid:88) =1(cid:16)(cid:10) θ, φ V g,k(cid:48),h(sk h(cid:48) , ak h(cid:48) )(cid:11) − V g,k(cid:48),h(sk h(cid:48) +1)(cid:17) , which has a closed-form solution given in (23). Leveraging θ and subtracting an exploration bonus term, g AdaGoal-UCRL·VTR builds optimistic goal-conditioned estimators Q (·, ·) (25) and V (·) (27) for the g,k,h g,k,h optimal action-value and value functions Q(cid:63) (·, ·) and V (cid:63) (·). The associated goal-conditioned policy is the g,h g,h greedy policy of the calculated optimistic Q-values (26). (cid:46) Error-targeted estimator. The main addition compared to existing works on linear mixture MDPs is that AdaGoal-UCRL·VTR also builds goal-conditioned errors denoted by U (28) that upper bound the goal- g,k,h conditioned gaps (defined as the difference between the value function of the current greedy policy and the optimistic value estimates). They rely on an additional estimator ˚θ and covariance matrix Σ˚ based on the g,k g,k errors {U g,k(cid:48),h} k(cid:48)≤k−1,h, instead of the values {V g,k(cid:48),h} k(cid:48)≤k−1,h as considered before. Specifically, ˚θ g minimizes the ridge regression problem with contexts φ U g,k(cid:48),h(sk h(cid:48) , ak h(cid:48) ) and targets U g,k(cid:48),h(sk h(cid:48) +1), i.e., k H ˚θ g,k+1 ← a θr ∈g Rm d+i 1n λ(cid:107)θ(cid:107)2 + k(cid:88) (cid:48)=1 h(cid:88) =1(cid:16)(cid:10) θ, φ U g,k(cid:48),h(sk h(cid:48) , a hk(cid:48) )(cid:11) − U g,k(cid:48),h(sk h(cid:48) +1)(cid:17) ,
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric which has a closed-form solution given in (24). (cid:46) Algorithmic notation and updates. Let B be an upper bound of the (cid:96) -norm of θ(cid:63) (see Definition 7) and set as regularization parameter λ (cid:44) 1/(B +1)2. 2 Also define the confidence radius (cid:112) β (cid:44) H d log(3(1 + kH3(B + 1)2)/δ) + 1. (22) k At the first episode indexed by k = 1, we initialize for every goal g ∈ G and h ∈ [H] the following quantities Σ , Σ˚ (cid:44) λI, b ,˚b (cid:44) 0, θ ,˚θ (cid:44) 0, V (·) (cid:44) 0, U (·) (cid:44) 0. g,1,h g,1,h g,1,h g,1,h g,1 g,1,h g,1,H+1 g,1,H+1 We now explain how the various estimates are updated during an episode k with goal state denoted by g . Over k the trajectory of episode k, given the current state visited at step h denoted by sk, the executed action is denoted h by ak (cid:44) πk (sk) and the next state is denoted by sk . Then for every goal g ∈ G and for h = 1, . . . , H, we set h gk,h h h+1 Σ (cid:44) Σ + φ (sk, ak)φ (sk, ak)(cid:62), g,k,h+1 g,k,h Vg,k,h h h Vg,k,h h h b (cid:44) b + φ (sk, ak)V (sk ), g,k,h+1 g,k,h Vg,k,h h h g,k,h h+1 Σ˚ (cid:44) Σ˚ + φ (sk, ak)φ (sk, ak)(cid:62), g,k,h+1 g,k,h Ug,k,h h h Ug,k,h h h ˚b (cid:44) ˚b + φ (sk, ak)U (sk ), g,k,h+1 g,k,h Ug,k,h h h g,k,h h+1 and for every goal g ∈ G, we set Σ (cid:44) Σ , b (cid:44) b , θ (cid:44) Σ−1 b , (23) g,k+1,1 g,k,H+1 g,k+1,1 g,k,H+1 g,k+1 g,k+1,1 g,k+1,1 Σ˚ (cid:44) Σ˚ , ˚b (cid:44) ˚b , ˚θ (cid:44) Σ˚−1 ˚b . (24) g,k+1,1 g,k,H+1 g,k+1,1 g,k,H+1 g,k+1 g,k+1,1 g,k+1,1 We proceed by recursively defining for every episode k, goal g ∈ G and h = H, . . . , 1, Q (·, ·) (cid:44) clip (cid:16) 1[· =(cid:54) g] + (cid:10) θ , φ (·, ·)(cid:11) − β (cid:13) (cid:13)Σ−1/2φ (·, ·)(cid:13) (cid:13) , 0, H(cid:17) , (25) g,k,h g,k Vg,k,h+1 k(cid:13) g,k,1 Vg,k,h+1 (cid:13) 2 πk (·) (cid:44) arg min Q (·, a), (26) g,h g,k,h a∈A V (·) (cid:44) min Q (·, a), (27) g,k,h g,k,h a∈A U (·) (cid:44) clip (cid:16) 2β (cid:13) (cid:13)Σ−1/2φ (s, πk (s))(cid:13) (cid:13) + (cid:10) φ (s, πk (s)),˚θ (cid:11) g,k,h k(cid:13) g,k,1 Vg,k,h+1 g,h (cid:13) 2 Ug,k,h+1 g,h g,k (cid:13) (cid:13) (cid:17) + β (cid:13)Σ˚−1/2φ (s, πk (s))(cid:13) , 0, H . (28) k(cid:13) g,k,1 Ug,k,h+1 g,h (cid:13) 2 (cid:46) Choice of estimates D, E, Q of AdaGoal-UCRL·VTR (line 14 of Algorithm 1): Q (s, a, g) (cid:44) Q (s, a), (29) k,h g,k,h D (g) (cid:44) V (s ), (30) k g,k,1 0 8ε E (g) (cid:44) U (s ) + . (31) k g,k,1 0 9 D.2 Proof sketch of Theorem 11 The analysis of AdaGoal-UCRL·VTR follows the same key steps considered in Section 4 for the analysis of AdaGoal-UCBVI. We now sketch the AdaGoal-UCRL·VTR equivalent of the various key steps. First, note that similar to Lemma 16 in the tabular case, for any state-action pair (s, a) ∈ S × A, goal state g ∈ G and vector Y ∈ RS such that Y (g) = 0, it holds that [pY ](s, a) = [p Y ](s, a). g We now build the high-probability events. By using the standard self-normalized concentration inequality for vector-valued martingales of Abbasi-Yadkori et al. (2011, Theorem 2), it holds that with probability at least 1 − δ/3, for any k ≥ 1 and g ∈ G, θ(cid:63) lies in the ellipsoid g (cid:110) (cid:13) (cid:13) (cid:111) C (cid:44) θ ∈ Rd+1 : (cid:13)Σ1/2 (θ − θ)(cid:13) ≤ β . g,k (cid:13) g,k,1 g,k (cid:13) k 2
Adaptive Multi-Goal Exploration The proof of the above statement follows the steps of e.g., Zhang et al. (2021a, Lemma A.2), the only slight difference being that we take an additional union bound over all goals g ∈ G, hence the presence of G in the confidence radius (22). Furthermore, following the exact same steps as above and by definition of Σ˚ and ˚θ, it holds that with probability at least 1 − δ/3, for any k ≥ 1 and g ∈ G, θ(cid:63) lies in the ellipsoid g (cid:110) (cid:13) (cid:13) (cid:111) C(cid:48) (cid:44) θ ∈ Rd+1 : (cid:13)Σ˚1/2 (˚θ − θ)(cid:13) ≤ β . g,k (cid:13) g,k,1 g,k (cid:13) k 2 Here the confidence radius is the same as the one in C since it is chosen to be proportional to the magnitude of g,k the U (·) function, which lies in [0, H], as does the value function V (·). In what follows, we assume g,k,h+1 g,k,h+1 that the two high-probability events considered above hold, i.e., that the following event holds (it does so with probability at least 1 − 2δ/3) (cid:8) ∀k ≥ 1, ∀g ∈ G, θ(cid:63) ∈ C ∩ C(cid:48) (cid:9) . (32) g g,k g,k (cid:46) Key step (cid:172): Optimism and gap bounds. The optimism property is standard: following e.g., Zhang et al. (2021a, Lemma A.1) (see also Zhou et al., 2021, Lemma C.4), it holds that Q (s, a) ≤ Q(cid:63) (s, a) and g,k,h g,h V (s) ≤ V (cid:63) (s) for any (s, a, g) ∈ S × A × G, h ∈ [H], k ≥ 1. g,k,h g,h We now depart from a usual regret minimization analysis and examine our errors U , proving that they upper bound the goal-conditioned gaps, formally defined as W (s) (cid:44) V π gk (s) − V (s). g,k,h g,h g,k,h We now prove by induction that W (s) ≤ U (s). The property holds at H +1 since W (s) = 0 = U (s). g,k,h g,k,h g,k,h g,k,h Assume that W (s) ≤ U (s), then we start by noticing, similar to Zhou et al. (2021, Equation C.10); g,k,h+1 g,k,h+1 Zhang et al. (2021a, Lemma A.1), that (cid:13) (cid:13) W (s) ≤ 2β (cid:13)Σ−1/2φ (s, πk (s))(cid:13) + [pV πk ](s, πk (s)) − [pV ](s, πk (s)) . g,k,h k(cid:13) g,k,1 Vg,k,h+1 g,h (cid:13) 2 (cid:124) g,h+1 g,h (cid:123)(cid:122) g,k,h+1 g,h (cid:125) (cid:44)X We bound X as follows X = [pW ](s, πk (s)) g,k,h+1 g,h (i) ≤ [pU ](s, πk (s)) g,k,h+1 g,h = (cid:10) φ (s, πk (s)), θ(cid:63)(cid:11) Ug,k,h+1 g,h g = (cid:10) φ (s, πk (s)),˚θ (cid:11) + (cid:10) φ (s, πk (s)), θ(cid:63) − ˚θ (cid:11) Ug,k,h+1 g,h g,k Ug,k,h+1 g,h g g,k ( ≤ii) (cid:10) φ (s, πk (s)),˚θ (cid:11) + (cid:13) (cid:13)Σ˚1/2 (˚θ − θ(cid:63))(cid:13) (cid:13) (cid:13) (cid:13)Σ˚−1/2φ (s, πk (s))(cid:13) (cid:13) Ug,k,h+1 g,h g,k (cid:13) g,k,1 g,k g (cid:13) 2(cid:13) g,k,1 Ug,k,h+1 g,h (cid:13) 2 ( ≤iii) (cid:10) φ (s, πk (s)),˚θ (cid:11) + β (cid:13) (cid:13)Σ˚−1/2φ (s, πk (s))(cid:13) (cid:13) , Ug,k,h+1 g,h g,k k(cid:13) g,k,1 Ug,k,h+1 g,h (cid:13) 2 where (i) comes from the induction hypothesis and because p is a monotone operator w.r.t. the partial ordering of functions, (ii) is by Cauchy-Schwarz, (iii) holds by event (32). Finally, using that W (s) ∈ [0, H] and by g,k,h definition of U (s), we conclude that W (s) ≤ U (s). g,k,h g,k,h g,k,h (cid:46) Key step (cid:173): Bounding the cumulative gap bounds. We now bound (cid:80)K U (s ). It holds that k=1 gk,k,1 0 U (s ) − U (s ) g,k,h k,h g,k,h+1 k,h+1 (cid:110) (cid:13) (cid:13) (cid:111) (cid:110) (cid:13) (cid:13) (cid:111) ≤ 2β min 1, (cid:13)Σ−1/2φ (s , πk (s ))(cid:13) + β min 1, (cid:13)Σ˚−1/2φ (s , πk (s ))(cid:13) k (cid:13) g,k,1 Vg,k,h+1 k,h g,h k,h (cid:13) 2 k (cid:13) g,k,1 Ug,k,h+1 k,h g,h k,h (cid:13) 2 + min (cid:110) (cid:10) φ (s , πk (s )),˚θ (cid:11) − U (s ), H(cid:111) , Ug,k,h+1 k,h g,h k,h g,k gk,k,h+1 k,h+1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:44)Y
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric where Y ≤ (cid:12) (cid:12)(cid:10) φ Ug,k,h+1(s k,h, π gk ,h(s k,h)), θ g(cid:63) − ˚θ g,k(cid:11)(cid:12) (cid:12) + (cid:10) φ Ug,k,h+1(s k,h, π gk ,h(s k,h)), θ g(cid:63)(cid:11) − U g,k,h+1(s k,h+1) (cid:13) (cid:13) (cid:13) (cid:13) ≤ (cid:13)Σ˚1/2 (˚θ − θ(cid:63))(cid:13) (cid:13)Σ˚−1/2φ (s , πk (s ))(cid:13) + [p U ](s , πk (s )) − U (s ) (cid:13) g,k,1 g,k g (cid:13) 2(cid:13) g,k,1 Ug,k,h+1 k,h g,h k,h (cid:13) 2 g g,k,h+1 k,h g,h k,h g,k,h+1 k,h+1 (cid:13) (cid:13) ≤ β (cid:13)Σ˚−1/2φ (s , πk (s ))(cid:13) + [p U ](s , πk (s )) − U (s ). k(cid:13) g,k,1 Ug,k,h+1 k,h g,h k,h (cid:13) 2 g g,k,h+1 k,h g,h k,h g,k,h+1 k,h+1 Therefore we get by telescopic sum K K H (cid:88) (cid:88) (cid:88) (cid:0) (cid:1) U (s ) = U (s ) − U (s ) gk,k,1 0 gk,k,h k,h gk,k,h+1 k,h+1 k=1 k=1 h=1 ≤ 2β (cid:88)K (cid:88)H min (cid:110) 1, (cid:13) (cid:13)Σ−1/2 φ (s , a )(cid:13) (cid:13) (cid:111) K (cid:13) gk,k,1 Vgk,k,h+1 k,h k,h (cid:13) 2 k=1 h=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:44)Z1 + 2β (cid:88)K (cid:88)H min (cid:110) 1, (cid:13) (cid:13)Σ˚−1/2 φ (s , a )(cid:13) (cid:13) (cid:111) K (cid:13) gk,k,1 Ugk,k,h+1 k,h k,h (cid:13) 2 k=1 h=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:44)Z2 K H (cid:88) (cid:88) + [p U ](s , a ) − U (s ) . gk gk,k,h+1 k,h k,h gk,k,h+1 k,h+1 k=1 h=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:44)Z3 We bound Z and Z using Cauchy-Schwarz and the elliptical potential lemma from linear bandits (Abbasi-Yadkori 1 2 et al., 2011, Lemma 11), see e.g., Zhang et al. (2021a, Proof of Lemma A.3). This yields (cid:118) Z ≤ √ KH(cid:117) (cid:117) (cid:116)(cid:88)K (cid:88)H min (cid:110) 1, (cid:13) (cid:13)Σ−1/2 φ (s , a )(cid:13) (cid:13)2(cid:111) 1 (cid:13) gk,k,1 Vgk,k,h+1 k,h k,h (cid:13) 2 k=1 h=1 (cid:118) ≤ √ 2√ KH(cid:117) (cid:117) (cid:116)(cid:88)K (cid:88)H min (cid:110) 1, (cid:13) (cid:13)Σ−1/2 φ (s , a )(cid:13) (cid:13)2(cid:111) + 2Hd log(1 + kH3/λ) (cid:13) gk,k,h Vgk,k,h+1 k,h k,h (cid:13) 2 k=1 h=1 (cid:112) ≤ 2KHd log(1 + KH3/(dλ) + 2Hd log(1 + kH3/λ), and likewise for Z . The term Z can be bounded by the Azuma-Hoeffding inequality since its summands form a 2 3 martingale difference sequence, thus with probability at least 1 − δ/3, it holds that Z ≤ H(cid:112) 2HK log(3/δ). 3 √ Putting everything together and using that β K = O(cid:101)(H d), we obtain (cid:88)K (cid:16) √ (cid:17) U gk,k,1(s 0) = O(cid:101) dH3/2 K + H2d3/2 . k=1 (cid:46) Key step (cid:174): Bounding the sample complexity. We follow the reasoning given in Section 4. By construction of the stopping rule (1), the algorithm terminates at an episode κ that verifies κ (cid:88)−1 8ε (cid:16) √ (cid:17) ε · (κ − 1) ≤ E k(g k) = · (κ − 1) + O(cid:101) dH3/2 κ + H2d3/2 . 9 k=1 Solving this functional inequality in κ yields (cid:18) H3d2 H2d3/2 (cid:19) κ = O(cid:101) + . ε2 ε Using that the sample complexity is bounded by κ(H + 1) and that H = O(cid:101)(L), we conclude the proof. (cid:46) Key step (cid:175): Connecting to the original MGE objective. The proof of this step is identical to the one of AdaGoal-UCBVI in Appendix C.3.4.
Adaptive Multi-Goal Exploration Figure 5: Goal sampling frequency of UniGoal-UCBVI (top row), RareGoal-UCBVI (middle row) and AdaGoal- UCBVI (bottom row) over 1000 episodes, split over episodes 0 − 333 (left column), episodes 334 − 666 (middle column) and episodes 667 − 999 (right column). Episodes are of length H = 50, the environment is a grid-world with S = 52 states, starting state s = (0, 0) (i.e., the top left state), A = 5 actions (the 4 cardinal actions plus a ). The black walls act as 0 reset reflectors, i.e., if the action leads against the wall, the agent stays in the current position with probability 1. An action fails with probability p = 0.1, in which case the agent follows (uniformly) one of the other directions. The 4 states of the f bottom right room can only be accessed from s by any cardinal action with probability η = 0.001, thus they are extremely 0 hard to reliably reach as their associated V (cid:63)(s → ·) is very large (scaling with η−1). We select L = 40 for AdaGoal, and 0 α = 0.1 for RareGoal. For the three methods we follow the practice of Ménard et al. (2021b, Section 4) and use their proposed simplified form for the exploration bonuses. The experiment is based on the rlberry framework (Domingues et al., 2021a). E ABLATION OF THE GOAL SELECTION SCHEME & PROOF OF CONCEPT EXPERIMENT In this section, we single out the role of the adaptive goal selection scheme of AdaGoal, i.e., step ○i in Algorithm 1. For simplicity we focus on the tabular case and consider that G = S. Keeping the remainder of the AdaGoal-UCBVI algorithm fixed, we compare it to two other ad-hoc goal sampling alternatives: • UniGoal: the goal state is sampled uniformly in S \ {s }, i.e., with probability 0 puni (g) (cid:44) (cid:0) S − 1(cid:1)−1 ; • RareGoal: the goal state is sampled proportionally to its rarity, i.e., with probability (nk (g))−1 pr αare (g) (cid:44) (cid:80) α (nk (s))−1 , s∈S\{s0} α where nk(s) (cid:44) (cid:80)kH 1[s = s] denotes the number of times state s was visited in the first k episodes, and t=1 t nk (s) (cid:44) max{nk(s), α} for α ∈ (0, 1]. α
Tarbouriech, Domingues, Ménard, Pirotta, Valko, Lazaric We can find equivalents of these two goal selection schemes in existing goal-conditioned deep RL methods. The case of a uniform goal sampling distribution prescribed by the environment (i.e., UniGoal) is the most common, see e.g., Schaul et al. (2015); Andrychowicz et al. (2017). Meanwhile, the goal sampling scheme of Skew-Fit (Pong et al., 2020), a recent state-of-the-art algorithm for deep GC-RL, gives higher sampling weight to rarer goal states, where rarity is measured by a learned generative model. In the tabular case, a goal state’s rarity can be characterized by the inverse of its visitation count, which corresponds to RareGoal. On the one hand, it is straightforward to show that UniGoal achieves a sample complexity of at most O(cid:101)(L3S2Aε−2). Intuitively, it pays for an extra S since it may sample goals that are too easy or too hard, in either case they are not very useful for the agent to improve its learning (and there may be in the worst case S − 2 of such non-informative goal states). On the other hand, we can see that by design RareGoal relies on the communicating assumption and may require poly(S, A, D, ε−1) samples to learn an ε-optimal goal-conditioned policy on G . Here the dependence L on the diameter D is somewhat problematic. Indeed, imagine there exist a set of states S such that hard 1 (cid:28) V (cid:63)(s → s) ≤ D for s ∈ S (i.e., very hard to reach states, e.g., by chance due to environment 0 hard stochasticity). Then throughout the learning process, RareGoal will strive to reach the states in S and select hard them as goals, which leads to unsuccessful episodes and a possible waste of samples. Consequently, when goals have varying reachability (e.g., if the environment is highly stochastic), RareGoal suffers from an issue of goal prioritizing, i.e., too-hard-to-reach states are given too much goal sampling importance. Finally, we empirically complement our discussion above on the sequence of goals selected by UniGoal, RareGoal and AdaGoal. We design a simple two-room grid-world with a very small probability of reaching the second room, and illustrate in Figure 5 the goal sampling frequency of UniGoal-UCBVI, RareGoal-UCBVI and AdaGoal-UCBVI. We see that over the course of the learning interaction, as opposed to the designs of RareGoal and UniGoal, our AdaGoal strategy is able to successfully discard the states from the bottom right room, which have a negligible probability of being reached. In addition, AdaGoal is able to target as goals the states in the first room that are furthest away from s , i.e., those at the center of the “spiral”, which effectively correspond to 0 the fringe of what the agent can reliably reach. F IMPLEMENTATION DETAILS OF SECTION 6 Here we provide the implementation details of our experiments reported in Section 6. Our implementation and hyperparameters of HER (Andrychowicz et al., 2017) are based on the PyTorch open-source codebase of https://github.com/TianhongDai/hindsight-experience-replay, which follows the official implementation of HER. As explained in Section 6, we approximate AdaGoal by computing the disagreement (i.e., standard deviation) of an ensemble of J goal-conditioned Q-functions and selecting a goal proportionally to it among N uniformly sampled goals. Then the policy is conditioned on this goal and executed for an episode of length H = 50. This recovers the VDS algorithm of Zhang et al. (2020b). We thus follow the implementation details given in the latter paper. In particular, the value ensemble (for goal selection) is treated as a separate module from the policy optimization (for goal-conditioned policy execution). In each training epoch, each Q-function in the ensemble performs Bellman updates with independently sampled mini-batches, and the policy is updated with DDPG (Lillicrap et al., 2015). Each Q-function in the ensemble is trained with its target network, with learning rate 1e-3, polyak coefficient 0.95, buffer size 1e6, and batch size 1000. Finally, we set J = 3 and N = 1000.
