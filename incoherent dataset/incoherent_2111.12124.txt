2202 nuJ 32 ]DS.sc[ 3v42121.1112:viXra TOWARDS LEARNING UNIVERSAL AUDIO REPRESENTATIONS Luyu Wang, Pauline Luc, Yan Wu, Adria` Recasens, Lucas Smaira, Andrew Brock, Andrew Jaegle, Jean-Baptiste Alayrac, Sander Dieleman, Joao Carreira, Aa¨ ron van den Oord DeepMind, London, UK ABSTRACT celerated progress in sound understanding thanks to its scale and difficulty; but similarly, it provides an incomplete pic- The ability to learn universal audio representations that can ture on the usefulness of representations. Likewise, Mag- solve diverse speech, music, and environment tasks can spur naTagATune [5] and NSynth [6] are limited to the music do- many applications that require general sound content under- main. These benchmarks have facilitated model development standing. In this work, we introduce a holistic audio repre- in each individual domain [7, 8, 9, 10, 11, 12, 13]. A natural sentation evaluation suite (HARES) spanning 12 downstream question is whether optimizing for performance on these dif- tasks across audio domains and provide a thorough empirical ferent domains jointly can provide benefits in each domain. study of recent sound representation learning systems on that The task suites used in [14, 15, 16] constitute early steps in benchmark. We discover that previous sound event classifica- this direction. We build on their proposed benchmark, re- tion or speech models do not generalize outside of their do- moving tasks where we noted performance saturation, and in- mains. We observe that more robust audio representations can cluding additional challenging tasks, such as AudioSet [4], be learned with the SimCLR objective; however, the model’s MagnaTagATune [5], and Fluent Speech Commands [17]. transferability depends heavily on the model architecture. We Equipped with this benchmark, we perform thorough em- find the Slowfast architecture is good at learning rich repre- pirical studies on recent frameworks and architectures. We sentations required by different domains, but its performance first perform an extensive empirical comparison of training is affected by the normalization scheme. Based on these find- objectives and models, especially focusing on the two dom- ings, we propose a novel normalizer-free Slowfast NFNet and inant paradigms of supervised classification [8, 12] and self- achieve state-of-the-art performance across all domains. supervised contrastive learning [15, 13], and comparing them Index Terms— audio representations, representation across a large set of model architectures. We find that mod- evaluation, speech, music, acoustic scenes els trained with contrastive learning tend to generalize better in the speech and music domain, while performing compa- 1. INTRODUCTION rably to supervised pretraining for environment sounds. We also measure the performance of heavily-tuned speech rep- Audio is a ubiquitous form of data that can convey rich infor- resentation systems [18, 9, 10], but we find that their strong mation, whether in the form of speech, music, or surround- performance does not extend to other domains. Motivated by ing environment sounds. An ability to understand all forms the complementary strengths of Slowfast ResNet50 [19] and of sound in a holistic manner would empower a wide range NFNet-F0 [20], we combine their design principles and eval- of diverse applications, from end-to-end audio content under- uate a novel architecture, Slowfast NFNet-F0, which obtains standing to human-computer interactions. Importantly, when improved performance across the board. We additionally pro- used on a new task, such an ability could reduce the amount vide a thorough study on how normalization schemes affect of labeled data required, thanks to similarities with other do- the representations. Putting these all together, we obtain bet- mains and synergies with tasks where data is abundant. In this ter scores across the speech, music, and environment domain, paper, we take a step in this direction by laying the ground- as measured by our meta benchmark. work for learning universal audio representations. To sum up, we make the following contributions: (i) We first turn our attention to evaluation benchmarks, and We introduce HARES, an evaluation benchmark designed to seek a compact and representative way to measure perfor- drive progress for universal audio representations; (ii) We mance of universal audio representations. In the text domain, perform empirical studies on models and learning frame- GLUE [1] has been effective in driving the progress of unified works. (iii) Based on our findings, we propose a novel ar- approaches, and we follow a similar approach here. Meta- chitecture, Slowfast NFNet-F0, which significantly improves benchmarks, such as NOSS [2] and SUPERB [3] have been performance across the board. (iv) Our final model obtains introduced with similar motivation, but are restricted to the state-of-the-art results, in particular improving by a large speech domain. The AudioSet benchmark [4] has greatly ac- margin in the music domain.
Table 1: HARES tasks. HARES unifies existing datasets mean average precision (mAP) on AudioSet and MagnaTa- with the goal of covering the three major audio domains to gATune due to their multi-class nature, and F1-score on all facilitate the development of models for general auditory con- other datasets. Note that although there is an overlap in the tent understanding. tasks, our results on speech tasks are not directly compara- ble to the SUPERB benchmark [3], where the authors use Dataset Task #Samples #Classes Used in weighted-sum of hidden representations from different layers Environment AudioSet Audio tagging 1.9m 527 [7, 8, 12] and an additional linear projection layer for the downstream Birdsong Animal sound 36k 2 [14, 15] evaluation. Our choice follows [14, 15] and keeps homogene- TUT18 Acoustic Scenes 8.6k 10 [14, 15] ity with the rest. ESC-50 Acoustic Scenes 2.0k 50 [8, 13] Speech Speech Comm. v1 Keyword 90k 12 [2, 15, 3, 16] Speech Comm. v2 Keyword 96k 35 [14, 15, 16] 2.2. Training Frameworks and Architectures Fluent Speech Comm. Intention 27k 31 [3] VoxForge Language id 145k 6 [2, 15, 16] We perform an extensive empirical comparison of training VoxCeleb Speaker id 147k 1251 [2, 15, 3, 16] objectives and models. We use AudioSet [4] as the training Music dataset, due to its combined advantages of size and availabil- NSynth Instrument Instrument id 293k 11 [14, 15, 16] NSynth Pitch Pitch estimation 293k 128 - ity of labels for the supervised setting. MagnaTagATune Music tagging 26k 50 [21] First, we focus our study on two dominant paradigms for pretraining: supervised classification [8, 12] and self- supervised contrastive learning (SimCLR) [15, 13, 23, 21]. 2. METHOD We compare the two training objectives across a large set of models used recently for audio representation learning, 2.1. Holistic Audio Representation Evaluation Suite listed in Table 2. Note that most models are adopted from the Intuitively, sound can be categorized into three domains: vision domain; they treat spectrograms as images and do not speech, music, and environment (sound of things, animal, na- distinguish time and frequency dimensions. We use example ture, etc). Previous evaluation benchmarks either contain sat- mixing to augment spectrograms, whose effectiveness has urated tasks and lack coverage in the music and environment been demonstrated in both settings [8, 13]. Also, for sound domain [14, 15], or focus entirely on the speech domain [2, 3]. classification, techniques including class-balanced data load- We introduce a new evaluation meta-benchmark, named the ing and model ensembles can boost the performance on the Holistic Audio Representation Evaluation Suite (HARES) AudioSet benchmark [8, 12], and pretraining on large image that contains 12 tasks spanning all three domains.1 We build datasets can also significantly improve the performance of on top of tasks previously used in [14, 15]. We exclude tasks methods like ViT [12]. For the sake of simplicity, we leave where performance is saturated (LibriSpeech-speaker-id and the exploration of such orthogonal directions for future work. MUSAN), and add widely used and challenging tasks, includ- Second, we extend our study to other strongly performing ing AudioSet [4], ESC-50 [22], Fluent Speech Commands approaches that explicitly model the time dimension, namely [17], NSynth-pitch [6], and MagnaTagATune [5]. We sum- BYOL-A [16], CPC [18, 9, 11] and masked prediction using marize all task characteristics (description, relevant statistics Wav2Vec2.0 [10]. We reproduce these methods to evaluate and occurrences in related work) in Table 1. Each of these them on our benchmark. benchmarks has been effective in driving model development, and we propose to jointly optimize performance across do- 2.3. Slowfast NFNets mains to obtain universal representations. HARES shares 4 tasks with the SUPERB speech benchmark [3]. We exclude Due to its strong performance, we build on the Slowfast net- speech and phoneme recognition tasks because the labels are work architecture [19], and seek to further improve its perfor- temporally structured and they require sequence-to-sequence mance in the audio domain. It uses separable convolutions to modeling. The models need to output representations with a model the time and frequency dimension independently, and high temporal resolution and it restricts the types of model two pathways to capture both slow and fast features in the that can be evaluated. spectrogram. Due to space limitations, we refer the reader to We evaluate by training a linear layer on the frozen pre- [19] for details. trained model for all tasks, except for AudioSet where we From our study on various architectures, we notice that follow the common 1-hidden-layer MLP protocol [7, 13, 23]. the NFNet architecture [20], which is normalizer-free, per- Global average pooling is applied to the representations from forms very well. Motivated by this finding, we perform a the last layer before the fine-tune module. We report the thorough study on various normalization schemes for Slow- fast networks. We compare Batch Norm [24], Layer Norm 1More details about HARES tasks and the [25], Instance Norm [26] as well as not using any normal- Slowfast NFNet-F0 model can be found at https://github.com/deepmind/slowfast_nfnets. izer. Finally, we combine the design principles of NFNet with
Table 2: Evaluating frameworks and architectures on HARES. We compare the impact of architecture choice under the classification and SimCLR objective. We also show the performance of several other recent strongly performing frameworks. Average scores are reported for tasks in each domain separately, and all three combined. All models are trained on AudioSet except for bidirectional CPC and Wav2Vec2.0, for which we also show results when they are trained on LibriSpeech (LS). Architecture #Params Input format Used in Env. Speech Music HARES AudioSet (mAP) Classification/SimCLR BYOL-A CNN 5.3m Spectrogram [16] 69.8/69.9 61.4/69.8 57.6/63.1 63.3/68.2 33.8/32.2 EfficientNet-B0 4.0m Spectrogram [15] 70.5/63.8 43.5/40.7 48.0/44.0 53.6/49.2 32.1/26.2 PANNs-CNN14 71m Spectrogram [8, 13] 74.8/66.4 56.0/37.3 56.4/44.8 62.4/48.9 38.9/28.8 ViT-Base 86m Spectrogram [12] 73.5/74.6 50.4/56.5 60.3/64.2 60.6/64.5 37.6/36.8 ResNet50 23m Spectrogram [7] 75.1/74.4 51.7/65.0 59.6/63.7 61.4/67.8 39.4/36.2 SF ResNet50 26m Spectrogram [19] 74.3/74.3 56.9/73.4 59.6/65.2 63.4/71.7 38.2/36.6 NFNet-F0 68m Spectrogram Ours 76.2/76.0 59.0/65.9 61.8/65.5 65.5/69.2 39.6/37.6 SF NFNet-F0 63m Spectrogram Ours 75.4/75.8 65.6/77.2 64.5/68.6 68.6/74.6 38.7/37.8 Bidir-CPC 38m Waveform [11] 69.1 72.4 57.6 67.6 28.7 Bidir-CPC (LS) 38m Waveform [11] 59.9 75.3 51.5 64.2 21.3 Wav2Vec2.0 95m Waveform [10] 48.3 41.2 35.4 42.1 14.8 Wav2Vec2.0 (LS) 95m Waveform [10] 48.5 75.6 35.5 56.6 14.8 BYOL-A 5.3m Spectrogram [16] 65.4 66.9 60.7 64.9 26.1 those of the Slowfast network, to propose a novel architecture, Table 3: Impact of normalizer on Slowfast networks denoted Slowfast NFNet-F0. It is free from the side effects of trained with the SimCLR objective. We consider Batch normalizers [27] and is optimized for training speed on recent Norm (BN), Layer Norm (LN), Instance Norm (IN), and hardware accelerators [20]. Normalizer-Free (NF) architectures. Latencies are given as the time required for a full training step on TPU. 3. EMPIRICAL STUDY Architectures Env. Speech Music HARES TPUv3 Train SF ResNet50 (no BN) 74.1 74.1 65.6 71.9 131.1 ms We perform thorough empirical studies using the proposed SF ResNet50 74.3 73.4 65.2 71.7 188.0 ms HARES benchmark. Unless further noted, we train classifi- SF ResNet50-LN 74.0 76.0 67.1 73.1 203.7 ms SF ResNet50-IN 73.7 77.7 66.0 73.5 207.9 ms cation models with batch size 1024 for 100k steps, as these SF NF-ResNet50 74.0 75.7 66.6 72.9 158.0 ms models converge fast and can overfit easily. We train Sim- SF NFNet-F0 75.8 77.2 68.6 74.6 280.9 ms CLR models with batch size 4096 for 200k steps. All models are trained on AudioSet [4] with Adam [28] and an initial learning rate of 2 × 10−4 on a Cloud TPU v3 Pod slice with possibly because they treat the time-frequency spectrograms 32 cores. For classification and SimCLR, we randomly crop as images with two equal spatial dimensions, which may ig- 3-second windows for training. We extract log-mel spectro- nore local features present in the audio. This hypothesis is grams with 80 features using a window size of 25 ms and a supported by the fact that approaches that handle the two di- stride of 10 ms from a waveform sampled at 16kHz. The pro- mensions differently tend to perform better on speech tasks: jector for the SimCLR loss is a MLP with 3 hidden layers of Slowfast networks thank to the use of separable convolutions, size 4096. and BYOL-A CNN thanks to its fully connected layers only operating on the frequency and channel dimension. Next, we replicate and evaluate the bidirectional ver- 3.1. Architectures and Losses sion of CPC/Wav2Vec [11], Wav2Vec2.0 [10], and BYOL-A We report the results of our comparison between supervised [16]. All three models show competitive performance on classification and SimCLR loss in Table 2. In general, we speech tasks, but they do not transfer well on the other two observe that the widely used AudioSet benchmark [4] does domains. CPC and Wav2Vec2.0 directly operate on raw au- not correlate with our HARES benchmark - it aligns with the dio sequences and use the contrastive objective over time overall score on the environment sub-suite. On the speech steps: this procedure preserves the temporal structure of the and music benchmarks SimCLR leads to better generalization data by design. Interestingly, we find Wav2Vec2.0 does not across architectures, and they perform similarly on the envi- perform well when trained with AudioSet. This is possibly ronment benchmark, with the exception of EfficientNet-B0 because the complexity of the data is beyond the capability and CNN14. We think this is because labels presented in su- of the masked prediction objective, which works the best on pervised pretraining bias the network towards extracting more clean speech datasets such as LibriSpeech [29]. BYOL-A overall and slow features than local traits. Architectures from [16] uses a Siamese framework but takes only one temporal the vision domain show strong performance on environment crop at random and augments it with random resize cropping: tasks but get relatively low scores on speech tasks. This is this effectively biases the model towards local semantics.
Table 4: Comparisons to the state-of-the-art on HARES. Both Slowfast networks are trained with the SimCLR loss. Wav2Vec2.0 is trained on LibriSpeech (LS) because it does not perform well when trained on AudioSet. CLMR is trained on MagnaTagATune and Million Songs. All other models are trained on AudioSet. *No class-balanced dataloader. SF NFNet-F0 SF ResNet50 Wav2Vec2.0 Bidir-CPC BYOL-A PANNs-CNN14 COLA BYOL-A PANNs-CNN14 CLMR Tasks (ours) [19] (ours) [10] (ours, LS) [11] (ours) [16] (ours) [8] (ours) [15] [16] [8] [21] Environment AudioSet 37.8 36.6 14.8 28.7 26.1 38.9 - - 37.5∗ - Birdsong 77.6 77.9 60.9 77.6 77.1 78.0 77.0 - - - TUT18 96.8 92.3 49.3 90.9 76.3 88.9 94.0 - - - ESC-50 91.1 90.5 69.2 79.2 82.3 93.6 - - 90.8 - Avg. 75.8 74.3 48.5 69.1 65.4 74.8 - - - - Speech Speech Comm. v1 91.7 88.8 95.1 92.0 90.1 71.4 71.7 91.0 - - Speech Comm. v2 93.0 91.5 94.8 92.4 89.7 76.4 62.4 92.2 - - VoxForge 90.4 89.2 91.2 89.1 83.9 78.5 71.3 90.2 - - VoxCeleb 64.9 59.4 41.6 40.7 39.5 30.9 29.9 40.1 - - Fluent Comm. 46.1 38.3 55.3 47.9 31.3 22.8 - - - - Avg. 77.2 73.4 75.6 72.4 66.9 56.0 - - - - Music NSynth-Pitch 88.0 83.2 34.6 78.1 71.4 58.4 - - - - NSynth-Instrument 78.2 73.5 40.2 58.4 73.6 71.4 63.4 74.1 - - MagnaTagATune 39.5 38.8 31.7 36.3 37.1 39.4 - - - 36.0 Avg. 68.6 65.2 35.5 57.6 60.7 56.4 - - - - HARES 74.6 71.7 56.6 67.6 64.9 62.4 - - - - Notably, we find that with the same CNN architecture, our benchmark as well as in each domain, including COLA [15] SimCLR-based framework obtains much better performance and BYOL-A [16] previously developed for general-purpose than the original BYOL framework proposed in [16]. We note audio understanding. It outperforms the previously proposed that this could be in part due to the more extensive study we Slowfast ResNet50 [19] on almost all tasks. In addition, it perform under the former setting. An in-depth comparison also outperforms strongly performing frameworks developed between SimCLR and BYOL for audio is beyond the scope in different audio domains. It has a slight edge over the su- of this work. pervised PANNs-CNN14 sound classification model on envi- Overall, NFNet-F0 performs remarkably on the environ- ronment tasks. It performs slightly worse on semantic speech ment sub-suite. When comparing the Slowfast networks with tasks than Wav2Vec2.0 (trained on LibriSpeech), but there is their vanilla counterparts, we find that they are comparable on a substantial improvement on the VoxCeleb speaker identifi- environment tasks, but better on the speech and music tasks. cation task. Meanwhile, it shows strong performance on the This shows the effectiveness of the Slowfast architecture. music sub-suite. It achieves an audio-only state-or-the-art on the MagnaTagATune task with an mAP of 39.5, outperform- ing CLMR trained on a music corpus at 36.0 [21]. 3.2. Normalizers Matter For Slowfast Networks We find that the choice of normalizer affects transferability. The original Slowfast ResNet50 [19] for audio makes use of 4. CONCLUSION Batch Norm. In Table 3, we observe that using Layer Norm or Instance Norm, which do not break the independence be- In this paper, we devised a task suite called HARES for tween training examples in the batch, improves the scores in holistic evaluation of audio representations. We study sev- all domains. We adapt the NF ResNet50 [27] and NFNet- eral recent pre-training frameworks, focusing in particular F0 [20] to the Slowfast setting. The latter is better optimized on supervised classification and self-supervised contrastive for performance on recent hardware accelerators. Even in the learning with many different models. We find that models absence of any normalizer, Slowfast NF ResNet50 performs trained with contrastive learning and Slowfast networks tend similarly to when layer or instance normalization is used in- to generalize better across different domains. We also per- stead. Furthermore, our Slowfast NFNet-F0 outperforms all form a thorough study of different normalization schemes, other models. building on the Slowfast network architecture, and obtain significant performance improvements using a new Slow- fast NFNet-F0. Our final model significantly outperforms 3.3. Comparison to the State-of-the-Art previous general-purpose audio representation models. We We compare our best models to previous works in Table 4. hope the HARES benchmark and the findings presented in Our Slowfast NFNet-F0 trained with the SimCLR objective this paper can spur future research towards further improving outperforms all previous frameworks on the overall HARES universal audio representations.
5. REFERENCES [14] M Tagliasacchi, B Gfeller, F de Chaumont Quitry, and D Roblek, “Pre-training audio representations with self- [1] A Wang, A Singh, J Michael, F Hill, O Levy, and supervision,” IEEE Signal Processing Letters, 2020. S Bowman, “Glue: A multi-task benchmark and analysis platform for natural language understanding,” [15] A Saeed, D Grangier, and N Zeghidour, “Con- arXiv:1804.07461, 2018. trastive learning of general-purpose audio representa- tions,” arXiv:2010.10915, 2020. [2] J Shor, A Jansen, R Maor, O Lang, O Tuval, F de Chau- mont Quitry, M Tagliasacchi, I Shavitt, D Emanuel, and [16] D Niizumi, D Takeuchi, Y Ohishi, N Harada, and Y Haviv, “Towards learning a universal non-semantic K Kashino, “BYOL for audio: Self-supervised representation of speech,” in Interspeech, 2020. learning for general-purpose audio representation,” arXiv:2103.06695, 2021. [3] S Yang, P Chi, Y Chuang, C Lai, K Lakhotia, Y Lin, A Liu, J Shi, X Chang, G Lin, et al., “SUPERB: [17] L Lugosch, M Ravanelli, P Ignoto, V Tomar, and Y Ben- Speech processing universal performance benchmark,” gio, “Speech model pre-training for end-to-end spoken arXiv:2105.01051, 2021. language understanding,” arXiv:1904.03670, 2019. [4] J Gemmeke, D Ellis, D Freedman, A Jansen, [18] A van den Oord, Y Li, and O Vinyals, “Repre- W Lawrence, C Moore, M Plakal, and M Ritter, “Audio sentation learning with contrastive predictive coding,” Set: An ontology and human-labeled dataset for audio arXiv:1807.03748, 2018. events,” in ICASSP, 2017. [19] E Kazakos, A Nagrani, A Zisserman, and D Damen, [5] E Law, K West, M Mandel, M Bay, and S Downie, “Slow-fast auditory streams for audio recognition,” in “Evaluation of algorithms using games: The case of mu- ICASSP, 2021. sic tagging.,” in ISMIR, 2009. [20] A Brock, S De, S L Smith, and K Simonyan, “High- [6] J Engel, C Resnick, A Roberts, S Dieleman, M Norouzi, performance large-scale image recognition without nor- D Eck, and K Simonyan, “Neural audio synthesis of mu- malization,” arXiv:2102.06171, 2021. sical notes with WaveNet autoencoders,” ICML, 2017. [21] J Spijkervet and J Burgoyne, “Contrastive learning of [7] A Jansen, M Plakal, R Pandya, D Ellis, S Hershey, J Liu, musical representations,” arXiv:2103.09410, 2021. C Moore, and R Saurous, “Unsupervised learning of semantic audio representations,” in ICASSP, 2018. [22] K Piczak, “ESC: Dataset for environmental sound clas- sification,” in ACM Multimedia, 2015. [8] Q Kong, Y Cao, T Iqbal, Y Wang, W Wang, and M Plumbley, “PANNs: Large-scale pretrained au- [23] L Wang, P Luc, A Recasens, J Alayrac, and A van den dio neural networks for audio pattern recognition,” Oord, “Multimodal self-supervised learning of general arXiv:1912.10211, 2019. audio representations,” arXiv:2104.12807, 2021. [9] S Schneider, A Baevski, R Collobert, and M Auli, [24] S Ioffe and C Szegedy, “Batch normalization: Accel- “wav2vec: Unsupervised pre-training for speech recog- erating deep network training by reducing internal co- nition,” in Interspeech, 2019. variate shift,” in International conference on machine learning, 2015. [10] A Baevski, Y Zhou, A Mohamed, and M Auli, “wav2vec2.0: A framework for self-supervised learning [25] J Ba, J Kiros, and G Hinton, “Layer normalization,” of speech representations,” in NeurIPS, 2020. arXiv:1607.06450, 2016. [11] L Wang, K Kawakami, and A van den Oord, “Con- [26] D Ulyanov, A Vedaldi, and V Lempitsky, “Instance nor- trastive predictive coding of audio with an adversary,” malization: The missing ingredient for fast stylization,” in Interspeech, 2020. arXiv:1607.08022, 2016. [12] Y Gong, Y Chung, and J Glass, “AST: Audio spectro- [27] A Brock, S De, and S Smith, “Characterizing signal gram transformer,” arXiv:2104.01778, 2021. propagation to close the performance gap in unnormal- ized resnets,” arXiv:2101.08692, 2021. [13] L Wang and A van den Oord, “Multi-format contrastive learning of audio representations,” NeurIPS Workshops, [28] D Kingma and J Ba, “Adam: A method for stochastic 2020. optimization,” arXiv:1412.6980, 2014.
[29] V Panayotov, G Chen, D Povey, and S Khudanpur, “Lib- A. HARES TASKS DETAILS rispeech: an asr corpus based on public domain audio books,” in ICASSP, 2015. In this section, we provide more relevant details about HARES tasks in addition to the information shown in Ta- [30] I Loshchilov and F Hutter, “SGDR: Stochastic gradient ble 1. All of the HARES tasks are public available2. We descent with warm restarts,” in ICLR, 2017. evaluate by training a linear layer on the frozen pretrained model for all tasks except for AudioSet, where we follow [31] D Stowell, M Wood, H Pamuła, Y Stylianou, and the common 1-hidden-layer MLP protocol on frozen features H Glotin, “Automatic acoustic detection of birds [7]. Global average pooling is applied before the fine-tune through deep learning: the first bird audio detection module. If the dataset has a validation set, we ignore it and challenge,” Methods in Ecology and Evolution, 2019. only report the score on the test set. Unless further noted, all downstream jobs are trained with a total batch size of 64 for 400k steps on Cloud TPU v2 slice with 8 cores. We use [32] T Heittola, A Mesaros, and T Virtanen, “TUT urban the Adam optimizer [28], first linearly increasing the learning acoustic scenes 2018,” 2018. rate from 0 to 2 × 10−4 over 5k steps, and then decaying it following a cosine schedule [30] down to 0. Except for Au- [33] A Recasens, P Luc, J Alayrac, L Wang, F Strub, dioSet and ESC-50, we do not further apply augmentations C Tallec, M Malinowski, V Patraucean, F Altche´, to downstream evaluations. M Valko, J Grill, A van den Oord, and A Zisserman, AudioSet [4]: Due the attrition of YouTube URLs3, our ver- “Broaden your views for self-supervised video learn- sion of AudioSet has around 1.9 million samples. Following ing,” arXiv:2103.16559, 2021. [7], the downsteam MLP has one hidden layer with 512 hid- den units. We train and evaluate models using crops of 3 sec- [34] F Pedregosa, G Varoquaux, A Gramfort, V Michel, onds. Same as previous works [8, 13, 23], we apply example B Thirion, O Grisel, M Blondel, P Prettenhofer, mixing to audio samples during training. During evaluation, R Weiss, V Dubourg, J Vanderplas, A Passos, and Cour- we average the scores of 10 overlapped and equally spaced napeau D, “Scikit-learn: Machine learning in python,” 3-second clips. AudioSet downstream tasks are trained with a Journal of Machine Learning Research, 2011. batch size of 128 for 200k steps. For supervised classification, we do not perform a second stage of fine-tuning to evaluate [35] P Warden, “Speech Commands: A dataset for limited- the representations on the AudioSet benchmark. vocabulary speech recognition,” arXiv:1804.03209, Birdsong [31]: This dataset is constructed from three de- 2018. velopment datasets (freefield1010, warblrb10k, BirdVox- DCASE-20k) of the Task 3 of DCASE2018 challenge4. Note [36] K MacLean, “VoxForge,” Ken MacLean.[Online]. that the training and test splits are different from ones pro- Available: http://www.voxforge.org/home.[Acedido em vided by the challenge as labels of the original test set is not 2012], 2018. accessible. Instead, we use the splits created and provided by the authors of [14]. Each sample is around 10 seconds long. [37] A Nagrani, J Chung, and A Zisserman, “Vox- We train models with 1-second crops. During evaluation, we Celeb: a large-scale speaker identification dataset,” average the scores over consecutive non-overlapped 1-second arXiv:1706.08612, 2017. subclips that cover the whole clip. TUT18 [32]: This dataset is created from the development [38] J Pons, O Nieto, M Prockup, E Schmidt, A Ehmann, and dataset of Task 1 - Subtask A of DCASE2018 challenge5. X Serra, “End-to-end learning for music audio tagging Similarly, the training and test splits are different from orig- at scale,” arXiv:1711.02520, 2017. inal ones. We use the splits created and provided by the au- thors of [14]. Each sample is around 10 seconds long. We [39] S Qiao, H Wang, C Liu, W Shen, and A Yuille, “Weight train models with 5-second crops. During evaluation, we av- standardization,” arXiv:1903.10520, 2019. erage the scores over consecutive non-overlapped 5-second subclips covering the entire clip. [40] G Huang, Y Sun, Z Liu, D Sedra, and K Weinberger, ESC-50 [22]: For the linear evaluation on ESC-506 we fol- “Deep networks with stochastic depth,” in ECCV, 2016. low the setup used in [33]. We use the SVM implementa- [41] N Srivastava, G Hinton, A Krizhevsky, I Sutskever, and 2https://github.com/deepmind/slowfast_nfnets 3research.google.com/audioset/ R Salakhutdinov, “Dropout: a simple way to prevent 4dcase.community/challenge2018/task-bird-audio-detection neural networks from overfitting,” Journal of Machine 5dcase.community/challenge2018/task-acoustic-scene-classification Learning Research, 2014. 6github.com/karolpiczak/ESC-50
tion of SciKit-Learn [34]. We use full 5-second clips for Table 5: Slowfast NFNet-F0. Slow and fast path has a group training and evaluation. For training, we process 10 epochs size of 128 and 16, respectively. Each block has a stride of worth of augmented samples. We sweep the value for the 1, 2 on the time and frequency dimension, respectively. Final SVM regularization parameter in the following set of values: features are obtained by global average pooling and concate- {3 × 10−5 , 10−4 , 3 × 10−4 , 10−3 , 3 × 10−3 , 10−2 , 3 × 10−2. nation of two pathways. We use the first split to pick the optimal value and report the Stage Slow path Fast path T × F average of all the splits. spectrogram - - 400 × 128 Speech Commands [35]: In this work we use both Speech data layer stride 4, 1 stride 1, 1 Slow : 100 × 128 Fast : 400 × 128 Commands dataset V17 and V28 with 12 and 35 labels, re- 1 × 3, 16 3 × 3, 2 Slow : 50 × 64 spectively. We train and evaluate models with the entire 1- stem1 stride 2, 2 stride 2, 2 Fast : 200 × 64 second audio clips. 1 × 3, 32 3 × 3, 4 Slow : 50 × 64 stem2 stride 1, 1 stride 1, 1 Fast : 200 × 64 Fluent Speech Commands [17]: In this dataset9, each au- 1 × 3, 64 3 × 3, 8 Slow : 50 × 64 dio is labeled with three slots. Unlike in [17], we do not use stem3 stride 1, 1 stride 1, 1 Fast : 200 × 64 any ASR module and directly use the model to predict the in- 3 × 3, 128 3 × 3, 16 Slow : 25 × 32 stem4 tent from the raw waveform. The model predicts three sets of stride 2, 2 stride 2, 2 Fast : 100 × 32 logits upon which three cross entropy losses are computed si- 1 × 1, 128  1 × 1, 16  1 × 1, 128 3 × 1, 16 Slow : 25 × 32 multaneously. The prediction is correct only if all three slots block1 1 × 3, 128 × 1 1 × 3, 16 × 1 Fast : 100 × 32     are predicted correctly. We train models with 3-second crops. 1 × 1, 256 1 × 1, 32 During evaluation, we average the scores over consecutive 1 × 1, 256 1 × 1, 32     1 × 1, 256 3 × 1, 32 Slow : 25 × 16 non-overlapped 3-second subclips covering the whole audio. block2 × 2 × 2 1 × 3, 256 1 × 3, 32 Fast : 100 × 16     1 × 1, 512 1 × 1, 64 VoxForge [36]: This dataset is available through Tensorflow 1 × 1, 768 1 × 1, 96 Dataset10. We train models with 3-second crops. During  3 × 1, 768   3 × 1, 96  Slow : 25 × 8 block3 × 6 × 6 evaluation, we average the scores over consecutive non-  1 × 3, 768   1 × 3, 96  Fast : 100 × 8     1 × 1, 1536 1 × 1, 192 overlapped 3-second subclips that cover the whole clip. 1 × 1, 768 1 × 1, 96     3 × 1, 768 3 × 1, 96 Slow : 25 × 4 VoxCeleb [37]: This dataset is available through Tensorflow block4 × 3 × 3  1 × 3, 768   1 × 3, 96  Fast : 100 × 4 Dataset11. We train models with 3-second crops. During  1 × 1, 1536   1 × 1, 192  evaluation, we average the scores over consecutive non- Global average pool & concatenate Feature dim: 1728 overlapped 3-second subclips that cover the whole clip. NSynth [6]: This dataset is available through Tensorflow B. SLOWFAST NFNET-F0 ARCHITECTURE Dataset12. It is used for both the instrument classification and pitch estimation task. For instrument classification, we We combine the design principles of Slowfast networks [19] directly use the 4-second clip for both training and evaluation. and NFNets [20] to devise a Slowfast NFNet-F015, whose For the pitch estimation task, we train models with 1-second architecture is shown in Table 5. Following [19], the slow crops, and during evaluation, we average the scores over four stream, compared to the fast stream, has 8 times more chan- non-overlapped 1-second subclips. nel capacity, whose input spectrogram is strided temporally MagnaTagATune [5]: For this dataset13, we use the same by 4 and thus has a lower temporal resolution. We also apply splits14 as previous works [38, 21]. We train and evaluate the multi-level fast-to-slow fusion before each block with the same configuration as [19], except that the Batch Norm layer models using crops of 3 seconds. During evaluation, we av- is removed, and Scaled Weight Standardization is applied to erage the scores over consecutive non-overlapped 3-second 2D convolutions [39, 27]. The ResNet blocks in the original subclips that cover the entire audio. We notice that the down- Slowfast network [19] are replaced with NFNet blocks, and stream training on this dataset converges quickly. Therefore, the stem modules are also changed to be the same as ones models are trained with a batch size of 128 for 20k steps. used in [20]. Each block has a stride of 1 and 2 on the time and frequency dimension, respectively. We follow the orig- inal choices for block hyperparameters, except that the slow 7download.tensorflow.org/data/speech_commands_v0.01.taanrd.fgazst path has a group size of 128 and 16 respectively, and 8download.tensorflow.org/data/speech_commands_v0.02.tthare.3g×z 3 convolutions are replaced by separable convolutions. 9fluent.ai/research/fluent-speech-commands/ Final features are obtained by global average pooling and con- 10tensorflow.org/datasets/catalog/voxforge 11tensorflow.org/datasets/catalog/voxceleb catenation of two pathways. We use Stochastic Depth [40] 12tensorflow.org/datasets/catalog/nsynth with a rate of 0.1. Unlike the original NFNets we do not ap- 13mirg.city.ac.uk/codeapps/the-magnatagatune-dataset 14github.com/jordipons/musicnn-training/tree/master/dat1a5/hitntdpesx://m/tgtithub.com/deepmind/slowfast_nfnets/blob/main/slowfast_nfnet.py
ply any Dropout [41]. Note that this design can be extended to adopting bigger versions of the NFNet architecture, which we leave for future work. In our experiments, we do not experience instability when training NFNets with large batch sizes (maximum 4096). Therefore, we do not use the Adaptive Gradient Clipping techniques which is employed by the original NFNet paper for the vision domain [20]. C. ADDITIONAL PRE-TRAINING DETAILS For supervised and SimCLR, we use the Adam optimizer [28], first linearly increasing the learning rate from 0 to 2 × 10−4 over 5k steps, and then decaying it following a cosine sched- ule [30] down to 0. All our SimCLR models are trained with a temperature τ of 0.1. The example mixing ratio is sampled on the fly from the β(5, 2) distribution. For spectrogram-based models, we standardize each input spectrogram by its mean and standard deviation. When pretraining Wav2Vec2.0 [10] and bi-directional CPC [11] models, we use crops of 10 seconds and a batch size of 256. We use the same optimizer setup as that used for SimCLR except that on LibriSpeech [29] we train the models for 80k steps. We do not use any audio augmentation for these two models. For Wav2Vec2.0, we use both con- tinuous inputs and targets for the target network [10]. For other hyperparameters, we follow the same choices in the original papers [10, 11]. For BYOL-A, we follow the exact pre-training setup in the original paper [16]. D. FULL EXPERIMENT RESULTS Full results for Table 2 and Table 3 are shown in Table 6, 7, 8, 9.
Table 6: Performance of models trained with classification loss. Architecture BYOL-A EfficientNet-B0 ResNet50 CNN14 NFNet-F0 ViT-Base Environment AudioSet 33.8 32.1 39.4 38.9 39.6 37.6 Birdsong 77.0 75.8 77.7 78.0 78.4 76.6 TUT18 83.3 83.3 89.6 88.9 92.3 87.4 ESC-50 84.9 90.9 93.5 93.6 94.6 92.6 Avg. 69.8 70.5 75.1 74.8 76.2 73.5 Speech Speech Comm. v1 81.8 58.6 64.4 71.4 75.2 62.4 Speech Comm. v2 85.1 60.1 69.0 76.4 79.9 70.1 VoxForge 78.6 70.3 75.5 78.5 79.3 74.7 VoxCeleb 33.4 13.8 30.1 30.9 35.9 29.6 Fluent Comm. 28.3 14.6 19.3 22.8 24.9 15.0 Avg. 61.4 43.5 51.7 56.0 59.0 50.4 Music NSynth-Pitch 64.4 41.5 69.3 58.4 73.9 71.6 NSynth-Instrument 70.1 64.4 70.7 71.4 73.0 70.7 MagnaTagATune 38.4 38.1 38.7 39.4 38.6 38.7 Avg. 57.6 48.0 59.6 56.4 61.8 60.3 HARES 63.3 53.6 61.4 62.4 65.5 60.6 Table 7: Performance of models trained with SimCLR loss. Architecture BYOL-A EfficientNet-B0 ResNet50 CNN14 NFNet-F0 ViT-Base Environment AudioSet 32.2 26.2 36.2 28.8 37.6 36.8 Birdsong 76.6 74.2 77.7 76.4 77.5 77.1 TUT18 83.8 76.8 94.0 82.2 96.9 93.7 ESC-50 87.0 77.9 89.7 78.0 92.2 90.6 Avg. 69.9 63.8 74.4 66.4 76.0 74.6 Speech Speech Comm. v1 89.5 44.5 78.9 43.2 82.3 65.1 Speech Comm. v2 91.4 55.4 83.2 44.7 86.2 74.6 VoxForge 84.3 67.5 84.4 68.2 86.0 83.7 VoxCeleb 55.0 22.1 50.0 19.1 50.1 44.4 Fluent Comm. 28.9 14.0 28.3 11.3 24.8 14.8 Avg. 69.8 40.7 65.0 37.3 65.9 56.5 Music NSynth-Pitch 76.1 48.0 78.5 36.1 82.9 81.1 NSynth-Instrument 75.0 58.0 73.8 63.7 74.0 71.8 MagnaTagATune 38.2 26.0 38.7 34.6 39.6 39.7 Avg. 63.1 44.0 63.7 44.8 65.5 64.2 HARES 68.2 49.2 67.8 48.9 69.2 64.5
Table 8: Performance of Slowfast models trained with classification loss. SF ResNet50 Architecture SF NF-ResNet50 SF NFNet-F0 no norm BN LN IN Environment AudioSet 36.4 38.2 37.9 34.8 36.2 38.7 Birdsong 77.3 77.8 77.7 77.5 77.2 77.4 TUT18 90.7 89.4 91.9 87.0 91.2 92.7 ESC-50 90.8 91.8 92.8 90.4 90.9 92.7 Avg. 73.8 74.3 75.1 72.4 73.9 75.4 Speech Speech Comm. v1 86.9 73.2 78.4 74.7 83.2 83.0 Speech Comm. v2 88.9 78.2 83.1 80.1 85.6 86.2 VoxForge 80.4 76.6 77.8 69.5 80.7 82.9 VoxCeleb 37.7 31.9 32.2 22.6 39.2 40.2 Fluent Comm. 37.1 24.6 26.5 25.6 35.0 35.5 Avg. 66.2 56.9 59.6 54.5 64.7 65.6 Music NSynth-Pitch 76.6 71.1 75.3 67.2 77.3 79.9 NSynth-Instrument 70.8 69.6 73.8 71.2 72.0 75.0 MagnaTagATune 38.4 38.1 38.7 38.8 38.0 38.5 Avg. 61.9 59.6 62.6 59.1 62.4 64.5 HARES 67.7 63.4 65.5 61.6 67.2 68.6 Table 9: Performance of Slowfast models trained with SimCLR loss. SF ResNet50 Architecture SF NF-ResNet50 SF NFNet-F0 no norm BN LN IN Environment AudioSet 35.9 36.6 36.2 36.0 36.2 37.8 Birdsong 77.9 77.9 78.0 77.3 78.0 77.6 TUT18 91.9 92.3 92.0 91.9 93.0 96.8 ESC-50 90.6 90.5 90.0 89.5 88.7 91.1 Avg. 74.1 74.3 74.0 73.7 74.0 75.8 Speech Speech Comm. v1 89.7 88.8 91.1 91.8 91.0 91.7 Speech Comm. v2 92.0 91.5 93.2 93.9 92.2 93.0 VoxForge 87.7 89.2 89.2 86.6 89.9 90.4 VoxCeleb 59.5 59.4 60.6 67.9 61.1 64.9 Fluent Comm. 41.5 38.3 45.9 48.5 44.3 46.1 Avg. 74.1 73.4 76.0 77.7 75.7 77.2 Music NSynth-Pitch 82.4 83.2 85.9 84.5 84.4 88.0 NSynth-Instrument 75.2 73.5 76.1 74.3 76.6 78.2 MagnaTagATune 39.1 38.8 39.2 39.3 38.9 39.5 Avg. 65.6 65.2 67.1 66.0 66.6 68.6 HARES 71.9 71.7 73.1 73.5 72.9 74.6
